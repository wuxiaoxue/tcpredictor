Issue key,Description,sec,config,perf,,,,,
HBASE-3406,"I had a RS fail due to GC pause while it was in the midst of opening a region, apparently. This got the region stuck in the following repeating sequence in the master log:

2011-01-03 17:24:33,884 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been OPENING for too long, reassigning region=usertable,user991629466,1293747979500.c6a54b4d07a44e113b3a4d2ab22daa70.
2011-01-03 17:24:33,885 DEBUG org.apache.hadoop.hbase.zookeeper.ZKUtil: master:60000-0x12ce26f6c0600e3 Retrieved 113 byte(s) of data from znode /hbase/unassigned/c6a54b4d07a44e113b3a4d2ab22daa70; data=region=usertable,user991629466,1293747979500.c6a54b4d07a44e113b3a4d2ab22daa70., server=haus03.sf.cloudera.com:60000, state=M_ZK_REGION_OFFLINE
2011-01-03 17:24:43,886 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  usertable,user991629466,1293747979500.c6a54b4d07a44e113b3a4d2ab22daa70. state=OPENING, ts=1293840977790
2011-01-03 17:24:43,886 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been OPENING for too long, reassigning region=usertable,user991629466,1293747979500.c6a54b4d07a44e113b3a4d2ab22daa70.
2011-01-03 17:24:43,887 DEBUG org.apache.hadoop.hbase.zookeeper.ZKUtil: master:60000-0x12ce26f6c0600e3 Retrieved 113 byte(s) of data from znode /hbase/unassigned/c6a54b4d07a44e113b3a4d2ab22daa70; data=region=usertable,user991629466,1293747979500.c6a54b4d07a44e113b3a4d2ab22daa70., server=haus03.sf.cloudera.com:60000, state=M_ZK_REGION_OFFLINE

etc... repeating every 10 seconds. Eventually I ran hbck -fix which forced it to OFFLINE in ZK and it reassigned just fine.", ,,,,,,,
HBASE-3408,"If AssignmentManager tries to move a region to an invalid destination server, rather than choosing a random server as intended, it throws an NPE.

Line 1009 should check if existingPlan.getDestination()!=null:

 if (existingPlan == null || forceNewPlan ||
          (existingPlan.getDestination() != null && existingPlan.getDestination().equals(serverToExclude))) {

I triggered it by trying to manually move regions around, probably to an invalid destination server.  I'm not currently able to build the project to test if that's the extent of the problem, so here's a little more info...  

It leaves a stranded region-in-transition until the master and/or regionserver are restarted and causes problems like the following.  ""hbck -fix"" was unable to repair it.

2011-01-04 00:14:10,948 DEBUG org.apache.hadoop.hbase.master.CatalogJanitor: Scanned 4287 catalog row(s) and gc'd 0 unreferenced parent region(s)
2011-01-04 00:14:18,574 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because 1 region(s) in transition: {23ebce9a5d174f87bfb96ed1da387fdc=RandomValue,,1291219068335.23ebce9a5d174f87bfb96ed1da387fdc. state=OFFLINE, ts=1294118046139}
2011-01-04 00:14:36,142 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  RandomValue,,1291219068335.23ebce9a5d174f87bfb96ed1da387fdc. state=OFFLINE, ts=1294118046139
2011-01-04 00:14:36,142 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been OFFLINE for too long, reassigning RandomValue,,1291219068335.23ebce9a5d174f87bfb96ed1da387fdc. to a random server
2011-01-04 00:14:36,142 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=RandomValue,,1291219068335.23ebce9a5d174f87bfb96ed1da387fdc. state=OFFLINE, ts=1294118046139
2011-01-04 00:14:36,142 ERROR org.apache.hadoop.hbase.master.AssignmentManager$TimeoutMonitor: Caught exception
java.lang.NullPointerException
        at org.apache.hadoop.hbase.master.AssignmentManager.getRegionPlan(AssignmentManager.java:934) (i think this is .90.0RC1, so same bug on a different line number)
        at org.apache.hadoop.hbase.master.AssignmentManager.getRegionPlan(AssignmentManager.java:909)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:822)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:663)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:643)
        at org.apache.hadoop.hbase.master.AssignmentManager$TimeoutMonitor.chore(AssignmentManager.java:1481)
        at org.apache.hadoop.hbase.Chore.run(Chore.java:66)
", ,,,,,,,
HBASE-3417,"Currently the block names used in the block cache are built using the filesystem path.  However, for cache on write, the path is a temporary output file.

The original COW patch actually made some modifications to block naming stuff to make it more consistent but did not do enough.  Should add a separate method somewhere for generating block names using some more easily mocked scheme (rather than just raw path as we generate a random unique file name twice, once for tmp and then again when moved into place).", ,,,,,,,
HBASE-3419,"The {{Progressable}} used on region open to tickle the ZK OPENING node to prevent the master from timing out a region open operation will currently abort the RegionServer if this fails for some reason.  However it could be ""normal"" for an RS to have a region open operation aborted by the master, so should just handle as it does other places by reverting the open.

We had a cluster trip over some other issue (for some reason, the tickle was not happening in < 30 seconds, so master was timing out every time).  Because of the abort on BadVersion, this eventually led to every single RS aborting itself eventually taking down the cluster.", ,1,,,,,,
HBASE-3420,"This is pretty ugly.  In short, on a heavily loaded cluster, we are queuing multiple instances of region close.  They all try to run confusing state.

Long version:

I have a messy cluster.  Its 16k regions on 8 servers.  One node has 5k or so regions on it.  Heaps are 1G all around.  My master had OOME'd.  Not sure why but not too worried about it for now.  So, new master comes up and is trying to rebalance the cluster:

{code}
2011-01-05 00:48:07,385 INFO org.apache.hadoop.hbase.master.LoadBalancer: Calculated a load balance in 14ms. Moving 3666 regions off of 6 overloaded servers onto 3 less loaded servers
{code}

The balancer ends up sending many closes to a single overloaded server are taking so long, the close times out in RIT.  We then do this:

{code}
              case CLOSED:
                LOG.info(""Region has been CLOSED for too long, "" +
                    ""retriggering ClosedRegionHandler"");
                AssignmentManager.this.executorService.submit(
                    new ClosedRegionHandler(master, AssignmentManager.this,
                        regionState.getRegion()));
                break;
{code}

We queue a new close (Should we?).

We time out a few more times (9 times) and each time we queue a new close.

Eventually the close succeeds, the region gets assigned a new location.

Then the next close pops off the eventhandler queue.

Here is the telltale signature of stuff gone amiss:

{code}
2011-01-05 00:52:19,379 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=TestTable,0487405776,1294125523541.b1fa38bb610943e9eadc604babe4d041. state=OPEN, ts=1294188709030
{code}

Notice how state is OPEN when we are forcing offline (It was actually just successfully opened).  We end up assigning same server because plan was still around:

{code}
2011-01-05 00:52:20,705 WARN org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Attempted open of TestTable,0487405776,1294125523541.b1fa38bb610943e9eadc604babe4d041. but already online on this server
{code}

But later when plan is cleared, we assign new server and we have dbl-assignment.



",,,1,,,,,
HBASE-3421,"From the list, see 'jvm oom' in http://mail-archives.apache.org/mod_mbox/hbase-user/201101.mbox/browser, it looks like wide rows -- 30M or so -- causes OOME during compaction.  We should check it out. Can the scanner used during compactions use the 'limit' when nexting?  If so, this should save our OOME'ing (or, we need to add to the next a max size rather than count of KVs).",,1,,,,,,
HBASE-3431,"Our man Ted Dunning found the following where RS checks in with one name, the master tells it use another name but we seem to go ahead and continue with our original name.

In RS logs I see:

{code}
2011-01-07 15:45:50,757 INFO  org.apache.hadoop.hbase.regionserver.HRegionServer [regionserver60020]: Master passed us address to use. Was=perfnode11:60020, Now=10.10.30.11:60020
{code}


On master I see

{code}
2011-01-07 15:45:38,613 INFO  org.apache.hadoop.hbase.master.ServerManager [IPC Server handler 0 on 60000]: Registering server=10.10.30.11,60020,1294443935414, regionCount=0, userLoad=false
{code}

....

then later

{code}
2011-01-07 15:45:44,247 INFO  org.apache.hadoop.hbase.master.ServerManager [IPC Server handler 2 on 60000]: Registering server=perfnode11,60020,1294443935414, regionCount=0, userLoad=true
{code}

This might be since we started letting servers register in other than with the reportStartup.",,1,,,,,,
HBASE-3433,"Here is offending code from inside in StoreScanner#next:

      // kv is no longer immutable due to KeyOnlyFilter! use copy for safety
      KeyValue copyKv = new KeyValue(kv.getBuffer(), kv.getOffset(), kv.getLength());
This looks wrong given philosophy up to this has been avoidance of garbage-making copies.

Maybe this has been looked into before and this is the only thing to be done but why is KeyOnlyFilter not making copies rather than mutating originals?

Making this critical against 0.92.",1,,,,,,,
HBASE-3443,"For incrementColumnValue() HBASE-3082 adds an optimization to check memstores first, and only if not present in the memstore then check the store files. In the presence of deletes, the above optimization is not reliable.

If the column is marked as deleted in the memstore, one should not look further into the store files. But currently, the code does so.

Sample test code outline:

{code}
admin.createTable(desc)

table = HTable.new(conf, tableName)

table.incrementColumnValue(Bytes.toBytes(""row""), cf1name, Bytes.toBytes(""column""), 5);

admin.flush(tableName)
sleep(2)

del = Delete.new(Bytes.toBytes(""row""))
table.delete(del)

table.incrementColumnValue(Bytes.toBytes(""row""), cf1name, Bytes.toBytes(""column""), 5);

get = Get.new(Bytes.toBytes(""row""))
keyValues = table.get(get).raw()
keyValues.each do |keyValue|
  puts ""Expect 5; Got Value=#{Bytes.toLong(keyValue.getValue())}"";
end
{code}

The above prints:
{code}
Expect 5; Got Value=10
{code}
",,,,,,,,
HBASE-3445,"While testing an upgrade to 0.90.0 RC3 I noticed that if I seeded our test data on one machine and transferred to another machine the HMaster on the new machine dies on startup.

Based on the following stack trace it looks as though it is attempting to find the .meta region with the ip address of the original machine.  Instead of waiting around for RegionServer's to register with new location data, HMaster throws it's hands up with a FATAL exception.

Note that deleting the zookeeper dir makes no difference.

Also note that so far I have only reproduced this in my own environment using the hbase-trx extension of HBase and an ApplicationStarter that starts the Master and RegionServer together in the same JVM.  While the issue seems likely isolated from those factors it is far from a vanilla HBase environment.

I will spend some time trying to reproduce the issue in a proper hbase test.  But perhaps someone can beat me to it?  How do I simulate the IP switch? May require a data.tar upload. 

[14/01/11 10:45:20] 6396   [     Thread-298] ERROR server.quorum.QuorumPeerConfig  - Invalid configuration, only one server specified (ignoring)
[14/01/11 10:45:21] 7178   [           main] INFO  ion.service.HBaseRegionService  - troove> region port:       60010
[14/01/11 10:45:21] 7180   [           main] INFO  ion.service.HBaseRegionService  - troove> region interface:  org.apache.hadoop.hbase.ipc.IndexedRegionInterface
[14/01/11 10:45:21] 7180   [           main] INFO  ion.service.HBaseRegionService  - troove> root dir: hdfs://localhost:8701/hbase
[14/01/11 10:45:21] 7180   [           main] INFO  ion.service.HBaseRegionService  - troove> Initializing region server.
[14/01/11 10:45:21] 7631   [           main] INFO  ion.service.HBaseRegionService  - troove> Starting region server thread.
[14/01/11 10:46:54] 100764 [        HMaster] FATAL he.hadoop.hbase.master.HMaster  - Unhandled exception. Starting shutdown.
java.net.SocketTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=192.168.1.102/192.168.1.102:60020]
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:213)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:404)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupIOstreams(HBaseClient.java:311)
	at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:865)
	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:732)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:258)
	at $Proxy14.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:419)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:393)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:444)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.waitForProxy(HBaseRPC.java:349)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:954)
	at org.apache.hadoop.hbase.catalog.CatalogTracker.getCachedConnection(CatalogTracker.java:384)
	at org.apache.hadoop.hbase.catalog.CatalogTracker.getMetaServerConnection(CatalogTracker.java:283)
	at org.apache.hadoop.hbase.catalog.CatalogTracker.verifyMetaRegionLocation(CatalogTracker.java:478)
	at org.apache.hadoop.hbase.master.HMaster.assignRootAndMeta(HMaster.java:435)
	at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:382)
	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:277)
",,,,,,,,
HBASE-3446,"I ran a rolling restart on a 5 node cluster with lots of regions, and afterwards had LOTS of regions left orphaned. The issue appears to be that ProcessServerShutdown failed because the server hosting META was restarted around the same time as another server was being processed",,,,,,,,
HBASE-3449,"I have a situation where both of my MASTER_META_SERVER_OPERATIONS handlers are handling server shutdowns, and both of them are waiting on ROOT, which isn't coming up. Unclear exactly how this happened, but I triggered it by doing a rolling restart.",,1,,,,,,
HBASE-3465,"I have been burned a few times lately while developing code by having the make sure that the hadoop jar in hbase/lib is exactly correct.  In my own deployment, there are actually 3 jars and a native library to keep in sync that hbase shouldn't have to know about explicitly.  A similar problem arises when using stock hbase with CDH3 because of the security patches changing the wire protocol.

All of these problems could be avoided by not assuming that the hadoop library is in the local directory.  Moreover, I think it might be possible to assemble the distribution such that the compile time hadoop dependency is in a cognate directory to lib and is referenced using a default value for HADOOP_HOME.

Does anybody have any violent antipathies to such a change?",,,,,,,,
HBASE-3478,"This looks like a variant of HBASE-3445:

One of our developers ran a seed program with configuration A to generate some test data on his local machine. He then moved that data into a development environment on the same machine with a different hbase configuration B.

On startup the HMaster waits for new regionserver to register itself:

[25/01/11 15:37:25] 162161 [  HRegionServer] INFO  ase.regionserver.HRegionServer  - Telling master at 10.0.1.4:7801 that we are up
[25/01/11 15:37:25] 162165 [ice-EventThread] DEBUG .hadoop.hbase.zookeeper.ZKUtil  - master:7801-0x12dbf879abe0000 Retrieved 13 byte(s) of data from znode /hbase/rs/10.0.1.4,7802,1295998613814 and set watcher; 10.0.1.4:7802

Then ROOT region comes online at the right place: 10.0.1.4,7802

[25/01/11 15:37:31] 168369 [yTasks:70236052] INFO  ase.catalog.RootLocationEditor  - Setting ROOT region location in ZooKeeper as 10.0.1.4:7802
3:57 [25/01/11 15:37:31] 168408 [10.0.1.4:7801-0] DEBUG er.handler.OpenedRegionHandler  - Opened region -ROOT-,,0.70236052 on 10.0.1.4,7802,1295998613814

But then HMaster chokes on the stale META region location.

[25/01/11 15:37:31] 168448 [        HMaster] ERROR he.hadoop.hbase.HServerAddress  - Could not resolve the DNS name of warren:60020
[25/01/11 15:37:31] 168448 [        HMaster] FATAL he.hadoop.hbase.master.HMaster  - Unhandled exception. Starting shutdown.
java.lang.IllegalArgumentException: Could not resolve the DNS name of warren:60020
   at org.apache.hadoop.hbase.HServerAddress.checkBindAddressCanBeResolved(HServerAddress.java:105)
   at org.apache.hadoop.hbase.HServerAddress.<init>(HServerAddress.java:66)
   at org.apache.hadoop.hbase.catalog.MetaReader.readLocation(MetaReader.java:344)
   at org.apache.hadoop.hbase.catalog.MetaReader.readMetaLocation(MetaReader.java:281)
   at org.apache.hadoop.hbase.catalog.CatalogTracker.getMetaServerConnection(CatalogTracker.java:280)
   at org.apache.hadoop.hbase.catalog.CatalogTracker.verifyMetaRegionLocation(CatalogTracker.java:482)
   at org.apache.hadoop.hbase.master.HMaster.assignRootAndMeta(HMaster.java:435)
   at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:382)
   at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:277)
   at java.lang.Thread.run(Thread.java:680)

First of all, we do not yet understand why in configuration A the RegionInfo resolved to ""warren:60020"" whereas in configuration B we get ""10.0.1.4:7802"".  The port numbers make sense but not the ""warren"" hostname. It's probably specific to Warren's mac environment somehow because no other developer gets this problem when doing the same thing.  ""warren"" isn't in his hosts file so that remains a mystery.

But irrespective of that, since the ports differ we expect the stale meta connection data to cause connection failure anyway.  Perhaps in the form of a SocketTimeoutException as in hbase-3445.

But shouldn't the HMaster handle that by catching the exception and letting verifyMetaRegionLocation() fail so that meta regions get reassigned to the new region server?

Probably the safeguards in CatalogTracker.getCachedConnection() should move up to getMetaServerConnection() so as to encompass MetaReader.readMetaLocation() also. Essentially if getMetaServerConnection() encounters ANY exception connection to meta RegionServer it should probably just return null to force meta region reassignment.


",,,,,,,,
HBASE-3481,"[While doing some cluster kill tests, I noticed some missing data after log recovery. Upon investigating further, and pretty printing contents of HFiles and recovered logs, this is my analysis of the situation/bug. Please confirm the theory and pitch in with suggestions.]

When memstores are flushed, the max sequence id recorded in  the HFile should be the max sequence id of all KVs in the memstore. However, we seem to simply obtain the current sequence id from the HRegion, and stamp the HFile's MAX_SEQ_ID with it.

From HRegion.java:
{code}
    sequenceId = (wal == null)? myseqid: wal.startCacheFlush();
{code}

where, startCacheFlush() is:

{code}
public long startCacheFlush() {
    this.cacheFlushLock.lock();
    return obtainSeqNum();
 }
{code}

where, obtainSeqNum() is simply: 

{code}
private long obtainSeqNum() {
    return this.logSeqNum.incrementAndGet();
  }
{code}

So let's say a memstore contains edits with sequence number 1..10.

Meanwhile, say more Puts come along, and are going through this flow (in pseudo-code)

{code}
1. HLog.append();
       1.1  obtainSeqNum()
       1.2 writeToWAL()

2 updateMemStore()
{code}

So it is possible that the sequence number has already been incremented to say 15 if there are 5 more outstanding puts. Say the writeToWAL() is still in progress for these puts. In this case, none of these edits (11..15) would have been written to memstore yet.

At this point if a cache flush of the memstore happens, then we'll record its MAX_SEQ_ID as 16 in the store file instead of 10 (because that's what obtainSeqNum() would return as the next sequence number to use, right?).

Assume that the edits 11..15 eventually complete. And so HLogs do contain the data for edits 11..15.

Now, at this point if the region server were to crash, and we run log recovery, the splits all go through correctly, and a correct recovered.edits file is generated with the edits 11..15. 

Next, when the region is opened, the HRegion notes that one of the store file says MAX_SEQ_ID is 16. So, when it replays the recovered.edits file, it  skips replaying edits 11..15. Or in other words, data loss.

----



",,,,,,,,
HBASE-3492,"I did a simple test on trunk where I create a table (after wiping the local /tmp/hbase-<username>):

{code}
hbase(main):001:0> create 'testtable', 'cf1', 'cf2'                                                                                                  
{code}

Then I inserted 17k rows:

{code}
hbase(main):002:0> for i in 'a'..'z' do for j in 'a'..'z' do for k in 'a'..'z' do put 'testtable', ""row-#{i}#{j}#{k}"", ""cf1:#{k}"", ""#{k}"" end end end
{code}

and called 

{code}
hbase(main):003:0> split 'testttable'
{code}

and the logs gave this NPE and _no_ split was performed:

{code}
2011-01-31 10:06:38,534 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memstore flush for testtable,,1296461957099.2528e23534565a83e2c8590d33f3a47c., current region memstore size 3.5m
2011-01-31 10:06:38,575 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished snapshotting, commencing flushing stores
2011-01-31 10:06:38,856 INFO org.apache.hadoop.hbase.regionserver.Store: Renaming flushed file at file:/tmp/hbase-larsgeorge/hbase/testtable/2528e23534565a83e2c8590d33f3a47c/.tmp/5265602271926296451 to file:/tmp/hbase-larsgeorge/hbase/testtable/2528e23534565a83e2c8590d33f3a47c/cf1/5349044325262044918
2011-01-31 10:06:38,861 INFO org.apache.hadoop.hbase.regionserver.Store: Added file:/tmp/hbase-larsgeorge/hbase/testtable/2528e23534565a83e2c8590d33f3a47c/cf1/5349044325262044918, entries=17576, sequenceid=17588, memsize=3.5m, filesize=549.9k
2011-01-31 10:06:38,863 INFO org.apache.hadoop.hbase.regionserver.HRegion: Finished memstore flush of ~3.5m for region testtable,,1296461957099.2528e23534565a83e2c8590d33f3a47c. in 328ms, sequenceid=17588, compaction requested=false
2011-01-31 10:06:38,869 INFO org.apache.hadoop.hbase.regionserver.HRegion: Starting compaction on region testtable,,1296461957099.2528e23534565a83e2c8590d33f3a47c.
2011-01-31 10:06:38,870 INFO org.apache.hadoop.hbase.regionserver.HRegion: aborted compaction on region testtable,,1296461957099.2528e23534565a83e2c8590d33f3a47c. after 0sec
2011-01-31 10:06:38,872 ERROR org.apache.hadoop.hbase.regionserver.CompactSplitThread: Compaction failed for region testtable,,1296461957099.2528e23534565a83e2c8590d33f3a47c.
java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.Store.checkSplit(Store.java:1367)
        at org.apache.hadoop.hbase.regionserver.Store.compact(Store.java:633)
        at org.apache.hadoop.hbase.regionserver.HRegion.compactStores(HRegion.java:793)
        at org.apache.hadoop.hbase.regionserver.CompactSplitThread.run(CompactSplitThread.java:81)
2011-01-31 10:06:38,873 DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitThread: Compaction requested for testtable,,1296461957099.2528e23534565a83e2c8590d33f3a47c. because User-triggered split; priority=1, compaction queue size=0
{code}

I added a row with data in cf2:

{code}
hbase(main):005:0> put 'testtable', 'row1', 'cf2', 'test1'                                                                                           
{code}

and the tried to split the table again like above and now it worked.",,,,,,,,
HBASE-3493,"During HMaster.finishInitialization(), assignRootAndMeta() is called, which at some point does:

{code}
this.assignmentManager.waitForAssignment(HRegionInfo.FIRST_META_REGIONINFO);
{code}

And waitForAssignment does this:

{code}
    synchronized(regions) {
      while(!regions.containsKey(regionInfo)) {
        regions.wait();
      }
    }
{code}

However, I could not find any call to regions.notify(), so this could wait forever.

I have not noticed this problem on a real cluster, only when using HBaseTestingUtility on a slow and low-memory Hudson server (I can reproduce it on my local machine by creating some background load). Adding a notify() call when regions.put() is called seems to fix the problem. Will attach patch.

For reference, this was based on seeing the following from jstack:

{noformat}
""Master:0;lat:44776"" prio=10 tid=0x08d5b400 nid=0x381a in Object.wait() [0x9c76d000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0xa5196fb8> (a java.util.TreeMap)
        at java.lang.Object.wait(Object.java:485)
        at org.apache.hadoop.hbase.master.AssignmentManager.waitForAssignment(AssignmentManager.java:1152)
        - locked <0xa5196fb8> (a java.util.TreeMap)
        at org.apache.hadoop.hbase.master.HMaster.assignRootAndMeta(HMaster.java:440)
        at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:382)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:277)
        at java.lang.Thread.run(Thread.java:619)
{noformat}",,,,,,,,
HBASE-3495,"While working on HBASE-3492 I came across another oddity with manual splits:

{code}
hbase(main):003:0> split 'testtable'                                                                                                                 
0 row(s) in 3.0590 seconds

hbase(main):004:0> scan '.META.', { COLUMNS => ['info:regioninfo'] }                                                                                 
ROW                                       COLUMN+CELL                                                                                                            
 testtable,,1296545855212.5e4ef9631cacb6b column=info:regioninfo, timestamp=1296545855770, value=REGION => {NAME => 'testtable,,1296545855212.5e4ef9631cacb6b2c6c
 2c6c338140c53cad4.                       338140c53cad4.', STARTKEY => '', ENDKEY => 'row-mdc', ENCODED => 5e4ef9631cacb6b2c6c338140c53cad4, TABLE => {{NAME => '
                                          testtable', FAMILIES => [{NAME => 'cf1', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION 
                                          => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'cf2', BLOO
                                          MFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => 
                                          '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}                                                                
 testtable,row-mdc,1296545855212.46e57f0c column=info:regioninfo, timestamp=1296545855774, value=REGION => {NAME => 'testtable,row-mdc,1296545855212.46e57f0ca4eb
 a4eba8d3e5bef6365159a660.                a8d3e5bef6365159a660.', STARTKEY => 'row-mdc', ENDKEY => '', ENCODED => 46e57f0ca4eba8d3e5bef6365159a660, TABLE => {{NA
                                          ME => 'testtable', FAMILIES => [{NAME => 'cf1', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPR
                                          ESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'cf2
                                          ', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKS
                                          IZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}                                                         
2 row(s) in 0.6690 seconds

hbase(main):005:0> split 'testtable'                                
0 row(s) in 0.4030 seconds

hbase(main):006:0> split 'testtable'

ERROR: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hbase.NotServingRegionException: Region is not online: testtable,,1296545855212.5e4ef9631cacb6b2c6c338140c53cad4.
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:2376)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.splitRegion(HRegionServer.java:2196)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:309)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1057)

Here is some help for this command:
Split entire table or pass a region to split individual region.  With the 
second parameter, you can specify an explicit split key for the region.  
Examples:
    split 'tableName'
    split 'regionName' # format: 'tableName,startKey,id'
    split 'tableName', 'splitKey'
    split 'regionName', 'splitKey'

{code}

It takes minutes for this to clear out eventually. Why is this not retried or flushed out right away?

A few minutes (!) later I see this in the logs:

{code}
2011-02-01 08:42:42,062 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted daughter reference testtable,,1296545879295.dfcc24e02e27e60160612dd5398cbd1e., qualifier=splitA, from parent testtable,,1296545855212.5e4ef9631cacb6b2c6c338140c53cad4.
2011-02-01 08:42:42,064 DEBUG org.apache.hadoop.hbase.ipc.HBaseRPC: Call: getRegionInfo 1
2011-02-01 08:42:42,064 DEBUG org.apache.hadoop.hbase.ipc.HBaseRPC: Call: delete 0
2011-02-01 08:42:42,064 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted daughter reference testtable,row-dau,1296545879295.4073eb6c82755aab57778af2dba39e22., qualifier=splitB, from parent testtable,,1296545855212.5e4ef9631cacb6b2c6c338140c53cad4.
2011-02-01 08:42:42,064 DEBUG org.apache.hadoop.hbase.master.CatalogJanitor: Deleting region testtable,,1296545855212.5e4ef9631cacb6b2c6c338140c53cad4. because daughter splits no longer hold references
2011-02-01 08:42:42,065 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: DELETING region file:/tmp/hbase-larsgeorge/hbase/testtable/5e4ef9631cacb6b2c6c338140c53cad4
2011-02-01 08:42:42,067 DEBUG org.apache.hadoop.hbase.ipc.HBaseRPC: Call: getRegionInfo 1
2011-02-01 08:42:42,067 DEBUG org.apache.hadoop.hbase.ipc.HBaseRPC: Call: delete 0
2011-02-01 08:42:42,067 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted region testtable,,1296545855212.5e4ef9631cacb6b2c6c338140c53cad4. from META
2011-02-01 08:42:42,069 DEBUG org.apache.hadoop.hbase.ipc.HBaseRPC: Call: getRegionInfo 0
2011-02-01 08:42:42,070 DEBUG org.apache.hadoop.hbase.ipc.HBaseRPC: Call: delete 1
2011-02-01 08:42:42,071 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted daughter reference testtable,row-mdc,1296545879558.94cb351e5dd36c269247dd8a1a79373c., qualifier=splitA, from parent testtable,row-mdc,1296545855212.46e57f0ca4eba8d3e5bef6365159a660.
2011-02-01 08:42:42,073 DEBUG org.apache.hadoop.hbase.ipc.HBaseRPC: Call: getRegionInfo 1
2011-02-01 08:42:42,074 DEBUG org.apache.hadoop.hbase.ipc.HBaseRPC: Call: delete 1
2011-02-01 08:42:42,074 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted daughter reference testtable,row-seq,1296545879558.43c5ffe1ca7dd6d1374b7b7430a7d261., qualifier=splitB, from parent testtable,row-mdc,1296545855212.46e57f0ca4eba8d3e5bef6365159a660.
2011-02-01 08:42:42,074 DEBUG org.apache.hadoop.hbase.master.CatalogJanitor: Deleting region testtable,row-mdc,1296545855212.46e57f0ca4eba8d3e5bef6365159a660. because daughter splits no longer hold references
2011-02-01 08:42:42,074 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: DELETING region file:/tmp/hbase-larsgeorge/hbase/testtable/46e57f0ca4eba8d3e5bef6365159a660
{code}

The the next split call works while the subsequent ones fail again. In other words the split is dropped somewhere and picked up by the catalog classes later while the shell does not see the new daughter regions?

Even .META. is off

{code}

hbase(main):011:0> scan '.META.', { COLUMNS => ['info:regioninfo'] }
ROW                                       COLUMN+CELL                                                                                                            
 testtable,,1296545879295.dfcc24e02e27e60 column=info:regioninfo, timestamp=1296546225693, value=REGION => {NAME => 'testtable,,1296545879295.dfcc24e02e27e601606
 160612dd5398cbd1e.                       12dd5398cbd1e.', STARTKEY => '', ENDKEY => 'row-dau', ENCODED => dfcc24e02e27e60160612dd5398cbd1e, OFFLINE => true, SPL
                                          IT => true, TABLE => {{NAME => 'testtable', FAMILIES => [{NAME => 'cf1', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0
                                          ', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE 
                                          => 'true'}, {NAME => 'cf2', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION => 'NONE', TT
                                          L => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}                                
 testtable,,1296546225506.f3a53bfa1bfd5ae column=info:regioninfo, timestamp=1296546225763, value=REGION => {NAME => 'testtable,,1296546225506.f3a53bfa1bfd5ae6cbb
 6cbb0641d43f8a242.                       0641d43f8a242.', STARTKEY => '', ENDKEY => 'row-aaa', ENCODED => f3a53bfa1bfd5ae6cbb0641d43f8a242, TABLE => {{NAME => '
                                          testtable', FAMILIES => [{NAME => 'cf1', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION 
                                          => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'cf2', BLOO
                                          MFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => 
                                          '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}                                                                
 testtable,row-aaa,1296546225506.4253ecd9 column=info:regioninfo, timestamp=1296546225761, value=REGION => {NAME => 'testtable,row-aaa,1296546225506.4253ecd9c94c
 c94c38b66bdf8cd17b07efcb.                38b66bdf8cd17b07efcb.', STARTKEY => 'row-aaa', ENDKEY => 'row-dau', ENCODED => 4253ecd9c94c38b66bdf8cd17b07efcb, TABLE 
                                          => {{NAME => 'testtable', FAMILIES => [{NAME => 'cf1', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3'
                                          , COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME 
                                          => 'cf2', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647',
                                           BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}                                                  
 testtable,row-dau,1296545879295.4073eb6c column=info:regioninfo, timestamp=1296546225913, value=REGION => {NAME => 'testtable,row-dau,1296545879295.4073eb6c8275
 82755aab57778af2dba39e22.                5aab57778af2dba39e22.', STARTKEY => 'row-dau', ENDKEY => 'row-mdc', ENCODED => 4073eb6c82755aab57778af2dba39e22, OFFLIN
                                          E => true, SPLIT => true, TABLE => {{NAME => 'testtable', FAMILIES => [{NAME => 'cf1', BLOOMFILTER => 'NONE', REPLICATI
                                          ON_SCOPE => '0', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false
                                          ', BLOCKCACHE => 'true'}, {NAME => 'cf2', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION
                                           => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}                  
 testtable,row-dau,1296546225769.529fdb6b column=info:regioninfo, timestamp=1296546225971, value=REGION => {NAME => 'testtable,row-dau,1296546225769.529fdb6bcca8
 cca8459349c81b518a24436b.                459349c81b518a24436b.', STARTKEY => 'row-dau', ENDKEY => 'row-gbo', ENCODED => 529fdb6bcca8459349c81b518a24436b, TABLE 
                                          => {{NAME => 'testtable', FAMILIES => [{NAME => 'cf1', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3'
                                          , COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME 
                                          => 'cf2', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647',
                                           BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}                                                  
 testtable,row-gbo,1296546225769.374d4364 column=info:regioninfo, timestamp=1296546225968, value=REGION => {NAME => 'testtable,row-gbo,1296546225769.374d4364574a
 574ad1c5f522aa55b3d81586.                d1c5f522aa55b3d81586.', STARTKEY => 'row-gbo', ENDKEY => 'row-mdc', ENCODED => 374d4364574ad1c5f522aa55b3d81586, TABLE 
                                          => {{NAME => 'testtable', FAMILIES => [{NAME => 'cf1', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3'
                                          , COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME 
                                          => 'cf2', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647',
                                           BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}                                                  
 testtable,row-mdc,1296545879558.94cb351e column=info:regioninfo, timestamp=1296545879815, value=REGION => {NAME => 'testtable,row-mdc,1296545879558.94cb351e5dd3
 5dd36c269247dd8a1a79373c.                6c269247dd8a1a79373c.', STARTKEY => 'row-mdc', ENDKEY => 'row-seq', ENCODED => 94cb351e5dd36c269247dd8a1a79373c, TABLE 
                                          => {{NAME => 'testtable', FAMILIES => [{NAME => 'cf1', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3'
                                          , COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME 
                                          => 'cf2', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647',
                                           BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}                                                  
 testtable,row-seq,1296545879558.43c5ffe1 column=info:regioninfo, timestamp=1296546226107, value=REGION => {NAME => 'testtable,row-seq,1296545879558.43c5ffe1ca7d
 ca7dd6d1374b7b7430a7d261.                d6d1374b7b7430a7d261.', STARTKEY => 'row-seq', ENDKEY => '', ENCODED => 43c5ffe1ca7dd6d1374b7b7430a7d261, OFFLINE => tr
                                          ue, SPLIT => true, TABLE => {{NAME => 'testtable', FAMILIES => [{NAME => 'cf1', BLOOMFILTER => 'NONE', REPLICATION_SCOP
                                          E => '0', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOC
                                          KCACHE => 'true'}, {NAME => 'cf2', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION => 'NO
                                          NE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}                         
 testtable,row-seq,1296546225975.c9188f86 column=info:regioninfo, timestamp=1296546226161, value=REGION => {NAME => 'testtable,row-seq,1296546225975.c9188f869822
 9822da3ff21215a98a99ff5a.                da3ff21215a98a99ff5a.', STARTKEY => 'row-seq', ENDKEY => 'row-vfk', ENCODED => c9188f869822da3ff21215a98a99ff5a, TABLE 
                                          => {{NAME => 'testtable', FAMILIES => [{NAME => 'cf1', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3'
                                          , COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME 
                                          => 'cf2', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647',
                                           BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}                                                  
 testtable,row-vfk,1296546225975.682a4dbf column=info:regioninfo, timestamp=1296546226156, value=REGION => {NAME => 'testtable,row-vfk,1296546225975.682a4dbf9800
 980035dc379c6ccd7418cb08.                35dc379c6ccd7418cb08.', STARTKEY => 'row-vfk', ENDKEY => '', ENCODED => 682a4dbf980035dc379c6ccd7418cb08, TABLE => {{NA
                                          ME => 'testtable', FAMILIES => [{NAME => 'cf1', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPR
                                          ESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'cf2
                                          ', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKS
                                          IZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}                                                         
10 row(s) in 0.2610 seconds
{code}

Look at the ENKDEYs.",,,,,,,,
HBASE-3497,"setConf() method in TableOutputFormat gets called and it replaces the hbase.zookeeper.quorum address in the job conf xml when you run a CopyTable job from one cluster to another. The conf gets set to the peer.addr that is specified, which makes the job read and write from/to the peer cluster instead of reading from the original cluster and writing to the peer.

Possibly caused due to the change in https://issues.apache.org/jira/browse/HBASE-3111",,,,,,,,
HBASE-3502,"Testing killing .META. I tripped over this one.  Last thing seen on regionserver killed was:

{code}
2011-02-02 21:44:48,379 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Instantiated TestTable,0591556500,1296683085472.76c9a32c5f068d16240e42a15fed8417.
{code}

... which means we could have been inside checkRegioninfoOnFilesystem when we were killed.

This tries to create the .regioninfo file.  Seems like that was started over at the NN but then the RS died shortly afterward.  Its stopping the Region opening.  I suppose I could try and open it for append to shut it then reopen?",,,,,,,,
HBASE-3515,"This is from Hudson build 1738, if a log is about to be rolled and the ZK connection is already closed then the replication code will fail at adding the new log in ZK but the log will still be rolled and it's possible that some edits will make it in.

From the log:

{quote}
2011-02-08 10:21:20,618 FATAL [RegionServer:0;vesta.apache.org,46117,1297160399378.logRoller] regionserver.HRegionServer(1383):
 ABORTING region server serverName=vesta.apache.org,46117,1297160399378, load=(requests=1525, regions=12,
 usedHeap=273, maxHeap=1244): Failed add log to list
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for 
 /1/replication/rs/vesta.apache.org,46117,1297160399378/2/vesta.apache.org%3A46117.1297160480509
...

2011-02-08 10:21:22,444 DEBUG [MASTER_META_SERVER_OPERATIONS-vesta.apache.org:56008-0] wal.HLogSplitter(258):
 Splitting hlog 8 of 8: hdfs://localhost:55474/user/hudson/.logs/vesta.apache.org,46117,1297160399378/vesta.apache.org%3A46117.1297160480509, length=0

2011-02-08 10:21:22,862 DEBUG [MASTER_META_SERVER_OPERATIONS-vesta.apache.org:56008-0] wal.HLogSplitter(436):
 Pushed=31 entries from hdfs://localhost:55474/user/hudson/.logs/vesta.apache.org,46117,1297160399378/vesta.apache.org%3A46117.1297160480509
{quote}

The easiest thing to do would be let the exception out and cancel the log roll.",,,,,,,,
HBASE-3524,"I recently updated production data to use HBase 0.90.0.
Now I'm periodically seeing:

[10/02/11 17:23:27] 30076066 [mpactionChecker] ERROR nServer$MajorCompactionChecker  - Caught exception
java.lang.NullPointerException
	at org.apache.hadoop.hbase.regionserver.Store.isMajorCompaction(Store.java:832)
	at org.apache.hadoop.hbase.regionserver.Store.isMajorCompaction(Store.java:810)
	at org.apache.hadoop.hbase.regionserver.HRegion.isMajorCompaction(HRegion.java:2800)
	at org.apache.hadoop.hbase.regionserver.HRegionServer$MajorCompactionChecker.chore(HRegionServer.java:1047)
	at org.apache.hadoop.hbase.Chore.run(Chore.java:66)

The only negative effect is that this is interrupting compactions from happening. But that is pretty serious and this might be a sign of data corruption?

Maybe it's just my data, but this task should at least involve improving the handling to catch the NPE and still iterate through the other onlineRegions that might compact without error.  The MajorCompactionChecker.chore() method only catches IOExceptions and so this NPE breaks out of that loop. 
",,,,,,,,
HBASE-3525,"Here is what our lib dir looks this in 0.90.1:

{code}
-rwxr-xr-x  1 Stack  staff    62983 Mar 16  2009 activation-1.1.jar
-rwxr-xr-x  1 Stack  staff  1034049 May 21  2009 ant-1.6.5.jar
-rwxr-xr-x  1 Stack  staff  1323005 Jul 20  2009 ant-1.7.1.jar
-rwxr-xr-x  1 Stack  staff    12143 Jul 20  2009 ant-launcher-1.7.1.jar
-rwxr-xr-x  1 Stack  staff    43033 May  5  2009 asm-3.1.jar
-rwxr-xr-x  1 Stack  staff   339831 Oct 18 10:05 avro-1.3.3.jar
-rwxr-xr-x  1 Stack  staff    41123 Dec  8  2009 commons-cli-1.2.jar
-rwxr-xr-x  1 Stack  staff    58160 Oct 18 10:05 commons-codec-1.4.jar
-rwxr-xr-x  1 Stack  staff   112341 Mar 16  2009 commons-el-1.0.jar
-rwxr-xr-x  1 Stack  staff   305001 Mar 16  2009 commons-httpclient-3.1.jar
-rwxr-xr-x  1 Stack  staff   279193 May 17  2010 commons-lang-2.5.jar
-rwxr-xr-x  1 Stack  staff    60686 Mar 13  2009 commons-logging-1.1.1.jar
-rwxr-xr-x  1 Stack  staff   180792 Mar  4  2010 commons-net-1.4.1.jar
-rwxr-xr-x  1 Stack  staff  3566844 Jun  5  2009 core-3.1.1.jar
-rwxr-xr-x  1 Stack  staff   936397 Oct 18 10:05 guava-r06.jar
-rwxr-xr-x  1 Stack  staff  2707856 Jan 11 13:26 hadoop-core-0.20-append-r1056497.jar
-rwxr-xr-x  1 Stack  staff  2241521 Feb  9 15:57 hbase-0.90.1.jar
-rwxr-xr-x  1 Stack  staff   706710 Mar  4  2010 hsqldb-1.8.0.10.jar
-rwxr-xr-x  1 Stack  staff   171958 Oct 18 10:05 jackson-core-asl-1.5.5.jar
-rwxr-xr-x  1 Stack  staff    17065 Oct 18 10:05 jackson-jaxrs-1.5.5.jar
-rwxr-xr-x  1 Stack  staff   386509 Oct 18 10:05 jackson-mapper-asl-1.4.2.jar
-rwxr-xr-x  1 Stack  staff    24745 Oct 18 10:05 jackson-xc-1.5.5.jar
-rwxr-xr-x  1 Stack  staff   408133 May 21  2010 jasper-compiler-5.5.23.jar
-rwxr-xr-x  1 Stack  staff    76844 May 17  2010 jasper-runtime-5.5.23.jar
-rwxr-xr-x  1 Stack  staff   103515 May  6  2009 jaxb-api-2.1.jar
-rwxr-xr-x  1 Stack  staff   867801 Mar  4  2010 jaxb-impl-2.1.12.jar
-rwxr-xr-x  1 Stack  staff   455517 Oct 18 10:05 jersey-core-1.4.jar
-rwxr-xr-x  1 Stack  staff   142827 Oct 18 10:05 jersey-json-1.4.jar
-rwxr-xr-x  1 Stack  staff   677600 Oct 18 10:05 jersey-server-1.4.jar
-rwxr-xr-x  1 Stack  staff   377780 Mar  4  2010 jets3t-0.7.1.jar
-rwxr-xr-x  1 Stack  staff    67758 May  6  2009 jettison-1.1.jar
-rwxr-xr-x  1 Stack  staff   539912 Jan  3 16:51 jetty-6.1.26.jar
-rwxr-xr-x  1 Stack  staff   177131 Jan  3 16:51 jetty-util-6.1.26.jar
-rwxr-xr-x  1 Stack  staff    87325 Jul 20  2009 jline-0.9.94.jar
-rwxr-xr-x  1 Stack  staff  4477138 Jan  3 16:51 jruby-complete-1.0.3.jar
-rwxr-xr-x  1 Stack  staff  1024680 May 17  2010 jsp-2.1-6.1.14.jar
-rwxr-xr-x  1 Stack  staff   134910 May 17  2010 jsp-api-2.1-6.1.14.jar
-rwxr-xr-x  1 Stack  staff    46367 Mar  4  2010 jsr311-api-1.1.1.jar
-rwxr-xr-x  1 Stack  staff   121070 Mar 13  2009 junit-3.8.1.jar
-rwxr-xr-x  1 Stack  staff    11981 Mar  4  2010 kfs-0.3.jar
-rwxr-xr-x  1 Stack  staff   481535 Oct 18 10:05 log4j-1.2.16.jar
-rwxr-xr-x  1 Stack  staff    65261 Apr 14  2009 oro-2.0.8.jar
-rwxr-xr-x  1 Stack  staff    29392 Jun 14  2010 paranamer-2.2.jar
-rwxr-xr-x  1 Stack  staff     5420 Jun 14  2010 paranamer-ant-2.2.jar
-rwxr-xr-x  1 Stack  staff     6931 Jun 14  2010 paranamer-generator-2.2.jar
-rwxr-xr-x  1 Stack  staff   328635 Mar  4  2010 protobuf-java-2.3.0.jar
-rwxr-xr-x  1 Stack  staff   173236 Jun 14  2010 qdox-1.10.1.jar
drwxr-xr-x  7 Stack  staff      238 Feb  8 16:23 ruby
-rwxr-xr-x  1 Stack  staff   132368 May 17  2010 servlet-api-2.5-6.1.14.jar
-rwxr-xr-x  1 Stack  staff    23445 Mar  4  2010 slf4j-api-1.5.8.jar
-rwxr-xr-x  1 Stack  staff     9679 Mar  4  2010 slf4j-log4j12-1.5.8.jar
-rwxr-xr-x  1 Stack  staff    26514 May  6  2009 stax-api-1.0.1.jar
-rwxr-xr-x  1 Stack  staff   187530 Mar  4  2010 thrift-0.2.0.jar
-rwxr-xr-x  1 Stack  staff    15010 Mar  4  2010 xmlenc-0.52.jar
-rwxr-xr-x  1 Stack  staff   598364 Dec 10 15:13 zookeeper-3.3.2.jar
{code}

We are picking up bunch of hadoop dependencies.  I'd think it harmless other than the bulk.",,,,,,,,
HBASE-3531,"Ted ran into this in cluster testing. If the largest region is unflushable (eg it's in the midst of closing during a split, and hence doing its own flush), the global memstore pressure code doesn't notice this. So, it keeps trying to flush it, and ignores the false return code from flushRegion.

Instead, we should iterate down the list of regions and keep trying to flush them until we find one that works.",,,,,,,,
HBASE-3534,"Action stores the regionName, BUT an action comes from a MultiAction, which contains:

  public Map<byte[], List<Action<R>>> actions 

Which means we are storing the regionName multiple times. In fact, no one even calls the accessor getRegionName!

It changes the serialization of Action and MultiAction, but reduces the byte overhead.",,,,,,,,
HBASE-3545,"As part of our evaluation of HBase we have been testing failure scenarios to see how HBase fails in certain situations.

One of these is the outright failure of a HBase master.

What presently happens, if a HBase master is shutdown, is that the standby master becomes the active master in the Zookeeper. At the same time the region servers fail to connect to the dead master and typically fail their own heartbeats as part of the reportForDuty() method.

Following this the region server attempts to get a connection to a working HBase master, inside the getMaster() method the first action is to get the address of a potentially working master server from zookeeper. Following this the code is put into a tight loop whereupon it keeps attempting to connect to the address of the master found in Zookeeper.

Unfortunately it appears that during master fail-over, it becomes possible to get the address of the old, broken master, this address is then put into the connection attempt loop, whereupon the region server attempts to infinitely connect to the failed, none existent master. At this point nothing is able to break the loop in getMaster so the RS is unable to contact the master.

At the same time the new master is waiting patiently for the existing region servers as reported in Zookeeper to re-establish contact with it.

Attached is a patch that rectifies this issue in our test cluster for both the 0.90.0 tag and trunk versions (as of git SHA b72a24f71b67598e4077a9d1452f903082b0a9b7) of HBase.

This patch is also available in a forked repository here https://github.com/GregBowyer/hbase/commit/543f5903731ef6bbfd58c990e04a2c635e5c94b4",,,,,,,,
HBASE-3550,"When FilterList is set to Operator.MUST_PASS_ALL, if a child Filter returns ReturnCode.SEEK_NEXT_USING_HINT, that return code gets swallowed and ReturnCode.INCLUDE gets returned instead. This causes false positives with ColumnPrefixFilter.",,,,,,,,
HBASE-3552,"If a region server is launched in a context such that its classloader is different from the system class loader, then the Class object used to represent the Coprocessor interface of the coprocessor will be different than the Coprocessor Class object that is used by RegionCoprocessorHost.loadSystemCoprocessors() .

There's a few options that come to mind to fix this problem:

1. Remove the logic where loadSystemCoprocessors changes the context's ClassLoader back to the system default classloader.

2. Remove the cast to Coprocessor in CoprocessorHost.load() and invoke methods via reflection.

3. Set the class loader back to the system default before launching any daemon threads.",,1,,,,,,
HBASE-3553,"I know hbase has already PageFilter.
But, sometime we need to get row data from specified position.

only for newbie:
If you want to write custom Filter, you also add filter class to an hbase server classpath.
RowPaginationFilter
/**
	 * Constructor that takes a maximum page size.
	 * 
	 * get row from offset to offset+limit ( offset<= row<=offset+limit )
	 * @param offset start position
	 * @param limit count from offset position
	 */
	public RowPaginationFilter(final int offset, final int limit) {
		this.offset = offset;
		this.limit = limit;
	}

	//true to exclude row, false to include row.
	@Override
	public boolean filterRow() {	
	
		boolean isExclude = this.rowsAccepted < this.offset || this.rowsAccepted>=this.limit+this.offset;

		rowsAccepted++;
		return isExclude;
	}",,,1,,,,,
HBASE-3561,"Some of the command line arguments constructed in the bash scripts are getting duplicated

Here is what my HMaster process looks like with a ps aux | grep java.

{code}
/Library/Java/Home/bin/java 
-Xmx1000m 
-ea -XX:+HeapDumpOnOutOfMemoryError -XX:+UseConcMarkSweepGC -XX:+CMSIncrementalMode 
-ea -XX:+HeapDumpOnOutOfMemoryError -XX:+UseConcMarkSweepGC -XX:+CMSIncrementalMode 
-ea -XX:+HeapDumpOnOutOfMemoryError -XX:+UseConcMarkSweepGC -XX:+CMSIncrementalMode 
-Dcom.sun.management.jmxremote.ssl=false 
-Dcom.sun.management.jmxremote.authenticate=false 
-Dcom.sun.management.jmxremote.port=10101 
-Dcom.sun.management.jmxremote.ssl=false 
-Dcom.sun.management.jmxremote.authenticate=false 
-Dcom.sun.management.jmxremote.port=10101 
-Dcom.sun.management.jmxremote.ssl=false 
-Dcom.sun.management.jmxremote.authenticate=false 
-Dcom.sun.management.jmxremote.port=10101 
-Dhbase.log.dir=/Users/tims/workspace/hbase-trunk/bin/../logs 
-Dhbase.log.file=hbase-tims-master-grassmann.local.log 
-Dhbase.home.dir=/Users/tims/workspace/hbase-trunk/bin/.. 
-Dhbase.id.str=tims -Dhbase.root.logger=INFO,DRFA 
-classpath <blablablablabla>
org.apache.hadoop.hbase.master.HMaster start
{code}

This wouldn't really be a problem except if you try to add a java agent in the hbase-env.sh like so:
{code}
export HBASE_MASTER_OPTS=""$HBASE_MASTER_OPTS $HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10101 -javaagent:lib/HelloWorldAgent.jar""
{code}
It adds the option 3 times and it starts three java agents. My example agent print hello world once per second, but I end up with three hello world lines per second.

I attached my HelloWorldAgent.jar to demonstrate.


",,,,,,,,
HBASE-3562,"When performing a Get operation where a both a column is specified and a ValueFilter, the ValueFilter is evaluated before making the column match as is indicated in the javadoc of Get.setFilter()  : "" {@link Filter#filterKeyValue(KeyValue)} is called AFTER all tests for ttl, column match, deletes and max versions have been run. ""

The is shown in the little test below, which uses a TestComparator extending a WritableByteArrayComparable.

public void testFilter() throws Exception {
	byte[] cf = Bytes.toBytes(""cf"");
	byte[] row = Bytes.toBytes(""row"");
	byte[] col1 = Bytes.toBytes(""col1"");
	byte[] col2 = Bytes.toBytes(""col2"");
	Put put = new Put(row);
	put.add(cf, col1, new byte[]{(byte)1});
	put.add(cf, col2, new byte[]{(byte)2});
	table.put(put);

	Get get = new Get(row);
	get.addColumn(cf, col2); // We only want to retrieve col2

	TestComparator testComparator = new TestComparator();
	Filter filter = new ValueFilter(CompareOp.EQUAL, testComparator);
	get.setFilter(filter);
	Result result = table.get(get);
}


public class TestComparator extends WritableByteArrayComparable {
    /**
     * Nullary constructor, for Writable
     */
    public TestComparator() {
        super();
    }
    
    @Override
    public int compareTo(byte[] theirValue) {
        if (theirValue[0] == (byte)1) {
            // If the column match was done before evaluating the filter, we should never get here.
            throw new RuntimeException(""I only expect (byte)2 in col2, not (byte)1 from col1"");
        }
        if (theirValue[0] == (byte)2) {
            return 0;
        }
        else return 1;
    }
}

When only one column should be retrieved, this can be worked around by using a SingleColumnValueFilter instead of the ValueFilter.",1,1,,,,,,
HBASE-3566,"Class org.apache.hadoop.hbase.client.Increment has a member
boolean writeToWAL;
that is not serialized/deserialized in write/readFields functions. As a result an operation to increment several columns within a single row always writes to WAL, even if a client calls
increment.setWriteToWAL(false);",,,,,,,,
HBASE-3572,"in Chunk.init() if new byte[] fails it leaves the Chunk in its uninitialized state, other threads will assume someone else will init it and get stuck in an infinite loop.",,,,,,,,
HBASE-3582,"Currently HBase can run on top of Hadoop 0.20 with security APIs, but only using the ""simple"" authentication method.  There is currently no configuration or hooks for the HBase process to obtain Kerberos credentials so it can authenticate against secure Hadoop.

We should extend the current {{org.apache.hadoop.hbase.security.User}} hooks to allow obtaining credentials from a keytab file when security is enabled.",1,,,,,,,
HBASE-3583,"RegionObserver upcalls are expected to be triggered by corresponding client calls. 

I found that if a HTable.get() is issued, ScannerNext, and ScannerClose hooks are also invoked. 

Here is the reason: HRegion.get() is implemented with an internal scanner:

{code}
    InternalScanner scanner = null;
    try {
      scanner = getScanner(scan);
      scanner.next(results);
    } finally {
      if (scanner != null)
        scanner.close();
    }
{code}

where scanner.next, and scanner.close() are implemented with RegionObserver hooks. 

",,,1,,,,,
HBASE-3585,org.apache.hadoop.hbase.HColumnDescriptor.isLegalFamilyName(byte[]) accesses byte[0] w/o first checking the array length.,,,,,,,,
HBASE-3587,"Follow-up to a discussion on the dev list: http://search-hadoop.com/m/jOovV1uAJBP

The CoprocessorHost ReentrantReadWriteLock is imposing some overhead on data read/write operations, even when no coprocessors are loaded. Currently execution of RegionCoprocessorHost pre/postXXX() methods are guarded by acquiring the coprocessor read lock. This is used to prevent coprocessor registration from modifying the coprocessor collection while upcall hooks are in progress.

On further discussion, and looking at the locking in HRegion, it should be sufficient to just use a CopyOnWriteArrayList for the coprocessor collection. We can then remove the coprocessor lock and eliminate the associated overhead without having to special case the ""no loaded coprocessors"" condition.",,,,,,,,
HBASE-3594,"HBASE-3525 turned off the inclusion of transitive dependencies in the hbase/lib/ dir. This means that we no longer get the asm library, which is needed by jersey.",,,,,,,,
HBASE-3596,"ReplicationSourceManager.transferQueues is running a little too fast at the moment and this has the bad side effect of making us run into HBASE-2611 at almost every cluster restart. The reason is that some servers might shut down faster than others so that the last RS that are notified will at the same time see their friends dying, and will try to pick their queues. What happens then is that they also get told to shutdown and might be able to close their ZK session before the queue transfer process is completed, which is what 2611 is about.

Currently the only to fix to that is to delete the lock znode by hand and bounce a region server so that it picks up the queue on startup.",,,,,,,,
HBASE-3610,"When running RegionSplitter on a 100-node cluster with 900 regions (and plenty of data), the utility took around 72 hours to run. Analysis revealed two major bottlenecks:

1. We are serialized on the logical split (i.e. waiting for the split request to be registered). Parallelizing this step will align configured and actual outstanding splits.
2. Outstanding splits are modeled like a queue. Changing this to a list with a scanner will allow handling splits that finish out of order.",,,1,,,,,
HBASE-3612,"HBaseAdmin::isTableAvailable( name )  returns true for a table in which HBaseAdmin::tableExists( name ) returns false.

It appears from the code that the default return value from isTableAvailable() is true and false is only returned in the case where the table is found and not all the region servers are online.",,,,,,,,
HBASE-3613,"every now and again in a 0.90.1-2 load run I get a NPE on this line:


      if (bestAnyRegion.memstoreSize.get() > 2 * bestFlushableRegion.memstoreSize.get()) {
",,,,,,,,
HBASE-3617,"Via Tatsuya up on the list:

{code}
2011-03-10 07:48:39,192 FATAL org.apache.hadoop.hbase.master.HMaster:
Remote unexpected exception
java.net.NoRouteToHostException: No route to host
      at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
      at
sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
      at
org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:
206)
      at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:408)
      at org.apache.hadoop.hbase.ipc.HBaseClient
$Connection.setupIOstreams(HBaseClient.java:328)
      at
org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:
883)
      at
org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:750)
      at org.apache.hadoop.hbase.ipc.HBaseRPC
$Invoker.invoke(HBaseRPC.java:257)
      at $Proxy6.closeRegion(Unknown Source)
      at
org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:
589)
      at
org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:
1093)
      at
org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:
1040)
      at
org.apache.hadoop.hbase.master.AssignmentManager.balance(AssignmentManager.java:
1831)
      at org.apache.hadoop.hbase.master.HMaster.balance(HMaster.java:
692)
      at org.apache.hadoop.hbase.master.HMaster$1.chore(HMaster.java:
583)
      at org.apache.hadoop.hbase.Chore.run(Chore.java:66)
2011-03-10 07:48:39,192 INFO org.apache.hadoop.hbase.master.HMaster:
Aborting
2011-03-10 07:48:39,192 INFO org.apache.hadoop.hbase.master.HMaster:
balance hri=SpecialObject_Speed_Test,,
1299710751983.f0e5544339870a510c338b3029979d3e.,
src=ap13.secur2,60020,1299710609447,
dest=ap12.secur2,60020,1299710609148
2011-03-10 07:48:39,192 DEBUG
org.apache.hadoop.hbase.master.AssignmentManager: Starting
unassignment of region SpecialObject_Speed_Test,,
1299710751983.f0e5544339870a510c338b3029979d3e. (offlining)
2011-03-10 07:48:39,852 DEBUG org.apache.hadoop.hbase.master.HMaster:
Stopping service threads
2011-03-10 07:48:39,852 INFO org.apache.hadoop.ipc.HBaseServer:
Stopping server on 60000
2011-03-10 07:48:39,852 FATAL org.apache.hadoop.hbase.master.HMaster:
Remote unexpected exception
java.io.InterruptedIOException: Interruped while waiting for IO on
channel java.nio.channels.SocketChannel[connection-pending remote=/
10.X.X.18:60020]. 19340 millis timeout left.
      at org.apache.hadoop.net.SocketIOWithTimeout
$SelectorPool.select(SocketIOWithTimeout.java:349)
      at
org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:
203)
      at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:408)
      at org.apache.hadoop.hbase.ipc.HBaseClient
$Connection.setupIOstreams(HBaseClient.java:328)
      at
org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:
883)
      at
org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:750)
      at org.apache.hadoop.hbase.ipc.HBaseRPC
$Invoker.invoke(HBaseRPC.java:257)
      at $Proxy6.closeRegion(Unknown Source)
      at
org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:
589)
      at
org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:
1093)
      at
org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:
1040)
      at
org.apache.hadoop.hbase.master.AssignmentManager.balance(AssignmentManager.java:
1831)
      at org.apache.hadoop.hbase.master.HMaster.balance(HMaster.java:
692)
      at org.apache.hadoop.hbase.master.HMaster$1.chore(HMaster.java:
583)
      at org.apache.hadoop.hbase.Chore.run(Chore.java:66)
2011-03-10 07:48:39,852 INFO org.apache.hadoop.hbase.master.HMaster:
Aborting
{code}",,,,,,,,
HBASE-3620,"Make the HBCK utility contact all region servers & HDFS directories in parallel. This will speedup hbck processing, especially when there are lots of region servers.",,,,,,,,
HBASE-3621,J-D found this debugging a failure on Dmitriy's cluster; we're RPC'ing under a synchronized(regionsInTransition).  Fix.,,,,,,,,
HBASE-3622,"On Dmitriy's cluster:

{code}

""IPC Reader 0 on port 60020"" prio=10 tid=0x00002aacb4a82800 nid=0x3a72 waiting on condition [0x00000000429ba000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaabf5fa6d0> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:747)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:778)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1114)
        at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:186)
        at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:262)
        at java.util.concurrent.LinkedBlockingQueue.signalNotEmpty(LinkedBlockingQueue.java:103)
        at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:267)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.processData(HBaseServer.java:985)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.readAndProcess(HBaseServer.java:946)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.doRead(HBaseServer.java:522)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:316)
        - locked <0x00002aaabf580fb0> (a org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
...
""IPC Server handler 29 on 60020"" daemon prio=10 tid=0x00002aacbc163800 nid=0x3acc waiting on condition [0x00000000462f3000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaabf5e3800> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1025)
""IPC Server handler 28 on 60020"" daemon prio=10 tid=0x00002aacbc161800 nid=0x3acb waiting on condition [0x00000000461f2000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00002aaabf5e3800> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1025
...
{code}

This region server stayed in this state for hours. The reader is waiting to put and the handlers are waiting to take, and they wait on different lock ids. It reminds me of the UseMembar thing about the JVM sometime missing to notify waiters. In any case, that RS needed to be closed in order to get out of that state. ",,1,,,,,,
HBASE-3627,"When a region takes too long to open, it will try to update the unassigned znode and will fail on an ugly NPE like this:

{quote}
DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x22dc571dde04ca7 Attempting to transition node 0519dc3b62a569347526875048c37faa from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
DEBUG org.apache.hadoop.hbase.zookeeper.ZKUtil: regionserver:60020-0x22dc571dde04ca7 Unable to get data of znode /hbase/unassigned/0519dc3b62a569347526875048c37faa because node does not exist (not necessarily an error)
ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_RS_OPEN_REGION
java.lang.NullPointerException
	at org.apache.hadoop.hbase.util.Writables.getWritable(Writables.java:75)
	at org.apache.hadoop.hbase.executor.RegionTransitionData.fromBytes(RegionTransitionData.java:198)
	at org.apache.hadoop.hbase.zookeeper.ZKAssign.transitionNode(ZKAssign.java:672)
	at org.apache.hadoop.hbase.zookeeper.ZKAssign.retransitionNodeOpening(ZKAssign.java:585)
	at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.tickleOpening(OpenRegionHandler.java:322)
	at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:97)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:151)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
{quote}

I think the region server in this case should be closing the region ASAP.",,,,,,,,
HBASE-3633,"This is a typo.  The predicate tests for the wrong answer.  Please see the patch, which is clear.",,,,,,,,
HBASE-3636,"When ROWCOL bloomfilter needs to decide whether this key is a new key or not,
it will call the matchingRowColumn function, which will compare the timestamp offset between this kv and last kv.
But when checking the timestamp offset, it didn't deduct the original offset of the keyvalue itself.

For example, when 2 keyvalue objects have the same row key and col key, but from different storefiles. It is highly likely that these 2 keyvalue objects have different offset value. So the timestamp offset of these 2 objects are totally different. They will be regard as new keys to add into bloomfilters.
So after compaction, the key count of bloomfilter will increase immediately, which is almost equal to the number of entries.

The solution is straightforward. Just compare the relevant timestamp offset, which is the timestamp offset - key_value offset.

This also may explain this jira: https://issues.apache.org/jira/browse/HBASE-3007",,,,,,,,
HBASE-3639,Currently you can run into a StackOverflowError if the hbase root dir is on the non-default filesystem. Making FSUtils.getRootDir qualify its path solves this issue.,,1,,,,,,
HBASE-3648,"ReplicationZookeeper is a bit sloppy in how it handles the znodes during failover:

- when creating the lock, it doesn't cleanly handle the situation where the parent znode might already be deleted.
- when deleting the znodes after a successful move, it doesn't make sure to delete the lock znode last.
- after deleting the lock, there's a window where another region server could have already created another lock and deleted the znodes which would abort the first region server (saw it on one cluster).",,,,,,,,
HBASE-3654,"Saw this when debugging something else:
{code}

""regionserver60020"" prio=10 tid=0x00007f538c1c0000 nid=0x4c7 runnable [0x00007f53931da000]
   java.lang.Thread.State: RUNNABLE
	at org.apache.hadoop.hbase.regionserver.Store.getStorefilesIndexSize(Store.java:1380)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.createRegionLoad(HRegionServer.java:916)
	- locked <0x0000000672aa0a00> (a java.util.concurrent.ConcurrentSkipListMap)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.buildServerLoad(HRegionServer.java:767)
	- locked <0x0000000656f62710> (a java.util.HashMap)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.tryRegionServerReport(HRegionServer.java:722)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:591)
	at java.lang.Thread.run(Thread.java:662)

""IPC Reader 9 on port 60020"" prio=10 tid=0x00007f538c1be000 nid=0x4c6 waiting for monitor entry [0x00007f53932db000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.getFromOnlineRegions(HRegionServer.java:2295)
	- waiting to lock <0x0000000656f62710> (a java.util.HashMap)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.getOnlineRegion(HRegionServer.java:2307)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:2333)
	at org.apache.hadoop.hbase.regionserver.HRegionServer$QosFunction.isMetaRegion(HRegionServer.java:379)
	at org.apache.hadoop.hbase.regionserver.HRegionServer$QosFunction.apply(HRegionServer.java:422)
	at org.apache.hadoop.hbase.regionserver.HRegionServer$QosFunction.apply(HRegionServer.java:361)
	at org.apache.hadoop.hbase.ipc.HBaseServer.getQosLevel(HBaseServer.java:1126)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.processData(HBaseServer.java:982)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.readAndProcess(HBaseServer.java:946)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.doRead(HBaseServer.java:522)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:316)
	- locked <0x0000000656e60068> (a org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
...

""IPC Reader 0 on port 60020"" prio=10 tid=0x00007f538c08b000 nid=0x4bd waiting for monitor entry [0x00007f5393be4000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.getFromOnlineRegions(HRegionServer.java:2295)
	- waiting to lock <0x0000000656f62710> (a java.util.HashMap)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.getOnlineRegion(HRegionServer.java:2307)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:2333)
	at org.apache.hadoop.hbase.regionserver.HRegionServer$QosFunction.isMetaRegion(HRegionServer.java:379)
	at org.apache.hadoop.hbase.regionserver.HRegionServer$QosFunction.apply(HRegionServer.java:422)
	at org.apache.hadoop.hbase.regionserver.HRegionServer$QosFunction.apply(HRegionServer.java:361)
	at org.apache.hadoop.hbase.ipc.HBaseServer.getQosLevel(HBaseServer.java:1126)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.processData(HBaseServer.java:982)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.readAndProcess(HBaseServer.java:946)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.doRead(HBaseServer.java:522)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:316)
	- locked <0x0000000656e635c8> (a org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}

All the readers are blocked! I have the feeling something much better could be done.",,,,,,,,
HBASE-3660,"later edit: I've mixed up two issues here. The main problem is that a client (that could be HMaster) will read stale data from -ROOT- or .META. and not deal correctly with the raised exceptions. 

I've noticed this when the IP on my machine changed (it's even easier to detect when LZO doesn't work)

Master loads .META. successfully and then starts assigning regions.
However LZO doesn't work so HRegionServer can't open the regions. 
A client attempts to get data from a table so it reads the location from .META. but goes to a totally different server (the old value in .META.)

This could happen without the LZO story too.",,,,,,,,
HBASE-3663,"This is an interesting starvation case. There are 2 conditions to trigger this problem.
Condition1: r/s - r/(s+1) << 1 
Let r: the number of regions
Let s: the number of servers

Condition2: for each server, the load of each server is less or equal the ceil of avg load.

Here is the unit test to verify this problem: 
For example, there are 16 servers and 62 regions. The avg load is 
3.875. And setting the slot to 0 to keep the load of each server either 3 or 4. 
When a new server is coming,  no server needs to assign regions to this new server, since no one is larger the ceil of the avg.
(Setting slot to 0 is to easily trigger this situation, otherwise it needs much larger numbers)

Solutions is pretty straightforward. Just compare the floor of the avg instead of the ceil. This solution will evenly balance the load from the servers which is little more loaded than others. 

I also attached the comparison result  for the case mentioned above between the old balance algorithm and new balance algorithm. (I set the slot = 0 when testing)
",,,1,,,,,
HBASE-3664,"RSM logs all the new hlogs in memory but if there's no slave then it won't be added in zookeeper. This is a problem when a slave is finally added because the first time it logs the progress in ZK it will try to clean the old logs and will try to delete a bunch of znodes that don't exist. This throws a NoNodeException which is handled by aborting the region server. It looks like this:

{quote}
2011-03-17 17:38:29,832 INFO org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager: Removing 42 logs in the list: [sv2borg172%3A60020.1300351782310, sv2borg172%3A60020.1300352001486, sv2borg172%3A60020.1300352121367, sv2borg172%3A60020.1300352772392, sv2borg172%3A60020.1300354158262, sv2borg172%3A60020.1300355578566, sv2borg172%3A60020.1300356840451, sv2borg172%3A60020.1300358106115, sv2borg172%3A60020.1300359494020, sv2borg172%3A60020.1300360803514, sv2borg172%3A60020.1300362078570, sv2borg172%3A60020.1300363300908, sv2borg172%3A60020.1300364449495, sv2borg172%3A60020.1300365539396, sv2borg172%3A60020.1300366546548, sv2borg172%3A60020.1300367485952, sv2borg172%3A60020.1300368371234, sv2borg172%3A60020.1300369227069, sv2borg172%3A60020.1300370079940, sv2borg172%3A60020.1300370899710, sv2borg172%3A60020.1300371697355, sv2borg172%3A60020.1300372472873, sv2borg172%3A60020.1300373238890, sv2borg172%3A60020.1300374001201, sv2borg172%3A60020.1300374738469, sv2borg172%3A60020.1300375453876, sv2borg172%3A60020.1300376155468, sv2borg172%3A60020.1300376860049, sv2borg172%3A60020.1300377555922, sv2borg172%3A60020.1300378246690, sv2borg172%3A60020.1300378917995, sv2borg172%3A60020.1300379573664, sv2borg172%3A60020.1300380218543, sv2borg172%3A60020.1300380861201, sv2borg172%3A60020.1300381485824, sv2borg172%3A60020.1300381550415, sv2borg172%3A60020.1300381596287, sv2borg172%3A60020.1300381655746, sv2borg172%3A60020.1300381720905, sv2borg172%3A60020.1300382140669, sv2borg172%3A60020.1300382598288, sv2borg172%3A60020.1300383006771]
2011-03-17 17:38:29,868 FATAL org.apache.hadoop.hbase.regionserver.HRegionServer: ABORTING region server serverName=sv2borg172,60020,1300351781748, load=(requests=3869, regions=526, usedHeap=4475, maxHeap=7973): Failed remove from list
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /hbase/replication/rs/sv2borg172,60020,1300351781748/2/sv2borg172%3A60020.1300351782310
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:102)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
        at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:728)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.deleteNode(ZKUtil.java:959)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.deleteNode(ZKUtil.java:948)
        at org.apache.hadoop.hbase.replication.ReplicationZookeeper.removeLogFromList(ReplicationZookeeper.java:413)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.logPositionAndCleanOldLogs(ReplicationSourceManager.java:141)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:332)

{quote}

If there's no slave, only the latest hlog should be kept in memory.",,1,,,,,,
HBASE-3668,"We got into a weird situation after hitting HBASE-3664 leaving almost half the region servers unable to open regions (we didn't kill the master but all RS were restarted).

Here's the relevant jstack from the region servers:

{code}

""regionserver60020-EventThread"" daemon prio=10 tid=0x0000000041297800 nid=0x5c5 in Object.wait() [0x00007fbd2b7f6000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00007fbd4e1ac6d0> (a java.util.concurrent.atomic.AtomicBoolean)
	at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMeta(CatalogTracker.java:328)
	- locked <0x00007fbd4e1ac6d0> (a java.util.concurrent.atomic.AtomicBoolean)
	at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMetaServerConnectionDefault(CatalogTracker.java:363)
	at org.apache.hadoop.hbase.zookeeper.MetaNodeTracker.nodeDeleted(MetaNodeTracker.java:64)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.nodeCreated(ZooKeeperNodeTracker.java:154)
	- locked <0x00007fbd4d7ab060> (a org.apache.hadoop.hbase.zookeeper.MetaNodeTracker)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.nodeDataChanged(ZooKeeperNodeTracker.java:179)
	- locked <0x00007fbd4d7ab060> (a org.apache.hadoop.hbase.zookeeper.MetaNodeTracker)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:268)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:530)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:506)
{code}

Every server in that state cannot receive any ZK event. See how we first do a nodeDataChanged, then nodeCreated, then nodeDeleted. This is correlated in the logs:

{code}
2011-03-17 17:57:03,636 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher: regionserver:60020-0x12d627b723e081f Received ZooKeeper Event, type=NodeDataChanged, state=SyncConnected, path=/hbase/unassigned/1028785192
2011-03-17 17:57:03,636 DEBUG org.apache.hadoop.hbase.zookeeper.ZKUtil: regionserver:60020-0x12d627b723e081f Unable to get data of znode /hbase/unassigned/1028785192 because node does not exist (not an error)
2011-03-17 17:57:03,637 DEBUG org.apache.hadoop.hbase.zookeeper.ZKUtil: regionserver:60020-0x12d627b723e081f Set watcher on existing znode /hbase/unassigned/1028785192
2011-03-17 17:57:03,637 INFO org.apache.hadoop.hbase.zookeeper.MetaNodeTracker: Detected completed assignment of META, notifying catalog tracker
2011-03-17 17:57:06,988 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation: Lookedup root region location, connection=org.
{code}

Node is updated, then is missing, than came back! The reason we're stuck is that CT.waitForMeta will wait forever:
{code}
     if (getMetaServerConnection(true) != null) {
        return metaLocation;
      }
      while(!stopped && !metaAvailable.get() &&
          (timeout == 0 || System.currentTimeMillis() < stop)) {
        metaAvailable.wait(timeout);
      }
{code}

So when it tried getMetaServerConnection the first time it didn't work, and then since the timeout is 0 then it waits. Even if it was looping, metaAvailable will never be true since we're already inside the event thread thus blocking any other event!

This is what happens when the RS then tries to update .META. after opening a region:

{code}
2011-03-17 17:59:06,557 DEBUG org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Interrupting thread Thread[PostOpenDeployTasks:f06b630822a161d0a8fe37481d851d05,5,main]
2011-03-17 17:59:06,557 WARN org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Exception running postOpenDeployTasks; region=f06b630822a161d0a8fe37481d851d05
org.apache.hadoop.hbase.NotAllMetaRegionsOnlineException: Interrupted
        at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMetaServerConnectionDefault(CatalogTracker.java:365)
        at org.apache.hadoop.hbase.catalog.MetaEditor.updateRegionLocation(MetaEditor.java:142)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.postOpenDeployTasks(HRegionServer.java:1359)

{code}

The master eventually tries to assign the region somewhere else where it may work, but then the balancer will screw everything up again.",,,,,,,,
HBASE-3670,"See HBASE-3634 for details. The get(List<Get> gets) call needs to catch (or rather use a try/finally) the exception thrown by batch() and copy the Result instances over and return it. If that is not intended then we need to fix the JavaDoc in HTableInterface to reflect the new behavior. 

In general it seems to make sense to check the various methods (list based put, get, delete compared to batch) and agree on the correct behavior.",,,,,,,,
HBASE-3674,"In short, a ChecksumException will fail log processing for a server so we skip out w/o archiving logs.  On restart, we'll then reprocess the logs -- hit the checksumexception anew, usually -- and so on.

Here is the splitLog method (edited):

{code}
  private List<Path> splitLog(final FileStatus[] logfiles) throws IOException {
    ....
    outputSink.startWriterThreads(entryBuffers);
    
    try {
      int i = 0;
      for (FileStatus log : logfiles) {
       Path logPath = log.getPath();
        long logLength = log.getLen();
        splitSize += logLength;
        LOG.debug(""Splitting hlog "" + (i++ + 1) + "" of "" + logfiles.length
            + "": "" + logPath + "", length="" + logLength);
        try {
          recoverFileLease(fs, logPath, conf);
          parseHLog(log, entryBuffers, fs, conf);
          processedLogs.add(logPath);
        } catch (EOFException eof) {
          // truncated files are expected if a RS crashes (see HBASE-2643)
          LOG.info(""EOF from hlog "" + logPath + "". Continuing"");
          processedLogs.add(logPath);
        } catch (FileNotFoundException fnfe) {
          // A file may be missing if the region server was able to archive it
          // before shutting down. This means the edits were persisted already
          LOG.info(""A log was missing "" + logPath +
              "", probably because it was moved by the"" +
              "" now dead region server. Continuing"");
          processedLogs.add(logPath);
        } catch (IOException e) {
          // If the IOE resulted from bad file format,
          // then this problem is idempotent and retrying won't help
          if (e.getCause() instanceof ParseException ||
              e.getCause() instanceof ChecksumException) {
            LOG.warn(""ParseException from hlog "" + logPath + "".  continuing"");
            processedLogs.add(logPath);
          } else {
            if (skipErrors) {
              LOG.info(""Got while parsing hlog "" + logPath +
                "". Marking as corrupted"", e);
              corruptedLogs.add(logPath);
            } else {
              throw e;
            }
          }
        }
      }
      if (fs.listStatus(srcDir).length > processedLogs.size()
          + corruptedLogs.size()) {
        throw new OrphanHLogAfterSplitException(
            ""Discovered orphan hlog after split. Maybe the ""
            + ""HRegionServer was not dead when we started"");
      }
      archiveLogs(srcDir, corruptedLogs, processedLogs, oldLogDir, fs, conf);      
    } finally {
      splits = outputSink.finishWritingAndClose();
    }
    return splits;
  }
{code}

Notice how we'll only archive logs only if we successfully split all logs.  We won't archive 31 of 35 files if we happen to get a checksum exception on file 32.

I think we should treat a ChecksumException the same as a ParseException; a retry will not fix it if HDFS could not get around the ChecksumException (seems like in our case all replicas were corrupt).

Here is a play-by-play from the logs:

{code}
813572 2011-03-18 20:31:44,687 DEBUG org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Splitting hlog 34 of 35: hdfs://sv2borg170:9000/hbase/.logs/sv2borg182,60020,1300384550664/sv2borg182%3A60020.1300461329481, length=150       65662813573 2011-03-18 20:31:44,687 INFO org.apache.hadoop.hbase.util.FSUtils: Recovering file hdfs://sv2borg170:9000/hbase/.logs/sv2borg182,60020,1300384550664/sv2borg182%3A60020.1300461329481
....
813617 2011-03-18 20:31:46,238 INFO org.apache.hadoop.fs.FSInputChecker: Found checksum error: b[0, 512]=000000cd000000502037383661376439656265643938636463343433386132343631323633303239371d6170695f6163636573735f746f6b656e5f7374       6174735f6275636b65740000000d9fa4d5dc0000012ec9c7cbaf00ffffffff000000010000006d0000005d00000008002337626262663764626431616561366234616130656334383436653732333132643a32390764656661756c746170695f616e64726f69645f6c6f67676564       696e5f73686172655f70656e64696e675f696e69740000012ec956b02804000000000000000100000000ffffffff4e128eca0eb078d0652b0abac467fd09000000cd000000502034663166613763666165333930666332653138346233393931303132623366331d6170695f6163       636573735f746f6b656e5f73746174735f6275636b65740000000d9fa4d5dd0000012ec9c7cbaf00ffffffff000000010000006d0000005d00000008002366303734323966643036323862636530336238333938356239316237386633353a32390764656661756c746170695f61       6e64726f69645f6c6f67676564696e5f73686172655f70656e64696e675f696e69740000012ec9569f1804000000000000000100000000000000d30000004e2066663763393964303633343339666531666461633761616632613964643631331b6170695f6163636573735f746f       6b656e5f73746174735f68
813618 org.apache.hadoop.fs.ChecksumException: Checksum error: /blk_7781725413191608261:of:/hbase/.logs/sv2borg182,60020,1300384550664/sv2borg182%3A60020.1300461329481 at 15064576
813619         at org.apache.hadoop.fs.FSInputChecker.verifySum(FSInputChecker.java:277)
813620         at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:241)
813621         at org.apache.hadoop.fs.FSInputChecker.fill(FSInputChecker.java:176)
813622         at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:193)
813623         at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:158)
813624         at org.apache.hadoop.hdfs.DFSClient$BlockReader.read(DFSClient.java:1175)
813625         at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.readBuffer(DFSClient.java:1807)
813626         at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:1859)
813627         at java.io.DataInputStream.read(DataInputStream.java:132)
813628         at java.io.DataInputStream.readFully(DataInputStream.java:178)
813629         at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:63)
813630         at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:101)
813631         at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1937)
813632         at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1837)
813633         at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1883)
813634         at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:198)
813635         at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:172)
813636         at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.parseHLog(HLogSplitter.java:429)
813637         at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLog(HLogSplitter.java:262)
813638         at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLog(HLogSplitter.java:188)
813639         at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:197)
813640         at org.apache.hadoop.hbase.master.MasterFileSystem.splitLogAfterStartup(MasterFileSystem.java:181)
813641         at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:384)
813642         at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:283)
813643 2011-03-18 20:31:46,239 WARN org.apache.hadoop.hdfs.DFSClient: Found Checksum error for blk_7781725413191608261_14589573 from 10.20.20.182:50010 at 15064576
813644 2011-03-18 20:31:46,240 INFO org.apache.hadoop.hdfs.DFSClient: Could not obtain block blk_7781725413191608261_14589573 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations        from namenode and retry...
813645 2011-03-18 20:31:49,243 DEBUG org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Pushed=80624 entries from hdfs://sv2borg170:9000/hbase/.logs/sv2borg182,60020,1300384550664/sv2borg182%3A60020.1300461329481
....
{code}

See code above.  On exception we'll dump edits read so far from this block, close out all writers tying off recovered.edits so far written.  We'll skip archiving these files because we only archive if all files are processed; we won't archive files 30 of 35 if we failed splitting on file 31.

I think checksumexception should be treated same as a ParseException

  

",,,,,,,,
HBASE-3685,"As reported by an Hbase user: 

""I have a ThreadMetadata column family, and there are two columns in it: v12:th: and v12:me. The following code only returns v12:me

get.addColumn(Bytes.toBytes(""ThreadMetadata""), Bytes.toBytes(""v12:th:"");
get.addColumn(Bytes.toBytes(""ThreadMetadata""), Bytes.toBytes(""v12:me:"");
List<Long> threadIds = new ArrayList<Long>();
threadIds.add(10709L);
TimestampFilter filter = new TimestampFilter(threadIds);
get.setFilter(filter);
get.setMaxVersions();
Result result = table.get(get);

I checked hbase for the key/value, they are present. Also other combinations like no timestampfilter, it returns both.""

Kannan was able to do a small repro of the issue and commented that if we drop the get.setMaxVersions(), then the problem goes away. ",,,,,,,,
HBASE-3686,"This can cause rows to be lost from a scan.

See this thread where the issue was brought up: http://search-hadoop.com/m/xITBQ136xGJ1

If hbase.regionserver.lease.period is higher on the client than the server we can get this series of events: 

1. Client is scanning along happily, and does something slow.
2. Scanner times out on region server
3. Client calls HTable.ClientScanner.next()
4. The region server throws an UnknownScannerException
5. Client catches exception and sees that it's not longer then it's hbase.regionserver.lease.period config, so it doesn't throw a ScannerTimeoutException. Instead, it treats it like a NSRE.

Right now the workaround is to make sure the configs are consistent. 

A possible fix would be to use whatever the region server's scanner timeout is, rather than the local one.",,,,,,,,
HBASE-3687,"On startup, we do bulk assign.  At the moment, if any problem during bulk assign, we consider startup failed and expectation is that you need to retry (We need to make this better but that is not what this issue is about).  One exception that we should handle is the case where a RS is slow coming up and its rpc is not yet up listening.  In this case it will throw: ServerNotRunningException.  We should retry at least this one exception during bulk assign.

We had this happen to us starting up a prod cluster.",,,,,,,,
HBASE-3688,"Setters ""setName()"" and ""setDeferredLogFlush()"" do not work properly because for example after calling setName() the internal property nameAsString is not modified and then if you call getNameAsString() you get the previous value and not the new one. Something similar happens to the setter ""setDeferredLogFlush()""",,,,,,,,
HBASE-3702,"Exec write method invokes getClass() on its arguments list for finding the argument's class, which gives a npe in case the argument is null. There is already an parameterClasses array in Invoker (its super class), which is populated with correct values (by method.getParameterTypes()). One can use this array.",,,,,,,,
HBASE-3711,"importtsv MapTask fails with this error, when long row key (exceeds MAX_ROW_LENGTH) was given.

11/03/30 04:59:16 INFO mapred.JobClient: Task Id : attempt_201103252231_0077_m_000003_0, Status : FAILED
java.lang.IllegalArgumentException: Row key is invalid
at org.apache.hadoop.hbase.client.Put.<init>(Put.java:101)
at org.apache.hadoop.hbase.client.Put.<init>(Put.java:80)
at org.apache.hadoop.hbase.client.Put.<init>(Put.java:71)
at org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvImporter.map(ImportTsv.java:235)
at org.apache.hadoop.hbase.mapreduce.ImportTsv$TsvImporter.map(ImportTsv.java:190)
at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:646)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:322)
at org.apache.hadoop.mapred.Child$4.run(Child.java:240)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:396)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1115)
at org.apache.hadoop.mapred.Child.main(Child.java:234)",,1,,,,,,
HBASE-3712,HTable.close() doesn't shutdown thread pool,,,,,,,,
HBASE-3714,"The completebulkupload tool should be using the HBaseConfiguration.create() method to get the HBase configuration in 0.90.*. In it's present state, you receive a connection error when running this tool.",,1,,,,,,
HBASE-3716,"See HBase-TRUNK build #1820

This could be due to HBASE-3681
In trunk, default value of ""hbase.regions.slop"" is 20%. It is possible for load balancer to see region distribution which falls within 20% of optimal distribution.
However, assertRegionsAreBalanced() uses 10% slop.

One solution is to align the slop in assertRegionsAreBalanced() with ""hbase.regions.slop"" value.",,,,,,,,
HBASE-3722,"I'm not sure exactly what arose it. there is some split failed logs .
the master should shutdown itself when the HDFS is crashed.

 The logs is :
 2011-03-22 13:21:55,056 WARN 
 org.apache.hadoop.hbase.master.LogCleaner: Error while cleaning the 
 logs
 java.net.ConnectException: Call to C4C1/157.5.100.1:9000 failed on connection exception: java.net.ConnectException: Connection refused
         at org.apache.hadoop.ipc.Client.wrapException(Client.java:844)
         at org.apache.hadoop.ipc.Client.call(Client.java:820)
         at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:221)
         at $Proxy5.getListing(Unknown Source)
         at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
         at java.lang.reflect.Method.invoke(Method.java:597)
         at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
         at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
         at $Proxy5.getListing(Unknown Source)
         at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:614)
         at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:252)
         at org.apache.hadoop.hbase.master.LogCleaner.chore(LogCleaner.java:121)
         at org.apache.hadoop.hbase.Chore.run(Chore.java:66)
         at 
 org.apache.hadoop.hbase.master.LogCleaner.run(LogCleaner.java:154)
 Caused by: java.net.ConnectException: Connection refused
         at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
         at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:574)
         at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
         at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:408)
         at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:332)
         at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:202)
         at org.apache.hadoop.ipc.Client.getConnection(Client.java:943)
         at org.apache.hadoop.ipc.Client.call(Client.java:788)
         ... 13 more
 2011-03-22 13:21:56,056 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 0 time(s).
 2011-03-22 13:21:57,057 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 1 time(s).
 2011-03-22 13:21:58,057 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 2 time(s).
 2011-03-22 13:21:59,057 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 3 time(s).
 2011-03-22 13:22:00,058 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 4 time(s).
 2011-03-22 13:22:01,058 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 5 time(s).
 2011-03-22 13:22:02,059 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 6 time(s).
 2011-03-22 13:22:03,059 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 7 time(s).
 2011-03-22 13:22:04,059 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 8 time(s).
 2011-03-22 13:22:05,060 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 9 time(s).
 2011-03-22 13:22:05,060 ERROR 
 org.apache.hadoop.hbase.master.MasterFileSystem: Failed splitting 
 hdfs://C4C1:9000/hbase/.logs/C4C9.site,60020,1300767633398
 java.net.ConnectException: Call to C4C1/157.5.100.1:9000 failed on connection exception: java.net.ConnectException: Connection refused
         at org.apache.hadoop.ipc.Client.wrapException(Client.java:844)
         at org.apache.hadoop.ipc.Client.call(Client.java:820)
         at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:221)
         at $Proxy5.getFileInfo(Unknown Source)
         at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
         at java.lang.reflect.Method.invoke(Method.java:597)
         at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
         at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
         at $Proxy5.getFileInfo(Unknown Source)
         at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:623)
         at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:461)
         at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:690)
         at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLog(HLogSplitter.java:177)
         at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:196)
         at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:95)
         at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:151)
         at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
         at java.lang.Thread.run(Thread.java:662)
 Caused by: java.net.ConnectException: Connection refused
         at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
         at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:574)
         at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
         at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:408)
         at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:332)
         at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:202)
         at org.apache.hadoop.ipc.Client.getConnection(Client.java:943)
         at org.apache.hadoop.ipc.Client.call(Client.java:788)
         ... 18 more
 2011-03-22 13:22:45,600 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 0 time(s).
 2011-03-22 13:22:46,600 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 1 time(s).
 2011-03-22 13:22:47,601 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 2 time(s).
 2011-03-22 13:22:48,601 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 3 time(s).
 2011-03-22 13:22:49,601 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 4 time(s).
 2011-03-22 13:22:50,602 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 5 time(s).
 2011-03-22 13:22:51,602 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 6 time(s).
 2011-03-22 13:22:52,602 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 7 time(s).
 2011-03-22 13:22:53,603 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 8 time(s).
 2011-03-22 13:22:54,603 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 9 time(s).
 2011-03-22 13:22:54,603 WARN 
 org.apache.hadoop.hbase.master.LogCleaner: Error while cleaning the 
 logs
 java.net.ConnectException: Call to C4C1/157.5.100.1:9000 failed on connection exception: java.net.ConnectException: Connection refused
         at org.apache.hadoop.ipc.Client.wrapException(Client.java:844)
         at org.apache.hadoop.ipc.Client.call(Client.java:820)
         at org.apache.hadoop.ipc.RPC$Invok

",,,,,,,,
HBASE-3723,"In the function store.isMajorCompaction:
      if (filesToCompact.size() == 1) {
        // Single file
        StoreFile sf = filesToCompact.get(0);
        long oldest =
            (sf.getReader().timeRangeTracker == null) ?
                Long.MIN_VALUE :
                now - sf.getReader().timeRangeTracker.minimumTimestamp;
        if (sf.isMajorCompaction() &&
            (this.ttl == HConstants.FOREVER || oldest < this.ttl)) {
          if (LOG.isDebugEnabled()) {
            LOG.debug(""Skipping major compaction of "" + this.storeNameStr +
                "" because one (major) compacted file only and oldestTime "" +
                oldest + ""ms is < ttl="" + this.ttl);
          }
        }
      } else {
When there is only one storefile in the store, and some keyvalues' TTL are overtime, the majorcompactchecker should send this region to the compactquene and run a majorcompact to clean these outdated data. But according to the code in 0.90.1, it will do nothing. ",,,,,,,,
HBASE-3724,Umbrella issue under which we hang all regions related to balancer,,,1,,,,,
HBASE-3725,"Deleted row values are sometimes used for starting points on new increments.

To reproduce:
Create a row ""r"". Set column ""x"" to some default value.
Force hbase to write that value to the file system (such as restarting the cluster).
Delete the row.
Call table.incrementColumnValue with ""some_value""
Get the row.
The returned value in the column was incremented from the old value before the row was deleted instead of being initialized to ""some_value"".

Code to reproduce:
{code}

import java.io.IOException;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.HColumnDescriptor;
import org.apache.hadoop.hbase.HTableDescriptor;
import org.apache.hadoop.hbase.client.Delete;
import org.apache.hadoop.hbase.client.Get;
import org.apache.hadoop.hbase.client.HBaseAdmin;
import org.apache.hadoop.hbase.client.HTableInterface;
import org.apache.hadoop.hbase.client.HTablePool;
import org.apache.hadoop.hbase.client.Increment;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.util.Bytes;

public class HBaseTestIncrement
{
	static String tableName  = ""testIncrement"";

	static byte[] infoCF = Bytes.toBytes(""info"");

	static byte[] rowKey = Bytes.toBytes(""test-rowKey"");

	static byte[] newInc = Bytes.toBytes(""new"");
	static byte[] oldInc = Bytes.toBytes(""old"");

	/**
	 * This code reproduces a bug with increment column values in hbase
	 * Usage: First run part one by passing '1' as the first arg
	 *        Then restart the hbase cluster so it writes everything to disk
	 *	  Run part two by passing '2' as the first arg
	 *
	 * This will result in the old deleted data being found and used for the increment calls
	 *
	 * @param args
	 * @throws IOException
	 */
	public static void main(String[] args) throws IOException
	{
		if(""1"".equals(args[0]))
			partOne();
		if(""2"".equals(args[0]))
			partTwo();
		if (""both"".equals(args[0]))
		{
			partOne();
			partTwo();
		}
	}

	/**
	 * Creates a table and increments a column value 10 times by 10 each time.
	 * Results in a value of 100 for the column
	 *
	 * @throws IOException
	 */
	static void partOne()throws IOException
	{

		Configuration conf = HBaseConfiguration.create();


		HBaseAdmin admin = new HBaseAdmin(conf);
		HTableDescriptor tableDesc = new HTableDescriptor(tableName);
		tableDesc.addFamily(new HColumnDescriptor(infoCF));
		if(admin.tableExists(tableName))
		{
			admin.disableTable(tableName);
			admin.deleteTable(tableName);
		}
		admin.createTable(tableDesc);

		HTablePool pool = new HTablePool(conf, Integer.MAX_VALUE);
		HTableInterface table = pool.getTable(Bytes.toBytes(tableName));

		//Increment unitialized column
		for (int j = 0; j < 10; j++)
		{
			table.incrementColumnValue(rowKey, infoCF, oldInc, (long)10);
			Increment inc = new Increment(rowKey);
			inc.addColumn(infoCF, newInc, (long)10);
			table.increment(inc);
		}

		Get get = new Get(rowKey);
		Result r = table.get(get);
		System.out.println(""initial values: new "" + Bytes.toLong(r.getValue(infoCF, newInc)) + "" old "" + Bytes.toLong(r.getValue(infoCF, oldInc)));

	}

	/**
	 * First deletes the data then increments the column 10 times by 1 each time
	 *
	 * Should result in a value of 10 but it doesn't, it results in a values of 110
	 *
	 * @throws IOException
	 */
	static void partTwo()throws IOException
	{
		Configuration conf = HBaseConfiguration.create();

		HTablePool pool = new HTablePool(conf, Integer.MAX_VALUE);
		HTableInterface table = pool.getTable(Bytes.toBytes(tableName));
		
		Delete delete = new Delete(rowKey);
		table.delete(delete);


		//Increment columns
		for (int j = 0; j < 10; j++)
		{
			table.incrementColumnValue(rowKey, infoCF, oldInc, (long)1);
			Increment inc = new Increment(rowKey);
			inc.addColumn(infoCF, newInc, (long)1);
			table.increment(inc);
		}


		Get get = new Get(rowKey);
		Result r = table.get(get);
		System.out.println(""after delete values: new "" + Bytes.toLong(r.getValue(infoCF, newInc)) + "" old "" + Bytes.toLong(r.getValue(infoCF, oldInc)));

	}
}
{code}",,,,,,,,
HBASE-3728,When I use HTablePool and try to close it on application shutdown I've got NPE calling closeTablePool method because I didn't borrow any tables with the given name. Could you please add a null check for queue in closeTablePool or add ability to get all table names used in a pool or just add a destroy method to close all existed table in a pool.,,,,,,,,
HBASE-3733,"v-himanshu found that since HRegion doesn't implement Comparable, it cannot be placed in TreeSet.",,,,,,,,
HBASE-3740,"Using hbck to fix a problem, I see that when it retries it doesn't reset the number of inconsistencies so the number doubles.",,,,,,,,
HBASE-3741,"This is a serious issue about a race between regions being opened and closed in region servers. We had this situation where the master tried to unassign a region for balancing, failed, force unassigned it, force assigned it somewhere else, failed to open it on another region server (took too long), and then reassigned it back to the original region server. A few seconds later, the region server processed the first closed and the region was left unassigned.

This is from the master log:

{quote}
11-04-05 15:11:17,758 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Sent CLOSE to serverName=sv4borg42,60020,1300920459477, load=(requests=187, regions=574, usedHeap=3918, maxHeap=6973) for region stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:12:10,021 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961 state=PENDING_CLOSE, ts=1302041477758
2011-04-05 15:12:10,021 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been PENDING_CLOSE for too long, running forced unassign again on region=stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
...
2011-04-05 15:14:45,783 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961 state=CLOSED, ts=1302041685733
2011-04-05 15:14:45,783 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x42ec2cece810b68 Creating (or updating) unassigned node for 1470298961 with OFFLINE state
...
2011-04-05 15:14:45,885 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Using pre-existing plan for region stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961; plan=hri=stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961, src=sv4borg42,60020,1300920459477, dest=sv4borg40,60020,1302041218196
2011-04-05 15:14:45,885 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961 to sv4borg40,60020,1302041218196
2011-04-05 15:15:39,410 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961 state=PENDING_OPEN, ts=1302041700944
2011-04-05 15:15:39,410 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been PENDING_OPEN for too long, reassigning region=stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:15:39,410 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961 state=PENDING_OPEN, ts=1302041700944
...
2011-04-05 15:15:39,410 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: No previous transition plan was found (or we are ignoring an existing plan) for stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961 so generated a random one; hri=stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961, src=, dest=sv4borg42,60020,1300920459477; 19 (online=19, exclude=null) available servers
2011-04-05 15:15:39,410 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961 to sv4borg42,60020,1300920459477
2011-04-05 15:15:40,951 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher: master:60000-0x42ec2cece810b68 Received ZooKeeper Event, type=NodeDataChanged, state=SyncConnected, path=/prodjobs/unassigned/1470298961
2011-04-05 15:15:40,952 DEBUG org.apache.hadoop.hbase.zookeeper.ZKUtil: master:60000-0x42ec2cece810b68 Retrieved 93 byte(s) of data from znode /prodjobs/unassigned/1470298961 and set watcher; region=stumbles_by_userid2,'6,1266566087256, server=sv4borg42,60020,1300920459477, state=RS_ZK_REGION_OPENED
2011-04-05 15:15:40,952 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENED, server=sv4borg42,60020,1300920459477, region=1470298961
2011-04-05 15:15:42,222 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Handling OPENED event for 1470298961; deleting unassigned node
...
2011-04-05 15:15:55,812 DEBUG org.apache.hadoop.hbase.zookeeper.ZKUtil: master:60000-0x42ec2cece810b68 Retrieved 93 byte(s) of data from znode /prodjobs/unassigned/1470298961 and set watcher; region=stumbles_by_userid2,'6,1266566087256, server=sv4borg42,60020,1300920459477, state=RS_ZK_REGION_CLOSING
2011-04-05 15:15:55,812 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling new unassigned node: /prodjobs/unassigned/1470298961 (region=stumbles_by_userid2,'6,1266566087256, server=sv4borg42,60020,1300920459477, state=RS_ZK_REGION_CLOSING)
2011-04-05 15:15:55,812 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_CLOSING, server=sv4borg42,60020,1300920459477, region=1470298961
2011-04-05 15:15:55,812 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received CLOSING for region 1470298961 from server sv4borg42,60020,1300920459477 but region was in  the state null and not in expected PENDING_CLOSE or CLOSING states
{quote}

And from sv4borg42:

{quote}
2011-04-05 15:09:58,755 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Received close region: stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:11:17,757 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Received close region: stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:12:10,021 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Received close region: stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:14:45,675 DEBUG org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler: Processing close of stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:14:45,700 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Closing stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961: disabling compactions & flushes
2011-04-05 15:14:45,701 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Updates disabled for region stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:14:45,701 INFO org.apache.hadoop.hbase.regionserver.HRegion: Closed stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:14:45,758 DEBUG org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler: Closed region stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:15:39,410 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Received request to open region: stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:15:39,410 DEBUG org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Processing open of stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:15:39,486 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Opening region: REGION => {NAME => 'stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256', STARTKEY => '\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6', ENDKEY => '\x00'\x9AU\x7F\xFF\xFE\xEBQ\xB0\xC3\xEF\x00Jr\xF2', ENCODED => 1470298961, TABLE => ...
2011-04-05 15:15:39,487 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Instantiated stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:15:40,399 INFO org.apache.hadoop.hbase.regionserver.HRegion: Onlined stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961; next sequenceid=37627407247
2011-04-05 15:15:40,488 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Updated row stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961 in region .META.,,1 with server=sv4borg42:60020, startcode=1300920459477
2011-04-05 15:15:40,582 DEBUG org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Opened stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:15:55,776 DEBUG org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler: Processing close of stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:15:55,809 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Closing stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961: disabling compactions & flushes
2011-04-05 15:15:55,809 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Updates disabled for region stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:15:55,809 INFO org.apache.hadoop.hbase.regionserver.HRegion: Closed stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:15:55,842 DEBUG org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler: Closed region stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:15:55,943 DEBUG org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler: Processing close of stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:15:55,943 WARN org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler: Received CLOSE for region stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961 but currently not serving

{quote}",,,,,,,,
HBASE-3744,"In HBASE-3305, the behavior of createTable was changed and introduced this bug: createTable now blocks until all regions have been assigned, since it uses BulkStartupAssigner. BulkStartupAssigner.waitUntilDone calls assignmentManager.waitUntilNoRegionsInTransition, which waits across all regions, not just the regions of the table that has just been created.

We saw an issue where one table had a region which was unable to be opened, so it was stuck in RegionsInTransition permanently (every open was failing). Since this was the case, waitUntilDone would always block indefinitely even though the newly created table had been assigned.",,,,,,,,
HBASE-3749,"When Hmaster crashed  and restart , The Hmaster is hung up.

    // start up all service threads.
    startServiceThreads();                                  ----this open port failed!

    // Wait for region servers to report in.  Returns count of regions.
    int regionCount = this.serverManager.waitForRegionServers();

    // TODO: Should do this in background rather than block master startup
    this.fileSystemManager.
      splitLogAfterStartup(this.serverManager.getOnlineServers());

    // Make sure root and meta assigned before proceeding.
assignRootAndMeta();                                       --- hung up this function, because of root can't be assigned.

  if (!catalogTracker.verifyRootRegionLocation(timeout)) {
      this.assignmentManager.assignRoot();
      this.catalogTracker.waitForRoot();                   --- This statement code is hung up. 
      assigned++;
}

Log is as?
2011-04-07 16:38:22,850 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2011-04-07 16:38:22,908 INFO org.apache.hadoop.http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 60010
2011-04-07 16:38:22,909 FATAL org.apache.hadoop.hbase.master.HMaster: Failed startup
java.net.BindException: Address already in use
         at sun.nio.ch.Net.bind(Native Method)
         at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:119)
         at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:59)
         at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)
         at org.apache.hadoop.http.HttpServer.start(HttpServer.java:445)
         at org.apache.hadoop.hbase.master.HMaster.startServiceThreads(HMaster.java:542)
         at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:373)
         at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:278)
2011-04-07 16:38:22,910 INFO org.apache.hadoop.hbase.master.HMaster: Aborting
2011-04-07 16:38:22,911 INFO org.apache.hadoop.hbase.master.ServerManager: Exiting wait on regionserver(s) to checkin; count=0, stopped=true, count of regions out on cluster=0
2011-04-07 16:38:22,914 DEBUG org.apache.hadoop.hbase.master.MasterFileSystem: No log files to split, proceeding...
2011-04-07 16:38:22,930 INFO org.apache.hadoop.ipc.HbaseRPC: Server at 167-6-1-12/167.6.1.12:60020 could not be reached after 1 tries, giving up.
2011-04-07 16:38:22,930 INFO org.apache.hadoop.hbase.catalog.RootLocationEditor: Unsetting ROOT region location in ZooKeeper
2011-04-07 16:38:22,941 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x22f2c49d2590021 Creating (or updating) unassigned node for 70236052 with OFFLINE state
2011-04-07 16:38:22,956 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Server stopped; skipping assign of -ROOT-,,0.70236052 state=OFFLINE, ts=1302165502941
2011-04-07 16:38:32,746 INFO org.apache.hadoop.hbase.master.AssignmentManager$TimeoutMonitor: 167-6-1-11:60000.timeoutMonitor exiting
2011-04-07 16:39:22,770 INFO org.apache.hadoop.hbase.master.LogCleaner: master-167-6-1-11:60000.oldLogCleaner exiting                  
",,,,,,,,
HBASE-3755,"0.90 has a different behavior regarding ZK connections, it tends to create too many of them and it's not obvious to users what they should do to fix. I think I've helped at least 5 different users this week with this error.

By catching ConnectionLossException and augmenting its message, we could say something like ""it's possible that the ZooKeeper server has too many connections from this IP, see doc at blah"" since the ZK server isn't nice enough to let us know what's going on.",,,,,,,,
HBASE-3756,"Fails with unknownregionexception:

{code}
ERROR: java.lang.reflect.UndeclaredThrowableException: org.apache.hadoop.hbase.UnknownRegionException: -ROOT-,,0,70236052
        at org.apache.hadoop.hbase.master.HMaster.move(HMaster.java:729)
        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:570)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1039)
{code}",,,,,,,,
HBASE-3758,"Currently RegionObserver pre/postScannerOpen() upcalls are injected at HRegion.instantiateInternalScanner(). If someone uses scanner methods at HRegion, ScannerOpen upcalls can be triggered unexpectedly. HBase-3583 pulled scannerNext and scannerClose from HRegion to HRegionServer. We also need to pull scannerOpen to HRegionObserver to prevent unexpected triggers. ",,,,,,,,
HBASE-3762,"Currently HTableFactory.releaseHTableInterface() wraps IOException in RuntimeException.
We should let HTableInterfaceFactory.releaseHTableInterface() throw IOException explicitly.",,,,,,,,
HBASE-3771,"Noticed by Dave Latham, refreshing the zk web page will eventually make that machine run out of connections with ZK. It's because we don't close the connection created inside HBA.",,,,,,,,
HBASE-3776,"Add Bloom Filter support for bulk imports. Lacking a bloom filter, even on a single imported file, can cause perf degradation. Since we now set our compression type based on the HBase CF configuration, it would be good to follow this path for the bloom filter addition.",,,1,,,,,
HBASE-3777,"Judging from the javadoc in HConnectionManager, sharing connections across multiple clients going to the same cluster is supposedly a good thing. However, the fact that there is a one-to-one mapping between a configuration and connection instance, kind of works against that goal. Specifically, when you create HTable instances using a given Configuration instance and a copy thereof, we end up with two distinct HConnection instances under the covers. Is this really expected behavior, especially given that the configuration instance gets cloned a lot?

Here, I'd like to play devil's advocate and propose that we ""deep-compare"" HBaseConfiguration instances, so that multiple HBaseConfiguration instances that have the same properties map to the same HConnection instance. In case one is ""concerned that a single HConnection is insufficient for sharing amongst clients"", to quote the javadoc, then one should be able to mark a given HBaseConfiguration instance as being ""uniquely identifiable"".

Note that ""sharing connections makes clean up of HConnection instances a little awkward"", unless of course, you apply the change described in HBASE-3766.",,,1,,,,,
HBASE-3800,"It reproduces when HMaster is started for the first time and NN is started without starting DN

Hmaster logs:
2011-04-19 16:49:09,208 DEBUG org.apache.hadoop.hbase.master.ActiveMasterManager: A master is now available
2011-04-19 16:49:09,400 WARN org.apache.hadoop.hbase.util.FSUtils: Version file was empty, odd, will try to set it.
2011-04-19 16:51:09,674 WARN org.apache.hadoop.hdfs.DFSClient: DataStreamer Exception: org.apache.hadoop.ipc.RemoteException: java.io.IOException: File /hbase/hbase.version could only be replicated to 0 nodes, instead of 1
...........

2011-04-19 16:51:09,674 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block null bad datanode[0] nodes == null
2011-04-19 16:51:09,674 WARN org.apache.hadoop.hdfs.DFSClient: Could not get block locations. Source file ""/hbase/hbase.version"" - Aborting...
2011-04-19 16:51:09,674 WARN org.apache.hadoop.hbase.util.FSUtils: Unable to create version file at hdfs://C4C1:9000/hbase, retrying: org.apache.hadoop.ipc.RemoteException: java.io.IOException: File /hbase/hbase.version could only be replicated to 0 nodes, instead of 1
...........

2011-04-19 16:56:19,695 WARN org.apache.hadoop.hbase.util.FSUtils: Unable to create version file at hdfs://C4C1:9000/hbase, retrying: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: failed to create file /hbase/hbase.version for DFSClient_hb_m_C4C1.site:60000_1303202948768 on client 157.5.100.1 because current leaseholder is trying to recreate file.
org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: failed to create file /hbase/hbase.version for DFSClient_hb_m_C4C1.site:60000_1303202948768 on client 157.5.100.1 because current leaseholder is trying to recreate file.
       at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1068)
....
",,,,,,,,
HBASE-3813,"Yesterday debugging w/ Jack we noticed that with few handlers on a big box, he was seeing stats like this:

{code}
2011-04-21 11:54:49,451 DEBUG org.apache.hadoop.ipc.HBaseServer: Server connection from X.X.X.X:60931; # active connections: 11; # queued calls: 2500
{code}

We had 2500 items in the rpc queue waiting to be processed.

Turns out he had too few handlers for number of clients (but also, it seems like he figured hw issues in that his RAM bus was running at 1/4 the rate that it should have been running at).

Chatting w/ J-D this morning, he asked if the queues hold 'data'.  The queues hold 'Calls'.  Calls are the client request.  They contain data.

Jack had 2500 items queued.  If each item to insert was 1MB, thats 2.5k * 1MB of memory that is outside of our generally accounting.

Currently the queue size is handlers * MAX_QUEUE_SIZE_PER_HANDLER where MAX_QUEUE_SIZE_PER_HANDLER is hardcoded to be 100.

If the queue is full we block (LinkedBlockingQueue).

Going to change the queue size from 100 to 10 by default -- but also will make it configurable and will doc. this as possible cause of OOME.  Will try it on production here before committing patch.

",,,1,,,,,
HBASE-3820,"I found this problem while the namenode went into safemode due to some unclear reasons. 
There's one patch about this problem:

   try {
      HLogSplitter splitter = HLogSplitter.createLogSplitter(
        conf, rootdir, logDir, oldLogDir, this.fs);
      try {
        splitter.splitLog();
      } catch (OrphanHLogAfterSplitException e) {
        LOG.warn(""Retrying splitting because of:"", e);
        // An HLogSplitter instance can only be used once.  Get new instance.
        splitter = HLogSplitter.createLogSplitter(conf, rootdir, logDir,
          oldLogDir, this.fs);
        splitter.splitLog();
      }
      splitTime = splitter.getTime();
      splitLogSize = splitter.getSize();
    } catch (IOException e) {
      checkFileSystem();
      LOG.error(""Failed splitting "" + logDir.toString(), e);
      master.abort(""Shutting down HBase cluster: Failed splitting hlog files..."", e);
    } finally {
      this.splitLogLock.unlock();
    }

And it was really give some useful help to some extent, while the namenode process exited or been killed, but not considered the Namenode safemode exception.
   I think the root reason is the method of checkFileSystem().
   It gives out an method to check whether the HDFS works normally(Read and write could be success), and that maybe the original propose of this method. This's how this method implements:

    DistributedFileSystem dfs = (DistributedFileSystem) fs;
    try {
      if (dfs.exists(new Path(""/""))) {  
        return;
      }
    } catch (IOException e) {
      exception = RemoteExceptionHandler.checkIOException(e);
    }
   
   I have check the hdfs code, and learned that while the namenode was in safemode ,the dfs.exists(new Path(""/"")) returned true. Because the file system could provide read-only service. So this method just checks the dfs whether could be read. I think it's not reasonable.
    
   ",,1,,,,,,
HBASE-3821,"""NOT flushing memstore for region"" keep on printing for half an hour in the regionserver. Then I restart hbase. I think there may be deadlock or cycling.
I know that when splitting region, it will doclose of region, and set writestate.writesEnabled = false  and may run close preflush. This will make flush fail and print ""NOT flushing memstore for region"". But It should be finished after a while.

logs:
2011-04-18 16:28:27,960 DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitThread: Compaction requested for ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610. because regionserver60020.cacheFlusher; priority=-1, compaction queue size=1
2011-04-18 16:28:30,171 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Flush requested on ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610.
2011-04-18 16:28:30,171 WARN org.apache.hadoop.hbase.regionserver.MemStoreFlusher: Region ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610. has too many store files; delaying flush up to 90000ms
2011-04-18 16:28:32,119 INFO org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter: Using syncFs -- HDFS-200
2011-04-18 16:28:32,285 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: Roll /hbase/.logs/linux253,60020,1303123943360/linux253%3A60020.1303124206693, entries=5226, filesize=255913736. New hlog /hbase/.logs/linux253,60020,1303123943360/linux253%3A60020.1303124311822
2011-04-18 16:28:32,287 DEBUG org.apache.hadoop.hbase.regionserver.wal.HLog: Found 1 hlogs to remove out of total 2; oldest outstanding sequenceid is 11037 from region 031f37c9c23fcab17797b06b90205610
2011-04-18 16:28:32,288 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: moving old hlog file /hbase/.logs/linux253,60020,1303123943360/linux253%3A60020.1303123945481 whose highest sequenceid is 6052 to /hbase/.oldlogs/linux253%3A60020.1303123945481
2011-04-18 16:28:42,701 INFO org.apache.hadoop.hbase.regionserver.Store: Completed major compaction of 4 file(s), new file=hdfs://10.18.52.108:9000/hbase/ufdr/031f37c9c23fcab17797b06b90205610/value/4398465741579485290, size=281.4m; total size for store is 468.8m
2011-04-18 16:28:42,712 INFO org.apache.hadoop.hbase.regionserver.HRegion: completed compaction on region ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610. after 1mins, 40sec
2011-04-18 16:28:42,741 INFO org.apache.hadoop.hbase.regionserver.SplitTransaction: Starting split of region ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610.
2011-04-18 16:28:42,770 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Closing ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610.: disabling compactions & flushes
2011-04-18 16:28:42,770 INFO org.apache.hadoop.hbase.regionserver.HRegion: Running close preflush of ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610.
2011-04-18 16:28:42,771 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memstore flush for ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610., current region memstore size 105.6m
2011-04-18 16:28:42,818 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished snapshotting, commencing flushing stores
2011-04-18 16:28:42,846 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: NOT flushing memstore for region ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610., flushing=false, writesEnabled=false
2011-04-18 16:28:42,849 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Flush requested on ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610.
2011-04-18 16:28:42,849 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: NOT flushing memstore for region ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610., flushing=false, writesEnabled=false ......
2011-04-18 17:04:08,803 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: NOT flushing memstore for region ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610., flushing=false, writesEnabled=false
2011-04-18 17:04:08,803 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Flush requested on ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610.
Mon Apr 18 17:04:24 IST 2011 Starting regionserver on linux253 ulimit -n 1024
",,,,,,,,
HBASE-3834,"If you corrupt one of the storefiles in a region (eg using vim to muck up some bytes), the region will still open, but that storefile will just be ignored with a log message. We should probably not do this in general - better to keep that region unassigned and force an admin to make a decision to remove the bad storefile.",,,,,,,,
HBASE-3843,splitlogworker should be started in startServiceThreads() instead of in initializeZookeeper(). This will ensure that the region server accepts a split-logging tasks only after it has successfully done reportForDuty() to the master.,,,,,,,,
HBASE-3845,"(I don't have a test case to prove this yet but I have run it by Dhruba and Kannan internally and wanted to put this up for some feedback.)

In this discussion let us assume that the region has only one column family. That way I can use region/memstore interchangeably.

After a memstore flush it is possible for lastSeqWritten to have a log-sequence-id for a region that is not the earliest log-sequence-id for that region's memstore.

HLog.append() does a putIfAbsent into lastSequenceWritten. This is to ensure that we only keep track  of the earliest log-sequence-number that is present in the memstore.

Every time the memstore is flushed we remove the region's entry in lastSequenceWritten and wait for the next append to populate this entry again. This is where the problem happens.

step 1:
flusher.prepare() snapshots the memstore under HRegion.updatesLock.writeLock().

step 2 :
as soon as the updatesLock.writeLock() is released new entries will be added into the memstore.

step 3 :
wal.completeCacheFlush() is called. This method removes the region's entry from lastSeqWritten.

step 4:
the next append will create a new entry for the region in lastSeqWritten(). But this will be the log seq id of the current append. All the edits that were added in step 2 are missing.

==

as a temporary measure, instead of removing the region's entry in step 3 I will replace it with the log-seq-id of the region-flush-event.

",,,,,,,,
HBASE-3846,"As I was talking in HBASE-3669, it is really easy with the current RIT timeout to end up in situations where regions are doubly assigned, not assigned at all or assigned but the master doesn't know about it. As a bandaid, we should set hbase.master.assignment.timeoutmonitor.timeout to what the ZK session timeout is.

We had to do that to one of our clusters to be able to start it, else the master kept racing with itself.",,,,,,,,
HBASE-3855,"The scanner use reseek to find the next row (or next column) as part of a scan. The reseek code iterates over a Set to position itself at the right place. If there are many thousands of kvs that need to be skipped over, then the time-cost is very high. In this case, a seek would be far lesser in cost than a reseek.",,,1,,,,,
HBASE-3861,"Currently the number of the client connections is hard-wired to 1000:

        standaloneServerFactory = new NIOServerCnxnFactory();
        standaloneServerFactory.configure(new InetSocketAddress(clientPort),1000);
      } catch (BindException e) {
 
This should be set according to the test environment's hbase configuration. The property in 
question is : hbase.zookeeper.property.maxClientCnxns.

Currently some tests such as org.apache.hadoop.hbase.client.TestHCM fail because the number of connections used by the HBase client exceeds 1000.

Recently MAX_CACHED_HBASE_INSTANCES increased from 31 to 2000 on 0.90 branch:

http://svn.apache.org/viewvc/hbase/branches/0.90/src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java?p2=%2Fhbase%2Fbranches%2F0.90%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhbase%2Fclient%2FHConnectionManager.java&p1=%2Fhbase%2Fbranches%2F0.90%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhbase%2Fclient%2FHConnectionManager.java&r1=1096818&r2=1096817&view=diff&pathrev=1096818

and correspondingly the hbase config on the Zookeeper server-side also increased in hbase-default.xml:

http://svn.apache.org/viewvc/hbase/branches/0.90/src/main/resources/hbase-default.xml?p2=%2Fhbase%2Fbranches%2F0.90%2Fsrc%2Fmain%2Fresources%2Fhbase-default.xml&p1=%2Fhbase%2Fbranches%2F0.90%2Fsrc%2Fmain%2Fresources%2Fhbase-default.xml&r1=1091594&r2=1091593&view=diff&pathrev=1091594

So if MiniZKCluster looks at this setting, the test won't have this failure.",,1,,,,,,
HBASE-3862,"The AggregationClient requests aggregations from multiple region servers in parallel. The calculations in the reducer callbacks of the AggregationClient are not thread safe, and therefore could return an incorrect result due to simultaneous/interleaved execution.",,,,,,,,
HBASE-3867,"When cluster stopped and romove server from cluster which contains meta region, then restart cluster,
From the following code throws ""NoRouteToHostException""

package org.apache.hadoop.hbase.catalog;
public class CatalogTracker 

 private HRegionInterface getMetaServerConnection(boolean refresh)
  throws IOException, InterruptedException {
    synchronized (metaAvailable) {
      if (metaAvailable.get()) {
        HRegionInterface current = getCachedConnection(metaLocation);
        if (!refresh) {
          return current;
        }
        if (verifyRegionLocation(current, this.metaLocation, META_REGION)) {
          return current;
        }
        resetMetaLocation();
      }
      HRegionInterface rootConnection = getRootServerConnection();
      if (rootConnection == null) {
        return null;
      }
      HServerAddress newLocation = MetaReader.readMetaLocation(rootConnection);
      if (newLocation == null) {
        return null;
      }
      ////////the following line throws the exception
HRegionInterface newConnection = getCachedConnection(newLocation);
      if (verifyRegionLocation(newConnection, this.metaLocation, META_REGION)) {
        setMetaLocation(newLocation);
        return newConnection;
      }
      return null;
    }
  }

/////////////the following method don't handle the exception.
public class CatalogTracker 
  public boolean verifyMetaRegionLocation(final long timeout)
  throws InterruptedException, IOException {
    return getMetaServerConnection(true) != null;
  }


//////////////////master call the CatalogTracker's method and don't handle the problem too.
package org.apache.hadoop.hbase.master;
public class HMaster
int assignRootAndMeta()
  throws InterruptedException, IOException, KeeperException {
    int assigned = 0;
    long timeout = this.conf.getLong(""hbase.catalog.verification.timeout"", 1000);

    // Work on ROOT region.  Is it in zk in transition?
    boolean rit = this.assignmentManager.
      processRegionInTransitionAndBlockUntilAssigned(HRegionInfo.ROOT_REGIONINFO);
    if (!catalogTracker.verifyRootRegionLocation(timeout)) {
      this.assignmentManager.assignRoot();
      this.catalogTracker.waitForRoot();
      assigned++;
    }
    LOG.info(""-ROOT- assigned="" + assigned + "", rit="" + rit +
      "", location="" + catalogTracker.getRootLocation());

    // Work on meta region
    rit = this.assignmentManager.
      processRegionInTransitionAndBlockUntilAssigned(HRegionInfo.FIRST_META_REGIONINFO);
///////////////////////////////
when restart cluster master break down here.
////////////////////////////////
    if (!this.catalogTracker.verifyMetaRegionLocation(timeout)) {
      this.assignmentManager.assignMeta();
      this.catalogTracker.waitForMeta();
      // Above check waits for general meta availability but this does not
      // guarantee that the transition has completed
      this.assignmentManager.waitForAssignment(HRegionInfo.FIRST_META_REGIONINFO);
      assigned++;
    }
    LOG.info("".META. assigned="" + assigned + "", rit="" + rit +
      "", location="" + catalogTracker.getMetaLocation());
    return assigned;
  }

Thanks to JunQiang Yuan in www.alipay.com  for providing information about this bug. ",,,,,,,,
HBASE-3872,"Saw this interesting one on a cluster of ours.  The cluster was configured with too few handlers so lots of the phenomeneon where actions were queued but then by the time they got into the server and tried respond to the client, the client had disconnected because of the timeout of 60 seconds.  Well, the meta edits for a split were queued at the regionserver carrying .META. and by the time it went to write back, the client had gone (the first insert of parent offline with daughter regions added as info:splitA and info:splitB).  The client presumed the edits failed and 'successfully' rolled back the transaction (failing to undo .META. edits thinking they didn't go through).

A few minutes later the .META. scanner on master runs.  It sees 'no references' in daughters -- the daughters had been cleaned up as part of the split transaction rollback -- so it thinks its safe to delete the parent.

Two things:

+ Tighten up check in master... need to check daughter region at least exists and possibly the daughter region has an entry in .META.
+ Dependent on the edit that fails, schedule rollback edits though it will seem like they didn't go through.

This is pretty critical one.",,,,,,,,
HBASE-3874,"By chance, we were able to revert the ulimit on one of our clusters to 1024 and it started dying non-stop on ""Too many open files"". Now the bad thing is that some region servers weren't completely ServerShutdownHandler'd because they failed on:

{quote}

2011-05-07 00:04:46,203 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_SERVER_SHUTDOWN
java.lang.NullPointerException
	at org.apache.hadoop.hbase.master.AssignmentManager.processServerShutdown(AssignmentManager.java:1804)
	at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:101)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:156)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{quote}

Reading the code, it seems the NPE is in the if statement:

{code}
Map.Entry<String, RegionPlan> e = i.next();
if (e.getValue().getDestination().equals(hsi)) {
  // Use iterator's remove else we'll get CME
  i.remove();
}
{code}

Which means that the destination (HSI) is null. Looking through the code, it seems we instantiate a RegionPlan with a null HSI when it's a random assignment. 

It means that if there's a random assignment going on while a node dies then this issue might happen.

Initially I thought that this could mean data loss, but the logs are already split so it's just the reassignment that doesn't happen (still bad).

Also it left the master with dead server being processed, so for two days the balancer didn't run failing on:

bq. org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): []

And the reason why the array is empty is because we are running 0.90.3 which removes the RS from the dead list if it comes back.",,,,,,,,
HBASE-3889,"There is an issue with the log splitting under the specific condition of edits belonging to a non existing region (which went away after a split for example). The HLogSplitter fails to check the condition, which is handled on a lower level, logging manifests it as 

{noformat}
2011-05-16 13:56:10,300 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: This region's directory doesn't exist: hdfs://localhost:8020/hbase/usertable/30c4d0a47703214845d0676d0c7b36f0. It is very likely that it was already split so it's safe to discard those edits.
{noformat}

The code returns a null reference which is not check in HLogSplitter.splitLogFileToTemp():

{code}
...
        WriterAndPath wap = (WriterAndPath)o;
        if (wap == null) {
          wap = createWAP(region, entry, rootDir, tmpname, fs, conf);
          if (wap == null) {
            logWriters.put(region, BAD_WRITER);
          } else {
            logWriters.put(region, wap);
          }
        }
        wap.w.append(entry);
...
{code}

The createWAP does return ""null"" when the above message is logged based on the obsolete region reference in the edit.

What made this difficult to detect is that the error (and others) are silently ignored in SplitLogWorker.grabTask(). I added a catch and error logging to see the NPE that was caused by the above.

{code}
...
          break;
      }
    } catch (Exception e) {
      LOG.error(""An error occurred."", e);
    } finally {
      if (t > 0) {
...
{code}

As a side note, there are other errors/asserts triggered that this try/finally not handles. For example

{noformat}
2011-05-16 13:58:30,647 WARN org.apache.hadoop.hbase.regionserver.SplitLogWorker: BADVERSION failed to assert ownership for /hbase/splitlog/hdfs%3A%2F%2Flocalhost%2Fhbase%2F.logs%2F10.0.0.65%2C60020%2C1305406356765%2F10.0.0.65%252C60020%252C1305406356765.1305409968389
org.apache.zookeeper.KeeperException$BadVersionException: KeeperErrorCode = BadVersion for /hbase/splitlog/hdfs%3A%2F%2Flocalhost%2Fhbase%2F.logs%2F10.0.0.65%2C60020%2C1305406356765%2F10.0.0.65%252C60020%252C1305406356765.1305409968389
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:106)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
        at org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1038)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.ownTask(SplitLogWorker.java:329)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.access$100(SplitLogWorker.java:68)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker$2.progress(SplitLogWorker.java:265)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFileToTemp(HLogSplitter.java:432)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFileToTemp(HLogSplitter.java:354)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker$1.exec(SplitLogWorker.java:113)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.grabTask(SplitLogWorker.java:260)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.taskLoop(SplitLogWorker.java:191)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.run(SplitLogWorker.java:164)
        at java.lang.Thread.run(Thread.java:680)
{noformat}

This should probably be handled - or at least documented - in another issue?

The NPE made the log split end and the SplitLogManager add an endless amount of RESCAN entries as this never came to an end.",,,,,,,,
HBASE-3890,"This is in continuation to HBASE-3889:

Note that there must be more slightly off here. Although the splitlogs znode is now empty the master is still stuck here:

{noformat}
Doing distributed log split in hdfs://localhost:8020/hbase/.logs/10.0.0.65,60020,1305406356765	
- Waiting for distributed tasks to finish. scheduled=2 done=1 error=0   4380s

Master startup	
- Splitting logs after master startup   4388s
{noformat}

There seems to be an issue with what is in ZK and what the TaskBatch holds. In my case it could be related to the fact that the task was already in ZK after many faulty restarts because of the NPE. Maybe it was added once (since that is keyed by path, and that is unique on my machine), but the reference count upped twice? Now that the real one is done, the done counter has been increased, but will never match the scheduled.

The code could also check if ZK is actually depleted, and therefore treat the scheduled task as bogus? This of course only treats the symptom, not the root cause of this condition. ",,,,,,,,
HBASE-3892,"In TimeoutMonitor : 
if node exists and node state is RS_ZK_REGION_CLOSED
We should send a zk message again when close region is timeout.
in this case, It may be loss some message.


I See. It seems like a bug. This is my analysis.

// disable table and master sent Close message to region server, Region state was set PENDING_CLOSE

2011-05-08 17:44:25,745 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Sent CLOSE to serverName=C4C4.site,60020,1304820199467, load=(requests=0, regions=123, usedHeap=4097, maxHeap=8175) for region ufdr,2011050812#8613817398167#4032,1304847764729.4418fb197685a21f77e151e401cf8b66.
2011-05-08 17:44:45,530 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: ufdr,2011050812#8613817306227#0516,1304845660567.8e9a3b05abe1c3a692999cf5e8dfd9dd.: Daughters; ufdr,2011050812#8613817306227#0516,1304847764729.5e4bca85c33fa6605ffc9a5c2eb94e62., ufdr,2011050812#8613817398167#4032,1304847764729.4418fb197685a21f77e151e401cf8b66. from C4C4.site,60020,1304820199467
2011-05-08 17:45:45,542 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: ufdr,2011050812#8613817306227#0516,1304845660567.8e9a3b05abe1c3a692999cf5e8dfd9dd.: Daughters; ufdr,2011050812#8613817306227#0516,1304847764729.5e4bca85c33fa6605ffc9a5c2eb94e62., ufdr,2011050812#8613817398167#4032,1304847764729.4418fb197685a21f77e151e401cf8b66. from C4C4.site,60020,1304820199467

// received splitting message and cleared Region state (PENDING_CLOSE)

2011-05-08 17:46:45,303 WARN org.apache.hadoop.hbase.master.AssignmentManager: Overwriting 4418fb197685a21f77e151e401cf8b66 on serverName=C4C4.site,60020,1304820199467, load=(requests=0, regions=123, usedHeap=4097, maxHeap=8175)
2011-05-08 17:46:45,538 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: ufdr,2011050812#8613817306227#0516,1304845660567.8e9a3b05abe1c3a692999cf5e8dfd9dd.: Daughters; ufdr,2011050812#8613817306227#0516,1304847764729.5e4bca85c33fa6605ffc9a5c2eb94e62., ufdr,2011050812#8613817398167#4032,1304847764729.4418fb197685a21f77e151e401cf8b66. from C4C4.site,60020,1304820199467
2011-05-08 17:47:45,548 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: ufdr,2011050812#8613817306227#0516,1304845660567.8e9a3b05abe1c3a692999cf5e8dfd9dd.: Daughters; ufdr,2011050812#8613817306227#0516,1304847764729.5e4bca85c33fa6605ffc9a5c2eb94e62., ufdr,2011050812#8613817398167#4032,1304847764729.4418fb197685a21f77e151e401cf8b66. from C4C4.site,60020,1304820199467
2011-05-08 17:48:45,545 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: ufdr,2011050812#8613817306227#0516,1304845660567.8e9a3b05abe1c3a692999cf5e8dfd9dd.: Daughters; ufdr,2011050812#8613817306227#0516,1304847764729.5e4bca85c33fa6605ffc9a5c2eb94e62., ufdr,2011050812#8613817398167#4032,1304847764729.4418fb197685a21f77e151e401cf8b66. from C4C4.site,60020,1304820199467
2011-05-08 17:49:46,108 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: ufdr,2011050812#8613817306227#0516,1304845660567.8e9a3b05abe1c3a692999cf5e8dfd9dd.: Daughters; ufdr,2011050812#8613817306227#0516,1304847764729.5e4bca85c33fa6605ffc9a5c2eb94e62., ufdr,2011050812#8613817398167#4032,1304847764729.4418fb197685a21f77e151e401cf8b66. from C4C4.site,60020,1304820199467
2011-05-08 17:50:46,105 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: ufdr,2011050812#8613817306227#0516,1304845660567.8e9a3b05abe1c3a692999cf5e8dfd9dd.: Daughters; ufdr,2011050812#8613817306227#0516,1304847764729.5e4bca85c33fa6605ffc9a5c2eb94e62., ufdr,2011050812#8613817398167#4032,1304847764729.4418fb197685a21f77e151e401cf8b66. from C4C4.site,60020,1304820199467
2011-05-08 17:51:46,117 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: ufdr,2011050812#8613817306227#0516,1304845660567.8e9a3b05abe1c3a692999cf5e8dfd9dd.: Daughters; ufdr,2011050812#8613817306227#0516,1304847764729.5e4bca85c33fa6605ffc9a5c2eb94e62., ufdr,2011050812#8613817398167#4032,1304847764729.4418fb197685a21f77e151e401cf8b66. from C4C4.site,60020,1304820199467
2011-05-08 17:52:46,112 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: ufdr,2011050812#8613817306227#0516,1304845660567.8e9a3b05abe1c3a692999cf5e8dfd9dd.: Daughters; ufdr,2011050812#8613817306227#0516,1304847764729.5e4bca85c33fa6605ffc9a5c2eb94e62., ufdr,2011050812#8613817398167#4032,1304847764729.4418fb197685a21f77e151e401cf8b66. from C4C4.site,60020,1304820199467
2011-05-08 17:52:47,309 DEBUG org.apache.hadoop.hbase.zookeeper.ZKUtil: master:60000-0x22fcd582836003d Retrieved 125 byte(s) of data from znode /hbase/unassigned/4418fb197685a21f77e151e401cf8b66 and set watcher; region=ufdr,2011050812#8613817398167#4032,1304847764729.4418fb197685a21f77e151e401cf8b66., server=C4C4.site,60020,1304820199467, state=RS_ZK_REGION_CLOSED
2011-05-08 17:52:47,388 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling new unassigned node: /hbase/unassigned/4418fb197685a21f77e151e401cf8b66 (region=ufdr,2011050812#8613817398167#4032,1304847764729.4418fb197685a21f77e151e401cf8b66., server=C4C4.site,60020,1304820199467, state=RS_ZK_REGION_CLOSED)
2011-05-08 17:52:47,388 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_CLOSED, server=C4C4.site,60020,1304820199467, region=4418fb197685a21f77e151e401cf8b66

// region server had closed region, but the region state had cleared. So it printed warning log.

2011-05-08 17:52:47,388 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received CLOSED for region 4418fb197685a21f77e151e401cf8b66 from server C4C4.site,60020,1304820199467 but region was in  the state null and not in expected PENDING_CLOSE or CLOSING states
2011-05-08 17:52:47,397 WARN org.apache.hadoop.hbase.master.AssignmentManager: Overwriting 4418fb197685a21f77e151e401cf8b66 on serverName=C4C4.site,60020,1304820199467, load=(requests=0, regions=123, usedHeap=4097, maxHeap=8175)

// The region state was set PENDING_CLOSE again.  the table couldn't disable and enable.
2011-05-08 17:52:47,398 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Starting unassignment of region ufdr,2011050812#8613817398167#4032,1304847764729.4418fb197685a21f77e151e401cf8b66. (offlining)


",,,,,,,,
HBASE-3893,"We just had a weird episode where one user was trying to insert a lot of data with overlapping keys into a single region (all of that is a separate problem), and the region server rapidly filled up all it's handlers + queues with those calls. Basically it wasn't deadlocked but almost.

Worse, now that we have a 60 seconds socket timeout the clients were eventually getting the timeout and then retrying another call to that same region server.

We should have a timeout on lockedRows.wait() in HRegion.internalObtainRowLock in order to survive this better.",,1,,,,,,
HBASE-3894,"HRegion maintains a set of row locks.  Whenever any thread attempts to lock or release a row it needs to acquire the monitor on that set.  We've been encountering cases with 30 handler threads all contending for that monitor, blocked progress on the region server.  Clients timeout, and retry making it worse, and the region server stops responding to new clients almost entirely.",,,,,,,,
HBASE-3906,"When HMaster is running,there are a lot of RegionLoad instances(far greater than the regions),it has risk of OOME.]",,,,,,,,
HBASE-3908,"
reported by Lucian Iordache on hbase-user mail list. will attach the patch asap
-------------------------------------------

Hi guys,

I've just found a problem with the class TableSplit. It implements ""equals"",
but it does not implement hashCode also, as it should have.
I've discovered it by trying to use a HashSet of TableSplit's, and I've
noticed that some duplicate splits are added to the set.

The only option I have for now is to extend TableSplit and to use the
subclass.
I use cloudera hbase cdh3u0 version.

Do you know about this problem? Should I open a Jira issue for that, or it
already exists?

Thanks,
Lucian
",,,,,,,,
HBASE-3914,"This could be happen under the following steps with little probability:
(I suppose the cluster nodes names are RS1/RS2/HM, and there's more than 10,000 regions in the cluster)

1.Root region was opened in RS1.
2.Due to some reason(Maybe the hdfs process was got abnormal),RS1 aborted.
3.ServerShutdownHandler process start.
4.HMaster was restarted, during the finishInitialization's handling, ROOT region was unsetted, and assigned to RS2. 
5.Root region was opened successfully in RS2.
6.But after while, ROOT region was unsetted again by RS1's ServerShutdownHandler. Then it was reassigned. Before that, the RS1 was restarted. So there's two possibilities:
 Case a:
   ROOT region was assigned to RS1. 
   It seemed nothing would be affected. But the root region was still online in RS2.  
   
 Case b:
   ROOT region was assigned to RS2.    
   The ROOT Region couldn't be opened until it would be reassigned to other regionserver, because it was showed online in this regionserver.

This could be proved from the logs:

1. ROOT region was opened with two times:
2011-05-17 10:32:59,188 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Opened region -ROOT-,,0.70236052 on 162-2-77-0,20020,1305598359031
2011-05-17 10:33:01,536 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Opened region -ROOT-,,0.70236052 on 162-2-16-6,20020,1305597548212

2.Regionserver 162-2-16-6 was aborted, so it was reassigned to 162-2-77-0, but already online on this server:
10:49:30,920 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Received request to open region: -ROOT-,,0.70236052 10:49:30,920 DEBUG org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Processing open of -ROOT-,,0.70236052 10:49:30,920 WARN org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Attempted open of -ROOT-,,0.70236052 but already online on this server

This could be cause a long break of ROOT region offline, though it happened under a special scenario. And I have checked the code, it seems a tiny bug here.

There's 2 references about assignRoot():

1.
HMaster# assignRootAndMeta:

    if (!catalogTracker.verifyRootRegionLocation(timeout)) {
      this.assignmentManager.assignRoot();
      this.catalogTracker.waitForRoot();
      assigned++;
    }

2.
ServerShutdownHandler# process: 
    
      if (isCarryingRoot()) { // -ROOT-      
        try {        
           this.services.getAssignmentManager().assignRoot();
        } catch (KeeperException e) {
           this.server.abort(""In server shutdown processing, assigning root"", e);
           throw new IOException(""Aborting"", e);
        }
      }    

I think each time call the method of assignRoot(), we should verify Root Region's Location first. Because before the assigning, the ROOT region could have been assigned by another place.



",,,,,,,,
HBASE-3933,"NullPointerException while hmaster starting.
{code}
      java.lang.NullPointerException
        at java.util.TreeMap.getEntry(TreeMap.java:324)
        at java.util.TreeMap.get(TreeMap.java:255)
        at org.apache.hadoop.hbase.master.AssignmentManager.addToServers(AssignmentManager.java:1512)
        at org.apache.hadoop.hbase.master.AssignmentManager.regionOnline(AssignmentManager.java:606)
        at org.apache.hadoop.hbase.master.AssignmentManager.processFailover(AssignmentManager.java:214)
        at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:402)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:283)
{code}",,1,,,,,,
HBASE-3946,"(The cluster has two HMatser, one active and one standby)

1.While the active HMaster shutdown, the standby one would become the active one, and went into the processFailover() method:
    if (regionCount == 0) {
      LOG.info(""Master startup proceeding: cluster startup"");
      this.assignmentManager.cleanoutUnassigned();
      this.assignmentManager.assignAllUserRegions();
    } else {
      
      LOG.info(""Master startup proceeding: master failover"");
      this.assignmentManager.processFailover();
    }
2.After that, the user regions would be rebuild.
  Map<HServerInfo,List<Pair<HRegionInfo,Result>>> deadServers = rebuildUserRegions(); 

3.Here's how the rebuildUserRegions worked. All the regions(contain the splitted regions) would be added to the offlineRegions of offlineServers.

   for (Result result : results) {
      Pair<HRegionInfo,HServerInfo> region =
        MetaReader.metaRowToRegionPairWithInfo(result);
      if (region == null) continue;
      HServerInfo regionLocation = region.getSecond();
      HRegionInfo regionInfo = region.getFirst();
      if (regionLocation == null) {
        // Region not being served, add to region map with no assignment
        // If this needs to be assigned out, it will also be in ZK as RIT
        this.regions.put(regionInfo, null);
      } else if (!serverManager.isServerOnline(
          regionLocation.getServerName())) {
        // Region is located on a server that isn't online
        List<Pair<HRegionInfo,Result>> offlineRegions =
          offlineServers.get(regionLocation);
        if (offlineRegions == null) {
          offlineRegions = new ArrayList<Pair<HRegionInfo,Result>>(1);
          offlineServers.put(regionLocation, offlineRegions);
        }
        offlineRegions.add(new Pair<HRegionInfo,Result>(regionInfo, result));
      } else {
        // Region is being served and on an active server
        regions.put(regionInfo, regionLocation);
        addToServers(regionLocation, regionInfo);
      }
    }

4.It seems that all the offline regions will be added to RIT and online again:
ZKAssign will creat node for each offline never consider the splitted ones. 

AssignmentManager# processDeadServers
  private void processDeadServers(
      Map<HServerInfo, List<Pair<HRegionInfo, Result>>> deadServers)
  throws IOException, KeeperException {
    for (Map.Entry<HServerInfo, List<Pair<HRegionInfo,Result>>> deadServer :
      deadServers.entrySet()) {
      List<Pair<HRegionInfo,Result>> regions = deadServer.getValue();
      for (Pair<HRegionInfo,Result> region : regions) {
        HRegionInfo regionInfo = region.getFirst();
        Result result = region.getSecond();
        // If region was in transition (was in zk) force it offline for reassign
        try {
          ZKAssign.createOrForceNodeOffline(watcher, regionInfo,
              master.getServerName());
        } catch (KeeperException.NoNodeException nne) {
          // This is fine
        }
        // Process with existing RS shutdown code
        ServerShutdownHandler.processDeadRegion(regionInfo, result, this,
            this.catalogTracker);
      }
    }
  }

AssignmentManager# processFailover
    // Process list of dead servers
    processDeadServers(deadServers);
    // Check existing regions in transition
    List<String> nodes = ZKUtil.listChildrenAndWatchForNewChildren(watcher,
        watcher.assignmentZNode);
    if (nodes.isEmpty()) {
      LOG.info(""No regions in transition in ZK to process on failover"");
      return;
    }
    LOG.info(""Failed-over master needs to process "" + nodes.size() +
        "" regions in transition"");
    for (String encodedRegionName: nodes) {
      processRegionInTransition(encodedRegionName, null);
    }

So I think before add the region into RIT, check it at first.
",,,,,,,,
HBASE-3950,"I discovered this while testing out HBASE-3789, I can recreate this bug without my patch.

When running TestFromClient, I get failures in testListTables and testJiraTest867. The assertion error is on a number mismatch, but when you look at the log you see:

{quote}
2011-06-02 16:51:24,602 WARN  [IPC Client (47) connection to hbasedev/10.10.1.177:56606 from an unknown user] ipc.HBaseClient$Connection(489): Unexpected exception receiving call responses
java.lang.IndexOutOfBoundsException
        at java.io.BufferedInputStream.read(BufferedInputStream.java:310)
        at java.io.DataInputStream.readFully(DataInputStream.java:178)
        at org.apache.hadoop.hbase.client.Result.readArray(Result.java:652)
        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:540)
        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readFields(HbaseObjectWritable.java:288)
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.receiveResponse(HBaseClient.java:563)
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.run(HBaseClient.java:486)
2011-06-02 16:51:24,603 WARN  [IPC Reader 2 on port 56606] ipc.HBaseServer$Listener(600): IPC Server listener on 56606: readAndProcess threw exception java.io.IOException: Connection reset by peer. Count of bytes read: 0
java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcher.read0(Native Method)
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:21)
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:233)
        at sun.nio.ch.IOUtil.read(IOUtil.java:206)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:236)
        at org.apache.hadoop.hbase.ipc.HBaseServer.channelRead(HBaseServer.java:1518)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.readAndProcess(HBaseServer.java:1001)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.doRead(HBaseServer.java:596)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:390)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)

{quote}

It's not clear to me how I can debug this, but adding some debug inside Result.readArray shows me that the last ints being read are out of whack:

{quote}
numKeys 3
0 keyLen 687
0 offset 55551
1 keyLen 127
1 offset 56242
2 keyLen 130
2 offset 56373
numKeys 3
0 keyLen 666
0 offset 56511
1 keyLen 120
1 offset 57181
2 keyLen 123
2 offset 57305
numKeys 1768842863
0 keyLen 1919248233
0 offset 57436
{quote}

Here I'm printing the tail of the reading of an array of Results where each has 3 KVs. As you can see, the last one has a pretty big number of keys and then the keyLen is also completely off. Looking at the server side when writing, I see that the real number of that last keyLen should be 448.",,,,,,,,
HBASE-3963,Schedule all log-spliiting at startup all at once,,,,,,,,
HBASE-3969,[Outdated data can not be cleaned in time],,,,,,,,
HBASE-3970,"When HMaster tries to migrate (after HBASE-451 goes live) the old HRI (with HTD) to new HRI (with out HTD) and if the Master or the migration process crashes/fails midway, it will leave the .META. in a corrupt state and may not allow successful cluster startup. 

",,,,,,,,
HBASE-3974,"While debugging an application consistency issue, we noticed that a single, synchronous Put request threw a NoServerForRegionException but eventually succeeded 90 seconds later.  The problem is that failed put requests are not actually removed from the HTable's writeBuffer.  This makes sense for asynchronous puts using setAutoFlush(false) but don't make sense for the default case where we expect synchronous operation.  We should discard all failed puts for the synchronous case and provide an API so asynchronous requests can have their failed puts cleared.",,,,,,,,
HBASE-3976,"Is there a good reason to believe that caching blocks during compactions is beneficial? Currently, if block cache is enabled on a certain family, then every time it's compacted, we load all of its blocks into the (LRU) cache, at the expense of the legitimately hot ones.
As a matter of fact, this concern was raised earlier in HBASE-1597, which rightly points out that, ""we should not bog down the LRU with unneccessary blocks"" during compaction. Even though that issue has been marked as ""fixed"", it looks like it ought to be reopened.
Should we err on the side of caution and not cache blocks during compactions period (as illustrated in the attached patch)? Or, can we be selectively aggressive about what blocks do get cached during compaction (e.g., only cache those blocks from the recent files)?",1,,,,,,,
HBASE-3978,"Region server will keep extending the lease on the rowlock as long as there is some action on the row. This is done inside HRegionServer.getLockFromId, where it renew the lease on the rowlock. However, when coprocessor ""pre-action"" observer is used, getLockFromId is skipped if default action is bypassed. Take HRegionServer.exists as example, RegionCoprocessorHost.preExists could return Boolean object if one of the regionobservers indicates default action should be bypassed; thus getLockFromId didn't called and the lease on the lock doesn't get renewed.

  public boolean exists(byte[] regionName, Get get) throws IOException {
    checkOpen();
    requestCount.incrementAndGet();
    try {
      HRegion region = getRegion(regionName);
      if (region.getCoprocessorHost() != null) {
        Boolean result = region.getCoprocessorHost().preExists(get);
        if (result != null) {
          return result.booleanValue();
        }
      }
      Result r = region.get(get, getLockFromId(get.getLockId()));
      boolean result = r != null && !r.isEmpty();
      if (region.getCoprocessorHost() != null) {
        result = region.getCoprocessorHost().postExists(get, result);
      }
      return result;
    } catch (Throwable t) {
      throw convertThrowableToIOE(cleanup(t));
    }
  }


The application scenario is:
a) client application passes in a rowlock object in an action.
b) there is custom coprocessor/observer used.
c) For a given action, the custom coprocessor/observer might tell coprocessor framework to bypass the default action.

From client application point of view, the behavior is sometimes the rowlock will timeout even though client is accessing the row all the time, depending on whether coprocessor/observer wants to bypass the default action.

This applies to several other actions as well, increment, checkAndPut, checkandDelete.",,1,,,,,,
HBASE-3984,"After some extensive debugging in the thread [A sudden msg of ""java.io.IOException: Server not running, aborting""|http://search-hadoop.com/m/Qb0BMnrTPZ1], we figured that the region servers weren't able to talk to the new .META. location because the old one was still alive but on it's way down after a OOME.

It translates into exceptions like ""Server not running"" coming from trying to edit .META. and digging in the code I see that CT.waitForMetaServerConnectionDefault -> waitForMeta -> getMetaServerConnection(true) calls verifyRegionLocation since we force the refresh. In this method we check if the RS is good by calling getRegionInfo which *does not* check if the region server is trying to close.

What this means is that a cluster can't recover a .META.-serving RS failure until it has fully shutdown since every time a RS tries to open a region (like right after the log splitting) or split it fails editing .META.",,,1,,,,,
HBASE-3985,"From the HMaster logs, I found something weird:

2011-05-24 11:12:11,152 INFO org.apache.hadoop.hbase.master.HMaster: balance hri=hello,122130,1305944329350.7d6c96428e2563c3d8676474d0a9f814., src=158-1-101-202,20020,1306205409671, dest=158-1-101-222,20020,1306205940117
2011-05-24 11:12:31,536 INFO org.apache.hadoop.hbase.master.HMaster: balance hri=hello,122130,1305944329350.7d6c96428e2563c3d8676474d0a9f814., src=158-1-101-202,20020,1306205409671, dest=158-1-101-222,20020,1306205940117

We can see that, the same region was balanced twice.

To describe the problem, I give out one simple example:

1. Suppose regions count is 10 in RegionServer A.
   Max: 5  Min:4
2. So the regions count need to move is: 5.
3. Before the movement of calculate, the list was shuffled.
4. The 5 moving region was picked out from the back.
5. The nextRegionForUnload value is 5.
6. So if the neededRegions is not zero. Maybe there's still one region should be picked out from RegionServer A.
   This time , the picked Index is 5 which has been picked once!!!!! 
                                  
                          |<-----5-------|                               
------------*--*--*--*--*--*--*--*--*--*----
                           |
                   getNextRegionForUnload                             

Here's the analysis from code:           

1. Walk down most loaded, pruning each to the max. Picked region from back of the list(by reverse order)   
Map<HServerInfo,BalanceInfo> serverBalanceInfo =
      new TreeMap<HServerInfo,BalanceInfo>();
    for(Map.Entry<HServerInfo, List<HRegionInfo>> server :
      serversByLoad.descendingMap().entrySet()) {
      HServerInfo serverInfo = server.getKey();
      int regionCount = serverInfo.getLoad().getNumberOfRegions();
      if(regionCount <= max) {
        serverBalanceInfo.put(serverInfo, new BalanceInfo(0, 0));
        break;
      }
      serversOverloaded++;
      List<HRegionInfo> regions = randomize(server.getValue());
      int numToOffload = Math.min(regionCount - max, regions.size());
      int numTaken = 0;
      for (int i = regions.size() - 1; i >= 0; i--) {
        HRegionInfo hri = regions.get(i);
        // Don't rebalance meta regions.
        if (hri.isMetaRegion()) continue;
        regionsToMove.add(new RegionPlan(hri, serverInfo, null));
        numTaken++; 
        if (numTaken >= numToOffload) break;
      }
      /**********************************************************/
      /***set the nextRegionForUnload  value is numToOffload ****/
      /**********************************************************/
      serverBalanceInfo.put(serverInfo,
          new BalanceInfo(numToOffload, (-1)*numTaken));
    }
2. The second pass of picked one region from the Max regionserver by order.
    if (neededRegions != 0) {
      // Walk down most loaded, grabbing one from each until we get enough
      for(Map.Entry<HServerInfo, List<HRegionInfo>> server :
        serversByLoad.descendingMap().entrySet()) {
        BalanceInfo balanceInfo = serverBalanceInfo.get(server.getKey());
        int idx =
          balanceInfo == null ? 0 : balanceInfo.getNextRegionForUnload();
        if (idx >= server.getValue().size()) break;
        HRegionInfo region = server.getValue().get(idx);
        if (region.isMetaRegion()) continue; // Don't move meta regions.
        regionsToMove.add(new RegionPlan(region, server.getKey(), null));
        if(--neededRegions == 0) {
          // No more regions needed, done shedding
          break;
        }
      }
    }
",,,,,,,,
HBASE-3987,"This is a fix for an NullPointerException that happens in passesBloomFilter. The meta block fails to load, and the IOException catch block sets the Bloom filter to null. Then all other threads waiting on the Bloom filter to load get a chance to try to load the meta block, and one of them eventually succeeds and goes on to query the Bloom filter in StoreFile.passesBloomFilter, but bloomFilter has been already set to null. The fix is to cache the bloomFilter variable in a local variable in passesBloomFilter so that it cannot be made null while the thread is waiting for another thread to load Bloom filter bits.",,,,,,,,
HBASE-3988,"There seems be a bug that the secondary master didn't come out when the primary master dead. 
Because the secondary master will be in a loop forever to watch a local variable before setting a zk watcher.
However this local variable is changed by the zk call back function.
So the secondary master will be in the infinite loop forever.",,,,,,,,
HBASE-3989,"I happened to fall across a problem, after some further analysis, I found the problem(The logs was attached at the end of the email)

Consider the following scenario which is similar with my problem :

1. Due to some unclear reason,  the report to master got error. And retrying several times, but also failed.
2. During this time , the standby master becomes the active one. So the endless loop is still running, and it won't success, for the master address has updated, but it didn't know. And won't know again.

   while (!stopped && (masterAddress = getMaster()) == null) {
      sleeper.sleep();
      LOG.warn(""Unable to get master for initialization"");
    }

    MapWritable result = null;
    long lastMsg = 0;
    while (!stopped) {
      try {
        this.requestCount.set(0);
        lastMsg = System.currentTimeMillis();
        ZKUtil.setAddressAndWatch(zooKeeper,
          ZKUtil.joinZNode(zooKeeper.rsZNode, ZKUtil.getNodeName(serverInfo)),
          this.serverInfo.getServerAddress());
        this.serverInfo.setLoad(buildServerLoad());
        LOG.info(""Telling master at "" + masterAddress + "" that we are up"");
        result = this.hbaseMaster.regionServerStartup(this.serverInfo,
            EnvironmentEdgeManager.currentTimeMillis());
        break;
      } catch (RemoteException e) {
        IOException ioe = e.unwrapRemoteException();
        if (ioe instanceof ClockOutOfSyncException) {
          LOG.fatal(""Master rejected startup because clock is out of sync"",
              ioe);
          // Re-throw IOE will cause RS to abort
          throw ioe;
        } else {
          LOG.warn(""remote error telling master we are up"", e);
        }
      } catch (IOException e) {
        LOG.warn(""error telling master we are up"", e);
      } catch (KeeperException e) {
        LOG.warn(""error putting up ephemeral node in zookeeper"", e);
      }
      sleeper.sleep(lastMsg);
    }


Here's the logs:

2011-06-13 11:25:12,236 WARN org.apache.hadoop.hbase.regionserver.HRegionServer: error telling master we are up
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:574)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:207)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:419)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupIOstreams(HBaseClient.java:328)
	at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:883)
	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:750)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:257)
	at $Proxy5.regionServerStartup(Unknown Source)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.reportForDuty(HRegionServer.java:1511)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.tryReportForDuty(HRegionServer.java:1479)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:571)
	at java.lang.Thread.run(Thread.java:662)
2011-06-13 11:25:15,231 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Telling master at 157-5-111-22:20000 that we are up
2011-06-13 11:25:15,232 WARN org.apache.hadoop.hbase.regionserver.HRegionServer: error telling master we are up
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:574)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:207)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:419)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupIOstreams(HBaseClient.java:328)
	at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:883)
	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:750)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:257)
	at $Proxy5.regionServerStartup(Unknown Source)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.reportForDuty(HRegionServer.java:1511)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.tryReportForDuty(HRegionServer.java:1479)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:571)
	at java.lang.Thread.run(Thread.java:662)
2011-06-13 11:25:18,225 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Telling master at 157-5-111-22:20000 that we are up

So I think, while the error orrured, we should re-get the master address. This problem could be solved.
",,,,,,,,
HBASE-3994,SplitTransaction has a window where clients can get RegionOfflineException,,,,,,,,
HBASE-4003,"In the event of a socket timeout, the {{HBaseClient}} iterates over the outstanding calls (on that socket), and notifies them that a {{SocketTimeoutException}} has occurred. Ideally, we should be cleanup up just those calls that have been outstanding for longer than the specified socket timeout.",,,1,,,,,
HBASE-4005,Lars found and fixed some issues with close_region.  See http://search-hadoop.com/m/L8dD1V5yts1/Issues+with+close_region&subj=Issues+with+close_region,,,,,,,,
HBASE-4007,"After the configured number of retries SplitLogManager is not going to resubmit log-split tasks. In this situation even if the splitLogWorker that owns the task dies the task will not get resubmitted.

When a regionserver goes away then all the split-log tasks that it owned should be resubmitted by the SplitLogMaster.",,,,,,,,
HBASE-4008,"stop-hbase.sh stops the server successfully if and only if the server is instantiated properly. 

When u Run 

start-hbase.sh; sleep 10; stop-hbase.sh; ( This works totally fine and has no issues )

Whereas when u run 

start-hbase.sh; stop-hbase.sh; ( This never stops the server and neither the server gets initialized and starts properly )",,,,,,,,
HBASE-4010,HMaster.createTable could be heavily optimized],,,1,,,,,
HBASE-4015,Refactor the TimeoutMonitor to make it less racy],,,,,,,,
HBASE-4016,"We wanted to use an int (32-bit) atomic counter and we initialize it with a certain value when the row is created. Later, we increment the counter using HTable.incrementColumnValue(). This call results in one of two outcomes. 

1. The call succeeds, but the column value now is a long (64-bit) and is corrupt (by additional data that was in the buffer read).
2. Throws IOException/IllegalArgumentException.
Java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: offset (65547) + length (8) exceed the capacity of the array: 65551
        at org.apache.hadoop.hbase.util.Bytes.explainWrongLengthOrOffset(Bytes.java:502)
        at org.apache.hadoop.hbase.util.Bytes.toLong(Bytes.java:480)
        at org.apache.hadoop.hbase.regionserver.HRegion.incrementColumnValue(HRegion.java:3139)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.incrementColumnValue(HRegionServer.java:2468)
        at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:570)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1039)

Based on our incorrect usage of counters (initializing it with a 32 bit value and later using it as a counter), I would expect that we fail consistently with mode 2 rather than silently corrupting data with mode 1. However, the exception is thrown only rarely and I am not sure what determines the case to be executed. I am wondering if this has something to do with flush.

Here is a HRegion unit test that can reproduce this problem. http://paste.lisp.org/display/122822

We modified our code to initialize the counter with a 64 bit value. But, I was also wondering if something has to change in HRegion.incrementColumnValue() to handle inconsistent counter sizes gracefully without corrupting existing data.

Please let me know if you need additional information.
",,1,,,,,,
HBASE-4024,"The trunk version of regionserver/Store.java, method   List<StoreFile> compactSelection(List<StoreFile> candidates) has this code to determine whether major compaction should be done or not: 

    // major compact on user action or age (caveat: we have too many files)
    boolean majorcompaction = (forcemajor || isMajorCompaction(filesToCompact))
      && filesToCompact.size() < this.maxFilesToCompact;


The isMajorCompaction(filesToCompact) method internally determines whether or not major compaction is required (and logs this as ""Major compaction triggered ... "" log message. However, after the call, the compactSelection method subsequently applies the filesToCompact.size() < this.maxFilesToCompact check which can turn off major compaction. 

This would result in a ""Major compaction triggered"" log message without actually triggering a major compaction.

The filesToCompact.size() check should probably be moved inside the isMajorCompaction(filesToCompact) method.",,,,,,,,
HBASE-4025,"2011-06-23 21:39:52,524 WARN org.apache.hadoop.hbase.monitoring.TaskMonitor: Status org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl@2f56f920 appears to have been leaked
2011-06-23 21:40:06,465 WARN org.apache.hadoop.hbase.master.HMaster: Failed getting all descriptors
java.io.FileNotFoundException: No status for hdfs://ciq.com:9000/hbase/.corrupt
	at org.apache.hadoop.hbase.util.FSUtils.getTableInfoModtime(FSUtils.java:888)
	at org.apache.hadoop.hbase.util.FSTableDescriptors.get(FSTableDescriptors.java:122)
	at org.apache.hadoop.hbase.util.FSTableDescriptors.getAll(FSTableDescriptors.java:149)
	at org.apache.hadoop.hbase.master.HMaster.getHTableDescriptors(HMaster.java:1442)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:340)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1138)
2011-06-23 21:40:26,790 WARN org.apache.hadoop.hbase.master.HMaster: Failed getting all descriptors
java.io.FileNotFoundException: No status for hdfs://ciq.com:9000/hbase/.corrupt
	at org.apache.hadoop.hbase.util.FSUtils.getTableInfoModtime(FSUtils.java:888)
	at org.apache.hadoop.hbase.util.FSTableDescriptors.get(FSTableDescriptors.java:122)
	at org.apache.hadoop.hbase.util.FSTableDescriptors.getAll(FSTableDescriptors.java:149)
	at org.apache.hadoop.hbase.master.HMaster.getHTableDescriptors(HMaster.java:1442)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:340)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1138)",,,,,,,,
HBASE-4028,"In my performance cluster(0.90.3), The Hmaster memory from 100 M up to 4G when one region server crashed.
I added some print in function doneWriting and found the values of totalBuffered is negative.

10:29:52,119 WARN org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: gjc:release Used -565832
hbase-root-master-157-5-111-21.log:2011-06-24 10:29:52,119 WARN org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: gjc:release Used -565832release size25168

void doneWriting(RegionEntryBuffer buffer) {
      synchronized (this) {
    	LOG.warn(""gjc1: relase currentlyWriting +biggestBufferKey "" + buffer.encodedRegionName );
        boolean removed = currentlyWriting.remove(buffer.encodedRegionName);
        assert removed;
      }
      long size = buffer.heapSize();

      synchronized (dataAvailable) {
        totalBuffered -= size;
        LOG.warn(""gjc:release Used "" + totalBuffered );
        // We may unblock writers
        dataAvailable.notifyAll();
      }
      LOG.warn(""gjc:release Used "" + totalBuffered + ""release size""+ size);
    }",,,,,,,,
HBASE-4029,"There is a condition check for Debug mode logging in HRegionServer.java . Because of this the region server never closes the META region while stopping hbase and thus never stops, if DEBUG mode is not enable in logging. ",,1,,,,,,
HBASE-4032,"After HBASE-451, HRegionInfo#getTableDesc has been modified to always return {{null}}. 

One immediate effect is broken unit tests.

That aside, it is not in the spirit of deprecation to actually break the method until after the deprecation cycle, it's a bug.",,,,,,,,
HBASE-4033,"The folling steps can easily recreate the problem:
1. There's thousands of regions in the cluster.
2. Stop the cluster.
3. Start the cluster. Killing one regionserver while the regions were opening. Restarted it after 10 seconds.

The shutted regionserver will appear in the AssignmentManager.servers list again.

For example:

Issue 1:

2011-06-23 14:14:30,775 DEBUG org.apache.hadoop.hbase.master.LoadBalancer: Server information: 167-6-1-12,20020,1308803390123=2220, 167-6-1-13,20020,1308803391742=2374, 167-6-1-11,20020,1308803386333=2205, 167-6-1-13,20020,1308803514394=2183

Two regionservers(One of it had aborted) had the same hostname but different startcode:
167-6-1-13,20020,1308803391742=2374
167-6-1-13,20020,1308803514394=2183

Issue 2:

(1).The Rs 167-6-1-11,20020,1308105402003 finished shutdown at ""10:46:37,774"":
10:46:37,774 INFO org.apache.hadoop.hbase.master.handler.ServerShutdownHandler: Finished processing of shutdown of 167-6-1-11,20020,1308105402003

(2).Overwriting happened, it seemed the RS was still exist in the set of AssignmentManager#regions:
10:45:55,081 WARN org.apache.hadoop.hbase.master.AssignmentManager: Overwriting 612342de1fe4733f72299d70addb6d11 on serverName=167-6-1-11,20020,1308105402003, load=(requests=0, regions=0, usedHeap=0, maxHeap=0)

(3).Region was assigned to this dead RS again at ""10:50:20,671"":
10:50:20,671 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region Jeason10,08058613800000030,1308032774777.612342de1fe4733f72299d70addb6d11. to 167-6-1-11,20020,1308105402003",,,,,,,,
HBASE-4034,"HRegionServer always makes sure one META region is hosted for it to stop. This should be changed so that even if no META regions are hosted, the HRegionServer should be stopped once all user regions are closed.",,,,,,,,
HBASE-4051,"I'm debugging a prePut hook which I've implemented as part of the coprocessor work being developed. This hook is loaded via a table COPROCESSOR attribute and I've noticed that the prePut method is being called twice for a single Put. After setting up the region server to run in a debugger, I'm noticing the call to loadTableCoprocessors() being invoked twice during region initialization, specifically: 

1.       HRegion.init => RegionCoprocessorHost.init => RegionCoprocessorHost.loadTableCoprocessors 

2.       ... => RegionCoprocessorHost.preOpen => RegionCoprocessorHost.loadTableCoprocessors 

This results in two RegionEnvironment instances, each containing a new instance of my coprocessor, being added to the RegionCoprocessorHost. When I issue a put, the list of RegionEnvironments is iterated over and each calls the prePut method in my coprocessor. Reason why this is posing a problem for me is that I modify the family map passed in to my prePut method. Since this family map is the same instance used in both prePut calls, the second prePut call operates on the modified family map, which leads to an unexpected result. 

Is the double loading of the same coprocessor class intentional, is this a bug?",,,1,,,,,
HBASE-4052,"Following is the scenario:

Start RS and Active and standby masters
Create table and insert data.
Disable the table.
Stop the active master and switch to the standby master.
Now enable the table.
Do a scan on the enabled table.
NotServingRegionException is Thrown.

But the same works well when we dont switch the master.
",,,,,,,,
HBASE-4053,"Here's the scenario of how did the problem happened:

1. When HMaster start, all regionservers checkin ok, and count of regions out on cluster is 10083, which is the actual region number count.
2. Then OpenedRegionHandler#process received zookeeper's events, and added 9923 regions to the hris list.
   but the 9923 regions already exists, force added.
3. The LoadBalancer get the wrong Region numbers of 20006 (10083 + 9923).

AssignmentManager#addToServers method:
private void addToServers(final HServerInfo hsi, final HRegionInfo hri) {
  List<HRegionInfo> hris = servers.get(hsi);
  if (hris == null) {
    hris = new ArrayList<HRegionInfo>();
    servers.put(hsi, hris);
  }
  hris.add(hri); // Same region was double added here
}

logs:
2011-06-27 16:13:06,845 INFO org.apache.hadoop.hbase.master.ServerManager: Exiting wait on regionserver(s) to checkin; count=3, stopped=false, count of regions out on cluster=10083
2011-06-27 16:13:17,334 INFO org.apache.hadoop.hbase.master.AssignmentManager: Failed-over master needs to process 9923 regions in transition
2011-06-27 16:21:45,135 DEBUG org.apache.hadoop.hbase.master.LoadBalancer: Balance parameter: numRegions=20006, numServers=3, max=6669, min=6668",,,,,,,,
HBASE-4059,"When a region is splitted during the RS shutdown process, RS just written the daughter region infos to META, but not make them online. Then, for master, in its ServerShutdownHandler, the function isDaughterMissing() uses FindDaughterVisitor to check whether daughter region is OK. However, this visitor doesn't check whether the value for HConstants.SERVER_QUALIFIER carries non-null value.

Therefore for the scenario, isDaughterMissing() returns false, skipping the following line:
     assignmentManager.assign(daughter, true);",,,,,,,,
HBASE-4061,"The getTableDirs() is missing extra checks:

{code}
  public static List<Path> getTableDirs(final FileSystem fs, final Path rootdir)
  throws IOException {
    // presumes any directory under hbase.rootdir is a table
    FileStatus [] dirs = fs.listStatus(rootdir, new DirFilter(fs));
    List<Path> tabledirs = new ArrayList<Path>(dirs.length);
    for (FileStatus dir: dirs) {
      Path p = dir.getPath();
      String tableName = p.getName();
      if (tableName.equals(HConstants.HREGION_LOGDIR_NAME) ||
          tableName.equals(Bytes.toString(HConstants.ROOT_TABLE_NAME)) ||
          tableName.equals(Bytes.toString(HConstants.META_TABLE_NAME)) ||
          tableName.equals(HConstants.HREGION_OLDLOGDIR_NAME) ) {
        continue;
      }
      tabledirs.add(p);
    }
    return tabledirs;
  }
{code}

It needs to also skip 
* .tmp
* .corrupt
* splitlog

A broader check should be performed to make sure it is all covered.

The missing .corrupt check causes for example:

{noformat}
2011-07-05 11:34:33,364 WARN org.apache.hadoop.hbase.master.HMaster: Failed getting all descriptors
java.io.FileNotFoundException: No status for hdfs://localhost:8020/hbase/.corrupt
        at org.apache.hadoop.hbase.util.FSUtils.getTableInfoModtime(FSUtils.java:888)
        at org.apache.hadoop.hbase.util.FSTableDescriptors.get(FSTableDescriptors.java:122)
        at org.apache.hadoop.hbase.util.FSTableDescriptors.getAll(FSTableDescriptors.java:149)
        at org.apache.hadoop.hbase.master.HMaster.getHTableDescriptors(HMaster.java:1429)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:312)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1065)
{noformat}

Not sure yet why others do not have this issue, could be me being on trunk and fiddling?",,,,,,,,
HBASE-4065,"If TableOutputFormat in the new API fails to create a table, it simply logs this at ERROR level and then continues on its way. Then, the first write() to the table will throw a NPE since table hasn't been set.

Instead, it should probably rethrow the exception as a RuntimeException in setConf, or do what the old-API TOF does and not create the HTable instance until getRecordWriter, where it can throw an IOE.",,,,,,,,
HBASE-4072,"This issue was found by Lars: http://search-hadoop.com/m/n04sthNcji2/zoo.cfg+vs+hbase-site.xml&subj=Re+zoo+cfg+vs+hbase+site+xml

Lets fix the inconsistency found and fix the places where we use non-zk attribute name for a zk attribute in hbase (There's only a few places that I remember -- maximum client connections is one IIRC)",,,,,,,,
HBASE-4077,"In the HRegion.delete function, If getLock throws a WrongRegionException, no lock id is ever returned, yet in the finally block, it tries to release the row lock using that lock id (which is null). This causes an NPE in the finally clause, and the closeRegionOperation() to never execute, keeping a read lock open forever.

ERROR org.apache.hadoop.hbase.regionserver.HRegionServer: 
java.lang.NullPointerException 
at org.apache.hadoop.hbase.util.Bytes.compareTo(Bytes.java:840) 
at org.apache.hadoop.hbase.util.Bytes$ByteArrayComparator.compare(Bytes.java:108) 
at org.apache.hadoop.hbase.util.Bytes$ByteArrayComparator.compare(Bytes.java:100) 
at java.util.TreeMap.getEntryUsingComparator(TreeMap.java:351) 
at java.util.TreeMap.getEntry(TreeMap.java:322) 
at java.util.TreeMap.remove(TreeMap.java:580) 
at java.util.TreeSet.remove(TreeSet.java:259) 
at org.apache.hadoop.hbase.regionserver.HRegion.releaseRowLock(HRegion.java:2145) 
at org.apache.hadoop.hbase.regionserver.HRegion.delete(HRegion.java:1174) 
at org.apache.hadoop.hbase.regionserver.HRegionServer.delete(HRegionServer.java:1914) 
at sun.reflect.GeneratedMethodAccessor22.invoke(Unknown Source) 
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 
at java.lang.reflect.Method.invoke(Method.java:597) 
at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:570) 
at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1039)

When the region later attempts to close, the write lock can never be acquired, and the region remains in transition forever.",,,,,,,,
HBASE-4081,"HRegion.java,

  byte [] compactStores(final boolean majorCompaction)
  throws IOException {
    if (majorCompaction) {
      this.triggerMajorCompaction();
    }
    return compactStores();
  }

  /**
   * Compact all the stores and return the split key of the first store that needs
   * to be split.
   */
  public byte[] compactStores() throws IOException {
    for(Store s : getStores().values()) {
      CompactionRequest cr = s.requestCompaction();
      if(cr != null) {
        try {
          compact(cr);
        } finally {
          s.finishRequest(cr);
        }
      }
      byte[] splitRow = s.checkSplit();
      if (splitRow != null) {
        return splitRow;
      }
    }
    return null;
  }

1. It seems the second method's intention is to compact all the stores. However, if a store requires split, the process will stop.
2. Only MetaUtils, HRegion.merge, HRegion.processTable use these two methods. No caller uses the return value.
3. HRegion.merge expects major compaction for each store after the call and has code like below to check error condition.

      // Because we compacted the source regions we should have no more than two
      // HStoreFiles per family and there will be no reference store
      if (srcFiles.size() == 2)


So it seems like the fixes are: a) take out s.CheckSplit() call inside compactStores. b) make the return type ""void"" for these two compactStores functions.",,,,,,,,
HBASE-4083,"Consider the following scenario
Start the Master, Backup master and RegionServer.
Create a table which in turn creates a region.
Disable the table.
Enable the table again. 
Kill the Active master exactly at the point before the actual region assignment is started.
Restart or switch master.
Scan the table.
NotServingRegionExcepiton is thrown.
",,,,,,,,
HBASE-4084,"Currently, MemStoreFlusher.flushRegion() is the driver of auto-splitting. It only decides to auto-split a region if there are too many store files per region. Since it's not guaranteed that the number of store files per region always grows above the ""too many"" count before compaction reduces the count, there is no guarantee that auto-split will ever happen. In my test setup, compaction seems to always win the race and I haven't noticed auto-splitting happen once.

It appears that the intention is to have split be mutually exclusive with compaction, and to have flushing be mutually exclusive with regions badly in need of compaction, but that resulted in auto-splitting being nested in a too-restrictive spot.

I'm not sure what the right fix is. Having one method that is essentially requestSplitOrCompact would probably help readability, and could be the ultimate solution if it replaces other calls of requestCompaction().",,,,,,,,
HBASE-4087,"Through HBASE-3777, HConnectionManager reuses the connection to HBase servers.
One challenge, discovered in troubleshooting HBASE-4052, is how we invalidate connection(s) to server which gets restarted.
There're at least two ways.
1. HConnectionManager utilizes background thread(s) to periodically perform validation of connections in HBASE_INSTANCES and remove stale connection(s).
2. Allow HBaseClient (including HBaseAdmin) to provide feedback to HConnectionManager.

The solution can be a combination of both of the above.",,,,,,,,
HBASE-4093,"When verifyAndAssignRoot throw exception, The deadServers state can not be changed.
The Hmaster log has a lot of 'Not running balancer because processing dead regionserver(s): []' information.


HMaster log:
2011-07-09 01:38:31,820 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs://162.2.16.6:9000/hbase/Htable_UFDR_035/fe7e51c0a74fac096cea8cdb3c9497a6/recovered.edits/0000000000204525422 (wrote 8 edits in 61583ms)
2011-07-09 01:38:31,836 ERROR org.apache.hadoop.hbase.master.MasterFileSystem: Failed splitting hdfs://162.2.16.6:9000/hbase/.logs/162-2-6-187,20020,1310107719056
java.io.IOException: hdfs://162.2.16.6:9000/hbase/.logs/162-2-6-187,20020,1310107719056/162-2-6-187%3A20020.1310143885352, entryStart=1878997244, pos=1879048192, end=2003890606, edit=80274
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.addFileInfoToException(SequenceFileLogReader.java:244)
	at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:200)
	at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:172)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.parseHLog(HLogSplitter.java:429)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLog(HLogSplitter.java:262)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLog(HLogSplitter.java:188)
	at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:201)
	at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:114)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:156)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Could not obtain block: blk_1310107715558_225636 file=/hbase/.logs/162-2-6-187,20020,1310107719056/162-2-6-187%3A20020.1310143885352
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.chooseDataNode(DFSClient.java:2491)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:2256)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:2441)
	at java.io.DataInputStream.read(DataInputStream.java:132)
	at java.io.DataInputStream.readFully(DataInputStream.java:178)
	at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:63)
	at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:101)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1984)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1884)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1930)
	at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:198)
	... 10 more
2011-07-09 01:38:33,052 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [162-2-6-187,20020,1310107719056]
2011-07-09 01:39:29,946 WARN org.apache.hadoop.hbase.master.CatalogJanitor: Failed scan of catalog table
java.net.SocketTimeoutException: Call to /162.2.6.187:20020 failed on socket timeout exception: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/162.2.6.187:38721 remote=/162.2.6.187:20020]
	at org.apache.hadoop.hbase.ipc.HBaseClient.wrapException(HBaseClient.java:802)
	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:775)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:257)
	at $Proxy6.getRegionInfo(Unknown Source)
	at org.apache.hadoop.hbase.catalog.CatalogTracker.verifyRegionLocation(CatalogTracker.java:424)
	at org.apache.hadoop.hbase.catalog.CatalogTracker.getMetaServerConnection(CatalogTracker.java:272)
	at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMeta(CatalogTracker.java:331)
	at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMetaServerConnectionDefault(CatalogTracker.java:364)
	at org.apache.hadoop.hbase.catalog.MetaReader.fullScan(MetaReader.java:255)
	at org.apache.hadoop.hbase.catalog.MetaReader.fullScan(MetaReader.java:237)
	at org.apache.hadoop.hbase.master.CatalogJanitor.scan(CatalogJanitor.java:116)
	at org.apache.hadoop.hbase.master.CatalogJanitor.chore(CatalogJanitor.java:85)
	at org.apache.hadoop.hbase.Chore.run(Chore.java:66)
Caused by: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/162.2.6.187:38721 remote=/162.2.6.187:20020]
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:165)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)
	at java.io.FilterInputStream.read(FilterInputStream.java:116)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection$PingInputStream.read(HBaseClient.java:299)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:237)
	at java.io.DataInputStream.readInt(DataInputStream.java:370)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.receiveResponse(HBaseClient.java:539)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.run(HBaseClient.java:477)
2011-07-09 01:39:29,946 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_META_SERVER_SHUTDOWN
java.net.SocketTimeoutException: Call to /162.2.6.187:20020 failed on socket timeout exception: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/162.2.6.187:38721 remote=/162.2.6.187:20020]
	at org.apache.hadoop.hbase.ipc.HBaseClient.wrapException(HBaseClient.java:802)
	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:775)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:257)
	at $Proxy6.getRegionInfo(Unknown Source)
	at org.apache.hadoop.hbase.catalog.CatalogTracker.verifyRegionLocation(CatalogTracker.java:424)
	at org.apache.hadoop.hbase.catalog.CatalogTracker.verifyRootRegionLocation(CatalogTracker.java:471)
	at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.verifyAndAssignRoot(ServerShutdownHandler.java:90)
	at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:126)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:156)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/162.2.6.187:38721 remote=/162.2.6.187:20020]
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:165)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)
	at java.io.FilterInputStream.read(FilterInputStream.java:116)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection$PingInputStream.read(HBaseClient.java:299)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:237)
	at java.io.DataInputStream.readInt(DataInputStream.java:370)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.receiveResponse(HBaseClient.java:539)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.run(HBaseClient.java:477)
2011-07-09 01:40:26,474 DEBUG org.apache.hadoop.hbase.master.ServerManager: Server 162-2-6-187,20020,1310146825674 came back up, removed it from the dead servers list
2011-07-09 01:40:26,515 INFO org.apache.hadoop.hbase.master.ServerManager: Registering server=162-2-6-187,20020,1310146825674, regionCount=0, userLoad=false
2011-07-09 01:40:28,410 INFO org.apache.hadoop.hbase.catalog.CatalogTracker: Failed verification of .META.,,1 at address=162-2-6-187:20020; org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region is not online: .META.,,1
...
2011-07-09 01:53:33,052 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): []
2011-07-09 01:58:33,060 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): []
2011-07-09 02:03:33,061 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): []
2011-07-09 02:08:33,061 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): []",,,,,,,,
HBASE-4095,"Some large Hlog files(Larger than 10G) appeared in our environment, and I got the reason why they got so huge:

1. The replicas is less than the expect number. So the method of checkLowReplication will be called each sync.

2. The method checkLowReplication request log-roll first, and set logRollRequested as true: 

{noformat}
private void checkLowReplication() {
// if the number of replicas in HDFS has fallen below the initial
// value, then roll logs.
try {
  int numCurrentReplicas = getLogReplication();
  if (numCurrentReplicas != 0 &&
	  numCurrentReplicas < this.initialReplication) {
	LOG.warn(""HDFS pipeline error detected. "" +
		""Found "" + numCurrentReplicas + "" replicas but expecting "" +
		this.initialReplication + "" replicas. "" +
		"" Requesting close of hlog."");
	requestLogRoll();
	logRollRequested = true;
  }
} catch (Exception e) {
  LOG.warn(""Unable to invoke DFSOutputStream.getNumCurrentReplicas"" + e +
	  "" still proceeding ahead..."");
}
}
{noformat}
3.requestLogRoll() just commit the roll request. It may not execute in time, for it must got the un-fair lock of cacheFlushLock.
But the lock may be carried by the cacheflush threads.

4.logRollRequested was true until the log-roll executed. So during the time, each request of log-roll in sync() was skipped.

Here's the logs while the problem happened(Please notice the file size of hlog ""193-195-5-111%3A20020.1309937386639"" in the last row):

2011-07-06 15:28:59,284 WARN org.apache.hadoop.hbase.regionserver.wal.HLog: HDFS pipeline error detected. Found 2 replicas but expecting 3 replicas.  Requesting close of hlog.
2011-07-06 15:29:46,714 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: Roll /hbase/.logs/193-195-5-111,20020,1309922880081/193-195-5-111%3A20020.1309937339119, entries=32434, filesize=239589754. New hlog /hbase/.logs/193-195-5-111,20020,1309922880081/193-195-5-111%3A20020.1309937386639
2011-07-06 15:29:56,929 WARN org.apache.hadoop.hbase.regionserver.wal.HLog: HDFS pipeline error detected. Found 2 replicas but expecting 3 replicas.  Requesting close of hlog.
2011-07-06 15:29:56,933 INFO org.apache.hadoop.hbase.regionserver.Store: Renaming flushed file at hdfs://193.195.5.112:9000/hbase/Htable_UFDR_034/a3780cf0c909d8cf8f8ed618b290cc95/.tmp/4656903854447026847 to hdfs://193.195.5.112:9000/hbase/Htable_UFDR_034/a3780cf0c909d8cf8f8ed618b290cc95/value/8603005630220380983
2011-07-06 15:29:57,391 INFO org.apache.hadoop.hbase.regionserver.Store: Added hdfs://193.195.5.112:9000/hbase/Htable_UFDR_034/a3780cf0c909d8cf8f8ed618b290cc95/value/8603005630220380983, entries=445880, sequenceid=248900, memsize=207.5m, filesize=130.1m
2011-07-06 15:29:57,478 INFO org.apache.hadoop.hbase.regionserver.HRegion: Finished memstore flush of ~207.5m for region Htable_UFDR_034,07664,1309936974158.a3780cf0c909d8cf8f8ed618b290cc95. in 10839ms, sequenceid=248900, compaction requested=false
2011-07-06 15:28:59,236 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: Roll /hbase/.logs/193-195-5-111,20020,1309922880081/193-195-5-111%3A20020.1309926531955, entries=216459, filesize=2370387468. New hlog /hbase/.logs/193-195-5-111,20020,1309922880081/193-195-5-111%3A20020.1309937339119
2011-07-06 15:29:46,714 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: Roll /hbase/.logs/193-195-5-111,20020,1309922880081/193-195-5-111%3A20020.1309937339119, entries=32434, filesize=239589754. New hlog /hbase/.logs/193-195-5-111,20020,1309922880081/193-195-5-111%3A20020.1309937386639
2011-07-06 16:29:58,775 DEBUG org.apache.hadoop.hbase.regionserver.LogRoller: Hlog roll period 3600000ms elapsed
2011-07-06 16:29:58,775 DEBUG org.apache.hadoop.hbase.regionserver.LogRoller: Hlog roll period 3600000ms elapsed
2011-07-06 16:30:01,978 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: Roll /hbase/.logs/193-195-5-111,20020,1309922880081/193-195-5-111%3A20020.1309937386639, entries=1135576, filesize=19220372830. New hlog /hbase/.logs/193-195-5-111,20020,1309922880081/193-195-5-111%3A20020.1309940998890

",,,1,,,,,
HBASE-4099,"The current implementation of HBase client authentication only works with the Java API. Alternate access gateways, like Thrift and REST are left out and will not work.
For the ThriftServer to be able to fully interoperate with the security implementation:
the ThriftServer should be able to login from a keytab file with it's own server principal on startup
thrift clients should be able to authenticate securely when connecting to the server
the ThriftServer should be able to act as a proxy for those clients so that the RPCs it issues will be correctly authorized as the original client identities
There is already some support for step 3 in UserGroupInformation and related classes.
For step #2, we really need to look at what thrift itself supports.
At a bare minimum, we need to implement step #1. If we do this, even without steps 2 & 3, this would at least allow deployments to use a ThriftServer per application user, and have the server login as that user on startup. Thrift clients may not be directly authenticated, but authorization checks for HBase could still be handled correctly this way.",1,,,,,,,
HBASE-4101,"We periodically see a situation where the regionserver process exists in the process list, zookeeper thread sends the keepalive so the master won't remove it from the active list, yet the regionserver will not serve data.

Hadoop(cdh3u0), HBase 0.90.3 (Apache version), under load from an internal testing tool.


Attached is the full JStack",,,,,,,,
HBASE-4107,"An issue was observed where upon shutdown of a regionserver the regionserver log was corrupt.  It appears from the following stacktrace that an Java heap memory exception occurred while writing the checksum to the WAL.  Corrupting the WAL can potentially cause data loss. 

2011-07-14 14:54:53,741 FATAL org.apache.hadoop.hbase.regionserver.wal.HLog: Could not append. Requesting close of hlog
java.io.IOException: Reflection
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.sync(SequenceFileLogWriter.java:147)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.sync(HLog.java:987)
        at org.apache.hadoop.hbase.regionserver.wal.HLog$LogSyncer.run(HLog.java:964)
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.GeneratedMethodAccessor1336.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.sync(SequenceFileLogWriter.java:145)
        ... 2 more
Caused by: java.lang.OutOfMemoryError: Java heap space
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$Packet.<init>(DFSClient.java:2375)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.writeChunk(DFSClient.java:3271)
        at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunk(FSOutputSummer.java:150)
        at org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:132)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3354)
        at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
        at org.apache.hadoop.io.SequenceFile$Writer.syncFs(SequenceFile.java:944)
        ... 6 more
",,,,,,,,
HBASE-4109,"If you are using an interface anything other than 'default' (literally that keyword) DNS.java 's getDefaultHost will return a string which will 
have a trailing period at the end. It seems javadoc of reverseDns in DNS.java (see below) is conflicting with what that function is actually doing. 
It is returning a PTR record while claims it returns a hostname. The PTR record always has period at the end , RFC:  http://irbs.net/bog-4.9.5/bog47.html 

We make call to DNS.getDefaultHost at more than one places and treat that as actual hostname.

Quoting HRegionServer for example
{code}
String machineName = DNS.getDefaultHost(conf.get(
        ""hbase.regionserver.dns.interface"", ""default""), conf.get(
        ""hbase.regionserver.dns.nameserver"", ""default""));
{code}

This causes inconsistencies. An example of such inconsistency was observed while debugging the issue ""Regions not getting reassigned if RS is brought down"". More here 
http://search-hadoop.com/m/CANUA1qRCkQ1 

We may want to sanitize the string returned from DNS class. Or better we can take a path of overhauling the way we do DNS name matching all over.
",,1,,,,,,
HBASE-4112," It happened in latest branch 0.90. but I can't reproduce it.
>
> It seems using api getHRegionInfoOrNull is better or check the input parameter before call getHRegionInfo.
>
> Code:
>  public static Writable getWritable(final byte [] bytes, final 
> Writable w)
>  throws IOException {
>    return getWritable(bytes, 0, bytes.length, w);
>  }
> return getWritable(bytes, 0, bytes.length, w);  // It seems input 
> parameter bytes is null
>
> logs:
> 11/07/15 10:15:42 INFO zookeeper.ClientCnxn: Socket connection 
> established to C4C3.site/157.5.100.3:2181, initiating session
> 11/07/15 10:15:42 INFO zookeeper.ClientCnxn: Session establishment 
> complete on server C4C3.site/157.5.100.3:2181, sessionid = 0x2312b8e3f700002, negotiated timeout = 180000 [INFO] Create : ufdr111 222!
> [INFO] Create : ufdr111 start!
> java.lang.NullPointerException
>        at 
> org.apache.hadoop.hbase.util.Writables.getWritable(Writables.java:75)
>        at 
> org.apache.hadoop.hbase.util.Writables.getHRegionInfo(Writables.java:1
> 19)
>        at 
> org.apache.hadoop.hbase.client.HBaseAdmin$1.processRow(HBaseAdmin.java
> :306)
>        at 
> org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:1
> 90)
>        at 
> org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:9
> 5)
>        at 
> org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:7
> 3)
>        at 
> org.apache.hadoop.hbase.client.HBaseAdmin.createTable(HBaseAdmin.java:
> 325)
>        at createTable.main(createTable.java:96)
>
",,1,,,,,,
HBASE-4116,"From user@hbase, Allan Yan writes:

There might be a bug for REST web service to get rows with given startRow and endRow.

For example, to get a list of rows with startRow=testrow1, endRow=testrow2, I send GET request:

curl http://localhost:8123/TestRowResource/testrow1,testrow2/a:1

And got StringIndexOutOfBoundsException.

This was because in the RowSpec.java, parseRowKeys method, startRow value was changed:

{code}

       startRow = sb.toString();
       int idx = startRow.indexOf(',');
       if (idx != -1) {
         startRow = URLDecoder.decode(startRow.substring(0, idx),
           HConstants.UTF8_ENCODING);
         endRow = URLDecoder.decode(startRow.substring(idx + 1),
           HConstants.UTF8_ENCODING);
       } else {
         startRow = URLDecoder.decode(startRow, HConstants.UTF8_ENCODING);
       }
{code}

 After change to this, it works:

{code}
       String row = sb.toString();
       int idx = row.indexOf(',');
       if (idx != -1) {
         startRow = URLDecoder.decode(row.substring(0, idx),
           HConstants.UTF8_ENCODING);
         endRow = URLDecoder.decode(row.substring(idx + 1),
           HConstants.UTF8_ENCODING);
       } else {
         startRow = URLDecoder.decode(row, HConstants.UTF8_ENCODING);
       }
{code}

I've also created a unit test method in TestRowResource.java,

{code}

   @Test
   public void testStartEndRowGetPutXML() throws IOException, JAXBException {
     String[] rows = {ROW_1,ROW_2,ROW_3};
     String[] values = {VALUE_1,VALUE_2,VALUE_3}; 
     Response response = null;
     for(int i=0; i<rows.length; i++){
         response = putValueXML(TABLE, rows[i], COLUMN_1, values[i]);
         assertEquals(200, response.getCode());
         checkValueXML(TABLE, rows[i], COLUMN_1, values[i]);
     }

     response = getValueXML(TABLE, rows[0], rows[2], COLUMN_1);
     assertEquals(200, response.getCode());
     CellSetModel cellSet = (CellSetModel)
       unmarshaller.unmarshal(new ByteArrayInputStream(response.getBody()));
     assertEquals(2, cellSet.getRows().size());
     for(int i=0; i<cellSet.getRows().size()-1; i++){
         RowModel rowModel = cellSet.getRows().get(i);
         for(CellModel cell : rowModel.getCells()){
             assertEquals(COLUMN_1, Bytes.toString(cell.getColumn()));
             assertEquals(values[i], Bytes.toString(cell.getValue()));
         }   
     }
    
     for(String row : rows){
         response = deleteRow(TABLE, row);
         assertEquals(200, response.getCode());
     }
   }

   private static Response getValueXML(String table, String startRow, String
 endRow, String column)
           throws IOException {
         StringBuilder path = new StringBuilder();
         path.append('/');
         path.append(table);
         path.append('/');
         path.append(startRow);
         path.append("","");
         path.append(endRow);
         path.append('/');
         path.append(column);
         return getValueXML(path.toString());
   }
{code}
",,,,,,,,
HBASE-4124,"ZK restarted while assigning a region, new active HM re-assign it but the RS warned 'already online on this server'.

Issue:
The RS failed besause of 'already online on this server' and return; The HM can not receive the message and report 'Regions in transition timed out'.
",,,,,,,,
HBASE-4138,"Change the zookeeper.znode.parent property (default is /hbase).
Now do not specify this change in the client code.

Use the HTable Object.
The HTable is not able to find the root region and keeps continuously looping.

Find the stack trace:
====================
Object.wait(long) line: not available [native method]		 
RootRegionTracker(ZooKeeperNodeTracker).blockUntilAvailable(long) line: 122

RootRegionTracker.waitRootRegionLocation(long) line: 73		 
HConnectionManager$HConnectionImplementation.locateRegion(byte[],
byte[], boolean) line: 578
HConnectionManager$HConnectionImplementation.locateRegion(byte[],
byte[]) line: 558
HConnectionManager$HConnectionImplementation.locateRegionInMeta(byte[],
byte[], byte[], boolean, Object) line: 687
HConnectionManager$HConnectionImplementation.locateRegion(byte[],
byte[], boolean) line: 589
HConnectionManager$HConnectionImplementation.locateRegion(byte[],
byte[]) line: 558
HConnectionManager$HConnectionImplementation.locateRegionInMeta(byte[],
byte[], byte[], boolean, Object) line: 687
HConnectionManager$HConnectionImplementation.locateRegion(byte[],
byte[], boolean) line: 593
HConnectionManager$HConnectionImplementation.locateRegion(byte[],
byte[]) line: 558
HTable.<init>(Configuration, byte[]) line: 171		 
HTable.<init>(Configuration, String) line: 145		 
HBaseTest.test() line: 45",,1,,,,,,
HBASE-4144," If any exception occurs while initialization of RS the RS doesnot get
 aborted whereas only the RPC server gets stopped.
  private void preRegistrationInitialization()
  throws IOException, InterruptedException {
    try {
      initializeZooKeeper();
      initializeThreads();
      int nbBlocks = conf.getInt(""hbase.regionserver.nbreservationblocks"",4);
      for (int i = 0; i < nbBlocks; i++) {
        reservedSpace.add(new
 byte[HConstants.DEFAULT_SIZE_RESERVATION_BLOCK]);
      }
    } catch (Throwable t) {
      // Call stop if error or process will stick around for ever since
 server
      // puts up non-daemon threads.
      LOG.error(""Stopping HRS because failed initialize"", t);
      this.rpcServer.stop();
    }
  }

 So if any exception occurs while initilization the RPC server gets stopped
 but RS process is still running. But the log says stopping HRegionServer.

 So in the below code the catch() block will be executed when the RPCServer
 stop fails?

 In all other cases it doesnt handle any initialization failure.

    try {
      // Do pre-registration initializations; zookeeper, lease threads,tc.

      preRegistrationInitialization();
    } catch (Exception e) {
      abort(""Fatal exception during initialization"", e);
    }",,,,,,,,
HBASE-4148,"When HFiles are flushed through the normal path, they include an attribute TIMERANGE_KEY which can be used to cull HFiles when performing a time-restricted scan. Files produced by HFileOutputFormat are currently missing this metadata.",,1,,,,,,
HBASE-4150,"See 'Problem with hbase.client.ipc.pool.type=threadlocal in trunk' discussion started by Lars George.

From Lars Hofhansl:
Looking at HBaseClient.getConnection(...) I see this:
{code}
     synchronized (connections) {
       connection = connections.get(remoteId);
       if (connection == null) {
         connection = new Connection(remoteId);
         connections.put(remoteId, connection);
       }
     }
{code}

At the same time PoolMap.ThreadLocalPool.put is defined like this:
{code}
   public R put(R resource) {
     R previousResource = get();
     if (previousResource == null) {
...
       if (poolSize.intValue() >= maxSize) {
         return null;
       }
...
   }
{code}
So... If the ThreadLocalPool reaches its capacity it always returns null and hence all new threads will create a
new connection every time getConnection is called!

I have also verified with a test program that works fine as long as the number of client threads (which include
the threads in HTable's threadpool of course) is < poolsize. Once that is no longer the case the number of
connections ""explodes"" and the program dies with OOMEs (mostly because each Connection is associated with
yet another thread).

It's not clear what should happen, though. Maybe (1) the ThreadLocalPool should not have a limit, or maybe
(2) allocations past the pool size should throw an exception (i.e. there's a hard limit), or maybe (3) in that case
a single connection is returned for all threads while the pool it over its limit or (4) we start round robin with the other
connection in the other thread locals.

For #1 means that the number of client threads needs to be more carefully managed by the client app.
In this case it would also be somewhat pointless that Connection have their own threads, we just pass stuff
between threads.
#2 would work, but puts more logic in the client.
#3 would lead to hard to debug performance issues.
And #4 is messy :)

From Ted Yu:
For HBaseClient, at least the javadoc doesn't match:
{code}
   * @param config configuration
   * @return either a {@link PoolType#Reusable} or {@link PoolType#ThreadLocal}
   */
  private static PoolType getPoolType(Configuration config) {
    return PoolType.valueOf(config.get(HConstants.HBASE_CLIENT_IPC_POOL_TYPE),
        PoolType.RoundRobin, PoolType.ThreadLocal);
{code}
I think for RoundRobinPool, we shouldn't allow maxSize to be Integer#MAX_VALUE. Otherwise connection explosion described by Lars may incur.
",,1,1,,,,,
HBASE-4153,"Comment from Stack over in HBASE-3741:

{quote}
Question: Looking at this patch again, if we throw a RegionAlreadyInTransitionException, won't we just assign the region elsewhere though RegionAlreadyInTransitionException in at least one case here is saying that the region is already open on this regionserver?
{quote}

Indeed looking at the code it's going to be handled the same way other exceptions are. Need to add special cases for assign and unassign.",,,,,,,,
HBASE-4156,"ZKConfig#makeZKProps() should use ""clientPort"" as the client port key in its output when defaulting instead of ""hbase.zookeeper.property.clientPort"".  This method strips the ""hbase.zookeeper.property."" prefix from all of the properties it returns, so the client port key should not have it.  The result is that the default is not properly picked up.",,1,,,,,,
HBASE-4161,"While opening a region, the HBase regionserver tries to list all the children in a ""recovered.edits"" directory. This directory may not exist and depending on the version of HDFS listStatus() might return null or an exception. If it does throw an exception the entire process of opening the region is aborted, just because the recovered.edits directory is not present.",,,,,,,,
HBASE-4167,"(Initially discussed in HBASE-4150)

In HTablePool, when obtaining a table:
{code}
private HTableInterface findOrCreateTable(String tableName) {
    HTableInterface table = tables.get(tableName);
    if (table == null) {
      table = createHTable(tableName);
    }
    return table;
  }
{code}

In the case of {{ThreadLocalPool}}, it seems like there's an exposure here between when the table is created initially and when {{ThreadLocalPool.put()}} is called to set the thread local variable (on {{PooledHTable.close()}}).


Potential solution described by Karthick Sankarachary:

For one thing, we might want to clear the tables variable when the {{HTablePool}} is closed (as shown below). For another, we should override ThreadLocalPool#get method so that it removes the resource, otherwise it might end up referencing a HTableInterface that's has been released.
{code}
1 diff --git a/src/main/java/org/apache/hadoop/hbase/client/HTablePool.java b/src/main/java/org/apache/hadoop/hbase/client/HTablePool.java
      2 index 952a3aa..c198f15 100755
      3 --- a/src/main/java/org/apache/hadoop/hbase/client/HTablePool.java
      4 +++ b/src/main/java/org/apache/hadoop/hbase/client/HTablePool.java
     13 @@ -309,6 +310,7 @@ public class HTablePool implements Closeable {
     14      for (String tableName : tables.keySet()) {
     15        closeTablePool(tableName);
     16      }
     17 +    this.tables.clear();
     18    }
{code}
",,,,,,,,
HBASE-4168,"Experiment-1

Started a dev cluster - META is on the same regionserver as my key-value. I kill the regionserver process but donot power down the machine.
The META is able to migrate to a new regionserver and the regions are also able to reopen elsewhere.
The client is able to talk to the META and find the new kv location and get it.

Experiment-2

Started a dev cluster - META is on a different regionserver as my key-value. I kill the regionserver process but donot power down the machine.
The META remains where it is and the regions are also able to reopen elsewhere.
The client is able to talk to the META and find the new kv location and get it.

Experiment-3

Started a dev cluster - META is on a different regionserver as my key-value. I power down the machine hosting this regionserver.
The META remains where it is and the regions are also able to reopen elsewhere.
The client is able to talk to the META and find the new kv location and get it.

Experiment-4 (This is the problematic one)

Started a dev cluster - META is on the same regionserver as my key-value. I power down the machine hosting this regionserver.
The META is able to migrate to a new regionserver - however - it takes a really long time (~30 minutes)
The regions on that regionserver DONOT reopen (I waited for 1 hour)
The client is able to find the new location of the META, however, the META keeps redirecting the client to powered down
regionserver as the location of the key-value it is trying to get. Thus the client's get is unsuccessful.",,,1,,,,,
HBASE-4169,"FSUtils.recoverFileLease uses HDFS's recoverLease method to get lease before splitting hlog file.
This might not work for other filesystem implementations. ",,,,,,,,
HBASE-4177,"As per the mailing thread with the heading
'Handling read failures during recovery? we found this problem.
As part of split Logs the HMaster calls Namenode recovery.  The recovery is an asynchronous process. 
In HDFS
=======
Even though client is getting the updated block info from Namenode on first
read failure, client is discarding the new info and using the old info only
to retrieve the data from datanode. So, all the read
retries are failing. [Method parameter reassignment - Not reflected in
caller]. 
In HBASE
=======
In HMaster code we tend to wait for  1sec.  But if the recovery had some failure then split log may not happen and may lead to dataloss.
So may be we need to decide upon the actual delay that needs to be introduced once Hmaster calls NN recovery.

",,,,,,,,
HBASE-4180,"Hadoop 0.21.0's UserGroupInfomation support the security check flag and always returns false.
HBase should check both the method existence and the return value.
",1,,,,,,,
HBASE-4181,"HRegionInterface getHRegionConnection(final String hostname,
        final int port, final InetSocketAddress isa, final boolean master)
        throws IOException 


/////////////////////////
	String rsName = isa != null ? isa.toString() : Addressing
          .createHostAndPortStr(hostname, port); 

                
////here,if isa is null, the Addressing created a address like ""node41:60010""
                                                                 ////should use ""isa.toString():new InetSocketAddress(hostname, port).toString();"" 
                                                                 ////instead of ""Addressing.createHostAndPortStr(hostname, port);""


 	server = this.servers.get(rsName);                                      
      if (server == null) {
        // create a unique lock for this RS (if necessary)
        this.connectionLock.putIfAbsent(rsName, rsName);
        // get the RS lock
        synchronized (this.connectionLock.get(rsName)) {
          // do one more lookup in case we were stalled above
          server = this.servers.get(rsName);
          if (server == null) {
            try {
              if (clusterId.hasId()) {
                conf.set(HConstants.CLUSTER_ID, clusterId.getId());
              }
              // Only create isa when we need to.
              InetSocketAddress address = isa != null ? isa
                  : new InetSocketAddress(hostname, port);
              // definitely a cache miss. establish an RPC for this RS
              server = (HRegionInterface) HBaseRPC.waitForProxy(
                  serverInterfaceClass, HRegionInterface.VERSION, address,
                  this.conf, this.maxRPCAttempts, this.rpcTimeout,
                  this.rpcTimeout);
              this.servers.put(address.toString(), server);    

          
////but here address.toString() send an address like ""node41/10.61.2l.171:60010
////so this method can never get cached address and make client request very slow due to it's synchronized.
	

                  
            } catch (RemoteException e) {
              LOG.warn(""RemoteException connecting to RS"", e);
              // Throw what the RemoteException was carrying.
              throw RemoteExceptionHandler.decodeRemoteException(e);
            }
          }
        }
///////////////////////",,,1,,,,,
HBASE-4184,"In our system, hbase.rootdir is set to a hdfs path and hbase can figure out the FileSystem and set ""fs.default.name"" accordingly on the Configuration object and pass around including to RS. That is handled in HMaster.java and MasterFileSystem.java.

CatalogJanitor uses deprecated HRegionInfo.getTableDesc. The method creates a default configuration and get FileSystem from there. That will be RawLocalFileSystem. It returns the following exception.


java.lang.IllegalArgumentException: Wrong FS: hdfs://sea-esxi-0:54310/tmp/hbase/
testtb/.tableinfo, expected: file:///
        at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:454)
        at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:67)
        at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1085)
        at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1110)
        at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:494)
        at org.apache.hadoop.hbase.util.FSUtils.getTableInfoModtime(FSUtils.java:833)
        at org.apache.hadoop.hbase.util.FSTableDescriptors.get(FSTableDescriptors.java:127)
        at org.apache.hadoop.hbase.util.FSTableDescriptors.get(FSTableDescriptors.java:99)
        at org.apache.hadoop.hbase.HRegionInfo.getTableDesc(HRegionInfo.java:560)
        at org.apache.hadoop.hbase.master.CatalogJanitor$1.compare(CatalogJanitor.java:118)
        at org.apache.hadoop.hbase.master.CatalogJanitor$1.compare(CatalogJanitor.java:110)
        at java.util.TreeMap.put(TreeMap.java:530)        at org.apache.hadoop.hbase.master.CatalogJanitor$2.visit(CatalogJanitor.java:138)",,1,,,,,,
HBASE-4195,"This follows the dicussion around HBASE-3855, and the random errors (20% failure on trunk) on the unit test org.apache.hadoop.hbase.regionserver.TestHRegion.testWritesWhileGetting

I saw some points related to numIterReseek, used in the MemStoreScanner#getNext (line 690):

{noformat}679	    protected KeyValue getNext(Iterator it) {
680	      KeyValue ret = null;
681	      long readPoint = ReadWriteConsistencyControl.getThreadReadPoint();
682	      //DebugPrint.println( "" MS@"" + hashCode() + "": threadpoint = "" + readPoint);
683	 
684	      while (ret == null && it.hasNext()) {
685	        KeyValue v = it.next();
686	        if (v.getMemstoreTS() <= readPoint) {
687	          // keep it.
688	          ret = v;
689	        }
690	        numIterReseek--;
691	        if (numIterReseek == 0) {
692	          break;
693	         }
694	      }
695	      return ret;
696	    }{noformat}

This function is called by seek, reseek, and next. The numIterReseek is only usefull for reseek.

There are some issues, I am not totally sure it's the root cause of the test case error, but it could explain partly the randomness of the error, and one point is for sure a bug.

1) In getNext, numIterReseek is decreased, then compared to zero. The seek function sets numIterReseek to zero before calling getNext. It means that the value will be actually negative, hence the test will always fail, and the loop will continue. It is the expected behaviour, but it's quite smart.

2) In ""reseek"", numIterReseek is not set between the loops on the two iterators. If the numIterReseek is equals to zero after the loop on the first one, the loop on the second one will never call seek, as numIterReseek will be negative.

3) Still in ""reseek"", the test to call ""seek"" is (kvsetNextRow == null && numIterReseek == 0). In other words, if kvsetNextRow is not null when numIterReseek equals zero, numIterReseek will start to be negative at the next iteration and seek will never be called.

4) You can have side effects if reseek ends with a numIterReseek > 0: the following calls to the ""next"" function will decrease numIterReseek to zero, and getNext will break instead of continuing the loop. As a result, later calls to next() may return null or not depending on how is configured the default value for numIterReseek.

To check if the issue comes from point 4, you can set the numIterReseek to zero before returning in reseek:

{noformat}      numIterReseek = 0;
      return (kvsetNextRow != null || snapshotNextRow != null);
    }{noformat}

On my env, on trunk, it seems to work, but as it's random I am not really sure. I also had to modify the test (I added a loop) to make it fails more often, the original test was working quite well here.

It has to be confirmed that this totally fix (it could be partial or unrelated) org.apache.hadoop.hbase.regionserver.TestHRegion.testWritesWhileGetting before implementing a complete solution.
",,,1,,,,,
HBASE-4196,"After the following scenario, the first record of region is skipped, without being sent to Mapper:
 - the reader is initialized with TableRecordReader.init()
 - then nextKeyValue is called, causing call to scanner.next() - here ScannerTimeoutException occurs
 - the scanner is restarted by call to restart() and then *two* calls to scanner.next() occur, causing we have lost the first row
",,,,,,,,
HBASE-4197,"Returning just an InternalScanner from RegionObsever.{pre|post}OpenScanner leads to the following exception when using the scanner.

java.io.IOException: InternalScanner implementation is expected to be HRegion.RegionScanner.
        at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:2023)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:314)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1225)

The problem is in HRegionServer.next(...):
{code} 
    InternalScanner s = this.scanners.get(scannerName);
...
      // Call coprocessor. Get region info from scanner.
      HRegion region = null;
      if (s instanceof HRegion.RegionScanner) {
        HRegion.RegionScanner rs = (HRegion.RegionScanner) s;
        region = getRegion(rs.getRegionName().getRegionName());
      } else {
        throw new IOException(""InternalScanner implementation is expected "" +
            ""to be HRegion.RegionScanner."");
      }
{code} ",,,,,,,,
HBASE-4212,"It seems a bug. The root in RIT can't be moved..
In the failover process, it enforces root on-line. But not clean zk node. 
test will wait forever.

  void processFailover() throws KeeperException, IOException, InterruptedException {
     
    // we enforce on-line root.
    HServerInfo hsi =
      this.serverManager.getHServerInfo(this.catalogTracker.getMetaLocation());
    regionOnline(HRegionInfo.FIRST_META_REGIONINFO, hsi);
    hsi = this.serverManager.getHServerInfo(this.catalogTracker.getRootLocation());
    regionOnline(HRegionInfo.ROOT_REGIONINFO, hsi);

It seems that we should wait finished as meta region 
  int assignRootAndMeta()
  throws InterruptedException, IOException, KeeperException {
    int assigned = 0;
    long timeout = this.conf.getLong(""hbase.catalog.verification.timeout"", 1000);

    // Work on ROOT region.  Is it in zk in transition?
    boolean rit = this.assignmentManager.
      processRegionInTransitionAndBlockUntilAssigned(HRegionInfo.ROOT_REGIONINFO);
    if (!catalogTracker.verifyRootRegionLocation(timeout)) {
      this.assignmentManager.assignRoot();
      this.catalogTracker.waitForRoot();

      //we need add this code and guarantee that the transition has completed
      this.assignmentManager.waitForAssignment(HRegionInfo.ROOT_REGIONINFO);
      assigned++;
    }

logs:
2011-08-16 07:45:40,715 DEBUG [RegionServer:0;C4S2.site,47710,1313495126115-EventThread] zookeeper.ZooKeeperWatcher(252): regionserver:47710-0x131d2690f780004 Received ZooKeeper Event, type=NodeDataChanged, state=SyncConnected, path=/hbase/unassigned/70236052
2011-08-16 07:45:40,715 DEBUG [RS_OPEN_ROOT-C4S2.site,47710,1313495126115-0] zookeeper.ZKAssign(712): regionserver:47710-0x131d2690f780004 Successfully transitioned node 70236052 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2011-08-16 07:45:40,715 DEBUG [Thread-760-EventThread] zookeeper.ZooKeeperWatcher(252): master:60701-0x131d2690f780009 Received ZooKeeper Event, type=NodeDataChanged, state=SyncConnected, path=/hbase/unassigned/70236052
2011-08-16 07:45:40,716 INFO  [PostOpenDeployTasks:70236052] catalog.RootLocationEditor(62): Setting ROOT region location in ZooKeeper as C4S2.site:47710
2011-08-16 07:45:40,716 DEBUG [Thread-760-EventThread] zookeeper.ZKUtil(1109): master:60701-0x131d2690f780009 Retrieved 52 byte(s) of data from znode /hbase/unassigned/70236052 and set watcher; region=-ROOT-,,0, server=C4S2.site,47710,1313495126115, state=RS_ZK_REGION_OPENING
2011-08-16 07:45:40,717 DEBUG [Thread-760-EventThread] master.AssignmentManager(477): Handling transition=RS_ZK_REGION_OPENING, server=C4S2.site,47710,1313495126115, region=70236052/-ROOT-
2011-08-16 07:45:40,725 DEBUG [RS_OPEN_ROOT-C4S2.site,47710,1313495126115-0] zookeeper.ZKAssign(661): regionserver:47710-0x131d2690f780004 Attempting to transition node 70236052/-ROOT- from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2011-08-16 07:45:40,727 DEBUG [RS_OPEN_ROOT-C4S2.site,47710,1313495126115-0] zookeeper.ZKUtil(1109): regionserver:47710-0x131d2690f780004 Retrieved 52 byte(s) of data from znode /hbase/unassigned/70236052; data=region=-ROOT-,,0, server=C4S2.site,47710,1313495126115, state=RS_ZK_REGION_OPENING
2011-08-16 07:45:40,740 DEBUG [RegionServer:0;C4S2.site,47710,1313495126115-EventThread] zookeeper.ZooKeeperWatcher(252): regionserver:47710-0x131d2690f780004 Received ZooKeeper Event, type=NodeDataChanged, state=SyncConnected, path=/hbase/unassigned/70236052
2011-08-16 07:45:40,740 DEBUG [Thread-760-EventThread] zookeeper.ZooKeeperWatcher(252): master:60701-0x131d2690f780009 Received ZooKeeper Event, type=NodeDataChanged, state=SyncConnected, path=/hbase/unassigned/70236052
2011-08-16 07:45:40,740 DEBUG [RS_OPEN_ROOT-C4S2.site,47710,1313495126115-0] zookeeper.ZKAssign(712): regionserver:47710-0x131d2690f780004 Successfully transitioned node 70236052 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2011-08-16 07:45:40,741 DEBUG [RS_OPEN_ROOT-C4S2.site,47710,1313495126115-0] handler.OpenRegionHandler(121): Opened -ROOT-,,0.70236052
2011-08-16 07:45:40,741 DEBUG [Thread-760-EventThread] zookeeper.ZKUtil(1109): master:60701-0x131d2690f780009 Retrieved 52 byte(s) of data from znode /hbase/unassigned/70236052 and set watcher; region=-ROOT-,,0, server=C4S2.site,47710,1313495126115, state=RS_ZK_REGION_OPENED
2011-08-16 07:45:40,741 DEBUG [Thread-760-EventThread] master.AssignmentManager(477): Handling transition=RS_ZK_REGION_OPENED, server=C4S2.site,47710,1313495126115, region=70236052/-ROOT-

//.............................................It said that zk node can't be cleaned because of we have enforced on-line the root.......................................
// The test will wait forever.

2011-08-16 07:45:40,741 WARN  [Thread-760-EventThread] master.AssignmentManager(540): Received OPENED for region 70236052/-ROOT- from server C4S2.site,47710,1313495126115 but region was in  the state null and not in expected PENDING_OPEN or OPENING states

2011-08-16 07:45:41,018 DEBUG [Master:0;C4S2.site:60701] zookeeper.ZKUtil(1109): master:60701-0x131d2690f780009 Retrieved 52 byte(s) of data from znode /hbase/unassigned/70236052 and set watcher; region=-ROOT-,,0, server=C4S2.site,47710,1313495126115, state=RS_ZK_REGION_OPENED
2011-08-16 07:45:41,233 DEBUG [Thread-760] zookeeper.ZKAssign(807): ZK RIT -> 70236052
2011-08-16 07:45:41,337 DEBUG [Thread-760] zookeeper.ZKAssign(807): ZK RIT -> 70236052
2011-08-16 07:45:41,439 DEBUG [Thread-760] zookeeper.ZKAssign(807): ZK RIT -> 70236052
2011-08-16 07:45:41,543 DEBUG [Thread-760] zookeeper.ZKAssign(807): ZK RIT -> 70236052
2011-08-16 07:45:41,645 DEBUG [Thread-760] zookeeper.ZKAssign(807): ZK RIT -> 70236052
2011-08-16 07:45:41,748 DEBUG [Thread-760] zookeeper.ZKAssign(807): ZK RIT -> 70236052
2011-08-16 07:45:41,900 DEBUG [Thread-760] zookeeper.ZKAssign(807): ZK RIT -> 70236052
2011-08-16 07:45:42,002 DEBUG [Thread-760] zookeeper.ZKAssign(807): ZK RIT -> 70236052
2011-08-16 07:45:42,105 DEBUG [Thread-760] zookeeper.ZKAssign(807): ZK RIT -> 70236052
2011-08-16 07:45:42,206 DEBUG [Thread-760] zookeeper.ZKAssign(807): ZK RIT -> 70236052
2011-08-16 07:45:42,308 DEBUG [Thread-760] zookeeper.ZKAssign(807): ZK RIT -> 70236052
2011-08-16 07:45:42,410 DEBUG [Thread-760] zookeeper.ZKAssign(807): ZK RIT -> 70236052
",,,,,,,,
HBASE-4215,"In testing trunk, I had YCSB reporting some 40,000 requests/second, but the summary info on the master webpage was consistently indicating somewhere around 3x that. I'm guessing that we may have a bug where we forgot to divide by time.",,,,,,,,
HBASE-4220,"In running a YCSB workload, I managed to DDOS a DNS server since it seems to be flooding lots of DNS requests. Installing nscd on the client machines improved throughput by a factor of 6 and stopped killing the server. These are long-running clients, so it's not clear why we do so many lookups.",,,1,,,,,
HBASE-4225,"It has been changed on trunk. But not on branch.
If rollback fail after poing-of-no-return and do nothing, the parent has been closed, and doesn't report to HMaster. This cause the inconsistance region state between HMaster and HRegionServer.",,,,,,,,
HBASE-4234,"It has been changed on trunk. But not on branch.
If rollback fail after poing-of-no-return and do nothing, the parent has been closed, and doesn't report to HMaster. This cause the inconsistance region state between HMaster and HRegionServer.",,,,,,,,
HBASE-4238,"I didn't dig a lot into this issue, but by splitting a table twice in a row I was able to trigger a situation where a daughter of the first split was deleted by the CatalogJanitor before it processed its parent. Will post log in a comment.",,,,,,,,
HBASE-4252,"Before I explain why it could happen, I describe how does this test(testLogRollOnDatanodeDeath) works:
1. There's two datanodes A & B in env. So the log has two replications first which is the expect and default value.
2. Add a new datanode C and wait it active.
3. Kill A who is in the pipelines. 
4. Write data. So trigger a new rollWriter while the next sync. And it only happens once. For the new log has two replications.
5. Kill another datanode B.
6. Write batch data to trigger consecutive rollWriter. So LowReplication-Roller will be disabled.
7. Add a new datanode D and wait it active.
8. Send a rollWriter request. So expect the new log will has the default replications.
9. Write batch data. Assert the LogReplication-Roller will be enabled.

Maybe the rollWriter request in step 8 can't be executed affected by the previous roll requests from step 6. So the current log replication is not the expect value.
{noformat}
  public byte [][] rollWriter() throws FailedLogCloseException, IOException {
    // Return if nothing to flush.
    if (this.writer != null && this.numEntries.get() <= 0) {
      return null;
    }
{noformat}

So the following assertion must be safeguarded.
{noformat}
  log.rollWriter();
  batchWriteAndWait(table, 14, true, 10000);
  assertTrue(""LowReplication Roller should've been enabled"",
      log.isLowReplicationRollEnabled());
{noformat}",,,,,,,,
HBASE-4270,"If the RS experiences an exception during the flush of a region while closing it, it currently catches the exception, logs a warning, and keeps going. If the exception was a DroppedSnapshotException, this means that it will silently drop any data that was in memstore when the region was closed.

Instead, the RS should do a hard abort so that its logs will be replayed.",,,,,,,,
HBASE-4271,"Couple fixes we can do w.r.t coprocessor's handlings of table operations.

1. Honor MasterObserver's requests to bypass default action.
2. Fix up the function signatures for preCreateTable to use HRegionInfo as parameter instead.
3. Invoke postEnableTable, etc. methods after the operations are done.",,,,,,,,
HBASE-4273,"This bug occurs in following scenario. 

1. For some reason, the regionLocation isn't set in .META. table for some regions. Perhaps createTable didn't complete successfully.
1. The table of those regions is being disabled.
2. HMaster restarted.
3. At HMaster startup, it tries to transition from disabling to disabled state. It got the following exception.

java.lang.NullPointerException: Passed server is null
        at
org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.
java:581)
        at
org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager
.java:1093)
        at
org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager
.java:1040)
        at
org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler$1.r
un(DisableTableHandler.java:132)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.j
ava:886)


In AssignmentManager.rebuildUserRegions, it added such regions to its regions list,

      if (regionLocation == null) {
        // Region not being served, add to region map with no assignment
        // If this needs to be assigned out, it will also be in ZK as RIT
        // add if the table is not in disabled and enabling state
        if (false == checkIfRegionBelongsToDisabled(regionInfo)
            && false == checkIfRegionsBelongsToEnabling(regionInfo)) {
          regions.put(regionInfo, regionLocation);
        }

Perhaps, it should be

      if (regionLocation == null) {
        // Region not being served, add to region map with no assignment
        // If this needs to be assigned out, it will also be in ZK as RIT
        // add if the table is not in disabled and enabling state
        if (true == checkIfRegionBelongsToEnabled(regionInfo) {
          regions.put(regionInfo, regionLocation);
        }


",,,,,,,,
HBASE-4277,"As suggested by Stack in HBASE-4217 creating a new issue to provide a patch for 0.90.x version.


We had some sort of an outage this morning due to a few racks losing power, and some regions were left in the following state:

ERROR: Region UNKNOWN_REGION on sv4r17s9:60020, key=e32bbe1f48c9b3633c557dc0291b90a3, not on HDFS or in META but deployed on sv4r17s9:60020

That region was deleted by the master but the region server never got the memo. Right now there's no way to force close it because HRS.closeRegion requires an HRI and the only way to create one is to get it from .META. which in our case doesn't contain a row for that region. Basically we have to wait until that server is dead to get rid of the region and make hbck happy.

The required change is to have closeRegion accept an encoded name in both HBA (when the RS address is provided) and HRS since it's able to find it anyways from it's list of live regions.
bq.If a 0.90 version, we maybe should do that in another issue.",,,,,,,,
HBASE-4280,replication] ReplicationSink can deadlock itself via handlers,,,,,,,,
HBASE-4287,"In the case that region-opening fails, we currently just close the region again, but don't do anything to the node in ZK. Instead, we should attempt to transition it from the OPENING state back to an OFFLINE state, or perhaps a new FAILED_OPEN state. Otherwise, we have to wait for the full timeoutMonitor period to elapse, which can be quite a long time.",,,1,,,,,
HBASE-4300,"I shut down an 0.90 cluster, and had to do so uncleanly. I then started a trunk (0.92) cluster before the old master znode had expired. This cased:

java.lang.StringIndexOutOfBoundsException: String index out of range: -1
        at java.lang.String.substring(String.java:1937)
        at org.apache.hadoop.hbase.ServerName.parseHostname(ServerName.java:81)
        at org.apache.hadoop.hbase.ServerName.<init>(ServerName.java:63)
        at org.apache.hadoop.hbase.master.ActiveMasterManager.blockUntilBecomingActiveMaster(ActiveMasterManager.java:148)
        at org.apache.hadoop.hbase.master.HMaster.becomeActiveMaster(HMaster.java:342)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:297)
",,,,,,,,
HBASE-4304,"Running trunk @ r1163343, all of the requestsPerSecond counters are showing 0 both in the master UI and in the RS UI. The writeRequestsCount metric is properly updating in the RS UI.",,,,,,,,
HBASE-4308,"When the master is processing a ZK event for REGION_OPENED, it calls delete() on the znode before it removes the node from RegionsInTransition. If the notification of that delete comes back into AssignmentManager before the region is removed from RIT, you see an error like:

2011-08-30 17:43:29,537 WARN  [main-EventThread] master.AssignmentManager(861): Node deleted but still in RIT: .META.,,1.1028785192 state=OPEN, ts=1314751409532, server=todd-w510,55655,1314751396840

Not certain if it causes issues, but it's a concerning log message.",,,,,,,,
HBASE-4331,"While testing some other scenario I found calling CoprocessorEnvironment.bypass() fails if all trailing puts in a batch are bypassed that way. By extension a single bypassed put will also fail.

The problem is that the puts are removed from the batch in a way that does not align them with the result-status, and in addition the result is never marked as success.

A possible fix is to just mark bypassed puts as SUCCESS and filter them in the following logic.
(I also contemplated a new BYPASSED OperationStatusCode, but that turned out to be not necessary).",,1,,,,,,
HBASE-4334,"If a client gets confused (possibly by a hole in .META., see HBASE-4333), it may send a request to the wrong region.  Paths through put, delete, incrementColumnValue, and checkAndMutate all call checkRow either directly or indirectly (through getLock).  But get apparently does not.  This can result in returning an incorrect empty result instead of a WrongRegionException.",,,,,,,,
HBASE-4335,"When a SplitTransaction is performed, three updates are done to .META.:
1. The parent region is marked as splitting (and hence offline)
2. The first daughter region is added (same start key as parent)
3. The second daughter region is added (split key is start key)
(later, the original parent region is deleted, but that's not important to this discussion)

Steps 2 and 3 are actually done concurrently by SplitTransaction.DaughterOpener threads.  While the master is notified when a split is complete, the only visibility that clients have is whether the daughter regions have appeared in .META.

If the second daughter is added to .META. first, then .META. will contain the (offline) parent region followed by the second daughter region.  If the client looks up a key that is greater than (or equal to) the split, the client will find the second daughter region and use it.  If the key is less than the split key, the client will find the parent region and see that it is offline, triggering a retry.

If the first daughter is added to .META. before the second daughter, there is a window during which .META. has a hole: the first daughter effectively hides the parent region (same start key), but there is no entry for the second daughter.  A region lookup will find the first daughter for all keys in the parent's range, but the first daughter does not include keys at or beyond the split key.

See HBASE-4333 and HBASE-4334 for details on how this causes problems and suggestions for mitigating this in the client and regionserver.
",,,,,,,,
HBASE-4336,"When we originally converted the build to maven we had a single ""core"" module defined, but later reverted this to a module-less build for the sake of simplicity.
It now looks like it's time to re-address this, as we have an actual need for modules to:
provide a trimmed down ""client"" library that applications can make use of
more cleanly support building against different versions of Hadoop, in place of some of the reflection machinations currently required
incorporate the secure RPC engine that depends on some secure Hadoop classes
I propose we start simply by refactoring into two initial modules:
core - common classes and utilities, and client-side code and interfaces
server - master and region server implementations and supporting code
This would also lay the groundwork for incorporating the HBase security features that have been developed. Once the module structure is in place, security-related features could then be incorporated into a third module C ""security"" C after normal review and approval. The security module could then depend on secure Hadoop, without modifying the dependencies of the rest of the HBase code.",1,,,,,,,
HBASE-4340,"Version: 0.90.4
Cluster : 40 boxes
As I saw below logs. It said that balance couldn't work because of a dead RS.
I dug deeply and found two issues:

1.       shutdownhandler didn't clear numProcessing deal with some exceptions. It seems whatever exceptions we should clear the flag or close master.

2.       ""dead regionserver(s): [158-1-130-12,20020,1314971097929]"" is inaccurate. The dead sever should be "" 158-1-130-10,20020,1315068597979""

//master logs:
2011-09-05 00:28:00,487 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 00:33:00,489 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 00:38:00,493 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 00:43:00,495 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 00:48:00,499 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 00:53:00,501 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 00:58:00,501 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 01:03:00,502 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 01:08:00,506 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 01:13:00,508 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 01:18:00,512 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 01:23:00,514 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 01:28:00,518 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 01:33:00,520 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 01:38:00,524 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 01:43:00,526 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 01:48:00,530 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 01:53:00,532 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 01:58:00,536 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 02:03:00,537 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 02:08:00,538 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 02:13:00,539 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 02:18:00,543 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]

// the exception logs :.
2011-09-03 18:13:27,550 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=158-1-133-11,20020,1315069437236, region=0db4088d75c58dd22f93f389d90ba6cc
2011-09-03 18:13:27,550 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_SERVER_SHUTDOWN java.lang.NullPointerException
         at org.apache.hadoop.hbase.util.Bytes.toLong(Bytes.java:480)
         at org.apache.hadoop.hbase.util.Bytes.toLong(Bytes.java:454)
         at org.apache.hadoop.hbase.catalog.MetaReader.metaRowToRegionPairWithInfo(MetaReader.java:400)
         at org.apache.hadoop.hbase.catalog.MetaReader.getServerUserRegions(MetaReader.java:591)
         at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:176)
         at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:156)
         at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
         at java.lang.Thread.run(Thread.java:662)
2011-09-03 18:13:27,550 INFO org.apache.hadoop.hbase.master.handler.ServerShutdownHandler: Splitting logs for 158-1-134-15,20020,1315065238916
2011-09-03 18:13:27,566 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: No previous transition plan was found (or we are ignoring an existing plan) for ufdr,001146,1314955304624.22f6d43e78c903196f206881fc149488. so generated a random one; hri=ufdr,001146,1314955304624.22f6d43e78c903196f206881fc149488., src=, dest=158-1-132-17,20020,1315069441916; 31 (online=31, exclude=null) available servers
201
",,,,,,,,
HBASE-4341,"This's the reason of why did ""https://builds.apache.org/job/hbase-0.90/282"" get failure . In this test, one case was timeout and cause the whole test process got killed.

[logs]
Here's the related logs(From org.apache.hadoop.hbase.mapreduce.TestTableMapReduce-output.txt):
{noformat}
2011-08-31 10:09:01,089 INFO  [RegionServer:0;vesta.apache.org,52257,1314785332968.leaseChecker] regionserver.Leases(124): RegionServer:0;vesta.apache.org,52257,1314785332968.leaseChecker closing leases
2011-08-31 10:09:01,089 INFO  [RegionServer:0;vesta.apache.org,52257,1314785332968.leaseChecker] regionserver.Leases(131): RegionServer:0;vesta.apache.org,52257,1314785332968.leaseChecker closed leases
2011-08-31 10:09:01,403 INFO  [RegionServer:0;vesta.apache.org,52257,1314785332968] regionserver.HRegionServer(709): Waiting on 1 regions to close
2011-08-31 10:09:01,403 DEBUG [RegionServer:0;vesta.apache.org,52257,1314785332968] regionserver.HRegionServer(713): {74a7a8befdf9561dc1d90c4241afeac7=mrtest,uuu,1314785328546.74a7a8befdf9561dc1d90c4241afeac7.}
2011-08-31 10:09:01,697 INFO  [Master:0;vesta.apache.org:50036] master.ServerManager(465): Waiting on regionserver(s) to go down vesta.apache.org,52257,1314785332968
2011-08-31 10:09:02,697 INFO  [Master:0;vesta.apache.org:50036] master.ServerManager(465): Waiting on regionserver(s) to go down vesta.apache.org,52257,1314785332968
2011-08-31 10:09:03,008 INFO  [vesta.apache.org:50036.timeoutMonitor] hbase.Chore(79): vesta.apache.org:50036.timeoutMonitor exiting
2011-08-31 10:09:03,697 INFO  [Master:0;vesta.apache.org:50036] master.ServerManager(465): Waiting on regionserver(s) to go down vesta.apache.org,52257,1314785332968
2011-08-31 10:09:04,697 INFO  [Master:0;vesta.apache.org:50036] master.ServerManager(465): Waiting on regionserver(s) to go down vesta.apache.org,52257,1314785332968
2011-08-31 10:09:05,698 INFO  [Master:0;vesta.apache.org:50036] master.ServerManager(465): Waiting on regionserver(s) to go down vesta.apache.org,52257,1314785332968
2011-08-31 10:09:06,698 INFO  [Master:0;vesta.apache.org:50036] master.ServerManager(465): Waiting on regionserver(s) to go down vesta.apache.org,52257,1314785332968
2011-08-31 10:09:07,698 INFO  [Master:0;vesta.apache.org:50036] master.ServerManager(465): Waiting on regionserver(s) to go down vesta.apache.org,52257,1314785332968
{noformat}
[Analysis]
One region was opened during the RS's stopping. 
This is method of ""HRS#closeAllRegions"":
{noformat}

  protected void closeAllRegions(final boolean abort) {
    closeUserRegions(abort);
    -------------------------
    if (meta != null) closeRegion(meta.getRegionInfo(), abort, false);
    if (root != null) closeRegion(root.getRegionInfo(), abort, false);
  }
{noformat}

HRS#onlineRegions is a ConcurrentHashMap. So walk down this map may not get all the data if some entries are been added during the traverse. Once one region was missed, it can't be closed anymore. And this regionserver will not be stopped normally. Then the following logs occurred:
{noformat}
2011-08-31 10:09:01,403 INFO  [RegionServer:0;vesta.apache.org,52257,1314785332968] regionserver.HRegionServer(709): Waiting on 1 regions to close
2011-08-31 10:09:01,403 DEBUG [RegionServer:0;vesta.apache.org,52257,1314785332968] regionserver.HRegionServer(713): {74a7a8befdf9561dc1d90c4241afeac7=mrtest,uuu,1314785328546.74a7a8befdf9561dc1d90c4241afeac7.}
2011-08-31 10:09:01,697 INFO  [Master:0;vesta.apache.org:50036] master.ServerManager(465): Waiting on regionserver(s) to go down vesta.apache.org,52257,1314785332968
{noformat}",,,,,,,,
HBASE-4351,"The following is the problem
Get the exact region name from UI and call
HBaseAdmin.unassign(regionname, true).
Here true is forceful option.
As part of unassign api
{code}
  public void unassign(final byte [] regionName, final boolean force)
  throws IOException {
    Pair<HRegionInfo, HServerAddress> pair =
      MetaReader.getRegion(this.catalogTracker, regionName);
    if (pair == null) throw new UnknownRegionException(Bytes.toStringBinary(regionName));
    HRegionInfo hri = pair.getFirst();
    if (force) this.assignmentManager.clearRegionFromTransition(hri);
    this.assignmentManager.unassign(hri, force);
  }
{code}
As part of clearRegionFromTransition()
{code}
    synchronized (this.regions) {
      this.regions.remove(hri);
      for (Set<HRegionInfo> regions : this.servers.values()) {
        regions.remove(hri);
      }
    }
{code}
the region is also removed.  Hence when the master tries to identify the region
{code}
      if (!regions.containsKey(region)) {
        debugLog(region, ""Attempted to unassign region "" +
          region.getRegionNameAsString() + "" but it is not "" +
          ""currently assigned anywhere"");
        return;
      }
{code}
It is not able to identify the region.  It exists in trunk and 0.90.x also.",,,,,,,,
HBASE-4357,"Got the following during testing, 

1. On a given machine, kill ""RS process id"". Then kill ""HMaster process id"".
2. Start RS first via ""bin/hbase-daemon.sh --config ./conf start regionserver."". Start HMaster via ""bin/hbase-daemon.sh --config ./conf start master"".

One region of a table stayed in closing state.

According to zookeeper,
794a6ff17a4de0dd0a19b984ba18eea9 miweng_500region,H\xB49X\x10bM\xB1,1315338786464.794a6ff17a4de0dd0a19b984ba18eea9. state=CLOSING, ts=Wed Sep 07 17:21:44 PDT 2011 (75701s ago), server=sea-esxi-0,60000,1315428682281 

According to .META. table, the region has been assigned to from sea-esxi-0 to sea-esxi-4.

miweng_500region,H\xB49X\x10bM\xB1,1315338786464.794a6ff17a4de0dd0a19b984ba18eea9. sea-esxi-4:60030  H\xB49X\x10bM\xB1 I7K\xC6\xA7\xEF\x9D\x90 0 ",,,,,,,,
HBASE-4363,"When trying to close a source, it will hang if it's already in shipEdits() and has issues reaching the sink. The reason is that in that method the while loop only checks if the RS is going down but not if the source was asked to shutdown.",,,,,,,,
HBASE-4367,"We observed a deadlock in production between the following threads:
- IPC handler thread holding the monitor lock on MemStoreFlusher inside reclaimMemStoreMemory, waiting to obtain MemStoreFlusher.lock (the reentrant lock member)
- cacheFlusher thread inside flushRegion holds MemStoreFlusher.lock, and then calls PriorityCompactionQueue.add, which calls PriorityCompactionQueue.addToRegionsInQueue, which calls CompactionRequest.toString(), which calls Date.toString. If this occurs just after a GC under memory pressure, Date.toString needs to reload locale information (stored in a soft reference), so it calls ResourceBundle.loadBundle, which uses Thread.currentThread() as a synchronizer (see sun bug http://bugs.sun.com/view_bug.do?bug_id=6915621). Since the current thread is the MemStoreFlusher itself, we have a lock order inversion and a deadlock.",,,,,,,,
HBASE-4378,hbck doesn't seem to complain or have an error condition if there is a region where startkey==endkey.,,,,,,,,
HBASE-4379,hbck does not detect or have an error condition when the last region of a table is missing (end key != '').,,,,,,,,
HBASE-4386,"Saw the following hitting /rs-status
<pre>    INTERNAL_SERVER_ERROR</pre></p><h3>Caused by:</h3><pre>java.lang.NullPointerException
        at org.apache.hadoop.hbase.monitoring.TaskMonitor.purgeExpiredTasks(TaskMonitor.java:97)
        at org.apache.hadoop.hbase.monitoring.TaskMonitor.getTasks(TaskMonitor.java:127)
        at org.apache.hbase.tmpl.common.TaskMonitorTmplImpl.renderNoFlush(TaskMonitorTmplImpl.java:50)
        at org.apache.hbase.tmpl.common.TaskMonitorTmpl.renderNoFlush(TaskMonitorTmpl.java:170)
        at org.apache.hbase.tmpl.regionserver.RSStatusTmplImpl.renderNoFlush(RSStatusTmplImpl.java:70)
        at org.apache.hbase.tmpl.regionserver.RSStatusTmpl.renderNoFlush(RSStatusTmpl.java:176)
        at org.apache.hbase.tmpl.regionserver.RSStatusTmpl.render(RSStatusTmpl.java:167)
        at org.apache.hadoop.hbase.regionserver.RSStatusServlet.doGet(RSStatusServlet.java:48)
",,,,,,,,
HBASE-4387,"In a billion-row load on ~25 servers, I see ""error while syncing"" reasonable often with the error ""DFSOutputStream is closed"" around a roll. We have some race where a roll at the same time as heavy inserts causes a problem.",,,,,,,,
HBASE-4397,"1. Shutdown all RSs.
2. Bring all RS back online.

The ""-ROOT-"", "".META."" stay in offline state until timeout monitor force assignment 30 minutes later. That is because HMaster can't find a RS to assign the tables to in assign operation.


011-09-13 13:25:52,743 WARN org.apache.hadoop.hbase.master.AssignmentManager: Failed assignment of -ROOT-,,0.70236052 to sea-lab-4,60020,1315870341387, trying to assign elsewhere instead; retry=0
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:373)
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupIOstreams(HBaseClient.java:345)
        at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:1002)
        at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:854)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:148)
        at $Proxy9.openRegion(Unknown Source)
        at org.apache.hadoop.hbase.master.ServerManager.sendRegionOpen(ServerManager.java:407)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1408)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1153)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1128)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1123)
        at org.apache.hadoop.hbase.master.AssignmentManager.assignRoot(AssignmentManager.java:1788)
        at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.verifyAndAssignRoot(ServerShutdownHandler.java:100)
        at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.verifyAndAssignRootWithRetries(ServerShutdownHandler.java:118)
        at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:181)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:167)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
2011-09-13 13:25:52,743 WARN org.apache.hadoop.hbase.master.AssignmentManager: Unable to find a viable location to assign region -ROOT-,,0.70236052



Possible fixes:

1. Have serverManager handle ""server online"" event similar to how RegionServerTracker.java calls servermanager.expireServer in the case server goes down.
2. Make timeoutMonitor handle the situation better. This is a special situation in the cluster. 30 minutes timeout can be skipped.",,,1,,,,,
HBASE-4398,"If HRegionPartitioner is used in MapReduce, client side configurations are overwritten by hbase-site.xml.

We can reproduce the problem by the following instructions:

{noformat}
- Add HRegionPartitioner.class to the 4th argument of

TableMapReduceUtil#initTableReducerJob()

at line around 133
in src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableMapReduce.java

- Change or remove ""hbase.zookeeper.property.clientPort"" property

in hbase-site.xml ( for example, changed to 12345 ).

- run testMultiRegionTable()
{noformat}

Then I got error messages as following:

{noformat}
2011-09-12 22:28:51,020 DEBUG [Thread-832] zookeeper.ZKUtil(93): hconnection opening connection to ZooKeeper with ensemble (localhost:12345)
2011-09-12 22:28:51,022 INFO  [Thread-832] zookeeper.RecoverableZooKeeper(89): The identifier of this process is 43200@imac.local
2011-09-12 22:28:51,123 WARN  [Thread-832] zookeeper.RecoverableZooKeeper(161): Possibly transient ZooKeeper exception: org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/master
2011-09-12 22:28:51,123 INFO  [Thread-832] zookeeper.RecoverableZooKeeper(173): The 1 times to retry ZooKeeper after sleeping 1000 ms

 =====

2011-09-12 22:29:02,418 ERROR [Thread-832] mapreduce.HRegionPartitioner(125): java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@2e54e48d closed
2011-09-12 22:29:02,422 WARN  [Thread-832] mapred.LocalJobRunner$Job(256): job_local_0001
java.lang.NullPointerException
       at org.apache.hadoop.hbase.mapreduce.HRegionPartitioner.setConf(HRegionPartitioner.java:128)
       at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:62)
       at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)
       at org.apache.hadoop.mapred.MapTask$NewOutputCollector.<init>(MapTask.java:527)
       at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:613)
       at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
       at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)
{noformat}

I think HTable should connect to ZooKeeper at port 21818 configured at client side instead of 12345 in hbase-site.xml
and It might be caused by ""HBaseConfiguration.addHbaseResources(conf);"" in HRegionPartitioner#setConf(Configuration).

And this might mean that all of client side configurations, also configured in hbase-site.xml, are overwritten caused by this problem.
",,1,,,,,,
HBASE-4400,"Start 2 RS.
The .META. is being hosted by RS2 but while processing it goes down.

Now restart the master and RS1.  Master gets the RS name from the znode in RS_ZK_REGION_OPENED.  But as RS2 is not online still the master is not able to process the META at all.  Please find the logs
{noformat}
2011-09-14 16:43:51,949 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=linux76,60020,1315998828523, region=70236052/-ROOT-
2011-09-14 16:43:51,968 INFO org.apache.hadoop.hbase.master.HMaster: -ROOT- assigned=1, rit=false, location=linux76:60020
2011-09-14 16:43:51,970 INFO org.apache.hadoop.hbase.master.AssignmentManager: Processing region .META.,,1.1028785192 in state RS_ZK_REGION_OPENED
2011-09-14 16:43:51,970 INFO org.apache.hadoop.hbase.master.AssignmentManager: Failed to find linux146,60020,1315998414623 in list of online servers; skipping registration of open of .META.,,1.1028785192
2011-09-14 16:43:51,971 INFO org.apache.hadoop.hbase.master.AssignmentManager: Waiting on 1028785192/.META.
2011-09-14 16:43:51,983 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENED, server=linux76,60020,1315998828523, region=70236052/-ROOT-
2011-09-14 16:43:51,986 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Handling OPENED event for 70236052; deleting unassigned node
2011-09-14 16:43:51,986 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x13267854032001d Deleting existing unassigned node for 70236052 that is in expected state RS_ZK_REGION_OPENED
2011-09-14 16:43:51,998 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x13267854032001d Successfully deleted unassigned node for region 70236052 in expected state RS_ZK_REGION_OPENED
2011-09-14 16:43:51,999 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Opened region -ROOT-,,0.70236052 on linux76,60020,1315998828523
2011-09-14 16:44:00,945 INFO org.apache.hadoop.hbase.master.ServerManager: Registering server=linux146,60020,1315998839724, regionCount=0, userLoad=false
2011-09-14 16:46:20,003 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  .META.,,1.1028785192 state=OPEN, ts=0
2011-09-14 16:46:20,004 ERROR org.apache.hadoop.hbase.master.AssignmentManager: Region has been OPEN for too long, we don't know where region was opened so can't do anything
{noformat}

{code}
        regionsInTransition.put(encodedRegionName, new RegionState(
            regionInfo, RegionState.State.OPEN, data.getStamp()));
          ................
        } else {
          HServerInfo hsi = this.serverManager.getServerInfo(sn);
          if (hsi == null) {
            LOG.info(""Failed to find "" + sn +
              "" in list of online servers; skipping registration of open of "" +
              regionInfo.getRegionNameAsString());
          } else {
            new OpenedRegionHandler(master, this, regionInfo, hsi).process();
          }
        }
{code}
So timeout monitor is not able to do anything here
{code}
          LOG.error(""Region has been OPEN for too long, "" +
          ""we don't know where region was opened so can't do anything"");
          synchronized(regionState) {
            regionState.update(regionState.getState());
          }
{code}",,,,,,,,
HBASE-4410,"HBaseAdmin.checkHBaseAvailable(conf) clones the passed connection, later in HBaseAdmin constructor the connection is cloned again. Thus a new HConnection object with ZooKeeper connections is created.",,,,,,,,
HBASE-4417,"HBaseAdmin.checkHBaseAvailable(conf) clones the passed connection, later in HBaseAdmin constructor the connection is cloned again. Thus a new HConnection object with ZooKeeper connections is created.",,,,,,,,
HBASE-4420,"We've standardized on IOException as the main way for coprocessors to communicate errors back out of the Observer hooks.  All Observer hooks throw IOE except for MasterObserver.preMove() and MasterObserver.postMove(), which throw UnknownRegionException, since that's what HMasterInterface.move() declares.  In hindsight, making these two MasterObserver methods inconsistent seems like a mistake.

I think we should change MasterObserver.preMove() and MasterObserver.postMove() to throw IOException for consistency with the other methods.  We could deprecate the existing HMasterInterface.move() method to have it switch over to throwing IOException as well, but this would require creating a version with a new name, which seems unnecessarily ugly.  So I'd suggest we just have HMaster.move() handle the IOException and use it to init an UnknownRegionException.  Wonky as that is, it seems the lesser evil.",,,,,,,,
HBASE-4424,"When developing access controls as a coprocessor, we needed to create an internal ""acl"" table for persisting the permission grants. We could have done this in the coprocessor through HBaseAdmin, but it seems silly to go through an RPC loop when we're already implementing a MasterObserver running on HMaster.
So the simplest approach was to expose createTable() in MasterServices:
  /**
   * Create a table using the given table definition.
   * @param desc The table definition
   * @param splitKeys Starting row keys for the initial table regions.  If null
   *     a single region is created.
   * @param sync If true, waits for all initial regions to be assigned before
   *     returning
   */
  public void createTable(HTableDescriptor desc, byte [][] splitKeys)
      throws IOException;
Other coprocessor implementations will likely have similar needs, so I propose we add this to MasterServices.
The alternative would be to expose some sort of HBaseAdmin like interface via MasterCoprocessorEnvironment, similar to what we do for HTable, but this would be a fair bit more work, and I still think we'll need a way to provide this capability in the short term.",1,,,,,,,
HBASE-4425,"In some cases, RegionObserver coprocessors may want to directly access the running RpcServer instance on the region server. For token based authentication, for example, this is needed for a coprocessor to interact with the SecretManager that validates authentication tokens in the secure RPC engine. With the addition of async call handling on the server-side, this becomes additionally important if coprocessors want to send back delayed responses to clients. In this case, the coprocessor would need to be able to call RpcServer.getCurrentCall() to send back the response.
So I propose we add access to the RpcServer in RegionServerServices:
  /**
   * Returns a reference to the region server's RPC server
   */
  public RpcServer getRpcServer();
We can simultaneously drop the existing RegionServerServices.getRpcMetrics() method, since this could then be accessed via RpcServer.getRpcMetrics().",1,,,,,,,
HBASE-4441,"Keep Master up all the time, do rolling restart of RSs like this - stop RS1, wait for 2 seconds, stop RS2, start RS1, wait for 2 seconds, stop RS3, start RS2, wait for 2 seconds, etc. Region sometimes can just stay in OPENING state even after timeoutmonitor period.


2011-09-19 08:10:33,131 WARN org.apache.hadoop.hbase.master.AssignmentManager: While timing out a region in state OPENING, found ZK node in unexpected state: RS_ZK_REGION_FAILED_OPEN

The issue - RS was shutdown when a region is being opened, it was transitioned to RS_ZK_REGION_FAILED_OPEN in ZK. In timeoutmonitor, it didn't take care of RS_ZK_REGION_FAILED_OPEN.

processOpeningState
...
   else if (dataInZNode.getEventType() != EventType.RS_ZK_REGION_OPENING &&
        LOG.warn(""While timing out a region in state OPENING, ""
            + ""found ZK node in unexpected state: ""
            + dataInZNode.getEventType());
        return;
      }

",,,,,,,,
HBASE-4446,"Keep Master up all the time, do rolling restart of RSs like this - stop RS1, wait for 2 seconds, stop RS2, start RS1, wait for 2 seconds, stop RS3, start RS2, wait for 2 seconds, etc. Region sometimes can just stay in OPENING state even after timeoutmonitor period.


2011-09-19 08:10:33,131 WARN org.apache.hadoop.hbase.master.AssignmentManager: While timing out a region in state OPENING, found ZK node in unexpected state: RS_ZK_REGION_FAILED_OPEN

The issue - RS was shutdown when a region is being opened, it was transitioned to RS_ZK_REGION_FAILED_OPEN in ZK. In timeoutmonitor, it didn't take care of RS_ZK_REGION_FAILED_OPEN.

processOpeningState
...
   else if (dataInZNode.getEventType() != EventType.RS_ZK_REGION_OPENING &&
        LOG.warn(""While timing out a region in state OPENING, ""
            + ""found ZK node in unexpected state: ""
            + dataInZNode.getEventType());
        return;
      }

",,,,,,,,
HBASE-4449,"When LoadIncrementalHFiles loads a store file that crosses region boundaries, it will split the file at the boundary to create two store files. If the store file is for a column family that has a bloom filter, then a ""java.lang.ArithmeticException: / by zero"" will be raised because ByteBloomFilter() is called with maxKeys of 0.

The included patch assumes that the number of keys in each split child will be equal to the number of keys in the parent's bloom filter (instead of 0). This is an overestimate, but it's safe and easy.",,,,,,,,
HBASE-4455,"Keep Master up all the time, do rolling restart of RSs like this - stop RS1, wait for 2 seconds, stop RS2, start RS1, wait for 2 seconds, stop RS3, start RS2, wait for 2 seconds, etc. After a while, you will find the -ROOT-, .META. regions aren't in ""regions in transtion"" from AssignmentManager point of view, but they aren't assigned to any regions. Here are the issues.

1. .-ROOT- or .META. location is stale when MetaServerShutdownHandler is invoked to check if it contains -ROOT- region. That is due to long delay from ZK notification and async nature of the system. Here is an example, even though new root region server sea-lab-1,60020,1316380133656 is set at T2, at T3 the shutdown process for sea-lab-1,60020,1316380133656, the root location still points to old server sea-lab-3,60020,1316380037898.




T1: 2011-09-18 14:08:52,470 DEBUG org.apache.hadoop.hbase.zookeeper.ZKUtil: master:6
0000-0x1327e43175e0000 Retrieved 29 byte(s) of data from znode /hbase/root-regio
n-server and set watcher; sea-lab-3,60020,1316380037898

T2: 2011-09-18 14:08:57,173 INFO org.apache.hadoop.hbase.catalog.RootLocationEditor: Setting ROOT region location in ZooKeeper as sea-lab-1,60020,1316380133656


T3: 2011-09-18 14:10:26,393 DEBUG org.apache.hadoop.hbase.master.ServerManager: Adde
d=sea-lab-1,60020,1316380133656 to dead servers, submitted shutdown handler to be executed, root=false, meta=true, current Root Location: sea-lab-3,60020,1316380037898

T4: 2011-09-18 14:12:37,314 DEBUG org.apache.hadoop.hbase.zookeeper.ZKUtil: master:6
0000-0x1327e43175e0000 Retrieved 29 byte(s) of data from znode /hbase/root-region-server and set watcher; sea-lab-1,60020,1316380133656


2. The MetaServerShutdownHandler worker thread that waits for -ROOT- or .META. availability could be blocked. If meanwhile, the new server that -ROOT- or .META. is being assigned restarted, another instance of MetaServerShutdownHandler is queued. Eventually, all MetaServerShutdownHandler worker threads are filled up. It looks like HBASE-4245.

",,,,,,,,
HBASE-4457,"When  ""hbase.regionserver.info.port"" is set to non-default port, Master UI has broken URL's in the region server table because it's hard coded to default port.",,1,,,,,,
HBASE-4465,Lazy-seek optimization for StoreFile scanners],,,1,,,,,
HBASE-4470,"I'm surprised we still have issues like that and I didn't get a hit while googling so forgive me if there's already a jira about it.

When the master starts it verifies the locations of root and meta before assigning them, if the server is started but not running you'll get this:

{quote}
2011-09-23 04:47:44,859 WARN org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation: RemoteException connecting to RS
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hbase.ipc.ServerNotRunningException: Server is not running yet
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1038)

        at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:771)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:257)
        at $Proxy6.getProtocolVersion(Unknown Source)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:419)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:393)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:444)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.waitForProxy(HBaseRPC.java:349)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:969)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.getCachedConnection(CatalogTracker.java:388)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.getMetaServerConnection(CatalogTracker.java:287)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.verifyMetaRegionLocation(CatalogTracker.java:484)
        at org.apache.hadoop.hbase.master.HMaster.assignRootAndMeta(HMaster.java:441)
        at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:388)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:282)
{quote}

I hit that 3-4 times this week while debugging something else. The worst is that when you restart the master it sees that as a failover, but none of the regions are assigned so it takes an eternity to get back fully online.",,1,,,,,,
HBASE-4472,"Running tests over in HBASE-4014 brought up this issue.  If the active master in a MiniHBaseCluster has aborted, then calling MiniHBaseCluster.shutdown() will just hang in JVMClusterUtil.shutdown(), waiting to join each of the region server threads.

Seems like we should explicitly stop each region server instead of just relying on an active master instance deleting the cluster status znode?",,,,,,,,
HBASE-4473,"Minor annoyance when shutting down a cluster and the master is still receiving events from Zookeeper:
{quote}
2011-09-22 23:53:01,552 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher: master:60000-0x3292d87deb004f Received InterruptedException, doing nothing here
java.lang.InterruptedException
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:485)
        at org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1317)
        at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:726)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.deleteNode(ZKUtil.java:938)
        at org.apache.hadoop.hbase.zookeeper.ZKAssign.deleteNode(ZKAssign.java:407)
        at org.apache.hadoop.hbase.zookeeper.ZKAssign.deleteOpenedNode(ZKAssign.java:284)
        at org.apache.hadoop.hbase.master.handler.OpenedRegionHandler.process(OpenedRegionHandler.java:88)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:156)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
...
2011-09-22 23:53:01,558 DEBUG org.apache.hadoop.hbase.executor.ExecutorService: Executor service [MASTER_OPEN_REGION-sv2borg170:60000] not found in {}
2011-09-22 23:53:01,558 ERROR org.apache.zookeeper.ClientCnxn: Error while calling watcher
java.lang.NullPointerException
        at org.apache.hadoop.hbase.executor.ExecutorService.submit(ExecutorService.java:220)
        at org.apache.hadoop.hbase.master.AssignmentManager.handleRegion(AssignmentManager.java:447)
        at org.apache.hadoop.hbase.master.AssignmentManager.nodeDataChanged(AssignmentManager.java:546)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:281)
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:530)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:506)
{quote}

It's annoying because it then spams you with a bunch of NPEs that have nothing to do with the reason the Master is shutting down. Googling I saw someone also had that issue in June: http://pastebin.com/5Tqrj0nq",,,,,,,,
HBASE-4476,"We found this in ScanWildcardColumnTracker:

    // new col < oldcol
    // if (cmp < 0) {
    // WARNING: This means that very likely an edit for some other family
    // was incorrectly stored into the store for this one. Continue, but
    // complain.
    LOG.error(""ScanWildcardColumnTracker.checkColumn ran "" +
  		""into a column actually smaller than the previous column: "" +

This went under the radar in our dark launch cluster when a column family name was misspelled first, but then was ""renamed"" by renaming directories in the HBase storage directory tree. We ended up with inconsistent data, but compactions still succeeded most of the time, likely discarding part of input data.
",,,,,,,,
HBASE-4482,ace Condition Concerning Eviction in SlabCache,,,,,,,,
HBASE-4487,The increment operation can release the rowlock before sync-ing the Hlog,,,1,,,,,
HBASE-4501,"When removing a peer it will call ReplicationSourceManager.removePeer which calls closeRecoveredQueue which does this:

{code}
LOG.info(""Done with the recovered queue "" + src.getPeerClusterZnode());
this.oldsources.remove(src);
this.zkHelper.deleteSource(src.getPeerClusterZnode(), false);
{code}

This works in the case where the recovered source is done and is calling this method, but when removing a peer it never calls terminate on thus it leaving it running.",,1,,,,,,
HBASE-4509,hbck] Improve region map output,,,1,,,,,
HBASE-4511,"It goes like this:

Master crashed ,  at the same time RS with meta is crashing, but RS doesn't eixt.
Master startups again and finds all living RS. 
Master verifies the meta failed,  because this RS is crashing.
Master reassigns the meta, but it doesn't split the Hlog. 

So some meta data is loss.

About the logs of a failover test case fail. 

//It said that we want to kill a RS

2011-09-28 19:54:45,694 INFO  [Thread-988] regionserver.HRegionServer(1443): STOPPED: Killing for unit test
2011-09-28 19:54:45,694 INFO  [Thread-988] master.TestMasterFailover(1007): 

RS 192.168.2.102,54385,1317264874629 killed 

//Rs didn't crash. 
2011-09-28 19:54:51,763 INFO  [Master:0;192.168.2.102,54557,1317264885720] master.HMaster(458): Registering server found up in zk: 192.168.2.102,54385,1317264874629
2011-09-28 19:54:51,763 INFO  [Master:0;192.168.2.102,54557,1317264885720] master.ServerManager(232): Registering server=192.168.2.102,54385,1317264874629
2011-09-28 19:54:51,770 DEBUG [Master:0;192.168.2.102,54557,1317264885720] zookeeper.ZKUtil(491): master:54557-0x132b31adbb30005 Unable to get data of znode /hbase/unassigned/1028785192 because node does not exist (not an error)
2011-09-28 19:54:51,771 DEBUG [Master:0;192.168.2.102,54557,1317264885720] zookeeper.ZKUtil(1003): master:54557-0x132b31adbb30005 Retrieved 33 byte(s) of data from znode /hbase/root-region-server and set watcher; 192.168.2.102,54383,131726487...

//Meta verification failed and ressigned the meta. So all the regions in the meta is loss.

2011-09-28 19:54:51,773 INFO  [Master:0;192.168.2.102,54557,1317264885720] catalog.CatalogTracker(476): Failed verification of .META.,,1 at address=192.168.2.102,54385,1317264874629; org.apache.hadoop.hbase.regionserver.RegionServerStoppedException: org.apache.hadoop.hbase.regionserver.RegionServerStoppedException: Server 192.168.2.102,54385,1317264874629 not running, aborting
2011-09-28 19:54:51,773 DEBUG [Master:0;192.168.2.102,54557,1317264885720] catalog.CatalogTracker(316): new .META. server: 192.168.2.102,54385,1317264874629 isn't valid. Cached .META. server: null
2011-09-28 19:54:52,274 DEBUG [Master:0;192.168.2.102,54557,1317264885720] zookeeper.ZKUtil(1003): master:54557-0x132b31adbb30005 Retrieved 33 byte(s) of data from znode /hbase/root-region-server and set watcher; 192.168.2.102,54383,131726487...
2011-09-28 19:54:52,277 INFO  [Master:0;192.168.2.102,54557,1317264885720] catalog.CatalogTracker(476): Failed verification of .META.,,1 at address=192.168.2.102,54385,1317264874629; org.apache.hadoop.hbase.regionserver.RegionServerStoppedException: org.apache.hadoop.hbase.regionserver.RegionServerStoppedException: Server 192.168.2.102,54385,1317264874629 not running, aborting
2011-09-28 19:54:52,277 DEBUG [Master:0;192.168.2.102,54557,1317264885720] catalog.CatalogTracker(316): new .META. server: 192.168.2.102,54385,1317264874629 isn't valid. Cached .META. server: null
2011-09-28 19:54:52,778 DEBUG [Master:0;192.168.2.102,54557,1317264885720] zookeeper.ZKUtil(1003): master:54557-0x132b31adbb30005 Retrieved 33 byte(s) of data from znode /hbase/root-region-server and set watcher; 192.168.2.102,54383,131726487...
2011-09-28 19:54:52,782 INFO  [Master:0;192.168.2.102,54557,1317264885720] catalog.CatalogTracker(476): Failed verification of .META.,,1 at address=192.168.2.102,54385,1317264874629; org.apache.hadoop.hbase.regionserver.RegionServerStoppedException: org.apache.hadoop.hbase.regionserver.RegionServerStoppedException: Server 192.168.2.102,54385,1317264874629 not running, aborting
2011-09-28 19:54:52,782 DEBUG [Master:0;192.168.2.102,54557,1317264885720] catalog.CatalogTracker(316): new .META. server: 192.168.2.102,54385,1317264874629 isn't valid. Cached .META. server: null
2011-09-28 19:54:52,782 DEBUG [Master:0;192.168.2.102,54557,1317264885720] zookeeper.ZKAssign(264): master:54557-0x132b31adbb30005 Creating (or updating) unassigned node for 1028785192 with OFFLINE state
2011-09-28 19:54:52,825 DEBUG [Thread-988-EventThread] zookeeper.ZooKeeperWatcher(233): master:54557-0x132b31adbb30005 Received ZooKeeper Event, type=NodeCreated, state=SyncConnected, path=/hbase/unassigned/1028785192


//It said that Master clean the cluster.
2011-09-28 19:54:52,889 INFO  [Master:0;192.168.2.102,54557,1317264885720] master.AssignmentManager(383): Clean cluster startup. Assigning userregions
2011-09-28 19:54:52,889 DEBUG [Master:0;192.168.2.102,54557,1317264885720] zookeeper.ZKAssign(494): master:54557-0x132b31adbb30005 Deleting any existing unassigned nodes",,,,,,,,
HBASE-4512,"In JVMClusterUtil.MasterThread createMasterThread
{code}
throw new RuntimeException(""Failed construction of RegionServer: "" +
{code}
It must be failed construction of Master.
Very trivial one.",,,,,,,,
HBASE-4515,"When testing with miniclusters that shutdown and are restarted, sometimes a call to User.getCurrent().getName() NPEs when attempting to restart hbase.  Oddly this happens consistently on particular branches and not on others. I don't know or understand why this happens but it has something to do with the getCurrentUGI call in  o.a.h.h.security.User.HadoopUser sometimes returning null and sometimes returning data.

{code}
   private HadoopUser() {
      try {
        ugi = (UserGroupInformation) callStatic(""getCurrentUGI"");
        if (ugi == null) {
          LOG.warn(""Although successfully retrieved UserGroupInformation"" 
              + ""  it was null!"");
        }
      } catch (RuntimeException re) {
{code}

This patch essentially is a workaround -- it propagates the null so that clients can check and avoid the NPE.",,,,,,,,
HBASE-4519,25s sleep when expiring sessions in tests,,1,1,,,,,
HBASE-4528,The put operation can release the rowlock before sync-ing the Hlog],,,1,,,,,
HBASE-4532,Avoid top row seek by dedicated bloom filter for delete family bloom filter],,,1,,,,,
HBASE-4539,"Steps to reproduce
==================
-> Region R1 is being opened in RS1.  
->After processing the znode to OPENED RS1 goes down.
->Now before the OpenedRegionHandler executor deletes the znode if ServerShutDownHandler tries to assign the region to RS2, RS2 transits the node to OPENED and this OpenedRegionHandler executor deletes the znode.  
->Now if the first OpenedRegionHandler tries deleting the znode it throws NoNode Exception and causes the HMaster to abort.

",,,,,,,,
HBASE-4536,[Allow CF to retain deleted rows],,,,,,,,
HBASE-4536,OpenedRegionHandler racing with itself in ServerShutDownhandler flow leading to HMaster abort],,,,,,,,
HBASE-4540,"-> OpenedRegionHandler has not yet deleted the znode of the region R1 opened by RS1.
-> RS1 goes down.
-> Servershutdownhandler assigns the region R1 to RS2.
-> The znode of R1 is moved to OFFLINE state by master or OPENING state by RS2 if RS2 has started",,,,,,,,
HBASE-4550,"when master passed regionserver different address, regionserver didn't create new zookeeper znode, master store new address in ServerManager, when call stop-hbase.sh , RegionServerTracker.nodeDeleted received path is old address, serverManager.expireServer is not be called. so stop-hbase.sh is hang.",,1,,,,,,
HBASE-4553,The update of .tableinfo is not atomic; we remove then rename],,1,,,,,,
HBASE-4562,"Follow below steps to replay the problem:
1. change the SplitTransaction.java as below,just like mock the timeout error.
   {code:title=SplitTransaction.java|borderStyle=solid}
      if (!testing) {
        MetaEditor.offlineParentInMeta(server.getCatalogTracker(),
           this.parent.getRegionInfo(), a.getRegionInfo(), b.getRegionInfo());
        throw new IOException(""some unexpected error in split"");
      }
   {code} 
2. update the regionserver code,restart;
3. create a table & put some data to the table;
4. split the table;
5. kill the regionserver hosted the table;
6. wait some time after master ServerShutdownHandler.process execute,then scan the table,u'll find the data wrote before lost.

We can fix the bug just use the patch.",,,,,,,,
HBASE-4563,"Follow below steps to replay the problem:
1. change the SplitTransaction.java as below,just like mock the hdfs error.
   {code:title=SplitTransaction.java|borderStyle=solid}
      List<StoreFile> hstoreFilesToSplit = this.parent.close(false);
      throw new IOException(""some unexpected error in close store files"");
   {code} 
2. update the regionserver code,restart;
3. create a table & put some data to the table;
4. split the table;
5. scan the table,then it'll fail.

We can fix the bug just use the patch.",,,,,,,,
HBASE-4568,"Follow below steps to replay the problem:
1. change the SplitTransaction.java as below,just like mock the hdfs error.
   {code:title=SplitTransaction.java|borderStyle=solid}
      List<StoreFile> hstoreFilesToSplit = this.parent.close(false);
      throw new IOException(""some unexpected error in close store files"");
   {code} 
2. update the regionserver code,restart;
3. create a table & put some data to the table;
4. split the table;
5. scan the table,then it'll fail.

We can fix the bug just use the patch.",,,1,,,,,
HBASE-4570,"When scanning a table sometimes rows that have multiple column families get split into two rows if there are concurrent writes.  In this particular case we are overwriting the contents of a Get directly back onto itself as a Put.

For example, this is a two cf row (with ""f1"", ""f2"", .. ""f9"" cfs).  It is actually returned as two rows (#55 and #56). Interestingly if the two were merged we would have a single proper row.

Row row0000024461 had time stamps: [55: keyvalues={row0000024461/f0:data/1318200440867/Put/vlen=1000, row0000024461/f0:qual/1318200440867/Put/vlen=10, row0000024461/f1:data/1318200440867/Put/vlen=1000, row0000024461/f1:qual/1318200440867/Put/vlen=10, row0000024461/f2:data/1318200440867/Put/vlen=1000, row0000024461/f2:qual/1318200440867/Put/vlen=10, row0000024461/f3:data/1318200440867/Put/vlen=1000, row0000024461/f3:qual/1318200440867/Put/vlen=10, row0000024461/f4:data/1318200440867/Put/vlen=1000, row0000024461/f4:qual/1318200440867/Put/vlen=10}, 
56: keyvalues={row0000024461/f5:data/1318200440867/Put/vlen=1000, row0000024461/f5:qual/1318200440867/Put/vlen=10, row0000024461/f6:data/1318200440867/Put/vlen=1000, row0000024461/f6:qual/1318200440867/Put/vlen=10, row0000024461/f7:data/1318200440867/Put/vlen=1000, row0000024461/f7:qual/1318200440867/Put/vlen=10, row0000024461/f8:data/1318200440867/Put/vlen=1000, row0000024461/f8:qual/1318200440867/Put/vlen=10, row0000024461/f9:data/1318200440867/Put/vlen=1000, row0000024461/f9:qual/1318200440867/Put/vlen=10}]

I've only tested this on 0.90.1+patches and 0.90.3+patches, but it is consistent and duplicatable.",,,,,,,,
HBASE-4578,"I'm still not a 100% sure on the source of this error, but here's what I was able to get twice while altering a table that was doing a bunch of splits:

{quote}

2011-10-11 23:48:59,344 INFO org.apache.hadoop.hbase.master.handler.SplitRegionHandler: Handled SPLIT report); parent=TestTable,0002608338,1318376880454.a75d6815fdfc513fb1c8aabe086c6763. daughter a=TestTable,0002608338,1318376938764.ef170ff6cd8695dc8aec92e542dc9ac1.daughter b=TestTable,0003301408,1318376938764.36eb2530341bd46888ede312c5559b5d.
2011-10-11 23:49:09,579 DEBUG org.apache.hadoop.hbase.master.handler.TableEventHandler: Ignoring table not disabled exception for supporting online schema changes.
2011-10-11 23:49:09,580 INFO org.apache.hadoop.hbase.master.handler.TableEventHandler: Handling table operation C_M_MODIFY_TABLE on table TestTable
2011-10-11 23:49:09,612 INFO org.apache.hadoop.hbase.util.FSUtils: TableInfoPath = hdfs://sv4r11s38:9100/hbase/TestTable/.tableinfo tmpPath = hdfs://sv4r11s38:9100/hbase/TestTable/.tmp/.tableinfo.1318376949612
2011-10-11 23:49:09,692 INFO org.apache.hadoop.hbase.util.FSUtils: TableDescriptor stored. TableInfoPath = hdfs://sv4r11s38:9100/hbase/TestTable/.tableinfo
2011-10-11 23:49:09,693 INFO org.apache.hadoop.hbase.util.FSUtils: Updated tableinfo=hdfs://sv4r11s38:9100/hbase/TestTable/.tableinfo to blah
2011-10-11 23:49:09,695 INFO org.apache.hadoop.hbase.master.handler.TableEventHandler: Bucketing regions by region server...
2011-10-11 23:49:09,695 DEBUG org.apache.hadoop.hbase.client.MetaScanner: Scanning .META. starting at row=TestTable,,00000000000000 for max=2147483647 rows
2011-10-11 23:49:09,709 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation: The connection to hconnection-0x132f043bbde02e9 has been closed.
2011-10-11 23:49:09,709 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event C_M_MODIFY_TABLE
java.lang.NullPointerException
	at java.util.TreeMap.getEntry(TreeMap.java:324)
	at java.util.TreeMap.containsKey(TreeMap.java:209)
	at org.apache.hadoop.hbase.master.handler.TableEventHandler.reOpenAllRegions(TableEventHandler.java:114)
	at org.apache.hadoop.hbase.master.handler.TableEventHandler.process(TableEventHandler.java:90)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:168)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)

{quote}

The first time the shell reported that all the regions were updated correctly, the second time it got stuck for a while:

{quote}

6/14 regions updated.
0/14 regions updated.
...
0/14 regions updated.
2/16 regions updated.
...
2/16 regions updated.
8/9 regions updated.
...
8/9 regions updated.
{quote}

After which I killed it, redid the alter and it worked.",,,,,,,,
HBASE-4579,"Another bug I'm not so sure what's going on. I see this in my log:

{quote}
2011-10-12 00:23:43,435 DEBUG org.apache.hadoop.hbase.regionserver.Store: info: no store files to compact
2011-10-12 00:23:44,335 DEBUG org.apache.hadoop.hbase.regionserver.Store: info: no store files to compact
2011-10-12 00:23:45,236 DEBUG org.apache.hadoop.hbase.regionserver.Store: info: no store files to compact
2011-10-12 00:23:46,136 DEBUG org.apache.hadoop.hbase.regionserver.Store: info: no store files to compact
2011-10-12 00:23:47,036 DEBUG org.apache.hadoop.hbase.regionserver.Store: info: no store files to compact
2011-10-12 00:23:47,936 DEBUG org.apache.hadoop.hbase.regionserver.Store: info: no store files to compact
{quote}

It spams for a while, and a little later instead I get:

{quote}
2011-10-12 00:26:52,139 DEBUG org.apache.hadoop.hbase.regionserver.Store: Skipped compaction of info.  Only 2 file(s) of size 176.4m have met compaction criteria.
2011-10-12 00:26:53,040 DEBUG org.apache.hadoop.hbase.regionserver.Store: Skipped compaction of info.  Only 2 file(s) of size 176.4m have met compaction criteria.
2011-10-12 00:26:53,940 DEBUG org.apache.hadoop.hbase.regionserver.Store: Skipped compaction of info.  Only 2 file(s) of size 176.4m have met compaction criteria.
2011-10-12 00:26:54,840 DEBUG org.apache.hadoop.hbase.regionserver.Store: Skipped compaction of info.  Only 2 file(s) of size 176.4m have met compaction criteria.
2011-10-12 00:26:55,741 DEBUG org.apache.hadoop.hbase.regionserver.Store: Skipped compaction of info.  Only 2 file(s) of size 176.4m have met compaction criteria.
2011-10-12 00:26:56,641 DEBUG org.apache.hadoop.hbase.regionserver.Store: Skipped compaction of info.  Only 2 file(s) of size 176.4m have met compaction criteria.
{quote}

I believe I also saw something like that for flushes, but the region was closing so at least I know why it was spamming (would be nice if it just unrequested the flush):

{quote}
2011-10-12 00:26:40,693 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Flush requested on TestTable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5.
2011-10-12 00:26:40,694 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: NOT flushing memstore for region TestTable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5., flushing=false, writesEnabled=false
2011-10-12 00:26:40,733 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Flush requested on TestTable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5.
2011-10-12 00:26:40,733 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: NOT flushing memstore for region TestTable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5., flushing=false, writesEnabled=false
2011-10-12 00:26:40,873 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Flush requested on TestTable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5.
2011-10-12 00:26:40,873 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: NOT flushing memstore for region TestTable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5., flushing=false, writesEnabled=false
2011-10-12 00:26:40,873 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Flush requested on TestTable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5.
2011-10-12 00:26:40,873 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: NOT flushing memstore for region TestTable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5., flushing=false, writesEnabled=false
2011-10-12 00:26:40,921 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Flush requested on TestTable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5.
2011-10-12 00:26:40,922 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: NOT flushing memstore for region TestTable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5., flushing=false, writesEnabled=false
2011-10-12 00:26:40,923 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Flush requested on TestTable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5.
2011-10-12 00:26:40,923 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: NOT flushing memstore for region TestTable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5., flushing=false, writesEnabled=false
2011-10-12 00:26:40,923 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Flush requested on TestTable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5.
2011-10-12 00:26:40,923 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: NOT flushing memstore for region TestTable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5., flushing=false, writesEnabled=false
{quote}",,1,,,,,,
HBASE-4580,"The below logs said that we created a invalid zk node when restarted a cluster.
it mistakenly believed that the regions belong to a dead server.

2011-10-11 05:05:29,127 INFO org.apache.hadoop.hbase.master.HMaster: Meta updated status = true
2011-10-11 05:05:29,127 INFO org.apache.hadoop.hbase.master.HMaster: ROOT/Meta already up-to date with new HRI.
2011-10-11 05:05:29,151 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Creating (or updating) unassigned node for 771d63e9327383159553619a4f2dc74f with OFFLINE state
2011-10-11 05:05:29,161 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Creating (or updating) unassigned node for 3cf860dd323fe6360f571aeafc129f95 with OFFLINE state
2011-10-11 05:05:29,170 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Creating (or updating) unassigned node for 4065350214452a9d5c55243c734bef08 with OFFLINE state
2011-10-11 05:05:29,178 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Creating (or updating) unassigned node for 4e81613f82a39fc6e5e89f96e7b3ccc4 with OFFLINE state
2011-10-11 05:05:29,187 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Creating (or updating) unassigned node for e21b9e1545a28953aba0098fda5c9cd9 with OFFLINE state
2011-10-11 05:05:29,195 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Creating (or updating) unassigned node for 5cd9f55eecd43d088bbd505f6795131f with OFFLINE state
2011-10-11 05:05:29,229 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Creating (or updating) unassigned node for db5f641452a70b09b85a92970e4198c7 with OFFLINE state
2011-10-11 05:05:29,237 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Creating (or updating) unassigned node for a7b20a653919e7f41bfb2ed349af7d21 with OFFLINE state
2011-10-11 05:05:29,253 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Creating (or updating) unassigned node for c9385619425f737eab1a6624d2e097a8 with OFFLINE state

// we cleaned all zk nodes.
2011-10-11 05:05:29,262 INFO org.apache.hadoop.hbase.master.AssignmentManager: Clean cluster startup. Assigning userregions
2011-10-11 05:05:29,262 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Deleting any existing unassigned nodes
2011-10-11 05:05:29,367 INFO org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 9 region(s) across 1 server(s), retainAssignment=true
2011-10-11 05:05:29,369 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Timeout-on-RIT=9000
2011-10-11 05:05:29,369 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 9 region(s) to C3S3,54366,1318323920153
2011-10-11 05:05:29,369 INFO org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning done
2011-10-11 05:05:29,371 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Async create of unassigned node for 771d63e9327383159553619a4f2dc74f with OFFLINE state
2011-10-11 05:05:29,371 INFO org.apache.hadoop.hbase.master.HMaster: Master has completed initialization
2011-10-11 05:05:29,371 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Async create of unassigned node for 3cf860dd323fe6360f571aeafc129f95 with OFFLINE state
2011-10-11 05:05:29,371 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Async create of unassigned node for 4065350214452a9d5c55243c734bef08 with OFFLINE state
2011-10-11 05:05:29,371 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Async create of unassigned node for 4e81613f82a39fc6e5e89f96e7b3ccc4 with OFFLINE state
2011-10-11 05:05:29,371 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Async create of unassigned node for e21b9e1545a28953aba0098fda5c9cd9 with OFFLINE state
2011-10-11 05:05:29,372 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Async create of unassigned node for 5cd9f55eecd43d088bbd505f6795131f with OFFLINE state
2011-10-11 05:05:29,372 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Async create of unassigned node for db5f641452a70b09b85a92970e4198c7 with OFFLINE state
2011-10-11 05:05:29,372 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Async create of unassigned node for a7b20a653919e7f41bfb2ed349af7d21 with OFFLINE state
2011-10-11 05:05:29,372 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Async create of unassigned node for c9385619425f737eab1a6624d2e097a8 with OFFLINE state
",,,,,,,,
HBASE-4583,"Currently Increment and Append operations do not work with RWCC and hence a client could see the results of multiple such operation mixed in the same Get/Scan.
The semantics might be a bit more interesting here as upsert adds and removes to and from the memstore.
",,,1,,,,,
HBASE-4590,"In the following case:

1.first,kill all the regionservers
2.then, start all the regionservers immediately
3.then, kill all the regionservers immediately
4.then, start all the regionservers immediately

Master may assign META region twice , and make two regionserver carrying META region.

Through logs, we find that:
Before 1, Regionserver A carrys the META region,

Between 2 and 3 , master receives Regionserver A startup message and is ready to assign meta region to Regionserver B

But, it fails because of 3, and then reassign meta region to Regionserver C,Then ,it is successful after 4,

when 4 , Regionserver C is started earlier than Regionserver A , and Regionserver A is started earlier than Regionserver C successfully online the META region. Therefore, when master receives Regionserver A startup message in 4, it consider Regionserver A carryed META region through  CatalogTracker and create a MetaServerShutdownHandler

In MetaServerShutdownHandler's process(), after completing split hlog, Regionserver C has already onlined META region,
however it will execute the following code all the same where isCarryingMeta() return true :
if (isCarryingMeta())this.services.getAssignmentManager().assignMeta();

After that, META region will online on two regionservers


In order to prevent it , we should verify meta region location before assign Meta region .
",,,,,,,,
HBASE-4603,"Currently Increment and Append operations do not work with RWCC and hence a client could see the results of multiple such operation mixed in the same Get/Scan.
The semantics might be a bit more interesting here as upsert adds and removes to and from the memstore.
",,,1,,,,,
HBASE-4608,"Over in HBASE-3380 we were having some TestMasterFailover flakiness.  We added some more config parameters to better control the master startup loop where it waits for RS to heartbeat in.  We had thought at the time that 92 would have a different solution but it is still relying on heartbeats to learn about RSs.

For now, we should definitely bring these config params into 92/trunk.  Otherwise this is an incompatible regression and adding these will also make things like what was just reported over in HBASE-4603 trivial to fix in an optimal way.",,,1,,,,,
HBASE-4610,"Over in HBASE-3380 we were having some TestMasterFailover flakiness.  We added some more config parameters to better control the master startup loop where it waits for RS to heartbeat in.  We had thought at the time that 92 would have a different solution but it is still relying on heartbeats to learn about RSs.

For now, we should definitely bring these config params into 92/trunk.  Otherwise this is an incompatible regression and adding these will also make things like what was just reported over in HBASE-4603 trivial to fix in an optimal way.",,,,,,,,
HBASE-4626,Just looked at SingleCol and ValueFilter... And on every column compared they create a copy of the column and/or value portion of the KV.,,,1,,,,,
HBASE-4633,"Relevant Jiras: https://issues.apache.org/jira/browse/HBASE-2937,
https://issues.apache.org/jira/browse/HBASE-4003

We have been using the 'hbase.client.operation.timeout' knob
introduced in 2937 for quite some time now. It helps us enforce SLA.
We have two HBase clusters and two HBase client clusters. One of them
is much busier than the other.

We have seen a deterministic behavior of clients running in busy
cluster. Their (client's) memory footprint increases consistently
after they have been up for roughly 24 hours.
This memory footprint almost doubles from its usual value (usual case
== RPC timeout disabled). After much investigation nothing concrete
came out and we had to put a hack
which keep heap size in control even when RPC timeout is enabled. Also
note , the same behavior is not observed in 'not so busy
cluster.

The patch is here : https://gist.github.com/1288023",,,,,,,,
HBASE-4634,"""test.build.data"" is overloaded in HBase.At the beginning, it's the ""Default parent directory for test output."", but then it's rewritten to be the directory itself in functions like HBaseTestingUtility#startMiniDFSCluster

It seems that this value is already used by MiniDFS (i.e. outside of HBase): 
""Name is as it is because mini dfs has hard-codings to put test data here.""

As it is today, there is at least a bug in HBaseTestingUtility:

{noformat}
  public void initTestDir() {
    if (System.getProperty(TEST_DIRECTORY_KEY) == null) {
      clusterTestBuildDir = setupClusterTestBuildDir();
      System.setProperty(TEST_DIRECTORY_KEY, clusterTestBuildDir.getPath());
    }
  }
{noformat}

if you set a value for ""test.build.data"", the test dir will be the parent directory and not a temp subdir, leading to issues as multiple tests will end-ups in the same (bad) directory. This function is barely used today, hence it's not visible, but I would like to use it in some new code.

A possible fix is to remove the check for null and continue with the overloading, but I don't think it would be a big issue to create a new key(like ""test.build.data.rootdirectory"") specific to the root directory and to use ""test.build.data"" only to communicate with MiniDFS. Feedback welcome.",,1,,,,,,
HBASE-4641,"After changes in the block cache instantiation over in HBASE-4422, it looks like the HMaster can now end up with a block cache instantiated.  Not a huge deal but prevents the process from shutting down properly.",,1,,,,,,
HBASE-4645,"There is a data loss happening (for some of the column families) when we do the replay logs.

The bug seems to be from the fact that during replay-logs we only choose to replay
the logs from the maximumSequenceID across *ALL* the stores. This is wrong. If a
column family is ahead of others (because the crash happened before all the column
families were flushed), then we lose data for the column families that have not yet
caught up.

The correct logic for replay should begin the replay from the minimum across the
maximum in each store. ",,,,,,,,
HBASE-4662,"When /etc/hosts contains following lines (and this is not uncommon) it will cause HBaseTestingUtility to malfunction.
127.0.0.1	localhost
127.0.1.1	myMachineName

Symptoms:
2011-10-25 17:38:30,875 WARN  master.AssignmentManager - Failed assignment of -ROOT-,,0.70236052 to serverName=localhost,34462,1319557102914, load=(requests=0, regions=0, usedHeap=46, maxHeap=865), trying to assign elsewhere instead; retry=0
org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed setting up proxy interface org.apache.hadoop.hbase.ipc.HRegionInterface to /127.0.0.1:34462 after attempts=1

because

2011-10-25 17:38:28,371 INFO  regionserver.HRegionServer - Serving as localhost,34462,1319557102914, RPC listening on /127.0.1.1:34462, sessionid=0x1333bbb7a180002

caused by /127.0.0.1:34462 vs /127.0.1.1:34462

Workaround:
Changing 127.0.1.1 to 127.0.0.1 works.

Permanent solution:
Dunno, my understanding of inner workings is not sufficient enough. Although it seems like it has something to do with changing the machine name from myMachineName to localhost during the test:
2011-10-25 17:38:28,056 INFO  regionserver.HRegionServer - Master passed us address to use. Was=myMachineName:34462, Now=localhost:34462",,1,,,,,,
HBASE-4671,"When /etc/hosts contains following lines (and this is not uncommon) it will cause HBaseTestingUtility to malfunction.
127.0.0.1	localhost
127.0.1.1	myMachineName

Symptoms:
2011-10-25 17:38:30,875 WARN  master.AssignmentManager - Failed assignment of -ROOT-,,0.70236052 to serverName=localhost,34462,1319557102914, load=(requests=0, regions=0, usedHeap=46, maxHeap=865), trying to assign elsewhere instead; retry=0
org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed setting up proxy interface org.apache.hadoop.hbase.ipc.HRegionInterface to /127.0.0.1:34462 after attempts=1

because

2011-10-25 17:38:28,371 INFO  regionserver.HRegionServer - Serving as localhost,34462,1319557102914, RPC listening on /127.0.1.1:34462, sessionid=0x1333bbb7a180002

caused by /127.0.0.1:34462 vs /127.0.1.1:34462

Workaround:
Changing 127.0.1.1 to 127.0.0.1 works.

Permanent solution:
Dunno, my understanding of inner workings is not sufficient enough. Although it seems like it has something to do with changing the machine name from myMachineName to localhost during the test:
2011-10-25 17:38:28,056 INFO  regionserver.HRegionServer - Master passed us address to use. Was=myMachineName:34462, Now=localhost:34462",,1,,,,,,
HBASE-4673,"On a test system got this exception when hfile.block.cache.size is set to 0:

java.lang.NullPointerException
at org.apache.hadoop.hbase.io.hfile.HFileReaderV2.close(HFileReaderV2.java:321)
at org.apache.hadoop.hbase.regionserver.StoreFile$Reader.close(StoreFile.java:1065)
at org.apache.hadoop.hbase.regionserver.StoreFile.closeReader(StoreFile.java:539)
at org.apache.hadoop.hbase.regionserver.StoreFile.deleteReader(StoreFile.java:549)
at org.apache.hadoop.hbase.regionserver.Store.completeCompaction(Store.java:1314)
at org.apache.hadoop.hbase.regionserver.Store.compact(Store.java:686)
at org.apache.hadoop.hbase.regionserver.HRegion.compact(HRegion.java:1016)
at org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.run(CompactionRequest.java:178)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:619) 

Minor issue as nobody in their right mind with have hfile.block.cache.size=0

Looks like this is due to HBASE-4422",,1,,,,,,
HBASE-4676,"When using null as a value for a mutation, HBasse thrift client failed and threw an error. We should instad check for a null byte buffer.
",,1,,,,,,
HBASE-4679,"When using null as a value for a mutation, HBasse thrift client failed and threw an error. We should instad check for a null byte buffer.
",,1,,,,,,
HBASE-4680,"The HDFS safe mode check workaround introduced by HBASE-4510 performs a {{FileSystem.setPermission()}} operation on the root directory (""/"") when attempting to trigger a {{SafeModeException}}.  As a result, it requires superuser privileges when running with DFS permission checking enabled.  Changing the operations to act on the HBase root directory should be safe, since the master process must have write access to it.",1,,,,,,,
HBASE-4683,"In r1182034 per-Store metrics were broken, because the aggregation of StoreFile metrics over all stores in a region was replaced by overriding them every time. We saw these metrics drop by a factor of numRegions on a production cluster -- thanks to Kannan for noticing this!  We need to fix the metrics and add a unit test to ensure regressions like this don't happen in the future.",,,1,,,,,
HBASE-4686,"In r1182034 per-Store metrics were broken, because the aggregation of StoreFile metrics over all stores in a region was replaced by overriding them every time. We saw these metrics drop by a factor of numRegions on a production cluster -- thanks to Kannan for noticing this!  We need to fix the metrics and add a unit test to ensure regressions like this don't happen in the future.",,,,,,,,
HBASE-4690,"In r1182034 per-Store metrics were broken, because the aggregation of StoreFile metrics over all stores in a region was replaced by overriding them every time. We saw these metrics drop by a factor of numRegions on a production cluster -- thanks to Kannan for noticing this!  We need to fix the metrics and add a unit test to ensure regressions like this don't happen in the future.",,1,,,,,,
HBASE-4695,"To replicate the problem do the following:

1. check /hbase/.logs/XXXX directory to see if you have WAL logs for the region server you are shutting down.
2. executing kill <pid> (where pid is a regionserver pid)
3. Watch the regionserver log to start flushing, you will see how many regions are left to flush:

09:36:54,665 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Waiting on 489 regions to close
09:56:35,779 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Waiting on 116 regions to close

4. Check /hbase/.logs/XXXX -- you will notice that it has dissapeared.
5. Check namenode logs:

09:26:41,607 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit: ugi=root ip=/10.101.1.5 cmd=delete src=/hbase/.logs/rdaa5.prod.imageshack.com,60020,1319749

Note that, if you kill -9 the RS now, and it crashes on flush, you won't have any WAL logs to replay.  We need to make sure that logs are deleted or moved out only when RS has fully flushed. Otherwise its possible to lose data.",,,,,,,,
HBASE-4700,"TestSplitTransactionOnCluster tries to have meta on one server and table region on a different server.  If the luck of the draw has the table and meta on same server, it'll move the table region elsewhere.  This has been failing since hbase-4300 because we've been doing HBaseAdmin#move passing versioned ServerName bytes when HBaseAdmin#move expects the raw, unversioned bytes.",,,,,,,,
HBASE-4707,"We recently hit an issue where upon each RS heartbeat we were looking up and resolving DNS name + address of the RS in the master, but needed it only for locality based assignment on startup.

Some flakiness in the DNS subsystem cause one of the threads to get stuck in the lookup and the synchronized call at:

ServerManager.java:528
processMsgs() {
...
synchronized (this.master.getRegionManager()) {
// does dns lookup
}
}

The offending stack trace was:

""IPC Server handler 232 on 60000"" daemon prio=10 tid=0x00007fcb64164000 nid=0x7d16 runnable [0x0000000052e7f000]
java.lang.Thread.State: RUNNABLE
at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)
at java.net.InetAddress$1.lookupAllHostAddr(InetAddress.java:849)
at java.net.InetAddress.getAddressFromNameService(InetAddress.java:1200)
at java.net.InetAddress.getAllByName0(InetAddress.java:1153)
at java.net.InetAddress.getAllByName0(InetAddress.java:1128)
at java.net.InetAddress.getHostFromNameService(InetAddress.java:550)
at java.net.InetAddress.getHostName(InetAddress.java:476)
at java.net.InetAddress.getHostName(InetAddress.java:448)
at java.net.InetSocketAddress.getHostName(InetSocketAddress.java:210)
at org.apache.hadoop.hbase.HServerAddress.getHostname(HServerAddress.java:117)
at org.apache.hadoop.hbase.master.RegionManager.regionsAwaitingAssignment(RegionManager.java:469)
at org.apache.hadoop.hbase.master.RegionManager.assignRegions(RegionManager.java:263)
at org.apache.hadoop.hbase.master.ServerManager.processMsgs(ServerManager.java:500)
- locked <0x00007fcb985b2030> (a org.apache.hadoop.hbase.master.RegionManager)
at org.apache.hadoop.hbase.master.ServerManager.processRegionServerAllsWell(ServerManager.java:425)
at org.apache.hadoop.hbase.master.ServerManager.regionServerReport(ServerManager.java:335)
at org.apache.hadoop.hbase.master.HMaster.regionServerReport(HMaster.java:841)
at sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:585)
at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:933)",,,,,,,,
HBASE-4710,"While {{HBaseRPC$UnknownProtocolException}} currently extends {{DoNotRetryIOException}}, it's still allowing retries of client RPCs when encountered in {{HConnectionManager.getRegionServerWithRetries()}}.  It turns out that {{UnknownProtocolException}} is missing a public constructor taking a single {{String}} argument, which is required when unwrapping an {{IOException}} from a {{RemoteException}} in {{RemoteExceptionHandler.decodeRemoteException()}}.",,,,,,,,
HBASE-4713,"HBASE-4552 changed the locking behavior for single column family bulk load, namely we don't need to take write lock.
A read lock would suffice in this scenario.",,,,,,,,
HBASE-4716,"HBASE-4552 changed the locking behavior for single column family bulk load, namely we don't need to take write lock.
A read lock would suffice in this scenario.",,,,,,,,
HBASE-4717,"fom the logs in my env
{noformat}
2011-11-01 15:48:40,744 WARN  [Master:0;localhost,39664,1320187706355] master.AssignmentManager(1471): Failed assignment of -ROOT-,,0.70236052 to localhost,44046,1320187706849, trying to assign elsewhere instead; retry=1
org.apache.hadoop.hbase.ipc.HBaseRPC$VersionMismatch: Protocol org.apache.hadoop.hbase.ipc.HRegionInterface version mismatch. (client = 28, server = 29)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine.getProxy(WritableRpcEngine.java:185)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:300)
{noformat}
Anyway, after this the logs finishes with:
{noformat}
2011-11-01 15:54:35,132 INFO  [Master:0;localhost,39664,1320187706355.oldLogCleaner] hbase.Chore(80): Master:0;localhost,39664,1320187706355.oldLogCleaner exiting
Process Thread Dump: Automatic Stack Trace every 60 seconds waiting on Master:0;localhost,39664,1320187706355
{noformat}
it's in
{noformat}
    sun.management.ThreadImpl.getThreadInfo1(Native Method)
    sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:156)
    sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:121)
    org.apache.hadoop.util.ReflectionUtils.printThreadInfo(ReflectionUtils.java:149)
    org.apache.hadoop.hbase.util.Threads.threadDumpingIsAlive(Threads.java:113)
    org.apache.hadoop.hbase.LocalHBaseCluster.join(LocalHBaseCluster.java:405)
    org.apache.hadoop.hbase.MiniHBaseCluster.join(MiniHBaseCluster.java:408)
    org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniHBaseCluster(HBaseTestingUtility.java:616)
    org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster(HBaseTestingUtility.java:590)
    org.apache.hadoop.hbase.client.TestAdmin.tearDownAfterClass(TestAdmin.java:89)
{noformat}
So that's at least why adding a timeout wont help and may be why it does not end at all. Adding a maximum retry to Threads#threadDumpingIsAlive could help.

I also wonder if the root cause of the non ending is my modif on the wal, with some threads surprised to have updates that were not written in the wal. Here is the full stack dump:
{noformat}
Thread 354 (IPC Client (47) connection to localhost/127.0.0.1:52227 from nkeywal):
  State: TIMED_WAITING
  Blocked count: 360
  Waited count: 359
  Stack:
    java.lang.Object.wait(Native Method)
    org.apache.hadoop.ipc.Client$Connection.waitForWork(Client.java:702)
    org.apache.hadoop.ipc.Client$Connection.run(Client.java:744)
Thread 272 (Master:0;localhost,39664,1320187706355-EventThread):
  State: WAITING
  Blocked count: 0
  Waited count: 4
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@107b954b
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:502)
Thread 271 (Master:0;localhost,39664,1320187706355-SendThread(localhost:21819)):
  State: RUNNABLE
  Blocked count: 2
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1107)
Thread 152 (Master:0;localhost,39664,1320187706355):
  State: WAITING
  Blocked count: 217
  Waited count: 174
  Waiting on org.apache.hadoop.hbase.zookeeper.RootRegionTracker@6621477c
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.Object.wait(Object.java:485)
    org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.blockUntilAvailable(ZooKeeperNodeTracker.java:131)
    org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.blockUntilAvailable(ZooKeeperNodeTracker.java:104)
    org.apache.hadoop.hbase.catalog.CatalogTracker.waitForRoot(CatalogTracker.java:277)
    org.apache.hadoop.hbase.master.HMaster.assignRootAndMeta(HMaster.java:523)
    org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:468)
    org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:309)
    java.lang.Thread.run(Thread.java:662)
Thread 165 (LruBlockCache.EvictionThread):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on org.apache.hadoop.hbase.io.hfile.LruBlockCache$EvictionThread@3e9d7b56
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.Object.wait(Object.java:485)
    org.apache.hadoop.hbase.io.hfile.LruBlockCache$EvictionThread.run(LruBlockCache.java:593)
    java.lang.Thread.run(Thread.java:662)
Thread 151 (main-EventThread):
  State: WAITING
  Blocked count: 40
  Waited count: 39
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@22531380
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:502)
Thread 150 (main-SendThread(localhost:21819)):
  State: RUNNABLE
  Blocked count: 55
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1107)
Thread 149 (IPC Server handler 4 on 39664):
  State: WAITING
  Blocked count: 3
  Waited count: 1968
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@73f5173f
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1224)
Thread 148 (IPC Server handler 3 on 39664):
  State: WAITING
  Blocked count: 7
  Waited count: 1969
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@73f5173f
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1224)
Thread 147 (IPC Server handler 2 on 39664):
  State: WAITING
  Blocked count: 11
  Waited count: 1968
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@73f5173f
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1224)
Thread 146 (IPC Server handler 1 on 39664):
  State: WAITING
  Blocked count: 6
  Waited count: 1969
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@73f5173f
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1224)
Thread 145 (IPC Server handler 0 on 39664):
  State: WAITING
  Blocked count: 3
  Waited count: 1970
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@73f5173f
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1224)
Thread 133 (IPC Server listener on 39664):
  State: RUNNABLE
  Blocked count: 1
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener.run(HBaseServer.java:580)
Thread 144 (IPC Server Responder):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.apache.hadoop.hbase.ipc.HBaseServer$Responder.run(HBaseServer.java:754)
Thread 143 (IPC Reader 9 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 142 (IPC Reader 8 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 141 (IPC Reader 7 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 140 (IPC Reader 6 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 139 (IPC Reader 5 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 138 (IPC Reader 4 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 137 (IPC Reader 3 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 136 (IPC Reader 2 on port 39664):
  State: RUNNABLE
  Blocked count: 1
  Waited count: 1
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 135 (IPC Reader 1 on port 39664):
  State: RUNNABLE
  Blocked count: 1
  Waited count: 1
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 134 (IPC Reader 0 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 124 (LeaseChecker):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 407
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.hdfs.DFSClient$LeaseChecker.run(DFSClient.java:1252)
    java.lang.Thread.run(Thread.java:662)
Thread 121 (ProcessThread:-1):
  State: WAITING
  Blocked count: 0
  Waited count: 269
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@7f5d84e0
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.zookeeper.server.PrepRequestProcessor.run(PrepRequestProcessor.java:103)
Thread 120 (SyncThread:0):
  State: WAITING
  Blocked count: 2
  Waited count: 252
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3b25b27c
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:94)
Thread 119 (SessionTracker):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 207
  Stack:
    java.lang.Object.wait(Native Method)
    org.apache.zookeeper.server.SessionTrackerImpl.run(SessionTrackerImpl.java:142)
Thread 118 (NIOServerCxn.Factory:0.0.0.0/0.0.0.0:21819):
  State: RUNNABLE
  Blocked count: 38
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.apache.zookeeper.server.NIOServerCnxn$Factory.run(NIOServerCnxn.java:232)
Thread 116 (org.apache.hadoop.hdfs.server.datanode.DataBlockScanner@2e6f947b):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 407
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.run(DataBlockScanner.java:620)
    java.lang.Thread.run(Thread.java:662)
Thread 113 (IPC Server handler 2 on 39899):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1b4c72c8
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 112 (IPC Server handler 1 on 39899):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1b4c72c8
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 111 (IPC Server handler 0 on 39899):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1b4c72c8
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 106 (IPC Server listener on 39899):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.ipc.Server$Listener.run(Server.java:439)
Thread 108 (IPC Server Responder):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.apache.hadoop.ipc.Server$Responder.run(Server.java:605)
Thread 102 (org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@45d1c3cd):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
    sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:152)
    sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:84)
    org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:131)
    java.lang.Thread.run(Thread.java:662)
Thread 109 (DataNode: [/tmp/d846d12d-19f3-439a-8cab-e05db67ae60b/dfscluster_2773516a-bc5a-4029-9ee0-fc499eee6ca8/dfs/data/data5,/tmp/d846d12d-19f3-439a-8cab-e05db67ae60b/dfscluster_2773516a-bc5a-4029-9ee0-fc499eee6ca8/dfs/data/data6]):
  State: TIMED_WAITING
  Blocked count: 137
  Waited count: 283
  Stack:
    java.lang.Object.wait(Native Method)
    org.apache.hadoop.hdfs.server.datanode.DataNode.offerService(DataNode.java:986)
    org.apache.hadoop.hdfs.server.datanode.DataNode.run(DataNode.java:1421)
    java.lang.Thread.run(Thread.java:662)
Thread 107 (pool-4-thread-1):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:333)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 105 (Timer-3):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:509)
    java.util.TimerThread.run(Timer.java:462)
Thread 104 (1353906974@qtp-1197660496-1 - Acceptor0 SelectChannelConnector@localhost:59858):
  State: RUNNABLE
  Blocked count: 1
  Waited count: 1
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
    org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
    org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
    org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
Thread 103 (1185716172@qtp-1197660496-0):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 8
  Stack:
    java.lang.Object.wait(Native Method)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
Thread 101 (refreshUsed-/tmp/d846d12d-19f3-439a-8cab-e05db67ae60b/dfscluster_2773516a-bc5a-4029-9ee0-fc499eee6ca8/dfs/data/data6):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 1
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:80)
    java.lang.Thread.run(Thread.java:662)
Thread 98 (refreshUsed-/tmp/d846d12d-19f3-439a-8cab-e05db67ae60b/dfscluster_2773516a-bc5a-4029-9ee0-fc499eee6ca8/dfs/data/data5):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 1
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:80)
    java.lang.Thread.run(Thread.java:662)
Thread 90 (org.apache.hadoop.hdfs.server.datanode.DataBlockScanner@605b28c9):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 408
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.run(DataBlockScanner.java:620)
    java.lang.Thread.run(Thread.java:662)
Thread 86 (IPC Server handler 2 on 56098):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@78ff22ed
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 85 (IPC Server handler 1 on 56098):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@78ff22ed
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 84 (IPC Server handler 0 on 56098):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@78ff22ed
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 80 (IPC Server listener on 56098):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.ipc.Server$Listener.run(Server.java:439)
Thread 82 (IPC Server Responder):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.apache.hadoop.ipc.Server$Responder.run(Server.java:605)
Thread 76 (org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@2b68989e):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
    sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:152)
    sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:84)
    org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:131)
    java.lang.Thread.run(Thread.java:662)
Thread 83 (DataNode: [/tmp/d846d12d-19f3-439a-8cab-e05db67ae60b/dfscluster_2773516a-bc5a-4029-9ee0-fc499eee6ca8/dfs/data/data3,/tmp/d846d12d-19f3-439a-8cab-e05db67ae60b/dfscluster_2773516a-bc5a-4029-9ee0-fc499eee6ca8/dfs/data/data4]):
  State: TIMED_WAITING
  Blocked count: 149
  Waited count: 290
  Stack:
    java.lang.Object.wait(Native Method)
    org.apache.hadoop.hdfs.server.datanode.DataNode.offerService(DataNode.java:986)
    org.apache.hadoop.hdfs.server.datanode.DataNode.run(DataNode.java:1421)
    java.lang.Thread.run(Thread.java:662)
Thread 81 (pool-3-thread-1):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:333)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 79 (Timer-2):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:509)
    java.util.TimerThread.run(Timer.java:462)
Thread 78 (348878159@qtp-164967086-1 - Acceptor0 SelectChannelConnector@localhost:36427):
  State: RUNNABLE
  Blocked count: 2
  Waited count: 1
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
    org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
    org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
    org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
Thread 77 (1905729203@qtp-164967086-0):
  State: TIMED_WAITING
  Blocked count: 0
  W",,,,,,,,
,,,,,,,,,
,,,,,,,,,
HBASE-4720,"fom the logs in my env
{noformat}
2011-11-01 15:48:40,744 WARN  [Master:0;localhost,39664,1320187706355] master.AssignmentManager(1471): Failed assignment of -ROOT-,,0.70236052 to localhost,44046,1320187706849, trying to assign elsewhere instead; retry=1
org.apache.hadoop.hbase.ipc.HBaseRPC$VersionMismatch: Protocol org.apache.hadoop.hbase.ipc.HRegionInterface version mismatch. (client = 28, server = 29)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine.getProxy(WritableRpcEngine.java:185)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:300)
{noformat}
Anyway, after this the logs finishes with:
{noformat}
2011-11-01 15:54:35,132 INFO  [Master:0;localhost,39664,1320187706355.oldLogCleaner] hbase.Chore(80): Master:0;localhost,39664,1320187706355.oldLogCleaner exiting
Process Thread Dump: Automatic Stack Trace every 60 seconds waiting on Master:0;localhost,39664,1320187706355
{noformat}
it's in
{noformat}
    sun.management.ThreadImpl.getThreadInfo1(Native Method)
    sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:156)
    sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:121)
    org.apache.hadoop.util.ReflectionUtils.printThreadInfo(ReflectionUtils.java:149)
    org.apache.hadoop.hbase.util.Threads.threadDumpingIsAlive(Threads.java:113)
    org.apache.hadoop.hbase.LocalHBaseCluster.join(LocalHBaseCluster.java:405)
    org.apache.hadoop.hbase.MiniHBaseCluster.join(MiniHBaseCluster.java:408)
    org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniHBaseCluster(HBaseTestingUtility.java:616)
    org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster(HBaseTestingUtility.java:590)
    org.apache.hadoop.hbase.client.TestAdmin.tearDownAfterClass(TestAdmin.java:89)
{noformat}
So that's at least why adding a timeout wont help and may be why it does not end at all. Adding a maximum retry to Threads#threadDumpingIsAlive could help.

I also wonder if the root cause of the non ending is my modif on the wal, with some threads surprised to have updates that were not written in the wal. Here is the full stack dump:
{noformat}
Thread 354 (IPC Client (47) connection to localhost/127.0.0.1:52227 from nkeywal):
  State: TIMED_WAITING
  Blocked count: 360
  Waited count: 359
  Stack:
    java.lang.Object.wait(Native Method)
    org.apache.hadoop.ipc.Client$Connection.waitForWork(Client.java:702)
    org.apache.hadoop.ipc.Client$Connection.run(Client.java:744)
Thread 272 (Master:0;localhost,39664,1320187706355-EventThread):
  State: WAITING
  Blocked count: 0
  Waited count: 4
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@107b954b
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:502)
Thread 271 (Master:0;localhost,39664,1320187706355-SendThread(localhost:21819)):
  State: RUNNABLE
  Blocked count: 2
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1107)
Thread 152 (Master:0;localhost,39664,1320187706355):
  State: WAITING
  Blocked count: 217
  Waited count: 174
  Waiting on org.apache.hadoop.hbase.zookeeper.RootRegionTracker@6621477c
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.Object.wait(Object.java:485)
    org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.blockUntilAvailable(ZooKeeperNodeTracker.java:131)
    org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.blockUntilAvailable(ZooKeeperNodeTracker.java:104)
    org.apache.hadoop.hbase.catalog.CatalogTracker.waitForRoot(CatalogTracker.java:277)
    org.apache.hadoop.hbase.master.HMaster.assignRootAndMeta(HMaster.java:523)
    org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:468)
    org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:309)
    java.lang.Thread.run(Thread.java:662)
Thread 165 (LruBlockCache.EvictionThread):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on org.apache.hadoop.hbase.io.hfile.LruBlockCache$EvictionThread@3e9d7b56
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.Object.wait(Object.java:485)
    org.apache.hadoop.hbase.io.hfile.LruBlockCache$EvictionThread.run(LruBlockCache.java:593)
    java.lang.Thread.run(Thread.java:662)
Thread 151 (main-EventThread):
  State: WAITING
  Blocked count: 40
  Waited count: 39
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@22531380
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:502)
Thread 150 (main-SendThread(localhost:21819)):
  State: RUNNABLE
  Blocked count: 55
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1107)
Thread 149 (IPC Server handler 4 on 39664):
  State: WAITING
  Blocked count: 3
  Waited count: 1968
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@73f5173f
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1224)
Thread 148 (IPC Server handler 3 on 39664):
  State: WAITING
  Blocked count: 7
  Waited count: 1969
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@73f5173f
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1224)
Thread 147 (IPC Server handler 2 on 39664):
  State: WAITING
  Blocked count: 11
  Waited count: 1968
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@73f5173f
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1224)
Thread 146 (IPC Server handler 1 on 39664):
  State: WAITING
  Blocked count: 6
  Waited count: 1969
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@73f5173f
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1224)
Thread 145 (IPC Server handler 0 on 39664):
  State: WAITING
  Blocked count: 3
  Waited count: 1970
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@73f5173f
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1224)
Thread 133 (IPC Server listener on 39664):
  State: RUNNABLE
  Blocked count: 1
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener.run(HBaseServer.java:580)
Thread 144 (IPC Server Responder):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.apache.hadoop.hbase.ipc.HBaseServer$Responder.run(HBaseServer.java:754)
Thread 143 (IPC Reader 9 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 142 (IPC Reader 8 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 141 (IPC Reader 7 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 140 (IPC Reader 6 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 139 (IPC Reader 5 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 138 (IPC Reader 4 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 137 (IPC Reader 3 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 136 (IPC Reader 2 on port 39664):
  State: RUNNABLE
  Blocked count: 1
  Waited count: 1
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 135 (IPC Reader 1 on port 39664):
  State: RUNNABLE
  Blocked count: 1
  Waited count: 1
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 134 (IPC Reader 0 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 124 (LeaseChecker):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 407
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.hdfs.DFSClient$LeaseChecker.run(DFSClient.java:1252)
    java.lang.Thread.run(Thread.java:662)
Thread 121 (ProcessThread:-1):
  State: WAITING
  Blocked count: 0
  Waited count: 269
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@7f5d84e0
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.zookeeper.server.PrepRequestProcessor.run(PrepRequestProcessor.java:103)
Thread 120 (SyncThread:0):
  State: WAITING
  Blocked count: 2
  Waited count: 252
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3b25b27c
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:94)
Thread 119 (SessionTracker):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 207
  Stack:
    java.lang.Object.wait(Native Method)
    org.apache.zookeeper.server.SessionTrackerImpl.run(SessionTrackerImpl.java:142)
Thread 118 (NIOServerCxn.Factory:0.0.0.0/0.0.0.0:21819):
  State: RUNNABLE
  Blocked count: 38
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.apache.zookeeper.server.NIOServerCnxn$Factory.run(NIOServerCnxn.java:232)
Thread 116 (org.apache.hadoop.hdfs.server.datanode.DataBlockScanner@2e6f947b):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 407
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.run(DataBlockScanner.java:620)
    java.lang.Thread.run(Thread.java:662)
Thread 113 (IPC Server handler 2 on 39899):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1b4c72c8
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 112 (IPC Server handler 1 on 39899):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1b4c72c8
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 111 (IPC Server handler 0 on 39899):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1b4c72c8
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 106 (IPC Server listener on 39899):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.ipc.Server$Listener.run(Server.java:439)
Thread 108 (IPC Server Responder):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.apache.hadoop.ipc.Server$Responder.run(Server.java:605)
Thread 102 (org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@45d1c3cd):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
    sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:152)
    sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:84)
    org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:131)
    java.lang.Thread.run(Thread.java:662)
Thread 109 (DataNode: [/tmp/d846d12d-19f3-439a-8cab-e05db67ae60b/dfscluster_2773516a-bc5a-4029-9ee0-fc499eee6ca8/dfs/data/data5,/tmp/d846d12d-19f3-439a-8cab-e05db67ae60b/dfscluster_2773516a-bc5a-4029-9ee0-fc499eee6ca8/dfs/data/data6]):
  State: TIMED_WAITING
  Blocked count: 137
  Waited count: 283
  Stack:
    java.lang.Object.wait(Native Method)
    org.apache.hadoop.hdfs.server.datanode.DataNode.offerService(DataNode.java:986)
    org.apache.hadoop.hdfs.server.datanode.DataNode.run(DataNode.java:1421)
    java.lang.Thread.run(Thread.java:662)
Thread 107 (pool-4-thread-1):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:333)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 105 (Timer-3):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:509)
    java.util.TimerThread.run(Timer.java:462)
Thread 104 (1353906974@qtp-1197660496-1 - Acceptor0 SelectChannelConnector@localhost:59858):
  State: RUNNABLE
  Blocked count: 1
  Waited count: 1
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
    org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
    org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
    org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
Thread 103 (1185716172@qtp-1197660496-0):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 8
  Stack:
    java.lang.Object.wait(Native Method)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
Thread 101 (refreshUsed-/tmp/d846d12d-19f3-439a-8cab-e05db67ae60b/dfscluster_2773516a-bc5a-4029-9ee0-fc499eee6ca8/dfs/data/data6):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 1
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:80)
    java.lang.Thread.run(Thread.java:662)
Thread 98 (refreshUsed-/tmp/d846d12d-19f3-439a-8cab-e05db67ae60b/dfscluster_2773516a-bc5a-4029-9ee0-fc499eee6ca8/dfs/data/data5):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 1
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:80)
    java.lang.Thread.run(Thread.java:662)
Thread 90 (org.apache.hadoop.hdfs.server.datanode.DataBlockScanner@605b28c9):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 408
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.run(DataBlockScanner.java:620)
    java.lang.Thread.run(Thread.java:662)
Thread 86 (IPC Server handler 2 on 56098):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@78ff22ed
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 85 (IPC Server handler 1 on 56098):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@78ff22ed
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 84 (IPC Server handler 0 on 56098):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@78ff22ed
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 80 (IPC Server listener on 56098):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.ipc.Server$Listener.run(Server.java:439)
Thread 82 (IPC Server Responder):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.apache.hadoop.ipc.Server$Responder.run(Server.java:605)
Thread 76 (org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@2b68989e):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
    sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:152)
    sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:84)
    org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:131)
    java.lang.Thread.run(Thread.java:662)
Thread 83 (DataNode: [/tmp/d846d12d-19f3-439a-8cab-e05db67ae60b/dfscluster_2773516a-bc5a-4029-9ee0-fc499eee6ca8/dfs/data/data3,/tmp/d846d12d-19f3-439a-8cab-e05db67ae60b/dfscluster_2773516a-bc5a-4029-9ee0-fc499eee6ca8/dfs/data/data4]):
  State: TIMED_WAITING
  Blocked count: 149
  Waited count: 290
  Stack:
    java.lang.Object.wait(Native Method)
    org.apache.hadoop.hdfs.server.datanode.DataNode.offerService(DataNode.java:986)
    org.apache.hadoop.hdfs.server.datanode.DataNode.run(DataNode.java:1421)
    java.lang.Thread.run(Thread.java:662)
Thread 81 (pool-3-thread-1):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:333)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 79 (Timer-2):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:509)
    java.util.TimerThread.run(Timer.java:462)
Thread 78 (348878159@qtp-164967086-1 - Acceptor0 SelectChannelConnector@localhost:36427):
  State: RUNNABLE
  Blocked count: 2
  Waited count: 1
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
    org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
    org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
    org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
Thread 77 (1905729203@qtp-164967086-0):
  State: TIMED_WAITING
  Blocked count: 0
  W",,,,,,,,
,,,,,,,,,
,,,,,,,,,
HBASE-4724,"fom the logs in my env
{noformat}
2011-11-01 15:48:40,744 WARN  [Master:0;localhost,39664,1320187706355] master.AssignmentManager(1471): Failed assignment of -ROOT-,,0.70236052 to localhost,44046,1320187706849, trying to assign elsewhere instead; retry=1
org.apache.hadoop.hbase.ipc.HBaseRPC$VersionMismatch: Protocol org.apache.hadoop.hbase.ipc.HRegionInterface version mismatch. (client = 28, server = 29)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine.getProxy(WritableRpcEngine.java:185)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:300)
{noformat}
Anyway, after this the logs finishes with:
{noformat}
2011-11-01 15:54:35,132 INFO  [Master:0;localhost,39664,1320187706355.oldLogCleaner] hbase.Chore(80): Master:0;localhost,39664,1320187706355.oldLogCleaner exiting
Process Thread Dump: Automatic Stack Trace every 60 seconds waiting on Master:0;localhost,39664,1320187706355
{noformat}
it's in
{noformat}
    sun.management.ThreadImpl.getThreadInfo1(Native Method)
    sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:156)
    sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:121)
    org.apache.hadoop.util.ReflectionUtils.printThreadInfo(ReflectionUtils.java:149)
    org.apache.hadoop.hbase.util.Threads.threadDumpingIsAlive(Threads.java:113)
    org.apache.hadoop.hbase.LocalHBaseCluster.join(LocalHBaseCluster.java:405)
    org.apache.hadoop.hbase.MiniHBaseCluster.join(MiniHBaseCluster.java:408)
    org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniHBaseCluster(HBaseTestingUtility.java:616)
    org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster(HBaseTestingUtility.java:590)
    org.apache.hadoop.hbase.client.TestAdmin.tearDownAfterClass(TestAdmin.java:89)
{noformat}
So that's at least why adding a timeout wont help and may be why it does not end at all. Adding a maximum retry to Threads#threadDumpingIsAlive could help.

I also wonder if the root cause of the non ending is my modif on the wal, with some threads surprised to have updates that were not written in the wal. Here is the full stack dump:
{noformat}
Thread 354 (IPC Client (47) connection to localhost/127.0.0.1:52227 from nkeywal):
  State: TIMED_WAITING
  Blocked count: 360
  Waited count: 359
  Stack:
    java.lang.Object.wait(Native Method)
    org.apache.hadoop.ipc.Client$Connection.waitForWork(Client.java:702)
    org.apache.hadoop.ipc.Client$Connection.run(Client.java:744)
Thread 272 (Master:0;localhost,39664,1320187706355-EventThread):
  State: WAITING
  Blocked count: 0
  Waited count: 4
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@107b954b
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:502)
Thread 271 (Master:0;localhost,39664,1320187706355-SendThread(localhost:21819)):
  State: RUNNABLE
  Blocked count: 2
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1107)
Thread 152 (Master:0;localhost,39664,1320187706355):
  State: WAITING
  Blocked count: 217
  Waited count: 174
  Waiting on org.apache.hadoop.hbase.zookeeper.RootRegionTracker@6621477c
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.Object.wait(Object.java:485)
    org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.blockUntilAvailable(ZooKeeperNodeTracker.java:131)
    org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.blockUntilAvailable(ZooKeeperNodeTracker.java:104)
    org.apache.hadoop.hbase.catalog.CatalogTracker.waitForRoot(CatalogTracker.java:277)
    org.apache.hadoop.hbase.master.HMaster.assignRootAndMeta(HMaster.java:523)
    org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:468)
    org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:309)
    java.lang.Thread.run(Thread.java:662)
Thread 165 (LruBlockCache.EvictionThread):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on org.apache.hadoop.hbase.io.hfile.LruBlockCache$EvictionThread@3e9d7b56
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.Object.wait(Object.java:485)
    org.apache.hadoop.hbase.io.hfile.LruBlockCache$EvictionThread.run(LruBlockCache.java:593)
    java.lang.Thread.run(Thread.java:662)
Thread 151 (main-EventThread):
  State: WAITING
  Blocked count: 40
  Waited count: 39
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@22531380
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:502)
Thread 150 (main-SendThread(localhost:21819)):
  State: RUNNABLE
  Blocked count: 55
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1107)
Thread 149 (IPC Server handler 4 on 39664):
  State: WAITING
  Blocked count: 3
  Waited count: 1968
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@73f5173f
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1224)
Thread 148 (IPC Server handler 3 on 39664):
  State: WAITING
  Blocked count: 7
  Waited count: 1969
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@73f5173f
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1224)
Thread 147 (IPC Server handler 2 on 39664):
  State: WAITING
  Blocked count: 11
  Waited count: 1968
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@73f5173f
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1224)
Thread 146 (IPC Server handler 1 on 39664):
  State: WAITING
  Blocked count: 6
  Waited count: 1969
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@73f5173f
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1224)
Thread 145 (IPC Server handler 0 on 39664):
  State: WAITING
  Blocked count: 3
  Waited count: 1970
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@73f5173f
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1224)
Thread 133 (IPC Server listener on 39664):
  State: RUNNABLE
  Blocked count: 1
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener.run(HBaseServer.java:580)
Thread 144 (IPC Server Responder):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.apache.hadoop.hbase.ipc.HBaseServer$Responder.run(HBaseServer.java:754)
Thread 143 (IPC Reader 9 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 142 (IPC Reader 8 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 141 (IPC Reader 7 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 140 (IPC Reader 6 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 139 (IPC Reader 5 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 138 (IPC Reader 4 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 137 (IPC Reader 3 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 136 (IPC Reader 2 on port 39664):
  State: RUNNABLE
  Blocked count: 1
  Waited count: 1
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 135 (IPC Reader 1 on port 39664):
  State: RUNNABLE
  Blocked count: 1
  Waited count: 1
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 134 (IPC Reader 0 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 124 (LeaseChecker):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 407
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.hdfs.DFSClient$LeaseChecker.run(DFSClient.java:1252)
    java.lang.Thread.run(Thread.java:662)
Thread 121 (ProcessThread:-1):
  State: WAITING
  Blocked count: 0
  Waited count: 269
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@7f5d84e0
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.zookeeper.server.PrepRequestProcessor.run(PrepRequestProcessor.java:103)
Thread 120 (SyncThread:0):
  State: WAITING
  Blocked count: 2
  Waited count: 252
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3b25b27c
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:94)
Thread 119 (SessionTracker):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 207
  Stack:
    java.lang.Object.wait(Native Method)
    org.apache.zookeeper.server.SessionTrackerImpl.run(SessionTrackerImpl.java:142)
Thread 118 (NIOServerCxn.Factory:0.0.0.0/0.0.0.0:21819):
  State: RUNNABLE
  Blocked count: 38
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.apache.zookeeper.server.NIOServerCnxn$Factory.run(NIOServerCnxn.java:232)
Thread 116 (org.apache.hadoop.hdfs.server.datanode.DataBlockScanner@2e6f947b):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 407
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.run(DataBlockScanner.java:620)
    java.lang.Thread.run(Thread.java:662)
Thread 113 (IPC Server handler 2 on 39899):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1b4c72c8
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 112 (IPC Server handler 1 on 39899):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1b4c72c8
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 111 (IPC Server handler 0 on 39899):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1b4c72c8
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 106 (IPC Server listener on 39899):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.ipc.Server$Listener.run(Server.java:439)
Thread 108 (IPC Server Responder):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.apache.hadoop.ipc.Server$Responder.run(Server.java:605)
Thread 102 (org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@45d1c3cd):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
    sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:152)
    sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:84)
    org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:131)
    java.lang.Thread.run(Thread.java:662)
Thread 109 (DataNode: [/tmp/d846d12d-19f3-439a-8cab-e05db67ae60b/dfscluster_2773516a-bc5a-4029-9ee0-fc499eee6ca8/dfs/data/data5,/tmp/d846d12d-19f3-439a-8cab-e05db67ae60b/dfscluster_2773516a-bc5a-4029-9ee0-fc499eee6ca8/dfs/data/data6]):
  State: TIMED_WAITING
  Blocked count: 137
  Waited count: 283
  Stack:
    java.lang.Object.wait(Native Method)
    org.apache.hadoop.hdfs.server.datanode.DataNode.offerService(DataNode.java:986)
    org.apache.hadoop.hdfs.server.datanode.DataNode.run(DataNode.java:1421)
    java.lang.Thread.run(Thread.java:662)
Thread 107 (pool-4-thread-1):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:333)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 105 (Timer-3):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:509)
    java.util.TimerThread.run(Timer.java:462)
Thread 104 (1353906974@qtp-1197660496-1 - Acceptor0 SelectChannelConnector@localhost:59858):
  State: RUNNABLE
  Blocked count: 1
  Waited count: 1
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
    org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
    org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
    org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
Thread 103 (1185716172@qtp-1197660496-0):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 8
  Stack:
    java.lang.Object.wait(Native Method)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
Thread 101 (refreshUsed-/tmp/d846d12d-19f3-439a-8cab-e05db67ae60b/dfscluster_2773516a-bc5a-4029-9ee0-fc499eee6ca8/dfs/data/data6):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 1
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:80)
    java.lang.Thread.run(Thread.java:662)
Thread 98 (refreshUsed-/tmp/d846d12d-19f3-439a-8cab-e05db67ae60b/dfscluster_2773516a-bc5a-4029-9ee0-fc499eee6ca8/dfs/data/data5):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 1
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:80)
    java.lang.Thread.run(Thread.java:662)
Thread 90 (org.apache.hadoop.hdfs.server.datanode.DataBlockScanner@605b28c9):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 408
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.run(DataBlockScanner.java:620)
    java.lang.Thread.run(Thread.java:662)
Thread 86 (IPC Server handler 2 on 56098):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@78ff22ed
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 85 (IPC Server handler 1 on 56098):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@78ff22ed
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 84 (IPC Server handler 0 on 56098):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@78ff22ed
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 80 (IPC Server listener on 56098):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.ipc.Server$Listener.run(Server.java:439)
Thread 82 (IPC Server Responder):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.apache.hadoop.ipc.Server$Responder.run(Server.java:605)
Thread 76 (org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@2b68989e):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
    sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:152)
    sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:84)
    org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:131)
    java.lang.Thread.run(Thread.java:662)
Thread 83 (DataNode: [/tmp/d846d12d-19f3-439a-8cab-e05db67ae60b/dfscluster_2773516a-bc5a-4029-9ee0-fc499eee6ca8/dfs/data/data3,/tmp/d846d12d-19f3-439a-8cab-e05db67ae60b/dfscluster_2773516a-bc5a-4029-9ee0-fc499eee6ca8/dfs/data/data4]):
  State: TIMED_WAITING
  Blocked count: 149
  Waited count: 290
  Stack:
    java.lang.Object.wait(Native Method)
    org.apache.hadoop.hdfs.server.datanode.DataNode.offerService(DataNode.java:986)
    org.apache.hadoop.hdfs.server.datanode.DataNode.run(DataNode.java:1421)
    java.lang.Thread.run(Thread.java:662)
Thread 81 (pool-3-thread-1):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:333)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 79 (Timer-2):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:509)
    java.util.TimerThread.run(Timer.java:462)
Thread 78 (348878159@qtp-164967086-1 - Acceptor0 SelectChannelConnector@localhost:36427):
  State: RUNNABLE
  Blocked count: 2
  Waited count: 1
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
    org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
    org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
    org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
Thread 77 (1905729203@qtp-164967086-0):
  State: TIMED_WAITING
  Blocked count: 0
  W",,,,,,,,
,,,,,,,,,
,,,,,,,,,
HBASE-4729,"I was running an online alter while regions were splitting, and suddenly the master died and left my table half-altered (haven't restarted the master yet).

What killed the master:

{quote}
2011-11-02 17:06:44,428 FATAL org.apache.hadoop.hbase.master.HMaster: Unexpected ZK exception creating node CLOSING
org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /hbase/unassigned/f7e1783e65ea8d621a4bc96ad310f101
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:110)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
        at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:637)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.createNonSequential(RecoverableZooKeeper.java:459)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.create(RecoverableZooKeeper.java:441)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.createAndWatch(ZKUtil.java:769)
        at org.apache.hadoop.hbase.zookeeper.ZKAssign.createNodeClosing(ZKAssign.java:568)
        at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1722)
        at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1661)
        at org.apache.hadoop.hbase.master.BulkReOpen$1.run(BulkReOpen.java:69)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{quote}

A znode was created because the region server was splitting the region 4 seconds before:

{quote}
2011-11-02 17:06:40,704 INFO org.apache.hadoop.hbase.regionserver.SplitTransaction: Starting split of region TestTable,0012469153,1320253135043.f7e1783e65ea8d621a4bc96ad310f101.
2011-11-02 17:06:40,704 DEBUG org.apache.hadoop.hbase.regionserver.SplitTransaction: regionserver:62023-0x132f043bbde0710 Creating ephemeral node for f7e1783e65ea8d621a4bc96ad310f101 in SPLITTING state
2011-11-02 17:06:40,751 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:62023-0x132f043bbde0710 Attempting to transition node f7e1783e65ea8d621a4bc96ad310f101 from RS_ZK_REGION_SPLITTING to RS_ZK_REGION_SPLITTING
...
2011-11-02 17:06:44,061 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:62023-0x132f043bbde0710 Successfully transitioned node f7e1783e65ea8d621a4bc96ad310f101 from RS_ZK_REGION_SPLITTING to RS_ZK_REGION_SPLIT
2011-11-02 17:06:44,061 INFO org.apache.hadoop.hbase.regionserver.SplitTransaction: Still waiting on the master to process the split for f7e1783e65ea8d621a4bc96ad310f101
{quote}

Now that the master is dead the region server is spewing those last two lines like mad.",,,,,,,,
HBASE-4739,"I saw this in the aftermath of HBASE-4729 on a 0.92 refreshed yesterday, when the master died it had just created the RIT znode for a region but didn't tell the RS to close it yet.

When the master restarted it saw the znode and started printing this:

{quote}
2011-11-03 00:02:49,130 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  TestTable,0007560564,1320253568406.f76899564cabe7e9857c3aeb526ec9dc. state=CLOSING, ts=1320253605285, server=sv4r11s38,62003,1320195046948
2011-11-03 00:02:49,130 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been CLOSING for too long, this should eventually complete or the server will expire, doing nothing
{quote}

It's never going to happen, and it's blocking balancing.

I'm marking this as minor since I believe this situation is pretty rare unless you hit other bugs while trying out stuff to root bugs out.",,,,,,,,
HBASE-4741,"Still after the fun I had over in HBASE-4729, I tried to finish altering my table (remove a family) since only half of it was changed so I did this:

{quote}
hbase(main):002:0> alter 'TestTable', NAME => 'allo', METHOD => 'delete' 
Updating all regions with the new schema...
244/244 regions updated.
Done.
0 row(s) in 1.2480 seconds
{quote}

Nice it all looks good, but over in the master log:

{quote}
org.apache.hadoop.hbase.InvalidFamilyOperationException: Family 'allo' does not exist so cannot be deleted
        at org.apache.hadoop.hbase.master.handler.TableDeleteFamilyHandler.handleTableOperation(TableDeleteFamilyHandler.java:56)
        at org.apache.hadoop.hbase.master.handler.TableEventHandler.process(TableEventHandler.java:86)
        at org.apache.hadoop.hbase.master.HMaster.deleteColumn(HMaster.java:1011)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:348)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1242)
{quote}

Maybe we should do checks before launching the async task.

Marking critical as this is a regression.",,1,,,,,,
HBASE-4742,"As reported by Roman in the thread entitled 'HBase 0.92/Hadoop 0.22 test results', table creation would result in the following if hadoop 0.22 is the underlying platform:
{code}
11/11/05 19:08:48 INFO handler.CreateTableHandler: Attemping to create
the table b
11/11/05 19:08:48 ERROR handler.CreateTableHandler: Error trying to
create the table b
java.io.FileNotFoundException: File
hdfs://ip-10-110-254-200.ec2.internal:17020/hbase/b does not exist.
       at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:387)
       at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1085)
       at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1110)
       at org.apache.hadoop.hbase.util.FSTableDescriptors.getTableInfoPath(FSTableDescriptors.java:257)
       at org.apache.hadoop.hbase.util.FSTableDescriptors.getTableInfoPath(FSTableDescriptors.java:243)
       at org.apache.hadoop.hbase.util.FSTableDescriptors.createTableDescriptor(FSTableDescriptors.java:566)
       at org.apache.hadoop.hbase.util.FSTableDescriptors.createTableDescriptor(FSTableDescriptors.java:535)
       at org.apache.hadoop.hbase.util.FSTableDescriptors.createTableDescriptor(FSTableDescriptors.java:519)
       at org.apache.hadoop.hbase.master.handler.CreateTableHandler.handleCreateTable(CreateTableHandler.java:140)
       at org.apache.hadoop.hbase.master.handler.CreateTableHandler.process(CreateTableHandler.java:126)
       at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:168)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:619)
{code}
This was due to how DistributedFileSystem.listStatus() in 0.22 handles non-existent directory:
{code}
  @Override
  public FileStatus[] listStatus(Path p) throws IOException {
    String src = getPathName(p);

    // fetch the first batch of entries in the directory
    DirectoryListing thisListing = dfs.listPaths(
        src, HdfsFileStatus.EMPTY_NAME);

    if (thisListing == null) { // the directory does not exist
      throw new FileNotFoundException(""File "" + p + "" does not exist."");
    }
{code}
So in FSTableDescriptors.getTableInfoPath(), we should catch FileNotFoundException and treat it the same way as status being null.",,,1,,,,,
HBASE-4746,"As reported by Roman in the thread entitled 'HBase 0.92/Hadoop 0.22 test results', table creation would result in the following if hadoop 0.22 is the underlying platform:
{code}
11/11/05 19:08:48 INFO handler.CreateTableHandler: Attemping to create
the table b
11/11/05 19:08:48 ERROR handler.CreateTableHandler: Error trying to
create the table b
java.io.FileNotFoundException: File
hdfs://ip-10-110-254-200.ec2.internal:17020/hbase/b does not exist.
       at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:387)
       at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1085)
       at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1110)
       at org.apache.hadoop.hbase.util.FSTableDescriptors.getTableInfoPath(FSTableDescriptors.java:257)
       at org.apache.hadoop.hbase.util.FSTableDescriptors.getTableInfoPath(FSTableDescriptors.java:243)
       at org.apache.hadoop.hbase.util.FSTableDescriptors.createTableDescriptor(FSTableDescriptors.java:566)
       at org.apache.hadoop.hbase.util.FSTableDescriptors.createTableDescriptor(FSTableDescriptors.java:535)
       at org.apache.hadoop.hbase.util.FSTableDescriptors.createTableDescriptor(FSTableDescriptors.java:519)
       at org.apache.hadoop.hbase.master.handler.CreateTableHandler.handleCreateTable(CreateTableHandler.java:140)
       at org.apache.hadoop.hbase.master.handler.CreateTableHandler.process(CreateTableHandler.java:126)
       at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:168)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:619)
{code}
This was due to how DistributedFileSystem.listStatus() in 0.22 handles non-existent directory:
{code}
  @Override
  public FileStatus[] listStatus(Path p) throws IOException {
    String src = getPathName(p);

    // fetch the first batch of entries in the directory
    DirectoryListing thisListing = dfs.listPaths(
        src, HdfsFileStatus.EMPTY_NAME);

    if (thisListing == null) { // the directory does not exist
      throw new FileNotFoundException(""File "" + p + "" does not exist."");
    }
{code}
So in FSTableDescriptors.getTableInfoPath(), we should catch FileNotFoundException and treat it the same way as status being null.",,,1,,,,,
HBASE-4752,"As reported by Roman in the thread entitled 'HBase 0.92/Hadoop 0.22 test results', table creation would result in the following if hadoop 0.22 is the underlying platform:
{code}
11/11/05 19:08:48 INFO handler.CreateTableHandler: Attemping to create
the table b
11/11/05 19:08:48 ERROR handler.CreateTableHandler: Error trying to
create the table b
java.io.FileNotFoundException: File
hdfs://ip-10-110-254-200.ec2.internal:17020/hbase/b does not exist.
       at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:387)
       at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1085)
       at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1110)
       at org.apache.hadoop.hbase.util.FSTableDescriptors.getTableInfoPath(FSTableDescriptors.java:257)
       at org.apache.hadoop.hbase.util.FSTableDescriptors.getTableInfoPath(FSTableDescriptors.java:243)
       at org.apache.hadoop.hbase.util.FSTableDescriptors.createTableDescriptor(FSTableDescriptors.java:566)
       at org.apache.hadoop.hbase.util.FSTableDescriptors.createTableDescriptor(FSTableDescriptors.java:535)
       at org.apache.hadoop.hbase.util.FSTableDescriptors.createTableDescriptor(FSTableDescriptors.java:519)
       at org.apache.hadoop.hbase.master.handler.CreateTableHandler.handleCreateTable(CreateTableHandler.java:140)
       at org.apache.hadoop.hbase.master.handler.CreateTableHandler.process(CreateTableHandler.java:126)
       at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:168)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:619)
{code}
This was due to how DistributedFileSystem.listStatus() in 0.22 handles non-existent directory:
{code}
  @Override
  public FileStatus[] listStatus(Path p) throws IOException {
    String src = getPathName(p);

    // fetch the first batch of entries in the directory
    DirectoryListing thisListing = dfs.listPaths(
        src, HdfsFileStatus.EMPTY_NAME);

    if (thisListing == null) { // the directory does not exist
      throw new FileNotFoundException(""File "" + p + "" does not exist."");
    }
{code}
So in FSTableDescriptors.getTableInfoPath(), we should catch FileNotFoundException and treat it the same way as status being null.",,,1,,,,,
HBASE-4754,"As reported by Roman in the thread entitled 'HBase 0.92/Hadoop 0.22 test results', table creation would result in the following if hadoop 0.22 is the underlying platform:
{code}
11/11/05 19:08:48 INFO handler.CreateTableHandler: Attemping to create
the table b
11/11/05 19:08:48 ERROR handler.CreateTableHandler: Error trying to
create the table b
java.io.FileNotFoundException: File
hdfs://ip-10-110-254-200.ec2.internal:17020/hbase/b does not exist.
       at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:387)
       at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1085)
       at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1110)
       at org.apache.hadoop.hbase.util.FSTableDescriptors.getTableInfoPath(FSTableDescriptors.java:257)
       at org.apache.hadoop.hbase.util.FSTableDescriptors.getTableInfoPath(FSTableDescriptors.java:243)
       at org.apache.hadoop.hbase.util.FSTableDescriptors.createTableDescriptor(FSTableDescriptors.java:566)
       at org.apache.hadoop.hbase.util.FSTableDescriptors.createTableDescriptor(FSTableDescriptors.java:535)
       at org.apache.hadoop.hbase.util.FSTableDescriptors.createTableDescriptor(FSTableDescriptors.java:519)
       at org.apache.hadoop.hbase.master.handler.CreateTableHandler.handleCreateTable(CreateTableHandler.java:140)
       at org.apache.hadoop.hbase.master.handler.CreateTableHandler.process(CreateTableHandler.java:126)
       at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:168)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:619)
{code}
This was due to how DistributedFileSystem.listStatus() in 0.22 handles non-existent directory:
{code}
  @Override
  public FileStatus[] listStatus(Path p) throws IOException {
    String src = getPathName(p);

    // fetch the first batch of entries in the directory
    DirectoryListing thisListing = dfs.listPaths(
        src, HdfsFileStatus.EMPTY_NAME);

    if (thisListing == null) { // the directory does not exist
      throw new FileNotFoundException(""File "" + p + "" does not exist."");
    }
{code}
So in FSTableDescriptors.getTableInfoPath(), we should catch FileNotFoundException and treat it the same way as status being null.",,,,,,,,
HBASE-4755,"As reported by Roman in the thread entitled 'HBase 0.92/Hadoop 0.22 test results', table creation would result in the following if hadoop 0.22 is the underlying platform:
{code}
11/11/05 19:08:48 INFO handler.CreateTableHandler: Attemping to create
the table b
11/11/05 19:08:48 ERROR handler.CreateTableHandler: Error trying to
create the table b
java.io.FileNotFoundException: File
hdfs://ip-10-110-254-200.ec2.internal:17020/hbase/b does not exist.
       at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:387)
       at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1085)
       at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1110)
       at org.apache.hadoop.hbase.util.FSTableDescriptors.getTableInfoPath(FSTableDescriptors.java:257)
       at org.apache.hadoop.hbase.util.FSTableDescriptors.getTableInfoPath(FSTableDescriptors.java:243)
       at org.apache.hadoop.hbase.util.FSTableDescriptors.createTableDescriptor(FSTableDescriptors.java:566)
       at org.apache.hadoop.hbase.util.FSTableDescriptors.createTableDescriptor(FSTableDescriptors.java:535)
       at org.apache.hadoop.hbase.util.FSTableDescriptors.createTableDescriptor(FSTableDescriptors.java:519)
       at org.apache.hadoop.hbase.master.handler.CreateTableHandler.handleCreateTable(CreateTableHandler.java:140)
       at org.apache.hadoop.hbase.master.handler.CreateTableHandler.process(CreateTableHandler.java:126)
       at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:168)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:619)
{code}
This was due to how DistributedFileSystem.listStatus() in 0.22 handles non-existent directory:
{code}
  @Override
  public FileStatus[] listStatus(Path p) throws IOException {
    String src = getPathName(p);

    // fetch the first batch of entries in the directory
    DirectoryListing thisListing = dfs.listPaths(
        src, HdfsFileStatus.EMPTY_NAME);

    if (thisListing == null) { // the directory does not exist
      throw new FileNotFoundException(""File "" + p + "" does not exist."");
    }
{code}
So in FSTableDescriptors.getTableInfoPath(), we should catch FileNotFoundException and treat it the same way as status being null.",,,,,,,,
HBASE-4762,"Patch in HBASE-3914 fixed root assigned in two regionservers. But it seemed like root region will never be assigned if verifyRootRegionLocation throws IOE.
Like following master logs:
{noformat}
2011-10-19 19:13:34,873 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_META_SERVER_S
HUTDOWN
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hbase.ipc.ServerNotRunningException: Server is not running yet
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1090)

        at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:771)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:256)
        at $Proxy7.getRegionInfo(Unknown Source)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.verifyRegionLocation(CatalogTracker.java:424)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.verifyRootRegionLocation(CatalogTracker.java:471)
        at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.verifyAndAssignRoot(ServerShutdownHandler.java:90)
        at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:126)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:151)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}
After this, -ROOT-'s region won't be assigned, like this:
{noformat}
2011-10-19 19:18:40,000 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation: locateRegionInMeta parent
Table=-ROOT-, metaLocation=address: dw79.kgb.sqa.cm4:60020, regioninfo: -ROOT-,,0.70236052, attempt=0 of 10 failed; retrying after s
leep of 1000 because: org.apache.hadoop.hbase.NotServingRegionException: Region is not online: -ROOT-,,0
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:2771)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getClosestRowBefore(HRegionServer.java:1802)
        at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:569)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1091)
{noformat}
So we should rewrite the verifyRootRegionLocation method.",,,,,,,,
HBASE-4769,"Currently, when the HRegionServer runs out of the memory, it will call master, which will cause more heap allocations and throw a second exception that it's run out of memory again. The easiest & safest way to avoid this OOME storm is to abort the RegionServer immediately when it hits the memory boundary.  Part of the 89-fb to trunk port.",,,,,,,,
HBASE-4773,"When master crashs, HBaseAdmin will leaks ZooKeeper connections
I think we should close the zk connetion when throw MasterNotRunningException

 public HBaseAdmin(Configuration c)
  throws MasterNotRunningException, ZooKeeperConnectionException {
    this.conf = HBaseConfiguration.create(c);
    this.connection = HConnectionManager.getConnection(this.conf);
    this.pause = this.conf.getLong(""hbase.client.pause"", 1000);
    this.numRetries = this.conf.getInt(""hbase.client.retries.number"", 10);
    this.retryLongerMultiplier = this.conf.getInt(""hbase.client.retries.longer.multiplier"", 10);

    //we should add this code and close the zk connection
    try{
      this.connection.getMaster();
    }catch(MasterNotRunningException e){
      HConnectionManager.deleteConnection(conf, false);
      throw e;      
    }
  }",,,,,,,,
HBASE-4776,Concurrency issue: HLog.closed is set inside the updateLock but not checked inside the lock.,,,,,,,,
HBASE-4778,"We used to ignore StoreFiles that failed to open, which led to a situation when only a subset of regions was opened, and HBase did not return results to clients for the affected set of keys. This change makes sure we propagate IOExceptions coming from an attempt to open a StoreFile all the way up to HRegionServer.openRegion, where it will lead to a failure to open the whole region. This way we can avoid returning corrupt data to the application.",,,,,,,,
HBASE-4780,"If a CoprocessorProtocol derived interface defines a method with a void return type, the method cannot be called using HTable.coprocessorExec().  Instead ExecResult will throw an IOException on the client trying to do a Class.forName() on ""void"".

Looking at ExecResult, it appears that the valueType field (which causes the error) is no longer even used, so I'd suggest we just get rid of it.",,1,1,,,,,
HBASE-4784,"If a CoprocessorProtocol derived interface defines a method with a void return type, the method cannot be called using HTable.coprocessorExec().  Instead ExecResult will throw an IOException on the client trying to do a Class.forName() on ""void"".

Looking at ExecResult, it appears that the valueType field (which causes the error) is no longer even used, so I'd suggest we just get rid of it.",,,,,,,,
HBASE-4785,"If a CoprocessorProtocol derived interface defines a method with a void return type, the method cannot be called using HTable.coprocessorExec().  Instead ExecResult will throw an IOException on the client trying to do a Class.forName() on ""void"".

Looking at ExecResult, it appears that the valueType field (which causes the error) is no longer even used, so I'd suggest we just get rid of it.",,1,1,,,,,
HBASE-4789,"Here is log for a particular region:

{code}
2011-11-15 05:46:31,382 INFO org.apache.hadoop.hbase.regionserver.SplitTransaction: Still waiting on the master to process the split for 8bbd7388262dc8cb1ce2cf4f04a7281d
2011-11-15 05:46:31,483 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:7003-0x1337b0b92cd000a-0x1337b0b92cd000a Attempting to transition node 8bbd7388262dc8cb1ce2cf4f04a7281d from RS_ZK_REGION_SPLIT to RS_ZK_REG
ION_SPLIT
2011-11-15 05:46:31,484 INFO org.apache.hadoop.hbase.regionserver.SplitRequest: Region split, META updated, and report to master. Parent=TestTable,0862220095,1321335865649.8bbd7388262dc8cb1ce2cf4f04a7281d., new regions: TestTab
le,0862220095,1321335989689.f00c683df3182d8ef33e315f77ca539c., TestTable,0892568091,1321335989689.a56ca1eff5b4401432fcba04b4e851f8.. Split took 1sec
2011-11-15 05:46:37,705 DEBUG org.apache.hadoop.hbase.regionserver.Store: Compacting hdfs://sv4r11s38:7000/hbase/TestTable/a56ca1eff5b4401432fcba04b4e851f8/info/9ce16d8fa94e4938964c04775a6fa1a7.8bbd7388262dc8cb1ce2cf4f04a7281d-
hdfs://sv4r11s38:7000/hbase/TestTable/8bbd7388262dc8cb1ce2cf4f04a7281d/info/9ce16d8fa94e4938964c04775a6fa1a7-top, keycount=717559, bloomtype=NONE, size=711.1m
2011-11-15 05:46:37,705 DEBUG org.apache.hadoop.hbase.regionserver.Store: Compacting hdfs://sv4r11s38:7000/hbase/TestTable/a56ca1eff5b4401432fcba04b4e851f8/info/9213f4d7ee9b4fda857a97603a001f9e.8bbd7388262dc8cb1ce2cf4f04a7281d-
hdfs://sv4r11s38:7000/hbase/TestTable/8bbd7388262dc8cb1ce2cf4f04a7281d/info/9213f4d7ee9b4fda857a97603a001f9e-top, keycount=416691, bloomtype=NONE, size=412.9m
2011-11-15 05:46:53,090 DEBUG org.apache.hadoop.hbase.regionserver.Store: Compacting hdfs://sv4r11s38:7000/hbase/TestTable/f00c683df3182d8ef33e315f77ca539c/info/9ce16d8fa94e4938964c04775a6fa1a7.8bbd7388262dc8cb1ce2cf4f04a7281d-
hdfs://sv4r11s38:7000/hbase/TestTable/8bbd7388262dc8cb1ce2cf4f04a7281d/info/9ce16d8fa94e4938964c04775a6fa1a7-bottom, keycount=717559, bloomtype=NONE, size=711.1m
2011-11-15 05:46:53,090 DEBUG org.apache.hadoop.hbase.regionserver.Store: Compacting hdfs://sv4r11s38:7000/hbase/TestTable/f00c683df3182d8ef33e315f77ca539c/info/9213f4d7ee9b4fda857a97603a001f9e.8bbd7388262dc8cb1ce2cf4f04a7281d-
hdfs://sv4r11s38:7000/hbase/TestTable/8bbd7388262dc8cb1ce2cf4f04a7281d/info/9213f4d7ee9b4fda857a97603a001f9e-bottom, keycount=416691, bloomtype=NONE, size=412.9m
2011-11-15 05:48:00,690 DEBUG org.apache.hadoop.hbase.regionserver.wal.HLog: Found 3 hlogs to remove out of total 12; oldest outstanding sequenceid is 5699 from region 8bbd7388262dc8cb1ce2cf4f04a7281d
2011-11-15 05:57:54,083 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 8bbd7388262dc8cb1ce2cf4f04a7281d
2011-11-15 05:57:54,083 WARN org.apache.hadoop.hbase.regionserver.LogRoller: Failed to schedule flush of 8bbd7388262dc8cb1ce2cf4f04a7281dr=null, requester=null
2011-11-15 05:58:01,358 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 8bbd7388262dc8cb1ce2cf4f04a7281d
2011-11-15 05:58:01,359 WARN org.apache.hadoop.hbase.regionserver.LogRoller: Failed to schedule flush of 8bbd7388262dc8cb1ce2cf4f04a7281dr=null, requester=null
{code}",,,,,,,,
HBASE-4792,"Saw this on a little test cluster, really easy to trigger.

First the master log:

{quote}
2011-11-15 22:28:57,900 DEBUG org.apache.hadoop.hbase.master.handler.SplitRegionHandler: Handling SPLIT event for e5be6551c8584a6a1065466e520faf4e; deleting node
2011-11-15 22:28:57,900 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:62003-0x132f043bbde08c1 Deleting existing unassigned node for e5be6551c8584a6a1065466e520faf4e that is in expected state RS_ZK_REGION_SPLIT
2011-11-15 22:28:57,975 WARN org.apache.hadoop.hbase.zookeeper.ZKAssign: master:62003-0x132f043bbde08c1 Attempting to delete unassigned node in RS_ZK_REGION_SPLIT state but after verifying state, we got a version mismatch
2011-11-15 22:28:57,975 INFO org.apache.hadoop.hbase.master.handler.SplitRegionHandler: Handled SPLIT report); parent=TestTable,0001355346,1321396080924.e5be6551c8584a6a1065466e520faf4e. daughter a=TestTable,0001355346,1321396132414.df9b549eb594a1f8728608a2a431224a.daughter b=TestTable,0001368082,1321396132414.de861596db4337dc341138f26b9c8bc2.
...
2011-11-15 22:28:58,052 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_SPLIT, server=sv4r28s44,62023,1321395865619, region=e5be6551c8584a6a1065466e520faf4e
2011-11-15 22:28:58,052 WARN org.apache.hadoop.hbase.master.AssignmentManager: Region e5be6551c8584a6a1065466e520faf4e not found on server sv4r28s44,62023,1321395865619; failed processing
2011-11-15 22:28:58,052 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received SPLIT for region e5be6551c8584a6a1065466e520faf4e from server sv4r28s44,62023,1321395865619 but it doesn't exist anymore, probably already processed its split
(repeated forever)
{quote}

The master processes the split but when it calls ZKAssign.deleteNode it doesn't check the boolean that's returned. In this case it was false. So for the master the split was completed, but for the region server it's another story:

{quote}
2011-11-15 22:28:57,661 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:62023-0x132f043bbde08d3 Attempting to transition node e5be6551c8584a6a1065466e520faf4e from RS_ZK_REGION_SPLITTING to RS_ZK_REGION_SPLIT
2011-11-15 22:28:57,775 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:62023-0x132f043bbde08d3 Successfully transitioned node e5be6551c8584a6a1065466e520faf4e from RS_ZK_REGION_SPLITTING to RS_ZK_REGION_SPLIT
2011-11-15 22:28:57,775 INFO org.apache.hadoop.hbase.regionserver.SplitTransaction: Still waiting on the master to process the split for e5be6551c8584a6a1065466e520faf4e
2011-11-15 22:28:57,876 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:62023-0x132f043bbde08d3 Attempting to transition node e5be6551c8584a6a1065466e520faf4e from RS_ZK_REGION_SPLIT to RS_ZK_REGION_SPLIT
2011-11-15 22:28:57,967 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:62023-0x132f043bbde08d3 Successfully transitioned node e5be6551c8584a6a1065466e520faf4e from RS_ZK_REGION_SPLIT to RS_ZK_REGION_SPLIT
2011-11-15 22:28:58,067 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:62023-0x132f043bbde08d3 Attempting to transition node e5be6551c8584a6a1065466e520faf4e from RS_ZK_REGION_SPLIT to RS_ZK_REGION_SPLIT
2011-11-15 22:28:58,108 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:62023-0x132f043bbde08d3 Successfully transitioned node e5be6551c8584a6a1065466e520faf4e from RS_ZK_REGION_SPLIT to RS_ZK_REGION_SPLIT
(printed forever)
{quote}

Since the znode isn't really deleted, it thinks the master just haven't got to process its region thus waits which leaves the region *unavailable*.

We need to just retry the delete master-side ASAP since the RS will wait 100ms between retries.

At the same time, it would be nice if ZKAssign.deleteNode always printed out the name of the region in its messages because it took me a while to see that the delete didn't take affect while looking at a grep.",,,,,,,,
HBASE-4796,"I just saw that multiple SplitRegionHandlers can be created for the same region because of the RS tickling, but it becomes deadly when more than 1 are trying to delete the znode at the same time:

{quote}
2011-11-16 02:25:28,778 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_SPLIT, server=sv4r7s38,62023,1321410237387, region=f80b6a904048a99ce88d61420b8906d1
2011-11-16 02:25:28,780 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_SPLIT, server=sv4r7s38,62023,1321410237387, region=f80b6a904048a99ce88d61420b8906d1
2011-11-16 02:25:28,796 DEBUG org.apache.hadoop.hbase.master.handler.SplitRegionHandler: Handling SPLIT event for f80b6a904048a99ce88d61420b8906d1; deleting node
2011-11-16 02:25:28,798 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:62003-0x132f043bbde094b Deleting existing unassigned node for f80b6a904048a99ce88d61420b8906d1 that is in expected state RS_ZK_REGION_SPLIT
2011-11-16 02:25:28,804 DEBUG org.apache.hadoop.hbase.master.handler.SplitRegionHandler: Handling SPLIT event for f80b6a904048a99ce88d61420b8906d1; deleting node
2011-11-16 02:25:28,806 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:62003-0x132f043bbde094b Deleting existing unassigned node for f80b6a904048a99ce88d61420b8906d1 that is in expected state RS_ZK_REGION_SPLIT
2011-11-16 02:25:28,821 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:62003-0x132f043bbde094b Successfully deleted unassigned node for region f80b6a904048a99ce88d61420b8906d1 in expected state RS_ZK_REGION_SPLIT
2011-11-16 02:25:28,821 INFO org.apache.hadoop.hbase.master.handler.SplitRegionHandler: Handled SPLIT report); parent=TestTable,0000006304,1321409743253.f80b6a904048a99ce88d61420b8906d1. daughter a=TestTable,0000006304,1321410325564.e0f5d201683bcabe14426817224334b8.daughter b=TestTable,0000007054,1321410325564.1b82eeb5d230c47ccc51c08256134839.
2011-11-16 02:25:28,829 WARN org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper: Node /hbase/unassigned/f80b6a904048a99ce88d61420b8906d1 already deleted, and this is not a retry
2011-11-16 02:25:28,830 FATAL org.apache.hadoop.hbase.master.HMaster: Error deleting SPLIT node in ZK for transition ZK node (f80b6a904048a99ce88d61420b8906d1)
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /hbase/unassigned/f80b6a904048a99ce88d61420b8906d1
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:102)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
	at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:728)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.delete(RecoverableZooKeeper.java:107)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.deleteNode(ZKUtil.java:884)
	at org.apache.hadoop.hbase.zookeeper.ZKAssign.deleteNode(ZKAssign.java:506)
	at org.apache.hadoop.hbase.zookeeper.ZKAssign.deleteNode(ZKAssign.java:453)
	at org.apache.hadoop.hbase.master.handler.SplitRegionHandler.process(SplitRegionHandler.java:95)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:168)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{quote}

Stack and I came up with the solution that we need just manage that exception because handleSplitReport is an in-memory thing.",,,,,,,,
HBASE-4797,"Testing 0.92, I crashed all servers out.  Another bug makes it so WALs are not getting cleaned so I had 7000 regions to replay.  The distributed split code did a nice job and cluster came back but interesting is that some hot regions ended up having loads of recovered.edits files -- tens if not hundreds -- to replay against the region (can we bulk load recovered.edits instead of replaying them?).  Each recovered.edits file is taking about a second to process (though only about 30 odd edits per file it seems).  The region is unavailable during this time.",,,1,,,,,
HBASE-4798,"When region split takes a significant amount of time, CatalogJanitor can cleanup one of SPLIT records, but left another in META. When another split finish, janitor cleans left SPLIT record, but parent regions haven't removed from FS and META not cleared.

The race condition is follows:
1. region split started
2. one of regions splitted, i.e. A (have no reference storefiles) but other (B) doesn't
3. janitor started and in routine checkDaughter removes SPLITA from meta, but see that SPLITB has references and does nothing.
4. region B completes split
5. janitor wakes up, removes SPLITB, but see that there is no records for A and does nothing again.

Result - parent region hangs forever.",,,1,,,,,
HBASE-4799,"When region split takes a significant amount of time, CatalogJanitor can cleanup one of SPLIT records, but left another in META. When another split finish, janitor cleans left SPLIT record, but parent regions haven't removed from FS and META not cleared.

The race condition is follows:
1. region split started
2. one of regions splitted, i.e. A (have no reference storefiles) but other (B) doesn't
3. janitor started and in routine checkDaughter removes SPLITA from meta, but see that SPLITB has references and does nothing.
4. region B completes split
5. janitor wakes up, removes SPLITB, but see that there is no records for A and does nothing again.

Result - parent region hangs forever.",,,,,,,,
HBASE-4800,"A coworker of mine (James Taylor) found a bug in Result.compareResults(...).
This condition:
{code}
      if (!ourKVs[i].equals(replicatedKVs[i]) &&
          !Bytes.equals(ourKVs[i].getValue(), replicatedKVs[i].getValue())) {
        throw new Exception(""This result was different: ""
{code}
should be
{code}
      if (!ourKVs[i].equals(replicatedKVs[i]) ||
          !Bytes.equals(ourKVs[i].getValue(), replicatedKVs[i].getValue())) {
        throw new Exception(""This result was different: ""
{code}

Just checked, this is wrong in all branches.",,,,,,,,
HBASE-4802,"During bulk load, the Configuration object may be set to null.  This caused an NPE in per-CF metrics because it consults the Configuration to determine whether to show the Table name.  Need to add simple change to allow the conf to be null & not specify table name in that instance.",,,,,,,,
HBASE-4803,This is an attempt to fix the fact that SplitLogWorker threads were not being terminated properly in some multi-master unit tests.,,,,,,,,
HBASE-4805,"From some internal discussions at Salesforce we concluded that we need better control over the resources (mostly threads) consumed by HTable when used in a AppServer with many client threads.

Since HTable is not thread safe, the only options are cache them (in a custom thread local or using HTablePool) or to create them on-demand.

I propose a simple change: Add a new constructor to HTable that takes an optional ExecutorService and HConnection instance. That would make HTable a pretty lightweight object and we would manage the ES and HC separately.

I'll upload a patch a soon to get some feedback.",,,1,,,,,
HBASE-4811,"From some internal discussions at Salesforce we concluded that we need better control over the resources (mostly threads) consumed by HTable when used in a AppServer with many client threads.

Since HTable is not thread safe, the only options are cache them (in a custom thread local or using HTablePool) or to create them on-demand.

I propose a simple change: Add a new constructor to HTable that takes an optional ExecutorService and HConnection instance. That would make HTable a pretty lightweight object and we would manage the ES and HC separately.

I'll upload a patch a soon to get some feedback.",,,1,,,,,
HBASE-4816,"A regionserver wouldn't go down because it was waiting on a user region to close only the user-space region had just been opened as part of a split transaction -- it was a new daughter -- just as we'd issued the bulk close to all user regions on receipt of a cluster shutdown call.

We need to add a check for this condition -- user tables that did not get the close.",,,,,,,,
HBASE-4830,"Running 0.20.205.1 (I was not at tip of the branch) I ran into the following hung regionserver:

{code}
""regionserver7003.logRoller"" daemon prio=10 tid=0x00007fd98028f800 nid=0x61af in Object.wait() [0x00007fd987bfa000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:485)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.waitForAckedSeqno(DFSClient.java:3606)
        - locked <0x00000000f8656788> (a java.util.LinkedList)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.flushInternal(DFSClient.java:3595)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.closeInternal(DFSClient.java:3687)
        - locked <0x00000000f8656458> (a org.apache.hadoop.hdfs.DFSClient$DFSOutputStream)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.close(DFSClient.java:3626)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:61)
        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:86)
        at org.apache.hadoop.io.SequenceFile$Writer.close(SequenceFile.java:966)
        - locked <0x00000000f8655998> (a org.apache.hadoop.io.SequenceFile$Writer)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.close(SequenceFileLogWriter.java:214)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.cleanupCurrentWriter(HLog.java:791)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.rollWriter(HLog.java:578)
        - locked <0x00000000c443deb0> (a java.lang.Object)
        at org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:94)
        at java.lang.Thread.run(Thread.java:662)
{code}


Other threads are like this (here's a sample):
{code}

""regionserver7003.logSyncer"" daemon prio=10 tid=0x00007fd98025e000 nid=0x61ae waiting for monitor entry [0x00007fd987cfb000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.syncer(HLog.java:1074)
        - waiting to lock <0x00000000c443deb0> (a java.lang.Object)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.sync(HLog.java:1195)
        at org.apache.hadoop.hbase.regionserver.wal.HLog$LogSyncer.run(HLog.java:1057)
        at java.lang.Thread.run(Thread.java:662)

....

""IPC Server handler 0 on 7003"" daemon prio=10 tid=0x00007fd98049b800 nid=0x61b8 waiting for monitor entry [0x00007fd9872f1000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.append(HLog.java:1007)
        - waiting to lock <0x00000000c443deb0> (a java.lang.Object)
        at org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchPut(HRegion.java:1798)
        at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1668)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.java:2980)
        at sun.reflect.GeneratedMethodAccessor636.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1325)

{code}

Looks like HDFS-1529?  (Todd?)",,,,,,,,
HBASE-4832,"The current implementation of HRegionServer#stop is

{noformat}
  public void stop(final String msg) {
    this.stopped = true;
    LOG.info(""STOPPED: "" + msg);
    synchronized (this) {
      // Wakes run() if it is sleeping
      notifyAll(); // FindBugs NN_NAKED_NOTIFY
    }
  }
{noformat}

The notification is sent on the wrong object and does nothing. As a consequence, the region server continues to sleep instead of waking up and stopping immediately. A correct implementation is:

{noformat}
  public void stop(final String msg) {
    this.stopped = true;
    LOG.info(""STOPPED: "" + msg);
    // Wakes run() if it is sleeping
    sleeper.skipSleepCycle();
  }
{noformat}

Then the region server stops immediately. This makes the region server stops 0,5s faster on average, which is quite useful for unit tests.

However, with this fix, TestRegionServerCoprocessorExceptionWithAbort does not work.
It likely because the code does no expect the region server to stop that fast.

The exception is:
{noformat}
testExceptionFromCoprocessorDuringPut(org.apache.hadoop.hbase.coprocessor.TestRegionServerCoprocessorExceptionWithAbort)  Time elapsed: 30.06 sec  <<< ERROR!
java.lang.Exception: test timed out after 30000 milliseconds
	at java.lang.Throwable.fillInStackTrace(Native Method)
	at java.lang.Throwable.<init>(Throwable.java:196)
	at java.lang.Exception.<init>(Exception.java:41)
	at java.lang.InterruptedException.<init>(InterruptedException.java:48)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:1019)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:804)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.relocateRegion(HConnectionManager.java:778)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionLocation(HConnectionManager.java:697)
	at org.apache.hadoop.hbase.client.ServerCallable.connect(ServerCallable.java:75)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionServerWithRetries(HConnectionManager.java:1280)
	at org.apache.hadoop.hbase.client.HTable.getRowOrBefore(HTable.java:585)
	at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:154)
	at org.apache.hadoop.hbase.client.MetaScanner.access$000(MetaScanner.java:52)
	at org.apache.hadoop.hbase.client.MetaScanner$1.connect(MetaScanner.java:130)
	at org.apache.hadoop.hbase.client.MetaScanner$1.connect(MetaScanner.java:127)
	at org.apache.hadoop.hbase.client.HConnectionManager.execute(HConnectionManager.java:357)
	at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:127)
	at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:103)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.prefetchRegionCache(HConnectionManager.java:866)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:920)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:808)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.processBatchCallback(HConnectionManager.java:1469)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.processBatch(HConnectionManager.java:1354)
	at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:892)
	at org.apache.hadoop.hbase.client.HTable.doPut(HTable.java:750)
	at org.apache.hadoop.hbase.client.HTable.put(HTable.java:725)
	at org.apache.hadoop.hbase.coprocessor.TestRegionServerCoprocessorExceptionWithAbort.testExceptionFromCoprocessorDuringPut(TestRegionServerCoprocessorExceptionWithAbort.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:62)
{noformat}

We have this exception because we entered a loop of retries.


",,,,,,,,
HBASE-4833,"The current implementation of HRegionServer#stop is

{noformat}
  public void stop(final String msg) {
    this.stopped = true;
    LOG.info(""STOPPED: "" + msg);
    synchronized (this) {
      // Wakes run() if it is sleeping
      notifyAll(); // FindBugs NN_NAKED_NOTIFY
    }
  }
{noformat}

The notification is sent on the wrong object and does nothing. As a consequence, the region server continues to sleep instead of waking up and stopping immediately. A correct implementation is:

{noformat}
  public void stop(final String msg) {
    this.stopped = true;
    LOG.info(""STOPPED: "" + msg);
    // Wakes run() if it is sleeping
    sleeper.skipSleepCycle();
  }
{noformat}

Then the region server stops immediately. This makes the region server stops 0,5s faster on average, which is quite useful for unit tests.

However, with this fix, TestRegionServerCoprocessorExceptionWithAbort does not work.
It likely because the code does no expect the region server to stop that fast.

The exception is:
{noformat}
testExceptionFromCoprocessorDuringPut(org.apache.hadoop.hbase.coprocessor.TestRegionServerCoprocessorExceptionWithAbort)  Time elapsed: 30.06 sec  <<< ERROR!
java.lang.Exception: test timed out after 30000 milliseconds
	at java.lang.Throwable.fillInStackTrace(Native Method)
	at java.lang.Throwable.<init>(Throwable.java:196)
	at java.lang.Exception.<init>(Exception.java:41)
	at java.lang.InterruptedException.<init>(InterruptedException.java:48)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:1019)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:804)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.relocateRegion(HConnectionManager.java:778)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionLocation(HConnectionManager.java:697)
	at org.apache.hadoop.hbase.client.ServerCallable.connect(ServerCallable.java:75)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionServerWithRetries(HConnectionManager.java:1280)
	at org.apache.hadoop.hbase.client.HTable.getRowOrBefore(HTable.java:585)
	at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:154)
	at org.apache.hadoop.hbase.client.MetaScanner.access$000(MetaScanner.java:52)
	at org.apache.hadoop.hbase.client.MetaScanner$1.connect(MetaScanner.java:130)
	at org.apache.hadoop.hbase.client.MetaScanner$1.connect(MetaScanner.java:127)
	at org.apache.hadoop.hbase.client.HConnectionManager.execute(HConnectionManager.java:357)
	at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:127)
	at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:103)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.prefetchRegionCache(HConnectionManager.java:866)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:920)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:808)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.processBatchCallback(HConnectionManager.java:1469)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.processBatch(HConnectionManager.java:1354)
	at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:892)
	at org.apache.hadoop.hbase.client.HTable.doPut(HTable.java:750)
	at org.apache.hadoop.hbase.client.HTable.put(HTable.java:725)
	at org.apache.hadoop.hbase.coprocessor.TestRegionServerCoprocessorExceptionWithAbort.testExceptionFromCoprocessorDuringPut(TestRegionServerCoprocessorExceptionWithAbort.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:62)
{noformat}

We have this exception because we entered a loop of retries.


",,,1,,,,,
HBASE-4841,"I'll attach a unit test for this. Basically if you call split, while inserting data you can get to the point to where the cluster becomes unstable, or rows will  disappear. The unit test gives you some flexibility of:

- How many rows
- How wide the rows are
- The frequency of the split. 


The default settings crash unit tests or cause the unit tests to fail on my laptop. On my macbook air, i could actually turn down the number of total rows, and the frequency of the splits which is surprising. I think this is because the macbook air has much better IO than my backup acer.",,,,,,,,
HBASE-4842,"Its seems that on the 0.92 branch in particular, TestHBaseFsck.testHBaseFsck is intermittently failing.

In the test, a region's assignment is purposely changed in META but not in ZK.  After the equivalent of 'hbck -fix', a subsequent check that should be clean comes up with a new ZK assignment but with META still being inconsistent with ZK.  The RS in ZK sometimes this points to the same RS, but sometimes it ""moves"" to another ZK. ",,,,,,,,
HBASE-4848,,,1,,,,,,
HBASE-4853,"Working w/ J-D on failing replication test turned up hole in seqids made by the patch over in hbase-4789.  With this patch in place we see lots of instances of the suspicious: 'Last sequenceid written is empty. Deleting all old hlogs'

At a minimum, these lines need removing:

{code}
diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java b/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
index 623edbe..a0bbe01 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
@@ -1359,11 +1359,6 @@ public class HLog implements Syncable {
       // Cleaning up of lastSeqWritten is in the finally clause because we
       // don't want to confuse getOldestOutstandingSeqNum()
       this.lastSeqWritten.remove(getSnapshotName(encodedRegionName));
-      Long l = this.lastSeqWritten.remove(encodedRegionName);
-      if (l != null) {
-        LOG.warn(""Why is there a raw encodedRegionName in lastSeqWritten? name="" +
-          Bytes.toString(encodedRegionName) + "", seqid="" + l);
-       }
       this.cacheFlushLock.unlock();
     }
   }
{code}

... but above is no good w/o figuring why WALs are not being rotated off.",,1,,,,,,
HBASE-4855,"Start a master and RS
RS goes down (kill -9)
Wait for ServerShutDownHandler to create the splitlog nodes. As no RS is there it cannot be processed.
Restart both master and bring up an RS.
The master hangs in SplitLogManager.waitforTasks().

I feel that batch.done is not getting incremented properly.  Not yet digged in fully.

This may be the reason for occasional failure of TestDistributedLogSplitting.testWorkerAbort(). 
",,,,,,,,
HBASE-4857,"Looking through stack traces for {{TestMasterFailover}}, I see a case where the leader {{AuthenticationTokenSecretManager}} can get into a recursive loop when a {{KeeperException}} is encountered:
{noformat}
Thread-1-EventThread"" daemon prio=10 tid=0x00007f9fb47b2800 nid=0x77f6 waiting on condition [0x00007f9fab376000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:302)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:328)
        at org.apache.hadoop.hbase.util.RetryCounter.sleepUntilNextRetry(RetryCounter.java:55)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.exists(RecoverableZooKeeper.java:206)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.createAndFailSilent(ZKUtil.java:891)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.createBaseZNodes(ZooKeeperWatcher.java:161)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.<init>(ZooKeeperWatcher.java:154)
        at org.apache.hadoop.hbase.master.HMaster.tryRecoveringExpiredZKSession(HMaster.java:1397)
        at org.apache.hadoop.hbase.master.HMaster.abortNow(HMaster.java:1435)
        at org.apache.hadoop.hbase.master.HMaster.abort(HMaster.java:1374)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.abort(ZooKeeperWatcher.java:450)
        at org.apache.hadoop.hbase.zookeeper.ZKLeaderManager.stepDownAsLeader(ZKLeaderManager.java:166)
        at org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager$LeaderElector.stop(AuthenticationTokenSecretManager.java:293)
        at org.apache.hadoop.hbase.zookeeper.ZKLeaderManager.stepDownAsLeader(ZKLeaderManager.java:167)
        at org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager$LeaderElector.stop(AuthenticationTokenSecretManager.java:293)
        at org.apache.hadoop.hbase.zookeeper.ZKLeaderManager.stepDownAsLeader(ZKLeaderManager.java:167)
        at org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager$LeaderElector.stop(AuthenticationTokenSecretManager.java:293)
        at org.apache.hadoop.hbase.zookeeper.ZKLeaderManager.handleLeaderChange(ZKLeaderManager.java:96)
        at org.apache.hadoop.hbase.zookeeper.ZKLeaderManager.nodeDeleted(ZKLeaderManager.java:78)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:286)
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:521)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:497)
{noformat}

The {{KeeperException}} causes {{ZKLeaderManager}} to call {{AuthenticationTokenSecretManager$LeaderElector.stop()}}, which calls {{ZKLeaderManager.stepDownAsLeader()}}, which will encounter another {{KeeperException}}, and so on...",1,,,,,,,
HBASE-4859,"See description at HBASE-3553.  We had a patch ready for this in HBASE-3620 but never applied it publicly.  Testing showed massive speedup in HBCK, especially when RegionServers were down or had long response times.",,,1,,,,,
HBASE-4862,"Case Description:
1.Split hlog thread creat writer for the file region A/recoverd.edits/123456 and is appending log entry
2.Regionserver is opening region A now, and in the process replayRecoveredEditsIfAny() ,it will delete the file region A/recoverd.edits/123456 
3.Split hlog thread catches the io exception, and stop parse this log file 
and if skipError = true , add it to the corrupt logs....However, data in other regions in this log file will loss 
4.Or if skipError = false, it will check filesystem.Of course, the file system is ok , and it only prints a error log, continue assigning regions. Therefore, data in other log files will also loss!!

The case may happen in the following:
1.Move region from server A to server B
2.kill server A and Server B
3.restart server A and Server B

We could prevent this exception throuth forbiding deleting  recover.edits file 
which is appending by split hlog thread",,,,,,,,
HBASE-4863,"Case Description:
1.Split hlog thread creat writer for the file region A/recoverd.edits/123456 and is appending log entry
2.Regionserver is opening region A now, and in the process replayRecoveredEditsIfAny() ,it will delete the file region A/recoverd.edits/123456 
3.Split hlog thread catches the io exception, and stop parse this log file 
and if skipError = true , add it to the corrupt logs....However, data in other regions in this log file will loss 
4.Or if skipError = false, it will check filesystem.Of course, the file system is ok , and it only prints a error log, continue assigning regions. Therefore, data in other log files will also loss!!

The case may happen in the following:
1.Move region from server A to server B
2.kill server A and Server B
3.restart server A and Server B

We could prevent this exception throuth forbiding deleting  recover.edits file 
which is appending by split hlog thread",,,,,,,,
HBASE-4877,"TestHCM takes 13 minutes for me on ubuntu and fails in testClosing.  It runs fine on a mac.  The problem test is not testClosing as I thought originally, its the test just previous, testConnectionUniqueness.  testConnectionUniqueness creates the maximum cached HConnections + 10 to verify each is unique if the passed in Configuration has a unique hash.  Problem comes when zk enforces its default max from single host of 30 connections which is < (max cached + 10).  The max does not seem to be enforced on mac for me.  The max connections runs up to max of 31 -- zk max + 1 -- and works fine until we do the +10.  On ubuntu, when we hit the zk max of 30, we'll then go into a fail mode where we cannot set up a zk session... each attempt takes a while.  Test passes, it just takes a while.

Only, the uniqueness test does not clean up after itself and so all sessions to zk are outstanding so then when the subsequent testClosing runs, it can't set up connections successfully so fails.",,1,,,,,,
HBASE-4878,"Let's see the code of HlogSplitter#splitLog(final FileStatus[] logfiles)
{code}
private List<Path> splitLog(final FileStatus[] logfiles) throws IOException {
 try {
  for (FileStatus log : logfiles) {
  parseHLog(in, logPath, entryBuffers, fs, conf, skipErrors);
 }
 archiveLogs(srcDir, corruptedLogs, processedLogs, oldLogDir, fs, conf);
 } finally {
      status.setStatus(""Finishing writing output logs and closing down."");
      splits = outputSink.finishWritingAndClose();
    }
}
{code}


If master is killed, after finishing archiveLogs(srcDir, corruptedLogs, processedLogs, oldLogDir, fs, conf), 
but before finishing splits = outputSink.finishWritingAndClose();
Log date would loss!
",,,,,,,,
HBASE-4880,"OpenRegionHandler in regionserver is processed as the following steps:
{code}
1.openregion()(Through it, closed = false, closing = false)
2.addToOnlineRegions(region)
3.update .meta. table 
4.update ZK's node state to RS_ZK_REGION_OPEND
{code}
We can find that region is on service before Step 4.
It means client could put data to this region after step 3.
What will happen if step 4 is failed processing?
It will execute OpenRegionHandler#cleanupFailedOpen which will do closing region, and master assign this region to another regionserver.
If closing region is failed, the data which is put between step 3 and step 4 may loss, because the region has been opend on another regionserver and be put new data. Therefore, it may not be recoverd through replayRecoveredEdit() because the edit's LogSeqId is smaller than current region SeqId.",,,,,,,,
HBASE-4881,"If region splitting is failed in the state of JournalEntry.CLOSED_PARENT_REGION
It will be rollback as the following steps:
{code}
1.case CLOSED_PARENT_REGION:
  this.parent.initialize();
        break;
2.case CREATE_SPLIT_DIR:
    	this.parent.writestate.writesEnabled = true;
        cleanupSplitDir(fs, this.splitdir);
        break;
3.case SET_SPLITTING_IN_ZK:
        if (server != null && server.getZooKeeper() != null) {
          cleanZK(server, this.parent.getRegionInfo());
        }
        break;
{code}
If this.parent.initialize() throws IOException in step 1,
If check filesystem is ok. it will do nothing.
However, the parent region is on service now.",,,,,,,,
HBASE-4890,"I was running YCSB against a 0.92 branch and encountered this error message:

{code}
11/11/29 08:47:16 WARN client.HConnectionManager$HConnectionImplementation: Failed all from region=usertable,user3917479014967760871,1322555655231.f78d161e5724495a9723bcd972f97f41., hostname=c0316.hal.cloudera.com, port=57020
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.NullPointerException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.processBatchCallback(HConnectionManager.java:1501)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.processBatch(HConnectionManager.java:1353)
        at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:898)
        at org.apache.hadoop.hbase.client.HTable.doPut(HTable.java:775)
        at org.apache.hadoop.hbase.client.HTable.put(HTable.java:750)
        at com.yahoo.ycsb.db.HBaseClient.update(Unknown Source)
        at com.yahoo.ycsb.DBWrapper.update(Unknown Source)
        at com.yahoo.ycsb.workloads.CoreWorkload.doTransactionUpdate(Unknown Source)
        at com.yahoo.ycsb.workloads.CoreWorkload.doTransaction(Unknown Source)
        at com.yahoo.ycsb.ClientThread.run(Unknown Source)
Caused by: java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionServerWithoutRetries(HConnectionManager.java:1315)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$3.call(HConnectionManager.java:1327)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$3.call(HConnectionManager.java:1325)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:158)
        at $Proxy4.multi(Unknown Source)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$3$1.call(HConnectionManager.java:1330)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$3$1.call(HConnectionManager.java:1328)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionServerWithoutRetries(HConnectionManager.java:1309)
        ... 7 more
{code}

It looks like the NPE is caused by server being null in the MultiRespone call() method.

{code}
     public MultiResponse call() throws IOException {
         return getRegionServerWithoutRetries(
             new ServerCallable<MultiResponse>(connection, tableName, null) {
               public MultiResponse call() throws IOException {
                 return server.multi(multi);
               }
               @Override
               public void connect(boolean reload) throws IOException {
                 server =
                   connection.getHRegionConnection(loc.getHostname(), loc.getPort());
               }
             }
         );
{code}",,,,,,,,
HBASE-4893,"In abort() of HConnectionManager$HConnectionImplementation, instance of HConnectionImplementation is marked as this.closed=true.

There is no way for client application to check the hbase client connection whether it is still opened/good (this.closed=false) or not. We need a method to validate the state of a connection like isClosed().

{code}
public boolean isClosed(){
   return this.closed;
} 
{code}

Once the connection is closed and it should get deleted. Client application still gets a connection from HConnectionManager.getConnection(Configuration) and tries to make a RPC call to RS, since connection is already closed, HConnectionImplementation.getRegionServerWithRetries throws RetriesExhaustedException with error message

{code}
Caused by: org.apache.hadoop.hbase.client.RetriesExhaustedException: Trying to contact region server null for region , row 'xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxx', but failed after 10 attempts.
Exceptions:
java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@7eab48a7 closed
java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@7eab48a7 closed
java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@7eab48a7 closed
java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@7eab48a7 closed
java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@7eab48a7 closed
java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@7eab48a7 closed
java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@7eab48a7 closed
java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@7eab48a7 closed
java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@7eab48a7 closed
java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@7eab48a7 closed
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionServerWithRetries(HConnectionManager.java:1008)
	at org.apache.hadoop.hbase.client.HTable.get(HTable.java:546)
{code}",,,,,,,,
HBASE-4899,"Before assigning region in ServerShutdownHandler#process, it will check whether region is in RIT,
however, this checking doesn't work as the excepted in the following case:
1.move region A from server B to server C
2.kill server B
3.start server B immediately

Let's see what happen in the code for the above case
{code}
for step1:
1.1 server B close the region A,
1.2 master setOffline for region A,(AssignmentManager#setOffline:this.regions.remove(regionInfo))
1.3 server C start to open region A.(Not completed)
for step3:
master ServerShutdownHandler#process() for server B
{
..
splitlog()
...
List<RegionState> regionsInTransition =
        this.services.getAssignmentManager()
        .processServerShutdown(this.serverName);
...
Skip regions that were in transition unless CLOSING or PENDING_CLOSE
...
assign region
}
{code}
In fact, when running ServerShutdownHandler#process()#this.services.getAssignmentManager().processServerShutdown(this.serverName), region A is in RIT (step1.3 not completed), but the return List<RegionState> regionsInTransition doesn't contain it, because region A has removed from AssignmentManager.regions by AssignmentManager#setOffline in step 1.2
Therefore, region A will be assigned twice.

Actually, one server killed and started twice will also easily cause region assigned twice.
Exclude the above reason, another probability : 
when execute ServerShutdownHandler#process()#MetaReader.getServerUserRegions ,region is included which is in RIT now.
But after completing MetaReader.getServerUserRegions, the region has been opened in other server and is not in RIT now.

In our testing environment where balancing,moving and killing are executed periodly, assigning region twice often happens, and it is hateful because it will affect other test cases.",,,,,,,,
HBASE-4918,"{code:title=HTablePool.java}
96   public HTablePool(final Configuration config, final int maxSize,
97       final HTableInterfaceFactory tableFactory) {
98     this(config, maxSize, null, PoolType.Reusable);
99   }
{code} 
I think that 3rd argument in line 98 should be ""tableFactory"".",,,,,,,,
HBASE-4926,"When reviewing HBASE-4238 backporting, Jon found this issue.

What happens if the split points are  (empty end key is the last key, empty start key is the first key)

Parent     [A,)
L daughter [A,B), 
R daughter [B,)

When sorted, we gets to end key comparision which results in this incorrector order:
[A,B), [A,), [B,) 

we wanted:
[A,), [A,B), [B,)",,,1,,,,,
HBASE-4927,"When reviewing HBASE-4238 backporting, Jon found this issue.

What happens if the split points are  (empty end key is the last key, empty start key is the first key)

Parent     [A,)
L daughter [A,B), 
R daughter [B,)

When sorted, we gets to end key comparision which results in this incorrector order:
[A,B), [A,), [B,) 

we wanted:
[A,), [A,B), [B,)",,1,,,,,,
HBASE-4932,Map Reduce tasks that create a writer to write HFiles inadvertently end up creating block cache.,,,,,,,,
HBASE-4936,"This isssue is unlikely to come up in a cluster test case. However, for development, the following thing happens: 

1. Start the HBase cluster locally, on network A (DNS A, etc)
2. The region locations are cached using the hostname (mycomputer.company.com, 211.x.y.z - real ip)
3. Change network location (go home)
4. Start the HBase cluster locally. My hostname / ips are not different (mycomputer, 192.168.0.130 - new ip)

If the region locations have been cached using the hostname, there is an UnknownHostException in CatalogTracker.getCachedConnection(ServerName sn), uncaught in the catch statements. The server will crash constantly. 

The error should be caught and not rethrown, so that the cached connection expires normally. ",,1,,,,,,
HBASE-4942,"This was reported by HH Zhu (zhh200910@gmail.com)
If the following is specified in hbase-site.xml?{code}
    <property>
        <name>hfile.format.version</name>
        <value>1</value>
    </property>
{code}
Clear the hdfs directory ""hbase.rootdir"" so that MasterFileSystem.bootstrap() is executed.
You would see:
{code}
java.lang.NullPointerException
    at org.apache.hadoop.hbase.io.hfile.HFileReaderV1.close(HFileReaderV1.java:358)
    at org.apache.hadoop.hbase.regionserver.StoreFile$Reader.close(StoreFile.java:1083)
    at org.apache.hadoop.hbase.regionserver.StoreFile.closeReader(StoreFile.java:570)
    at org.apache.hadoop.hbase.regionserver.Store.close(Store.java:441)
    at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:782)
    at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:717)
    at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:688)
    at org.apache.hadoop.hbase.master.MasterFileSystem.bootstrap(MasterFileSystem.java:390)
    at org.apache.hadoop.hbase.master.MasterFileSystem.checkRootDir(MasterFileSystem.java:356)
    at org.apache.hadoop.hbase.master.MasterFileSystem.createInitialFileSystemLayout(MasterFileSystem.java:128)
    at org.apache.hadoop.hbase.master.MasterFileSystem.<init>(MasterFileSystem.java:113)
    at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:435)
    at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:314)
    at java.lang.Thread.run(Thread.java:619)
{code}
The above exception would lead to:
{code}
java.lang.RuntimeException: HMaster Aborted
    at org.apache.hadoop.hbase.master.HMasterCommandLine.startMaster(HMasterCommandLine.java:152)
    at org.apache.hadoop.hbase.master.HMasterCommandLine.run(HMasterCommandLine.java:103)
    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
    at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:76)
    at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:1512)
{code}

In org.apache.hadoop.hbase.master.HMaster.HMaster(Configuration conf), we have:
{code}
this.conf.setFloat(CacheConfig.HFILE_BLOCK_CACHE_SIZE_KEY, 0.0f);
{code}
When CacheConfig is instantiated, the following is called:
{code}
org.apache.hadoop.hbase.io.hfile.CacheConfig.instantiateBlockCache(Configuration conf)
{code}
Since ""hfile.block.cache.size"" is 0.0, instantiateBlockCache() would return null, resulting in blockCache field of CacheConfig to be null.
When master closes Root region, org.apache.hadoop.hbase.io.hfile.HFileReaderV1.close(boolean evictOnClose) would be called. cacheConf.getBlockCache() returns null, leading to master abort.

The following should be called in HFileReaderV1.close(), similar to the code in HFileReaderV2.close():
{code}
if (evictOnClose && cacheConf.isBlockCacheEnabled())
{code}",,1,,,,,,
HBASE-4945,"Was playing with ""completebulkload"", and ran into an NPE.
The problem is here (HRegion.bulkLoadHFiles(...)).

{code}
Store store = getStore(familyName);
if (store == null) {
  IOException ioe = new DoNotRetryIOException(
      ""No such column family "" + Bytes.toStringBinary(familyName));
  ioes.add(ioe);
  failures.add(p);
}

try {
  store.assertBulkLoadHFileOk(new Path(path));
} catch (WrongRegionException wre) {
  // recoverable (file doesn't fit in region)
  failures.add(p);
} catch (IOException ioe) {
  // unrecoverable (hdfs problem)
  ioes.add(ioe);
}
{code}

This should be 
{code}
Store store = getStore(familyName);
if (store == null) {
...
} else {
  try {
    store.assertBulkLoadHFileOk(new Path(path));
...
}
{code}
",,,,,,,,
HBASE-4946,"Loading coprocessors jars from hdfs works fine. I load it from the shell, after setting the attribute, and it gets loaded:

{noformat}
INFO org.apache.hadoop.hbase.regionserver.HRegion: Setting up tabledescriptor config now ...
INFO org.apache.hadoop.hbase.coprocessor.CoprocessorHost: Class com.MyCoprocessorClass needs to be loaded from a file - hdfs://localhost:9000/coproc/rt-      >0.0.1-SNAPSHOT.jar.
INFO org.apache.hadoop.hbase.coprocessor.CoprocessorHost: loadInstance: com.MyCoprocessorClass
INFO org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost: RegionEnvironment createEnvironment
DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Registered protocol handler: region=t1,,1322572939753.6409aee1726d31f5e5671a59fe6e384f. protocol=com.MyCoprocessorClassProtocol
INFO org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost: Load coprocessor com.MyCoprocessorClass from HTD of t1 successfully.
{noformat}

The problem is that this coprocessors simply extends BaseEndpointCoprocessor, with a dynamic method. When calling this method from the client with HTable.coprocessorExec, I get errors on the HRegionServer, because the call cannot be deserialized from writables. 

The problem is that Exec tries to do an ""early"" resolve of the coprocessor class. The coprocessor class is loaded, but it is in the context of the HRegionServer / HRegion. So, the call fails:

{noformat}
2011-12-02 00:34:17,348 ERROR org.apache.hadoop.hbase.io.HbaseObjectWritable: Error in readFields
java.io.IOException: Protocol class com.MyCoprocessorClassProtocol not found
  at org.apache.hadoop.hbase.client.coprocessor.Exec.readFields(Exec.java:125)
  at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:575)
  at org.apache.hadoop.hbase.ipc.Invocation.readFields(Invocation.java:105)
  at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.processData(HBaseServer.java:1237)
  at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.readAndProcess(HBaseServer.java:1167)
  at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.doRead(HBaseServer.java:703)
  at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.doRunLoop(HBaseServer.java:495)
  at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:470)
  at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
  at java.lang.Thread.run(Thread.java:680)
Caused by: java.lang.ClassNotFoundException: com.MyCoprocessorClassProtocol
  at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
  at java.security.AccessController.doPrivileged(Native Method)
  at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
  at java.lang.Class.forName0(Native Method)
  at java.lang.Class.forName(Class.java:247)
  at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:943)
  at org.apache.hadoop.hbase.client.coprocessor.Exec.readFields(Exec.java:122)
  ... 10 more
{noformat}

Probably the correct way to fix this is to make Exec really smart, so that it knows all the class definitions loaded in CoprocessorHost(s).

I created a small patch that simply doesn't resolve the class definition in the Exec, instead passing it as string down to the HRegion layer. This layer knows all the definitions, and simply loads it by name. 

",,1,,,,,,
HBASE-4951,"It is easy to reproduce by following step:
step1:start master process.(do not start regionserver process in the cluster).
the master will wait the regionserver to check in:
org.apache.hadoop.hbase.master.ServerManager: Waiting on regionserver(s) to checkin

step2:stop the master by sh command bin/hbase master stop

result:the master process will never die because catalogTracker.waitForRoot() method will block unitl the root region assigned.
",,1,,,,,,
HBASE-4956,"It is easy to reproduce by following step:
step1:start master process.(do not start regionserver process in the cluster).
the master will wait the regionserver to check in:
org.apache.hadoop.hbase.master.ServerManager: Waiting on regionserver(s) to checkin

step2:stop the master by sh command bin/hbase master stop

result:the master process will never die because catalogTracker.waitForRoot() method will block unitl the root region assigned.
",,1,,,,,,
HBASE-4966,"When using the IdentityTableReducer, which expects input values of either a Put or Delete object, testing with MRUnit the Mapper with MRUnit is not possible because neither Put nor Delete implement equals().

We should implement equals() on both such that equality means:
* Both objects are of the same class (in this case, Put or Delete)
* Both objects are for the same key.
* Both objects contain an equal set of KeyValues (applicable only to Put)

KeyValue.equals() appears to already be implemented, but only checks for equality of row key, column family and column qualifier - two KeyValues can be considered ""equal"" if they contain different values. This won't work for testing.

Instead, the Put.equals() and Delete.equals() implementations should do a ""deep"" equality check on their KeyValues, like this:

{code:java}
myKv.equals(theirKv) && Bytes.equals(myKv.getValue(), theirKv.getValue());
{code}

NOTE: This would impact any code that relies on the existing ""identity"" implementation of Put.equals() and Delete.equals(), therefore cannot be guaranteed to be backwards-compatible.",,,,,,,,
HBASE-4967,"When using the IdentityTableReducer, which expects input values of either a Put or Delete object, testing with MRUnit the Mapper with MRUnit is not possible because neither Put nor Delete implement equals().

We should implement equals() on both such that equality means:
* Both objects are of the same class (in this case, Put or Delete)
* Both objects are for the same key.
* Both objects contain an equal set of KeyValues (applicable only to Put)

KeyValue.equals() appears to already be implemented, but only checks for equality of row key, column family and column qualifier - two KeyValues can be considered ""equal"" if they contain different values. This won't work for testing.

Instead, the Put.equals() and Delete.equals() implementations should do a ""deep"" equality check on their KeyValues, like this:

{code:java}
myKv.equals(theirKv) && Bytes.equals(myKv.getValue(), theirKv.getValue());
{code}

NOTE: This would impact any code that relies on the existing ""identity"" implementation of Put.equals() and Delete.equals(), therefore cannot be guaranteed to be backwards-compatible.",,1,,,,,,
HBASE-4970,"Relevant Stack trace: 

2011-11-30 13:10:26,557 [IPC Client (47) connection to xx.xx.xx/172.22.4.68:60020 from an unknown user] WARN  org.apache.hadoop.ipc.HBaseClient - Unexpected exception receiving call responses
java.lang.NullPointerException
>-at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.receiveResponse(HBaseClient.java:583)
>-at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.run(HBaseClient.java:511)


{code}
  if (LOG.isDebugEnabled())
          LOG.debug(getName() + "" got value #"" + id);
        Call call = calls.remove(id);
        // Read the flag byte
        byte flag = in.readByte();
        boolean isError = ResponseFlag.isError(flag);
        if (ResponseFlag.isLength(flag)) {
          // Currently length if present is unused.
          in.readInt();
        }
        int state = in.readInt(); // Read the state.  Currently unused.
        if (isError) {
          //noinspection ThrowableInstanceNeverThrown
          call.setException(new RemoteException( WritableUtils.readString(in),
              WritableUtils.readString(in)));
        } else {
{code}

This line {code}Call call = calls.remove(id);{code}  may return a null 'call'. It is so because if you have rpc timeout enable, we proactively clean up other calls which have expired their lifetime along with the call for which socket timeout exception happend.",,,1,,,,,
HBASE-4973,"Relevant Stack trace: 

2011-11-30 13:10:26,557 [IPC Client (47) connection to xx.xx.xx/172.22.4.68:60020 from an unknown user] WARN  org.apache.hadoop.ipc.HBaseClient - Unexpected exception receiving call responses
java.lang.NullPointerException
>-at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.receiveResponse(HBaseClient.java:583)
>-at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.run(HBaseClient.java:511)


{code}
  if (LOG.isDebugEnabled())
          LOG.debug(getName() + "" got value #"" + id);
        Call call = calls.remove(id);
        // Read the flag byte
        byte flag = in.readByte();
        boolean isError = ResponseFlag.isError(flag);
        if (ResponseFlag.isLength(flag)) {
          // Currently length if present is unused.
          in.readInt();
        }
        int state = in.readInt(); // Read the state.  Currently unused.
        if (isError) {
          //noinspection ThrowableInstanceNeverThrown
          call.setException(new RemoteException( WritableUtils.readString(in),
              WritableUtils.readString(in)));
        } else {
{code}

This line {code}Call call = calls.remove(id);{code}  may return a null 'call'. It is so because if you have rpc timeout enable, we proactively clean up other calls which have expired their lifetime along with the call for which socket timeout exception happend.",,,1,,,,,
HBASE-4980,"Relevant Stack trace: 

2011-11-30 13:10:26,557 [IPC Client (47) connection to xx.xx.xx/172.22.4.68:60020 from an unknown user] WARN  org.apache.hadoop.ipc.HBaseClient - Unexpected exception receiving call responses
java.lang.NullPointerException
>-at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.receiveResponse(HBaseClient.java:583)
>-at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.run(HBaseClient.java:511)


{code}
  if (LOG.isDebugEnabled())
          LOG.debug(getName() + "" got value #"" + id);
        Call call = calls.remove(id);
        // Read the flag byte
        byte flag = in.readByte();
        boolean isError = ResponseFlag.isError(flag);
        if (ResponseFlag.isLength(flag)) {
          // Currently length if present is unused.
          in.readInt();
        }
        int state = in.readInt(); // Read the state.  Currently unused.
        if (isError) {
          //noinspection ThrowableInstanceNeverThrown
          call.setException(new RemoteException( WritableUtils.readString(in),
              WritableUtils.readString(in)));
        } else {
{code}

This line {code}Call call = calls.remove(id);{code}  may return a null 'call'. It is so because if you have rpc timeout enable, we proactively clean up other calls which have expired their lifetime along with the call for which socket timeout exception happend.",,,,,,,,
HBASE-4987,@Ramakrishna found and analyzed an issue in SplitLogManager. But I don't think that the fix is correct. Will upload a patch shortly.,,,,,,,,
HBASE-4988,"If metaserver crash now,
All the splitting regionserver will abort theirself.
Becasue the code
{code}
this.journal.add(JournalEntry.PONR);
MetaEditor.offlineParentInMeta(server.getCatalogTracker(),
            this.parent.getRegionInfo(), a.getRegionInfo(), b.getRegionInfo());
{code}
If the JournalEntry is PONR, split's roll back will abort itselef.

It is terrible in huge putting environment when metaserver crash
",,,,,,,,
HBASE-4993,"Side effect of 4610: the mini cluster needs 4,5 seconds to start",,,1,,,,,
HBASE-4995,"Side effect of 4610: the mini cluster needs 4,5 seconds to start",,,1,,,,,
HBASE-4997,This is a continuation of HBASE-4987. Will put up a patch shortly.,,1,,,,,,
HBASE-4999,This is a continuation of HBASE-4987. Will put up a patch shortly.,,1,,,,,,
HBASE-5003,"Reported by a new user on IRC who tried to set hbase.rootdir to file:///~/hbase, the master gets stuck and cannot be killed. I tried something similar on my machine and it spins while logging:

{quote}
2011-12-09 16:11:17,002 WARN org.apache.hadoop.hbase.util.FSUtils: Unable to create version file at file:/bin/hbase, retrying: Mkdirs failed to create file:/bin/hbase
2011-12-09 16:11:27,002 WARN org.apache.hadoop.hbase.util.FSUtils: Unable to create version file at file:/bin/hbase, retrying: Mkdirs failed to create file:/bin/hbase
2011-12-09 16:11:37,003 WARN org.apache.hadoop.hbase.util.FSUtils: Unable to create version file at file:/bin/hbase, retrying: Mkdirs failed to create file:/bin/hbase
{quote}

The reason it cannot be stopped is that the master's main thread is stuck in there and will never be notified:

{quote}
""Master:0;su-jdcryans-01.local,51116,1323475535684"" prio=5 tid=7f92b7a3c000 nid=0x1137ba000 waiting on condition [1137b9000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hbase.util.FSUtils.setVersion(FSUtils.java:297)
	at org.apache.hadoop.hbase.util.FSUtils.setVersion(FSUtils.java:268)
	at org.apache.hadoop.hbase.master.MasterFileSystem.checkRootDir(MasterFileSystem.java:339)
	at org.apache.hadoop.hbase.master.MasterFileSystem.createInitialFileSystemLayout(MasterFileSystem.java:128)
	at org.apache.hadoop.hbase.master.MasterFileSystem.<init>(MasterFileSystem.java:113)
	at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:435)
	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:314)
	at org.apache.hadoop.hbase.master.HMasterCommandLine$LocalHMaster.run(HMasterCommandLine.java:218)
	at java.lang.Thread.run(Thread.java:680)
{quote}

It seems we should do a better handling of the exceptions we get in there, and die if we need to. It would make a better user experience.

Maybe also do a check on hbase.rootdir before even starting the master.",,1,,,,,,
HBASE-5007,"Please running this example:

public class Test {
  public static void main(String[] args) throws Exception {
    HBaseAdmin admin = new HBaseAdmin(HBaseConfiguration.create());
    admin.stopRegionServer(""your.rs.hostname:60020"");
  }
}

then, you can see:

Exception in thread ""main"" java.lang.RuntimeException: The interface org.apache.hadoop.hbase.Stoppable
    at org.apache.hadoop.hbase.ipc.Invocation.<init>(Invocation.java:61)
    at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:151)
    at $Proxy2.stop(Unknown Source)
    at org.apache.hadoop.hbase.client.HBaseAdmin.stopRegionServer(HBaseAdmin.java:1492)
    at Test.main(Test.java:7)
Caused by: java.lang.NoSuchFieldException: VERSION
    at java.lang.Class.getField(Class.java:1520)
    at org.apache.hadoop.hbase.ipc.Invocation.<init>(Invocation.java:57)
    ... 4 more





When invoking the ""HBaseAdmin.stopRegionServer"" method?we obtain a ""proxy"" for org.apache.hadoop.hbase.ipc.HRegionInterface?(HRegionInterface extends org.apache.hadoop.hbase.Stoppable)
but the ""stop"" method declared in Stoppable.

In the constructor of ""org.apache.hadoop.hbase.ipc.Invocation""?the ""method"" argument is ""public abstract void org.apache.hadoop.hbase.Stoppable.stop(java.lang.String)""?so, ""method.getDeclaringClass()"" is ""org.apache.hadoop.hbase.Stoppable""?but, the ""Stoppable"" interface no ""VERSION"" field.



[fix suggestion]:

Override the ""stop"" method in org.apache.hadoop.hbase.ipc.HRegionInterface as follows:
==================================
@Override
public void stop(String why);


of courese, another attempt is ok.
(e.g. declare ""VERSION"" field in Stoppable interface?then modify some code fragment of Invocation and org.apache.hadoop.hbase.ipc.WritableRpcEngine.Server)
",,,,,,,,
HBASE-5008,"Hbase version 0.90.4 + patches

My analysis is as follows:

//Started splitting region b24d8ccb852ff742f2a27d01b7f5853e and closed region.

2011-12-10 17:32:48,653 INFO org.apache.hadoop.hbase.regionserver.SplitTransaction: Starting split of region Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.
2011-12-10 17:32:49,759 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Closing Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.: disabling compactions & flushes
2011-12-10 17:32:49,759 INFO org.apache.hadoop.hbase.regionserver.HRegion: Running close preflush of Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.

//Processed a flush request and skipped , But flushRequested had set to true
2011-12-10 17:33:06,963 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memstore flush for Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e., current region memstore size 12.6m
2011-12-10 17:33:17,277 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Skipping flush on Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e. because closing

//split region b24d8ccb852ff742f2a27d01b7f5853 failed and rolled back, flushRequested flag was true, So all handle was blocked 

2011-12-10 17:34:01,293 INFO org.apache.hadoop.hbase.regionserver.SplitTransaction: Cleaned up old failed split transaction detritus: hdfs://193.195.18.121:9000/hbase/Htable_UFDR_004/b24d8ccb852ff742f2a27d01b7f5853e/splits
2011-12-10 17:34:01,294 INFO org.apache.hadoop.hbase.regionserver.HRegion: Onlined Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.; next sequenceid=15494173
2011-12-10 17:34:01,295 INFO org.apache.hadoop.hbase.regionserver.CompactSplitThread: Successful rollback of failed split of Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.
2011-12-10 17:43:10,147 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 19 on 20020' on region 
Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.: memstore size 384.0m is >= than blocking 384.0m size


// All handles had been blocked. The clusters could not provide services

2011-12-10 17:34:01,295 INFO org.apache.hadoop.hbase.regionserver.CompactSplitThread: Successful rollback of failed split of Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.
2011-12-10 17:43:10,147 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 19 on 20020' on region Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.: memstore size 384.0m is >= than blocking 384.0m size
2011-12-10 17:43:10,192 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 34 on 20020' on region Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.: memstore size 384.0m is >= than blocking 384.0m size
2011-12-10 17:43:10,193 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 51 on 20020' on region Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.: memstore size 384.0m is >= than blocking 384.0m size
2011-12-10 17:43:10,196 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 85 on 20020' on region Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.: memstore size 384.0m is >= than blocking 384.0m size
2011-12-10 17:43:10,199 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 88 on 20020' on region Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.: memstore size 384.0m is >= than blocking 384.0m size
2011-12-10 17:43:10,202 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 44 on 20020' on region Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.: memstore size 384.0m is >= than blocking 384.0m size
2011-12-10 17:43:11,663 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 2 on 20020' on region Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.: memstore size 384.0m is >= than blocking 384.0m size
2011-12-10 17:43:11,665 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 10 on 20020' on region Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.: memstore size 384.0m is >= than blocking 384.0m size
2011-12-10 17:43:11,670 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 75 on 20020' on region Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.: memstore size 384.0m is >= than blocking 384.0m size
2011-12-10 17:43:11,671 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 98 on 20020' on region Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.: memstore size 384.0m is >= than blocking 384.0m size
2011-12-10 17:43:11,680 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 11 on 20020' on region 



",,,,,,,,
HBASE-5033,"Reported by a new user on IRC who tried to set hbase.rootdir to file:///~/hbase, the master gets stuck and cannot be killed. I tried something similar on my machine and it spins while logging:

{quote}
2011-12-09 16:11:17,002 WARN org.apache.hadoop.hbase.util.FSUtils: Unable to create version file at file:/bin/hbase, retrying: Mkdirs failed to create file:/bin/hbase
2011-12-09 16:11:27,002 WARN org.apache.hadoop.hbase.util.FSUtils: Unable to create version file at file:/bin/hbase, retrying: Mkdirs failed to create file:/bin/hbase
2011-12-09 16:11:37,003 WARN org.apache.hadoop.hbase.util.FSUtils: Unable to create version file at file:/bin/hbase, retrying: Mkdirs failed to create file:/bin/hbase
{quote}

The reason it cannot be stopped is that the master's main thread is stuck in there and will never be notified:

{quote}
""Master:0;su-jdcryans-01.local,51116,1323475535684"" prio=5 tid=7f92b7a3c000 nid=0x1137ba000 waiting on condition [1137b9000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hbase.util.FSUtils.setVersion(FSUtils.java:297)
	at org.apache.hadoop.hbase.util.FSUtils.setVersion(FSUtils.java:268)
	at org.apache.hadoop.hbase.master.MasterFileSystem.checkRootDir(MasterFileSystem.java:339)
	at org.apache.hadoop.hbase.master.MasterFileSystem.createInitialFileSystemLayout(MasterFileSystem.java:128)
	at org.apache.hadoop.hbase.master.MasterFileSystem.<init>(MasterFileSystem.java:113)
	at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:435)
	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:314)
	at org.apache.hadoop.hbase.master.HMasterCommandLine$LocalHMaster.run(HMasterCommandLine.java:218)
	at java.lang.Thread.run(Thread.java:680)
{quote}

It seems we should do a better handling of the exceptions we get in there, and die if we need to. It would make a better user experience.

Maybe also do a check on hbase.rootdir before even starting the master.",,,1,,,,,
HBASE-5050,"Currently the REST gateway can authenticate to a HBase cluster using a preconfigured principal. This provides a limited form of secure operation where one or more gateways can be deployed with distinct principals granting appropriate levels of privilege, but the service ports must be protected through network ACLs. This is at best a stopgap.
SPNEGO is the standard mechanism for Kerberos authentication over HTTP. Enhance the REST gateway such that it provides this option, and issues requests to the HBase cluster with the established context.",1,,,,,,,
HBASE-5052,"When loading a coprocessor from hdfs, the jar file gets copied to a path on the local filesystem, which depends on the region name, and the region start key. The name is ""cleaned"", but not enough, so when you have filesystem unfriendly characters (/?:, etc), the coprocessor is not loaded, and an error is thrown",,1,,,,,,
HBASE-5060,"Since the client had a temporary network failure, After it recovered.
I found my client thread was blocked. 
Looks below stack and logs, It said that we use a invalid CatalogTracker in function ""tableExists"".

Block stack:
""WriteHbaseThread33"" prio=10 tid=0x00007f76bc27a800 nid=0x2540 in Object.wait() [0x00007f76af4f3000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
         at java.lang.Object.wait(Native Method)
         at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMeta(CatalogTracker.java:331)
         - locked <0x00007f7a67817c98> (a java.util.concurrent.atomic.AtomicBoolean)
         at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMetaServerConnectionDefault(CatalogTracker.java:366)
         at org.apache.hadoop.hbase.catalog.MetaReader.tableExists(MetaReader.java:427)
         at org.apache.hadoop.hbase.client.HBaseAdmin.tableExists(HBaseAdmin.java:164)
         at com.huawei.hdi.hbase.HbaseFileOperate.checkHtableState(Unknown Source)
         at com.huawei.hdi.hbase.HbaseReOper.reCreateHtable(Unknown Source)
         - locked <0x00007f7a4c5dc578> (a com.huawei.hdi.hbase.HbaseReOper)
         at com.huawei.hdi.hbase.HbaseFileOperate.writeToHbase(Unknown Source)
         at com.huawei.hdi.hbase.WriteHbaseThread.run(Unknown Source)

In ZooKeeperNodeTracker, We don't throw the KeeperException to high level.
So in CatalogTracker level, We think ZooKeeperNodeTracker start success and
continue to process .

[WriteHbaseThread33]2011-12-16 17:07:33,153[WARN ]  | hconnection-0x334129cf6890051-0x334129cf6890051-0x334129cf6890051 Unable to get data of znode /hbase/root-region-server | org.apache.hadoop.hbase.zookeeper.ZKUtil.getDataAndWatch(ZKUtil.java:557)
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/root-region-server
         at org.apache.zookeeper.KeeperException.create(KeeperException.java:90)
         at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
         at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:931)
         at org.apache.hadoop.hbase.zookeeper.ZKUtil.getDataAndWatch(ZKUtil.java:549)
         at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.start(ZooKeeperNodeTracker.java:73)
         at org.apache.hadoop.hbase.catalog.CatalogTracker.start(CatalogTracker.java:136)
         at org.apache.hadoop.hbase.client.HBaseAdmin.getCatalogTracker(HBaseAdmin.java:111)
         at org.apache.hadoop.hbase.client.HBaseAdmin.tableExists(HBaseAdmin.java:162)
         at com.huawei.hdi.hbase.HbaseFileOperate.checkHtableState(Unknown Source)
         at com.huawei.hdi.hbase.HbaseReOper.reCreateHtable(Unknown Source)
         at com.huawei.hdi.hbase.HbaseFileOperate.writeToHbase(Unknown Source)
         at com.huawei.hdi.hbase.WriteHbaseThread.run(Unknown Source)
[WriteHbaseThread33]2011-12-16 17:07:33,361[ERROR]  | hconnection-0x334129cf6890051-0x334129cf6890051-0x334129cf6890051 Received unexpected KeeperException, re-throwing exception | org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.keeperException(ZooKeeperWatcher.java:385)
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/root-region-server
         at org.apache.zookeeper.KeeperException.create(KeeperException.java:90)
         at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
         at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:931)
         at org.apache.hadoop.hbase.zookeeper.ZKUtil.getDataAndWatch(ZKUtil.java:549)
         at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.start(ZooKeeperNodeTracker.java:73)
         at org.apache.hadoop.hbase.catalog.CatalogTracker.start(CatalogTracker.java:136)
         at org.apache.hadoop.hbase.client.HBaseAdmin.getCatalogTracker(HBaseAdmin.java:111)
         at org.apache.hadoop.hbase.client.HBaseAdmin.tableExists(HBaseAdmin.java:162)
         at com.huawei.hdi.hbase.HbaseFileOperate.checkHtableState(Unknown Source)
         at com.huawei.hdi.hbase.HbaseReOper.reCreateHtable(Unknown Source)
         at com.huawei.hdi.hbase.HbaseFileOperate.writeToHbase(Unknown Source)
         at com.huawei.hdi.hbase.WriteHbaseThread.run(Unknown Source)


[WriteHbaseThread33]2011-12-16 17:07:33,361[FATAL]  | Unexpected exception during initialization, aborting | org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.abort(HConnectionManager.java:1351)
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/root-region-server
         at org.apache.zookeeper.KeeperException.create(KeeperException.java:90)
         at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
         at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:931)
         at org.apache.hadoop.hbase.zookeeper.ZKUtil.getDataAndWatch(ZKUtil.java:549)
         at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.start(ZooKeeperNodeTracker.java:73)
         at org.apache.hadoop.hbase.catalog.CatalogTracker.start(CatalogTracker.java:136)
         at org.apache.hadoop.hbase.client.HBaseAdmin.getCatalogTracker(HBaseAdmin.java:111)
         at org.apache.hadoop.hbase.client.HBaseAdmin.tableExists(HBaseAdmin.java:162)
         at com.huawei.hdi.hbase.HbaseFileOperate.checkHtableState(Unknown Source)
         at com.huawei.hdi.hbase.HbaseReOper.reCreateHtable(Unknown Source)
         at com.huawei.hdi.hbase.HbaseFileOperate.writeToHbase(Unknown Source)
         at com.huawei.hdi.hbase.WriteHbaseThread.run(Unknown Source)
",,,,,,,,
HBASE-5063,"# Setup cluster with two HMasters
# Observe that HM1 is up and that all RS's are in the RegionServer list on web page.
# Kill (not even -9) the active HMaster
# Wait for ZK to time out (default 3 minutes).
# Observe that HM2 is now active.  Tables may show up but RegionServers never report on web page.  Existing connections are fine.  New connections cannot find regionservers.

Note: 
* If we replace a new HM1 in the same place and kill HM2, the cluster functions normally again after recovery.  This sees to indicate that regionservers are stuck trying to talk to the old HM1.


",,,,,,,,
HBASE-5075,"SoftValueSortedMap is backed by a TreeMap. All the methods in this class are synchronized. If we use this method to add/delete elements, it's ok.
But in HConnectionManager#getCachedLocation, it use headMap to get a view from SoftValueSortedMap#internalMap. Once we operate 
on this view map(like add/delete) in other threads, a concurrency issue may occur.

",,,,,,,,
HBASE-5088,"SoftValueSortedMap is backed by a TreeMap. All the methods in this class are synchronized. If we use this method to add/delete elements, it's ok.
But in HConnectionManager#getCachedLocation, it use headMap to get a view from SoftValueSortedMap#internalMap. Once we operate 
on this view map(like add/delete) in other threads, a concurrency issue may occur.

",,,,,,,,
HBASE-5092,"  
Region is in PENDING_OPEN state and disable and enable are blocked.

We occasionally find if two assignments which have a short interval time will lead to a PENDING_OPEN state staying in the regionInTransition map and blocking the disable and enable table actions.

We found that the second assignment will set the zknode of this region to M_ZK_REGION_OFFLINE then set the state in assignmentMananger's regionInTransition map to PENDING_OPEN and abort its further operation because of finding the the region is already in the regionserver by a RegionAlreadyInTransitionException.
At the same time the first assignment is tickleOpening and find the version of the zknode is messed up by the  second assignment, so the OpenRegionHandler print out the following two lines:

{noformat} 
2011-12-23 22:12:15,197 WARN  [RS_OPEN_REGION-data16,59892,1324649528415-0] zookeeper.ZKAssign(788): regionserver:59892-0x1346b43b91e0002 Attempt to transition the unassigned node for 15237599c632752b8cfd3d5a86349768 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING failed, the node existed but was version 2 not the expected version 1
2011-12-23 22:12:15,197 WARN  [RS_OPEN_REGION-data16,59892,1324649528415-0] handler.OpenRegionHandler(403): Failed refreshing OPENING; region=15237599c632752b8cfd3d5a86349768, context=post_region_open
{noformat} 

After that it tries to turn the state to FAILED_OPEN, but also failed due to wrong version,

this is the output:

{noformat} 
2011-12-23 22:12:15,199 WARN  [RS_OPEN_REGION-data16,59892,1324649528415-0] zookeeper.ZKAssign(812): regionserver:59892-0x1346b43b91e0002 Attempt to transition the unassigned node for 15237599c632752b8cfd3d5a86349768 from RS_ZK_REGION_OPENING to RS_ZK_REGION_FAILED_OPEN failed, the node existed but was in the state M_ZK_REGION_OFFLINE set by the server data16,59892,1324649528415
2011-12-23 22:12:15,199 WARN  [RS_OPEN_REGION-data16,59892,1324649528415-0] handler.OpenRegionHandler(307): Unable to mark region {NAME => 'table1,,1324649533045.15237599c632752b8cfd3d5a86349768.', STARTKEY => '', ENDKEY => '', ENCODED => 15237599c632752b8cfd3d5a86349768,} as FAILED_OPEN. It's likely that the master already timed out this open attempt, and thus another RS already has the region.
{noformat} 

So after all that, the PENDING_OPEN state is left in the assignmentMananger's regionInTransition map and none will deal with it further,
This kind of situation will wait until the master find the state out of time.


The following is the test code:

{code:title=test.java|borderStyle=solid}
@Test
  public void testDisableTables() throws IOException {
    for (int i = 0; i < 20; i++) {
      HTableDescriptor des = admin.getTableDescriptor(Bytes.toBytes(table1));
      List<HRegionInfo> hris = TEST_UTIL.getHBaseCluster().getMaster()
          .getAssignmentManager().getRegionsOfTable(Bytes.toBytes(table1));
      TEST_UTIL.getHBaseCluster().getMaster()
          .assign(hris.get(0).getRegionName());
  
      TEST_UTIL.getHBaseCluster().getMaster()
          .assign(hris.get(0).getRegionName());
  
      admin.disableTable(Bytes.toBytes(table1));
      admin.modifyTable(Bytes.toBytes(table1), des);
      admin.enableTable(Bytes.toBytes(table1));
    }
  }

{code}

To fix this,we add a line to 

public static int ZKAssign.transitionNode() to make endState.RS_ZK_REGION_FAILED_OPEN transition pass.

{code:title=ZKAssign.java|borderStyle=solid}
   if((!existingData.getEventType().equals(beginState))
      //add the following line to make endState.RS_ZK_REGION_FAILED_OPEN transition pass.
      &&(!endState.equals(endState.RS_ZK_REGION_FAILED_OPEN))) {
      LOG.warn(zkw.prefix(""Attempt to transition the "" +
        ""unassigned node for "" + encoded +
        "" from "" + beginState + "" to "" + endState + "" failed, "" +
        ""the node existed but was in the state "" + existingData.getEventType() +
        "" set by the server "" + serverName));
      return -1;
    }
{code}

Run the test case again we found that before the first assignment trans the state from offline to opening, the second assignment could set the state to offline again and messed up the version of zknode.


In OpenRegionHandler.process() the following part failed and make the process() return.
{code:title=OpenRegionHandler.java|borderStyle=solid}
 if (!transitionZookeeperOfflineToOpening(encodedName,
          versionOfOfflineNode)) {
        LOG.warn(""Region was hijacked? It no longer exists, encodedName="" +
          encodedName);
        return;
{code}      }

//So we add the following code to the part to make this open region process to FAILED_OPEN.

{code:title=OpenRegionHandler.java|borderStyle=solid}
 if (!transitionZookeeperOfflineToOpening(encodedName,
          versionOfOfflineNode)) {
        LOG.warn(""Region was hijacked? It no longer exists, encodedName="" +
          encodedName);
        tryTransitionToFailedOpen(regionInfo);
        return;
      }
{code}

After the two amendments, two adjacent assignments will not lead to an unhandled PENDING_OPEN state.
",,,,,,,,
HBASE-5094,"{code}
RegionState rit = this.services.getAssignmentManager().isRegionInTransition(e.getKey());
            ServerName addressFromAM = this.services.getAssignmentManager()
                .getRegionServerOfRegion(e.getKey());
            if (rit != null && !rit.isClosing() && !rit.isPendingClose()) {
              // Skip regions that were in transition unless CLOSING or
              // PENDING_CLOSE
              LOG.info(""Skip assigning region "" + rit.toString());
            } else if (addressFromAM != null
                && !addressFromAM.equals(this.serverName)) {
              LOG.debug(""Skip assigning region ""
                    + e.getKey().getRegionNameAsString()
                    + "" because it has been opened in ""
                    + addressFromAM.getServerName());
              }
{code}
In ServerShutDownHandler we try to get the address in the AM.  This address is initially null because it is not yet updated after the region was opened .i.e. the CAll back after node deletion is not yet done in the master side.
But removal from RIT is completed on the master side.  So this will trigger a new assignment.
So there is a small window between the online region is actually added in to the online list and the ServerShutdownHandler where we check the existing address in AM.",,,,,,,,
HBASE-5099,"A RS died.  The ServerShutdownHandler kicked in and started the logspliting.  SpliLogManager
installed the tasks asynchronously, then started to wait for them to complete.

The task znodes were not created actually.  The requests were just queued.
At this time, the zookeeper connection expired.  HMaster tried to recover the expired ZK session.
During the recovery, a new zookeeper connection was created.  However, this master became the
new master again.  It tried to assign root and meta.

Because the dead RS got the old root region, the master needs to wait for the log splitting to complete.
This waiting holds the zookeeper event thread.  So the async create split task is never retried since
there is only one event thread, which is waiting for the root region assigned.
",,,,,,,,
HBASE-5141,"I got a pretty reliable way of OOME'ing my region servers. Using a big payload (64MB in my case), a default heap and default number of handlers, it's not too long that all the MonitoredRPCHandlerImpl hold on a 64MB reference and once a compaction kicks in it kills everything.

The issue is that even after the RPC call is done, the packet still lives in MonitoredRPCHandlerImpl.

Will attach a screen shot of jprofiler's analysis in a moment.

This is a blocker for 0.92.0, anyone using a high number of handlers and bigish values will kill themselves.",,,,,,,,
HBASE-5152,,,,,,,,,
HBASE-5153,"HBASE-4893 is related to this issue. In that issue, we know, if multi-threads share a same connection, once this connection got abort in one thread, the other threads will got a ""HConnectionManager$HConnectionImplementation@18fb1f7 closed"" exception.

It solve the problem of ""stale connection can't removed"". But the orignal HTable instance cann't be continue to use. The connection in HTable should be recreated.

Actually, there's two aproach to solve this:
1. In user code, once catch an IOE, close connection and re-create HTable instance. We can use this as a workaround.
2. In HBase Client side, catch this exception, and re-create connection.
",,,,,,,,
HBASE-5155,"ServerShutDownHandler and disable/delete table handler races.  This is not an issue due to TM.
-> A regionserver goes down.  In our cluster the regionserver holds lot of regions.
-> A region R1 has two daughters D1 and D2.
-> The ServerShutdownHandler gets called and scans the META and gets all the user regions
-> Parallely a table is disabled. (No problem in this step).
-> Delete table is done.
-> The tables and its regions are deleted including R1, D1 and D2.. (So META is cleaned)
-> Now ServerShutdownhandler starts to processTheDeadRegion
{code}
 if (hri.isOffline() && hri.isSplit()) {
      LOG.debug(""Offlined and split region "" + hri.getRegionNameAsString() +
        ""; checking daughter presence"");
      fixupDaughters(result, assignmentManager, catalogTracker);
{code}
As part of fixUpDaughters as the daughers D1 and D2 is missing for R1 
{code}
    if (isDaughterMissing(catalogTracker, daughter)) {
      LOG.info(""Fixup; missing daughter "" + daughter.getRegionNameAsString());
      MetaEditor.addDaughter(catalogTracker, daughter, null);

      // TODO: Log WARN if the regiondir does not exist in the fs.  If its not
      // there then something wonky about the split -- things will keep going
      // but could be missing references to parent region.

      // And assign it.
      assignmentManager.assign(daughter, true);
{code}
we call assign of the daughers.  
Now after this we again start with the below code.
{code}
        if (processDeadRegion(e.getKey(), e.getValue(),
            this.services.getAssignmentManager(),
            this.server.getCatalogTracker())) {
          this.services.getAssignmentManager().assign(e.getKey(), true);
{code}
Now when the SSH scanned the META it had R1, D1 and D2.
So as part of the above code D1 and D2 which where assigned by fixUpDaughters
is again assigned by 
{code}
this.services.getAssignmentManager().assign(e.getKey(), true);
{code}
Thus leading to a zookeeper issue due to bad version and killing the master.
The important part here is the regions that were deleted are recreated which i think is more critical.",,,,,,,,
HBASE-5162,"Concurrent processing of DeleteTableHandler and ServerShutdownHandler may cause following situation
1.Table has already be disabled.
2.ServerShutdownHandler is doing MetaReader.getServerUserRegions.
3.When step2 is processing or is completed just now, DeleteTableHandler starts to delete region(Remove region from META and Delete region from FS)
4.DeleteTableHandler set table enabled.
4.ServerShutdownHandler is starting to assign region which is alread deleted by DeleteTableHandler.

The result of above operations is producing an invalid record in .META.  and can't be fixed by hbck ",,,1,,,,,
HBASE-5165,"Concurrent processing of DeleteTableHandler and ServerShutdownHandler may cause following situation
1.Table has already be disabled.
2.ServerShutdownHandler is doing MetaReader.getServerUserRegions.
3.When step2 is processing or is completed just now, DeleteTableHandler starts to delete region(Remove region from META and Delete region from FS)
4.DeleteTableHandler set table enabled.
4.ServerShutdownHandler is starting to assign region which is alread deleted by DeleteTableHandler.

The result of above operations is producing an invalid record in .META.  and can't be fixed by hbck ",,,,,,,,
HBASE-5166,"Concurrent processing of DeleteTableHandler and ServerShutdownHandler may cause following situation
1.Table has already be disabled.
2.ServerShutdownHandler is doing MetaReader.getServerUserRegions.
3.When step2 is processing or is completed just now, DeleteTableHandler starts to delete region(Remove region from META and Delete region from FS)
4.DeleteTableHandler set table enabled.
4.ServerShutdownHandler is starting to assign region which is alread deleted by DeleteTableHandler.

The result of above operations is producing an invalid record in .META.  and can't be fixed by hbck ",,,1,,,,,
HBASE-5179,"If master's processing its failover and ServerShutdownHandler's processing happen concurrently, it may appear following  case.
1.master completed splitLogAfterStartup()
2.RegionserverA restarts, and ServerShutdownHandler is processing.
3.master starts to rebuildUserRegions, and RegionserverA is considered as dead server.
4.master starts to assign regions of RegionserverA because it is a dead server by step3.

However, when doing step4(assigning region), ServerShutdownHandler may be doing split log, Therefore, it may cause data loss.",,,,,,,,
HBASE-5190,"If region split fails after PONR, it relies on the master ServerShutdown handler to fix it.  However, if the master doesn't get a chance to fix it.  There will be a hole in the region chain.",,,1,,,,,
HBASE-5196,"If region split fails after PONR, it relies on the master ServerShutdown handler to fix it.  However, if the master doesn't get a chance to fix it.  There will be a hole in the region chain.",,,,,,,,
HBASE-5199,"If region split fails after PONR, it relies on the master ServerShutdown handler to fix it.  However, if the master doesn't get a chance to fix it.  There will be a hole in the region chain.",,,,,,,,
HBASE-5200,"This is the scenario
Consider a case where the balancer is going on thus trying to close regions in a RS.
Before we could close a master switch happens.  
On Master switch the set of nodes that are in RIT is collected and we first get Data and start watching the node
After that the node data is added into RIT.
Now by this time (before adding to RIT) if the RS to which close was called does a transition in AM.handleRegion() we miss the handling saying RIT state was null.
{code}
2012-01-13 10:50:46,358 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received CLOSED for region a66d281d231dfcaea97c270698b26b6f from server HOST-192-168-47-205,20020,1326363111288 but region was in  the state null and not in expected PENDING_CLOSE or CLOSING states
2012-01-13 10:50:46,358 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received CLOSED for region c12e53bfd48ddc5eec507d66821c4d23 from server HOST-192-168-47-205,20020,1326363111288 but region was in  the state null and not in expected PENDING_CLOSE or CLOSING states
2012-01-13 10:50:46,358 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received CLOSED for region 59ae13de8c1eb325a0dd51f4902d2052 from server HOST-192-168-47-205,20020,1326363111288 but region was in  the state null and not in expected PENDING_CLOSE or CLOSING states
2012-01-13 10:50:46,359 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received CLOSED for region f45bc9614d7575f35244849af85aa078 from server HOST-192-168-47-205,20020,1326363111288 but region was in  the state null and not in expected PENDING_CLOSE or CLOSING states
2012-01-13 10:50:46,359 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received CLOSED for region cc3ecd7054fe6cd4a1159ed92fd62641 from server HOST-192-168-47-204,20020,1326342744518 but region was in  the state null and not in expected PENDING_CLOSE or CLOSING states
2012-01-13 10:50:46,359 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received CLOSED for region 3af40478a17fee96b4a192b22c90d5a2 from server HOST-192-168-47-205,20020,1326363111288 but region was in  the state null and not in expected PENDING_CLOSE or CLOSING states
2012-01-13 10:50:46,359 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received CLOSED for region e6096a8466e730463e10d3d61f809b92 from server HOST-192-168-47-204,20020,1326342744518 but region was in  the state null and not in expected PENDING_CLOSE or CLOSING states
2012-01-13 10:50:46,359 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received CLOSED for region 4806781a1a23066f7baed22b4d237e24 from server HOST-192-168-47-204,20020,1326342744518 but region was in  the state null and not in expected PENDING_CLOSE or CLOSING states
2012-01-13 10:50:46,359 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received CLOSED for region d69e104131accaefe21dcc01fddc7629 from server HOST-192-168-47-205,20020,1326363111288 but region was in  the state null and not in expected PENDING_CLOSE or CLOSING states

{code}

In branch the CLOSING node is created by RS thus leading to more inconsistency.
",,,,,,,,
HBASE-5201,"The following NPE can occur during master failover:

{code}
2012-01-15 17:45:00,314 FATAL [Master:1;ip-10-166-123-193.us-west-1.compute.internal:36708] master.HMaster(944): Unhandled exception. Starting shutdown.
java.lang.NullPointerException
        at org.apache.hadoop.hbase.master.AssignmentManager.regionOnline(AssignmentManager.java:724)
        at org.apache.hadoop.hbase.master.AssignmentManager.processFailover(AssignmentManager.java:214)
        at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:396)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:279)
        at java.lang.Thread.run(Thread.java:636)
{code}

This is caused by regionOnline() being passed a null serverInfo (its second parameter). 

The AssignmentManager's processFailover() method is passing a null to regionOnline() because the value that regionOnline is passing, hsi, is set as:

{code}
hsi = this.serverManager.getHServerInfo(this.catalogTracker.getMetaLocation());
{code}

and
 
{code}
hsi = this.serverManager.getHServerInfo(this.catalogTracker.getRootLocation());
{code}

getHServerInfo() is defined as:

{code}
  public HServerInfo getHServerInfo(final HServerAddress hsa) {
    synchronized(this.onlineServers) {
      // TODO: This is primitive.  Do a better search.
      for (Map.Entry<String, HServerInfo> e: this.onlineServers.entrySet()) {
        if (e.getValue().getServerAddress().equals(hsa)) {
          return e.getValue();
        }
      }
    }
    return null;
  }
{code}

This will return null if the onlineServers map does not yet have a value corresponding to the key supplied by the catalogTracker's getRootLocation() or getMetaLocation(). 

Since the catalogTracker uses zookeeper to establish the server locations of {{-ROOT-}} and {{.META.}}, while the onlineServers map is set according to the these servers' registering with the master, there can be an inconsistency between the catalogTracker and the onlineServers if either of these regionservers is online with respect to zookeeper, but haven't yet registered with the master (perhaps due to a high latency network between the master and the regionserver).

The attached testMasterFailoverWithSlowRS.txt patch can be used to modify TestMasterFailover to cause this NPE. 

The proposed fix (provided along with the above test in a separate attachment) is for the master to use the new verifyMetaTablesAreUp() to wait for both of the servers named by the catalog tracker's getRootLocation() and getMetaLocation() to register with the master before the master can continue with failover.
",,1,,,,,,
HBASE-5202,"The following NPE can occur during master failover:

{code}
2012-01-15 17:45:00,314 FATAL [Master:1;ip-10-166-123-193.us-west-1.compute.internal:36708] master.HMaster(944): Unhandled exception. Starting shutdown.
java.lang.NullPointerException
        at org.apache.hadoop.hbase.master.AssignmentManager.regionOnline(AssignmentManager.java:724)
        at org.apache.hadoop.hbase.master.AssignmentManager.processFailover(AssignmentManager.java:214)
        at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:396)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:279)
        at java.lang.Thread.run(Thread.java:636)
{code}

This is caused by regionOnline() being passed a null serverInfo (its second parameter). 

The AssignmentManager's processFailover() method is passing a null to regionOnline() because the value that regionOnline is passing, hsi, is set as:

{code}
hsi = this.serverManager.getHServerInfo(this.catalogTracker.getMetaLocation());
{code}

and
 
{code}
hsi = this.serverManager.getHServerInfo(this.catalogTracker.getRootLocation());
{code}

getHServerInfo() is defined as:

{code}
  public HServerInfo getHServerInfo(final HServerAddress hsa) {
    synchronized(this.onlineServers) {
      // TODO: This is primitive.  Do a better search.
      for (Map.Entry<String, HServerInfo> e: this.onlineServers.entrySet()) {
        if (e.getValue().getServerAddress().equals(hsa)) {
          return e.getValue();
        }
      }
    }
    return null;
  }
{code}

This will return null if the onlineServers map does not yet have a value corresponding to the key supplied by the catalogTracker's getRootLocation() or getMetaLocation(). 

Since the catalogTracker uses zookeeper to establish the server locations of {{-ROOT-}} and {{.META.}}, while the onlineServers map is set according to the these servers' registering with the master, there can be an inconsistency between the catalogTracker and the onlineServers if either of these regionservers is online with respect to zookeeper, but haven't yet registered with the master (perhaps due to a high latency network between the master and the regionserver).

The attached testMasterFailoverWithSlowRS.txt patch can be used to modify TestMasterFailover to cause this NPE. 

The proposed fix (provided along with the above test in a separate attachment) is for the master to use the new verifyMetaTablesAreUp() to wait for both of the servers named by the catalog tracker's getRootLocation() and getMetaLocation() to register with the master before the master can continue with failover.
",,1,,,,,,
HBASE-5208,"The following NPE can occur during master failover:

{code}
2012-01-15 17:45:00,314 FATAL [Master:1;ip-10-166-123-193.us-west-1.compute.internal:36708] master.HMaster(944): Unhandled exception. Starting shutdown.
java.lang.NullPointerException
        at org.apache.hadoop.hbase.master.AssignmentManager.regionOnline(AssignmentManager.java:724)
        at org.apache.hadoop.hbase.master.AssignmentManager.processFailover(AssignmentManager.java:214)
        at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:396)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:279)
        at java.lang.Thread.run(Thread.java:636)
{code}

This is caused by regionOnline() being passed a null serverInfo (its second parameter). 

The AssignmentManager's processFailover() method is passing a null to regionOnline() because the value that regionOnline is passing, hsi, is set as:

{code}
hsi = this.serverManager.getHServerInfo(this.catalogTracker.getMetaLocation());
{code}

and
 
{code}
hsi = this.serverManager.getHServerInfo(this.catalogTracker.getRootLocation());
{code}

getHServerInfo() is defined as:

{code}
  public HServerInfo getHServerInfo(final HServerAddress hsa) {
    synchronized(this.onlineServers) {
      // TODO: This is primitive.  Do a better search.
      for (Map.Entry<String, HServerInfo> e: this.onlineServers.entrySet()) {
        if (e.getValue().getServerAddress().equals(hsa)) {
          return e.getValue();
        }
      }
    }
    return null;
  }
{code}

This will return null if the onlineServers map does not yet have a value corresponding to the key supplied by the catalogTracker's getRootLocation() or getMetaLocation(). 

Since the catalogTracker uses zookeeper to establish the server locations of {{-ROOT-}} and {{.META.}}, while the onlineServers map is set according to the these servers' registering with the master, there can be an inconsistency between the catalogTracker and the onlineServers if either of these regionservers is online with respect to zookeeper, but haven't yet registered with the master (perhaps due to a high latency network between the master and the regionserver).

The attached testMasterFailoverWithSlowRS.txt patch can be used to modify TestMasterFailover to cause this NPE. 

The proposed fix (provided along with the above test in a separate attachment) is for the master to use the new verifyMetaTablesAreUp() to wait for both of the servers named by the catalog tracker's getRootLocation() and getMetaLocation() to register with the master before the master can continue with failover.
",,1,,,,,,
HBASE-5210,"We run an overnight map/reduce job that loads data from an external source and adds that data to an existing HBase table.  The input files have been loaded into hdfs.  The map/reduce job uses the HFileOutputFormat (and the TotalOrderPartitioner) to create HFiles which are subsequently added to the HBase table.  On at least two separate occasions (that we know of), a range of output would be missing for a given day.  The range of keys for the missing values corresponded to those of a particular region.  This implied that a complete HFile somehow went missing from the job.  Further investigation revealed the following:

 * Two different reducers (running in separate JVMs and thus separate class loaders)
 * in the same server can end up using the same file names for their
 * HFiles.  The scenario is as follows:
 * 	1.	Both reducers start near the same time.
 * 	2.	The first reducer reaches the point where it wants to write its first file.
 * 	3.	It uses the StoreFile class which contains a static Random object 
 * 		which is initialized by default using a timestamp.
 * 	4.	The file name is generated using the random number generator.
 * 	5.	The file name is checked against other existing files.
 * 	6.	The file is written into temporary files in a directory named
 * 		after the reducer attempt.
 * 	7.	The second reduce task reaches the same point, but its StoreClass
 * 		(which is now in the file system's cache) gets loaded within the
 * 		time resolution of the OS and thus initializes its Random()
 * 		object with the same seed as the first task.
 * 	8.	The second task also checks for an existing file with the name
 * 		generated by the random number generator and finds no conflict
 * 		because each task is writing files in its own temporary folder.
 * 	9.	The first task finishes and gets its temporary files committed
 * 		to the ""real"" folder specified for output of the HFiles.
 *     10.	The second task then reaches its own conclusion and commits its
 * 		files (moveTaskOutputs).  The released Hadoop code just overwrites
 * 		any files with the same name.  No warning messages or anything.
 * 		The first task's HFiles just go missing.
 * 
 *  Note:  The reducers here are NOT different attempts at the same 
 *  	reduce task.  They are different reduce tasks so data is
 *  	really lost.

I am currently testing a fix in which I have added code to the Hadoop 
FileOutputCommitter.moveTaskOutputs method to check for a conflict with
an existing file in the final output folder and to rename the HFile if
needed.  This may not be appropriate for all uses of FileOutputFormat.
So I have put this into a new class which is then used by a subclass of
HFileOutputFormat.  Subclassing of FileOutputCommitter itself was a bit 
more of a problem due to private declarations.

I don't know if my approach is the best fix for the problem.  If someone
more knowledgeable than myself deems that it is, I will be happy to share
what I have done and by that time I may have some information on the
results.",,1,,,,,,
HBASE-5201,"We run an overnight map/reduce job that loads data from an external source and adds that data to an existing HBase table.  The input files have been loaded into hdfs.  The map/reduce job uses the HFileOutputFormat (and the TotalOrderPartitioner) to create HFiles which are subsequently added to the HBase table.  On at least two separate occasions (that we know of), a range of output would be missing for a given day.  The range of keys for the missing values corresponded to those of a particular region.  This implied that a complete HFile somehow went missing from the job.  Further investigation revealed the following:

 * Two different reducers (running in separate JVMs and thus separate class loaders)
 * in the same server can end up using the same file names for their
 * HFiles.  The scenario is as follows:
 * 	1.	Both reducers start near the same time.
 * 	2.	The first reducer reaches the point where it wants to write its first file.
 * 	3.	It uses the StoreFile class which contains a static Random object 
 * 		which is initialized by default using a timestamp.
 * 	4.	The file name is generated using the random number generator.
 * 	5.	The file name is checked against other existing files.
 * 	6.	The file is written into temporary files in a directory named
 * 		after the reducer attempt.
 * 	7.	The second reduce task reaches the same point, but its StoreClass
 * 		(which is now in the file system's cache) gets loaded within the
 * 		time resolution of the OS and thus initializes its Random()
 * 		object with the same seed as the first task.
 * 	8.	The second task also checks for an existing file with the name
 * 		generated by the random number generator and finds no conflict
 * 		because each task is writing files in its own temporary folder.
 * 	9.	The first task finishes and gets its temporary files committed
 * 		to the ""real"" folder specified for output of the HFiles.
 *     10.	The second task then reaches its own conclusion and commits its
 * 		files (moveTaskOutputs).  The released Hadoop code just overwrites
 * 		any files with the same name.  No warning messages or anything.
 * 		The first task's HFiles just go missing.
 * 
 *  Note:  The reducers here are NOT different attempts at the same 
 *  	reduce task.  They are different reduce tasks so data is
 *  	really lost.

I am currently testing a fix in which I have added code to the Hadoop 
FileOutputCommitter.moveTaskOutputs method to check for a conflict with
an existing file in the final output folder and to rename the HFile if
needed.  This may not be appropriate for all uses of FileOutputFormat.
So I have put this into a new class which is then used by a subclass of
HFileOutputFormat.  Subclassing of FileOutputCommitter itself was a bit 
more of a problem due to private declarations.

I don't know if my approach is the best fix for the problem.  If someone
more knowledgeable than myself deems that it is, I will be happy to share
what I have done and by that time I may have some information on the
results.",,,1,,,,,
HBASE-5229,"RSA, RS B and RS C are 3 region servers.
RS A -> META
RS B -> ROOT
RS C -> NON META and NON ROOT

Kill RS B and wait for server shutdown handler to start.  
Start RS B again before assigning ROOT to RS C.
Now the cluster will try to assign new regions to RS B.  
But as ROOT is not yet assigned the OpenRegionHandler.updateMeta will fail to update the regions just because ROOT is not online.
{code}
a87109263ed53e67158377a149c5a7be from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2012-01-30 16:23:25,126 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x1352e27539c0009 Attempting to transition node a87109263ed53e67158377a149c5a7be from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2012-01-30 16:23:25,159 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x1352e27539c0009 Successfully transitioned node a87109263ed53e67158377a149c5a7be from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2012-01-30 16:23:35,385 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x1352e27539c0009 Attempting to transition node a87109263ed53e67158377a149c5a7be from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2012-01-30 16:23:35,449 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x1352e27539c0009 Successfully transitioned node a87109263ed53e67158377a149c5a7be from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2012-01-30 16:24:16,666 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x1352e27539c0009 Attempting to transition node a87109263ed53e67158377a149c5a7be from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2012-01-30 16:24:16,701 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x1352e27539c0009 Successfully transitioned node a87109263ed53e67158377a149c5a7be from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2012-01-30 16:24:20,788 DEBUG org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Interrupting thread Thread[PostOpenDeployTasks:a87109263ed53e67158377a149c5a7be,5,main]
2012-01-30 16:24:30,699 WARN org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Exception running postOpenDeployTasks; region=a87109263ed53e67158377a149c5a7be
org.apache.hadoop.hbase.NotAllMetaRegionsOnlineException: Interrupted
	at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMetaServerConnectionDefault(CatalogTracker.java:439)
	at org.apache.hadoop.hbase.catalog.MetaEditor.updateRegionLocation(MetaEditor.java:142)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.postOpenDeployTasks(HRegionServer.java:1382)
	at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler$PostOpenDeployTasksThread.run(OpenRegionHandler.java:221)
{code}

So we need to wait for TM to assign the regions again. ",,,1,,,,,
HBASE-5241,"Like HADOOP-7119, the same motivations:
Hadoop RPC already supports Kerberos authentication. 
As does the HBase secure RPC engine.
Kerberos enables single sign-on.
Popular browsers (Firefox and Internet Explorer) have support for Kerberos HTTP SPNEGO.
Adding support for Kerberos HTTP SPNEGO to [HBase] web consoles would provide a unified authentication mechanism and single sign-on for web UI and RPC.
Also like HADOOP-7119, the same solution:
A servlet filter is configured in front of all Hadoop web consoles for authentication.
This filter verifies if the incoming request is already authenticated by the presence of a signed HTTP cookie. If the cookie is present, its signature is valid and its value didn't expire; then the request continues its way to the page invoked by the request. If the cookie is not present, it is invalid or it expired; then the request is delegated to an authenticator handler. The authenticator handler then is responsible for requesting/validating the user-agent for the user credentials. This may require one or more additional interactions between the authenticator handler and the user-agent (which will be multiple HTTP requests). Once the authenticator handler verifies the credentials and generates an authentication token, a signed cookie is returned to the user-agent for all subsequent invocations.
The authenticator handler is pluggable and 2 implementations are provided out of the box: pseudo/simple and kerberos.
1. The pseudo/simple authenticator handler is equivalent to the Hadoop pseudo/simple authentication. It trusts the value of the user.name query string parameter. The pseudo/simple authenticator handler supports an anonymous mode which accepts any request without requiring the user.name query string parameter to create the token. This is the default behavior, preserving the behavior of the HBase web consoles before this patch.
2. The kerberos authenticator handler implements the Kerberos HTTP SPNEGO implementation. This authenticator handler will generate a token only if a successful Kerberos HTTP SPNEGO interaction is performed between the user-agent and the authenticator. Browsers like Firefox and Internet Explorer support Kerberos HTTP SPNEGO.
We can build on the support added to Hadoop via HADOOP-7119. Should just be a matter of wiring up the filter to our infoservers in a similar manner. 
And from https://issues.apache.org/jira/browse/HBASE-5050?focusedCommentId=13171086&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13171086
Hadoop 0.23 onwards has a hadoop-auth artifact that provides SPNEGO/Kerberos authentication for webapps via a filter. You should consider using it. You don't have to move Hbase to 0.23 for that, just consume the hadoop-auth artifact, which has no dependencies on the rest of Hadoop 0.23 artifacts.",1,,,,,,,
HBASE-5259,"Like HADOOP-7119, the same motivations:
Hadoop RPC already supports Kerberos authentication. 
As does the HBase secure RPC engine.
Kerberos enables single sign-on.
Popular browsers (Firefox and Internet Explorer) have support for Kerberos HTTP SPNEGO.
Adding support for Kerberos HTTP SPNEGO to [HBase] web consoles would provide a unified authentication mechanism and single sign-on for web UI and RPC.
Also like HADOOP-7119, the same solution:
A servlet filter is configured in front of all Hadoop web consoles for authentication.
This filter verifies if the incoming request is already authenticated by the presence of a signed HTTP cookie. If the cookie is present, its signature is valid and its value didn't expire; then the request continues its way to the page invoked by the request. If the cookie is not present, it is invalid or it expired; then the request is delegated to an authenticator handler. The authenticator handler then is responsible for requesting/validating the user-agent for the user credentials. This may require one or more additional interactions between the authenticator handler and the user-agent (which will be multiple HTTP requests). Once the authenticator handler verifies the credentials and generates an authentication token, a signed cookie is returned to the user-agent for all subsequent invocations.
The authenticator handler is pluggable and 2 implementations are provided out of the box: pseudo/simple and kerberos.
1. The pseudo/simple authenticator handler is equivalent to the Hadoop pseudo/simple authentication. It trusts the value of the user.name query string parameter. The pseudo/simple authenticator handler supports an anonymous mode which accepts any request without requiring the user.name query string parameter to create the token. This is the default behavior, preserving the behavior of the HBase web consoles before this patch.
2. The kerberos authenticator handler implements the Kerberos HTTP SPNEGO implementation. This authenticator handler will generate a token only if a successful Kerberos HTTP SPNEGO interaction is performed between the user-agent and the authenticator. Browsers like Firefox and Internet Explorer support Kerberos HTTP SPNEGO.
We can build on the support added to Hadoop via HADOOP-7119. Should just be a matter of wiring up the filter to our infoservers in a similar manner. 
And from https://issues.apache.org/jira/browse/HBASE-5050?focusedCommentId=13171086&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13171086
Hadoop 0.23 onwards has a hadoop-auth artifact that provides SPNEGO/Kerberos authentication for webapps via a filter. You should consider using it. You don't have to move Hbase to 0.23 for that, just consume the hadoop-auth artifact, which has no dependencies on the rest of Hadoop 0.23 artifacts.",1,,,,,,,
HBASE-5291,"Like HADOOP-7119, the same motivations:
Hadoop RPC already supports Kerberos authentication. 
As does the HBase secure RPC engine.
Kerberos enables single sign-on.
Popular browsers (Firefox and Internet Explorer) have support for Kerberos HTTP SPNEGO.
Adding support for Kerberos HTTP SPNEGO to [HBase] web consoles would provide a unified authentication mechanism and single sign-on for web UI and RPC.
Also like HADOOP-7119, the same solution:
A servlet filter is configured in front of all Hadoop web consoles for authentication.
This filter verifies if the incoming request is already authenticated by the presence of a signed HTTP cookie. If the cookie is present, its signature is valid and its value didn't expire; then the request continues its way to the page invoked by the request. If the cookie is not present, it is invalid or it expired; then the request is delegated to an authenticator handler. The authenticator handler then is responsible for requesting/validating the user-agent for the user credentials. This may require one or more additional interactions between the authenticator handler and the user-agent (which will be multiple HTTP requests). Once the authenticator handler verifies the credentials and generates an authentication token, a signed cookie is returned to the user-agent for all subsequent invocations.
The authenticator handler is pluggable and 2 implementations are provided out of the box: pseudo/simple and kerberos.
1. The pseudo/simple authenticator handler is equivalent to the Hadoop pseudo/simple authentication. It trusts the value of the user.name query string parameter. The pseudo/simple authenticator handler supports an anonymous mode which accepts any request without requiring the user.name query string parameter to create the token. This is the default behavior, preserving the behavior of the HBase web consoles before this patch.
2. The kerberos authenticator handler implements the Kerberos HTTP SPNEGO implementation. This authenticator handler will generate a token only if a successful Kerberos HTTP SPNEGO interaction is performed between the user-agent and the authenticator. Browsers like Firefox and Internet Explorer support Kerberos HTTP SPNEGO.
We can build on the support added to Hadoop via HADOOP-7119. Should just be a matter of wiring up the filter to our infoservers in a similar manner. 
And from https://issues.apache.org/jira/browse/HBASE-5050?focusedCommentId=13171086&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13171086
Hadoop 0.23 onwards has a hadoop-auth artifact that provides SPNEGO/Kerberos authentication for webapps via a filter. You should consider using it. You don't have to move Hbase to 0.23 for that, just consume the hadoop-auth artifact, which has no dependencies on the rest of Hadoop 0.23 artifacts.",1,,,,,,,
HBASE-5295,"RSA, RS B and RS C are 3 region servers.
RS A -> META
RS B -> ROOT
RS C -> NON META and NON ROOT

Kill RS B and wait for server shutdown handler to start.  
Start RS B again before assigning ROOT to RS C.
Now the cluster will try to assign new regions to RS B.  
But as ROOT is not yet assigned the OpenRegionHandler.updateMeta will fail to update the regions just because ROOT is not online.
{code}
a87109263ed53e67158377a149c5a7be from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2012-01-30 16:23:25,126 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x1352e27539c0009 Attempting to transition node a87109263ed53e67158377a149c5a7be from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2012-01-30 16:23:25,159 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x1352e27539c0009 Successfully transitioned node a87109263ed53e67158377a149c5a7be from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2012-01-30 16:23:35,385 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x1352e27539c0009 Attempting to transition node a87109263ed53e67158377a149c5a7be from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2012-01-30 16:23:35,449 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x1352e27539c0009 Successfully transitioned node a87109263ed53e67158377a149c5a7be from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2012-01-30 16:24:16,666 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x1352e27539c0009 Attempting to transition node a87109263ed53e67158377a149c5a7be from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2012-01-30 16:24:16,701 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x1352e27539c0009 Successfully transitioned node a87109263ed53e67158377a149c5a7be from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2012-01-30 16:24:20,788 DEBUG org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Interrupting thread Thread[PostOpenDeployTasks:a87109263ed53e67158377a149c5a7be,5,main]
2012-01-30 16:24:30,699 WARN org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Exception running postOpenDeployTasks; region=a87109263ed53e67158377a149c5a7be
org.apache.hadoop.hbase.NotAllMetaRegionsOnlineException: Interrupted
	at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMetaServerConnectionDefault(CatalogTracker.java:439)
	at org.apache.hadoop.hbase.catalog.MetaEditor.updateRegionLocation(MetaEditor.java:142)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.postOpenDeployTasks(HRegionServer.java:1382)
	at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler$PostOpenDeployTasksThread.run(OpenRegionHandler.java:221)
{code}

So we need to wait for TM to assign the regions again. ",,,1,,,,,
HBASE-5299,"RSA, RS B and RS C are 3 region servers.
RS A -> META
RS B -> ROOT
RS C -> NON META and NON ROOT

Kill RS B and wait for server shutdown handler to start.  
Start RS B again before assigning ROOT to RS C.
Now the cluster will try to assign new regions to RS B.  
But as ROOT is not yet assigned the OpenRegionHandler.updateMeta will fail to update the regions just because ROOT is not online.
{code}
a87109263ed53e67158377a149c5a7be from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2012-01-30 16:23:25,126 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x1352e27539c0009 Attempting to transition node a87109263ed53e67158377a149c5a7be from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2012-01-30 16:23:25,159 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x1352e27539c0009 Successfully transitioned node a87109263ed53e67158377a149c5a7be from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2012-01-30 16:23:35,385 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x1352e27539c0009 Attempting to transition node a87109263ed53e67158377a149c5a7be from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2012-01-30 16:23:35,449 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x1352e27539c0009 Successfully transitioned node a87109263ed53e67158377a149c5a7be from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2012-01-30 16:24:16,666 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x1352e27539c0009 Attempting to transition node a87109263ed53e67158377a149c5a7be from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2012-01-30 16:24:16,701 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x1352e27539c0009 Successfully transitioned node a87109263ed53e67158377a149c5a7be from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2012-01-30 16:24:20,788 DEBUG org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Interrupting thread Thread[PostOpenDeployTasks:a87109263ed53e67158377a149c5a7be,5,main]
2012-01-30 16:24:30,699 WARN org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Exception running postOpenDeployTasks; region=a87109263ed53e67158377a149c5a7be
org.apache.hadoop.hbase.NotAllMetaRegionsOnlineException: Interrupted
	at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMetaServerConnectionDefault(CatalogTracker.java:439)
	at org.apache.hadoop.hbase.catalog.MetaEditor.updateRegionLocation(MetaEditor.java:142)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.postOpenDeployTasks(HRegionServer.java:1382)
	at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler$PostOpenDeployTasksThread.run(OpenRegionHandler.java:221)
{code}

So we need to wait for TM to assign the regions again. ",,,1,,,,,
HBASE-5310,"In case the master dies after a regionserver writes region state as OPENED or CLOSED in ZK but before the update is received by master and written to meta, the new master that comes up has to pick up the region state from ZK and write it to meta. Otherwise we can get multiply-assigned regions.",,,1,,,,,
HBASE-5313,"In case the master dies after a regionserver writes region state as OPENED or CLOSED in ZK but before the update is received by master and written to meta, the new master that comes up has to pick up the region state from ZK and write it to meta. Otherwise we can get multiply-assigned regions.",,,1,,,,,
HBASE-5334,"In case the master dies after a regionserver writes region state as OPENED or CLOSED in ZK but before the update is received by master and written to meta, the new master that comes up has to pick up the region state from ZK and write it to meta. Otherwise we can get multiply-assigned regions.",,,1,,,,,
HBASE-5335,"In case the master dies after a regionserver writes region state as OPENED or CLOSED in ZK but before the update is received by master and written to meta, the new master that comes up has to pick up the region state from ZK and write it to meta. Otherwise we can get multiply-assigned regions.",,,1,,,,,
HBASE-5344,"In case the master dies after a regionserver writes region state as OPENED or CLOSED in ZK but before the update is received by master and written to meta, the new master that comes up has to pick up the region state from ZK and write it to meta. Otherwise we can get multiply-assigned regions.",,,,,,,,
HBASE-5347,"In case the master dies after a regionserver writes region state as OPENED or CLOSED in ZK but before the update is received by master and written to meta, the new master that comes up has to pick up the region state from ZK and write it to meta. Otherwise we can get multiply-assigned regions.",,,1,,,,,
HBASE-5349,"In case the master dies after a regionserver writes region state as OPENED or CLOSED in ZK but before the update is received by master and written to meta, the new master that comes up has to pick up the region state from ZK and write it to meta. Otherwise we can get multiply-assigned regions.",,1,1,,,,,
HBASE-5351,"I have a test that tests vanilla use of importtsv with importtsv.bulk.output option followed by completebulkload to a new table.

This sometimes fails as follows:
11/12/19 15:02:39 WARN client.HConnectionManager$HConnectionImplementation: Encountered problems when prefetch META table:
org.apache.hadoop.hbase.TableNotFoundException: Cannot find row in .META. for table: ml_items_copy, row=ml_items_copy,,99999999999999
at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:157)
at org.apache.hadoop.hbase.client.MetaScanner.access$000(MetaScanner.java:52)
at org.apache.hadoop.hbase.client.MetaScanner$1.connect(MetaScanner.java:130)
at org.apache.hadoop.hbase.client.MetaScanner$1.connect(MetaScanner.java:127)
at org.apache.hadoop.hbase.client.HConnectionManager.execute(HConnectionManager.java:359)
at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:127)
at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:103)
at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.prefetchRegionCache(HConnectionManager.java:875)
at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:929)
at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:817)
at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:781)
at org.apache.hadoop.hbase.client.HTable.finishSetup(HTable.java:247)
at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:211)
at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:171)
at org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.createTable(LoadIncrementalHFiles.java:673)
at org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.run(LoadIncrementalHFiles.java:697)
at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)
at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:83)
at org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.main(LoadIncrementalHFiles.java:707)

The race appears to be calling HbAdmin.createTableAsync(htd, keys) and then creating an HTable object before that call has actually completed.
The following change to /src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java appears to fix the problem, but I have not been able to reproduce the race reliably, in order to write a test.

{code}
-    HTable table = new HTable(this.cfg, tableName);
-
-    HConnection conn = table.getConnection();
     int ctr = 0;
-    while (!conn.isTableAvailable(table.getTableName()) && (ctr<TABLE_CREATE_MA
+    while (!this.hbAdmin.isTableAvailable(tableName) && (ctr<TABLE_CREATE_MAX_R
{code}",,,,,,,,
HBASE-5355,"I was testing the region_mover.rb script on a loaded hbase and noticed that it can hang (thus hanging graceful shutdown) if a region that it is attempting to move gets deleted (by a table delete operation).

Here's the start of the relevent stack dump
{code}
12/02/08 13:27:13 WARN client.HConnectionManager$HConnectionImplementation: Encountered problems when prefetch META table:
org.apache.hadoop.hbase.TableNotFoundException: Cannot find row in .META. for table: TestLoadAndVerify_1328735001040, row=TestLoadAnd\
Verify_1328735001040,yC^P\xD7\x945\xD4,99999999999999
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:136)
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:95)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.prefetchRegionCache(HConnectionManager.java:64\
9)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:703\
)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:594)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.relocateRegion(HConnectionManager.java:565)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionLocation(HConnectionManager.java:416)
        at org.apache.hadoop.hbase.client.ServerCallable.instantiateServer(ServerCallable.java:57)
        at org.apache.hadoop.hbase.client.ScannerCallable.instantiateServer(ScannerCallable.java:63)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionServerWithRetries(HConnectionManager.\
java:1018)
        at org.apache.hadoop.hbase.client.HTable$ClientScanner.nextScanner(HTable.java:1104)
        at org.apache.hadoop.hbase.client.HTable$ClientScanner.initialize(HTable.java:1027)
        at org.apache.hadoop.hbase.client.HTable.getScanner(HTable.java:535)
        at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.jruby.javasupport.JavaMethod.invokeDirectWithExceptionHandling(JavaMethod.java:525)
        at org.jruby.javasupport.JavaMethod.invokeDirect(JavaMethod.java:380)
        at org.jruby.java.invokers.InstanceMethodInvoker.call(InstanceMethodInvoker.java:58)
        at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:137)
        at usr.lib.hbase.bin.region_mover.method__7$RUBY$isSuccessfulScan(/usr/lib/hbase/bin/region_mover.rb:133)
        at usr$lib$hbase$bin$region_mover#method__7$RUBY$isSuccessfulScan.call(usr$lib$hbase$bin$region_mover#method__7$RUBY$isSucces\
sfulScan:65535)
        at usr$lib$hbase$bin$region_mover#method__7$RUBY$isSuccessfulScan.call(usr$lib$hbase$bin$region_mover#method__7$RUBY$isSucces\
sfulScan:65535)
        at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:171)
        at usr.lib.hbase.bin.region_mover.block_4$RUBY$__for__(/usr/lib/hbase/bin/region_mover.rb:326)
        at usr$lib$hbase$bin$region_mover#block_4$RUBY$__for__.call(usr$lib$hbase$bin$region_mover#block_4$RUBY$__for__:65535)
        at org.jruby.runtime.CompiledBlock.yield(CompiledBlock.java:133)
        at org.jruby.runtime.BlockBody.call(BlockBody.java:73)
        at org.jruby.runtime.Block.call(Block.java:89)
        at org.jruby.RubyProc.call(RubyProc.java:268)
        at org.jruby.RubyProc.call(RubyProc.java:228)
        at org.jruby.RubyProc$i$0$0$call.call(RubyProc$i$0$0$call.gen:65535)
        at org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:209)
        at org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:205)
        at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:137)
        at org.jruby.ast.CallOneArgNode.interpret(CallOneArgNode.java:57)
        at org.jruby.ast.NewlineNode.interpret(NewlineNode.java:103)
        at org.jruby.ast.WhileNode.interpret(WhileNode.java:131)
        at org.jruby.ast.NewlineNode.interpret(NewlineNode.java:103)
        at org.jruby.ast.BlockNode.interpret(BlockNode.java:71)
        at org.jruby.evaluator.ASTInterpreter.INTERPRET_METHOD(ASTInterpreter.java:74)
        at org.jruby.internal.runtime.methods.InterpretedMethod.call(InterpretedMethod.java:169)
        at org.jruby.internal.runtime.methods.DefaultMethod.call(DefaultMethod.java:171)
        at org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:272)
        at org.jruby.runtime.callsite.CachingCallSite.callBlock(CachingCallSite.java:114)
        at org.jruby.runtime.callsite.CachingCallSite.callIter(CachingCallSite.java:123)
        at usr.lib.hbase.bin.region_mover.chained_26_rescue_4$RUBY$SYNTHETICunloadRegions(/usr/lib/hbase/bin/region_mover.rb:319)
        at usr.lib.hbase.bin.region_mover.method__25$RUBY$unloadRegions(/usr/lib/hbase/bin/region_mover.rb:313)
        at usr$lib$hbase$bin$region_mover#method__25$RUBY$unloadRegions.call(usr$lib$hbase$bin$region_mover#method__25$RUBY$unloadRegions:65535)
        at usr$lib$hbase$bin$region_mover#method__25$RUBY$unloadRegions.call(usr$lib$hbase$bin$region_mover#method__25$RUBY$unloadRegions:65535)
        at org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:302)
        at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:173)
        at usr.lib.hbase.bin.region_mover.__file__(/usr/lib/hbase/bin/region_mover.rb:430)
        at usr.lib.hbase.bin.region_mover.load(/usr/lib/hbase/bin/region_mover.rb)
        at org.jruby.Ruby.runScript(Ruby.java:670)
        at org.jruby.Ruby.runNormally(Ruby.java:574)
        at org.jruby.Ruby.runFromMain(Ruby.java:423)
        at org.jruby.Main.doRunFromMain(Main.java:278)
        at org.jruby.Main.internalRun(Main.java:198)
        at org.jruby.Main.run(Main.java:164)
        at org.jruby.Main.run(Main.java:148)
        at org.jruby.Main.main(Main.java:128)
{code}",,,1,,,,,
HBASE-5356,"I was testing the region_mover.rb script on a loaded hbase and noticed that it can hang (thus hanging graceful shutdown) if a region that it is attempting to move gets deleted (by a table delete operation).

Here's the start of the relevent stack dump
{code}
12/02/08 13:27:13 WARN client.HConnectionManager$HConnectionImplementation: Encountered problems when prefetch META table:
org.apache.hadoop.hbase.TableNotFoundException: Cannot find row in .META. for table: TestLoadAndVerify_1328735001040, row=TestLoadAnd\
Verify_1328735001040,yC^P\xD7\x945\xD4,99999999999999
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:136)
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:95)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.prefetchRegionCache(HConnectionManager.java:64\
9)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:703\
)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:594)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.relocateRegion(HConnectionManager.java:565)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionLocation(HConnectionManager.java:416)
        at org.apache.hadoop.hbase.client.ServerCallable.instantiateServer(ServerCallable.java:57)
        at org.apache.hadoop.hbase.client.ScannerCallable.instantiateServer(ScannerCallable.java:63)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionServerWithRetries(HConnectionManager.\
java:1018)
        at org.apache.hadoop.hbase.client.HTable$ClientScanner.nextScanner(HTable.java:1104)
        at org.apache.hadoop.hbase.client.HTable$ClientScanner.initialize(HTable.java:1027)
        at org.apache.hadoop.hbase.client.HTable.getScanner(HTable.java:535)
        at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.jruby.javasupport.JavaMethod.invokeDirectWithExceptionHandling(JavaMethod.java:525)
        at org.jruby.javasupport.JavaMethod.invokeDirect(JavaMethod.java:380)
        at org.jruby.java.invokers.InstanceMethodInvoker.call(InstanceMethodInvoker.java:58)
        at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:137)
        at usr.lib.hbase.bin.region_mover.method__7$RUBY$isSuccessfulScan(/usr/lib/hbase/bin/region_mover.rb:133)
        at usr$lib$hbase$bin$region_mover#method__7$RUBY$isSuccessfulScan.call(usr$lib$hbase$bin$region_mover#method__7$RUBY$isSucces\
sfulScan:65535)
        at usr$lib$hbase$bin$region_mover#method__7$RUBY$isSuccessfulScan.call(usr$lib$hbase$bin$region_mover#method__7$RUBY$isSucces\
sfulScan:65535)
        at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:171)
        at usr.lib.hbase.bin.region_mover.block_4$RUBY$__for__(/usr/lib/hbase/bin/region_mover.rb:326)
        at usr$lib$hbase$bin$region_mover#block_4$RUBY$__for__.call(usr$lib$hbase$bin$region_mover#block_4$RUBY$__for__:65535)
        at org.jruby.runtime.CompiledBlock.yield(CompiledBlock.java:133)
        at org.jruby.runtime.BlockBody.call(BlockBody.java:73)
        at org.jruby.runtime.Block.call(Block.java:89)
        at org.jruby.RubyProc.call(RubyProc.java:268)
        at org.jruby.RubyProc.call(RubyProc.java:228)
        at org.jruby.RubyProc$i$0$0$call.call(RubyProc$i$0$0$call.gen:65535)
        at org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:209)
        at org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:205)
        at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:137)
        at org.jruby.ast.CallOneArgNode.interpret(CallOneArgNode.java:57)
        at org.jruby.ast.NewlineNode.interpret(NewlineNode.java:103)
        at org.jruby.ast.WhileNode.interpret(WhileNode.java:131)
        at org.jruby.ast.NewlineNode.interpret(NewlineNode.java:103)
        at org.jruby.ast.BlockNode.interpret(BlockNode.java:71)
        at org.jruby.evaluator.ASTInterpreter.INTERPRET_METHOD(ASTInterpreter.java:74)
        at org.jruby.internal.runtime.methods.InterpretedMethod.call(InterpretedMethod.java:169)
        at org.jruby.internal.runtime.methods.DefaultMethod.call(DefaultMethod.java:171)
        at org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:272)
        at org.jruby.runtime.callsite.CachingCallSite.callBlock(CachingCallSite.java:114)
        at org.jruby.runtime.callsite.CachingCallSite.callIter(CachingCallSite.java:123)
        at usr.lib.hbase.bin.region_mover.chained_26_rescue_4$RUBY$SYNTHETICunloadRegions(/usr/lib/hbase/bin/region_mover.rb:319)
        at usr.lib.hbase.bin.region_mover.method__25$RUBY$unloadRegions(/usr/lib/hbase/bin/region_mover.rb:313)
        at usr$lib$hbase$bin$region_mover#method__25$RUBY$unloadRegions.call(usr$lib$hbase$bin$region_mover#method__25$RUBY$unloadRegions:65535)
        at usr$lib$hbase$bin$region_mover#method__25$RUBY$unloadRegions.call(usr$lib$hbase$bin$region_mover#method__25$RUBY$unloadRegions:65535)
        at org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:302)
        at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:173)
        at usr.lib.hbase.bin.region_mover.__file__(/usr/lib/hbase/bin/region_mover.rb:430)
        at usr.lib.hbase.bin.region_mover.load(/usr/lib/hbase/bin/region_mover.rb)
        at org.jruby.Ruby.runScript(Ruby.java:670)
        at org.jruby.Ruby.runNormally(Ruby.java:574)
        at org.jruby.Ruby.runFromMain(Ruby.java:423)
        at org.jruby.Main.doRunFromMain(Main.java:278)
        at org.jruby.Main.internalRun(Main.java:198)
        at org.jruby.Main.run(Main.java:164)
        at org.jruby.Main.run(Main.java:148)
        at org.jruby.Main.main(Main.java:128)
{code}",,,,,,,,
HBASE-5357,"I was testing the region_mover.rb script on a loaded hbase and noticed that it can hang (thus hanging graceful shutdown) if a region that it is attempting to move gets deleted (by a table delete operation).

Here's the start of the relevent stack dump
{code}
12/02/08 13:27:13 WARN client.HConnectionManager$HConnectionImplementation: Encountered problems when prefetch META table:
org.apache.hadoop.hbase.TableNotFoundException: Cannot find row in .META. for table: TestLoadAndVerify_1328735001040, row=TestLoadAnd\
Verify_1328735001040,yC^P\xD7\x945\xD4,99999999999999
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:136)
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:95)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.prefetchRegionCache(HConnectionManager.java:64\
9)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:703\
)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:594)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.relocateRegion(HConnectionManager.java:565)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionLocation(HConnectionManager.java:416)
        at org.apache.hadoop.hbase.client.ServerCallable.instantiateServer(ServerCallable.java:57)
        at org.apache.hadoop.hbase.client.ScannerCallable.instantiateServer(ScannerCallable.java:63)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionServerWithRetries(HConnectionManager.\
java:1018)
        at org.apache.hadoop.hbase.client.HTable$ClientScanner.nextScanner(HTable.java:1104)
        at org.apache.hadoop.hbase.client.HTable$ClientScanner.initialize(HTable.java:1027)
        at org.apache.hadoop.hbase.client.HTable.getScanner(HTable.java:535)
        at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.jruby.javasupport.JavaMethod.invokeDirectWithExceptionHandling(JavaMethod.java:525)
        at org.jruby.javasupport.JavaMethod.invokeDirect(JavaMethod.java:380)
        at org.jruby.java.invokers.InstanceMethodInvoker.call(InstanceMethodInvoker.java:58)
        at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:137)
        at usr.lib.hbase.bin.region_mover.method__7$RUBY$isSuccessfulScan(/usr/lib/hbase/bin/region_mover.rb:133)
        at usr$lib$hbase$bin$region_mover#method__7$RUBY$isSuccessfulScan.call(usr$lib$hbase$bin$region_mover#method__7$RUBY$isSucces\
sfulScan:65535)
        at usr$lib$hbase$bin$region_mover#method__7$RUBY$isSuccessfulScan.call(usr$lib$hbase$bin$region_mover#method__7$RUBY$isSucces\
sfulScan:65535)
        at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:171)
        at usr.lib.hbase.bin.region_mover.block_4$RUBY$__for__(/usr/lib/hbase/bin/region_mover.rb:326)
        at usr$lib$hbase$bin$region_mover#block_4$RUBY$__for__.call(usr$lib$hbase$bin$region_mover#block_4$RUBY$__for__:65535)
        at org.jruby.runtime.CompiledBlock.yield(CompiledBlock.java:133)
        at org.jruby.runtime.BlockBody.call(BlockBody.java:73)
        at org.jruby.runtime.Block.call(Block.java:89)
        at org.jruby.RubyProc.call(RubyProc.java:268)
        at org.jruby.RubyProc.call(RubyProc.java:228)
        at org.jruby.RubyProc$i$0$0$call.call(RubyProc$i$0$0$call.gen:65535)
        at org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:209)
        at org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:205)
        at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:137)
        at org.jruby.ast.CallOneArgNode.interpret(CallOneArgNode.java:57)
        at org.jruby.ast.NewlineNode.interpret(NewlineNode.java:103)
        at org.jruby.ast.WhileNode.interpret(WhileNode.java:131)
        at org.jruby.ast.NewlineNode.interpret(NewlineNode.java:103)
        at org.jruby.ast.BlockNode.interpret(BlockNode.java:71)
        at org.jruby.evaluator.ASTInterpreter.INTERPRET_METHOD(ASTInterpreter.java:74)
        at org.jruby.internal.runtime.methods.InterpretedMethod.call(InterpretedMethod.java:169)
        at org.jruby.internal.runtime.methods.DefaultMethod.call(DefaultMethod.java:171)
        at org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:272)
        at org.jruby.runtime.callsite.CachingCallSite.callBlock(CachingCallSite.java:114)
        at org.jruby.runtime.callsite.CachingCallSite.callIter(CachingCallSite.java:123)
        at usr.lib.hbase.bin.region_mover.chained_26_rescue_4$RUBY$SYNTHETICunloadRegions(/usr/lib/hbase/bin/region_mover.rb:319)
        at usr.lib.hbase.bin.region_mover.method__25$RUBY$unloadRegions(/usr/lib/hbase/bin/region_mover.rb:313)
        at usr$lib$hbase$bin$region_mover#method__25$RUBY$unloadRegions.call(usr$lib$hbase$bin$region_mover#method__25$RUBY$unloadRegions:65535)
        at usr$lib$hbase$bin$region_mover#method__25$RUBY$unloadRegions.call(usr$lib$hbase$bin$region_mover#method__25$RUBY$unloadRegions:65535)
        at org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:302)
        at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:173)
        at usr.lib.hbase.bin.region_mover.__file__(/usr/lib/hbase/bin/region_mover.rb:430)
        at usr.lib.hbase.bin.region_mover.load(/usr/lib/hbase/bin/region_mover.rb)
        at org.jruby.Ruby.runScript(Ruby.java:670)
        at org.jruby.Ruby.runNormally(Ruby.java:574)
        at org.jruby.Ruby.runFromMain(Ruby.java:423)
        at org.jruby.Main.doRunFromMain(Main.java:278)
        at org.jruby.Main.internalRun(Main.java:198)
        at org.jruby.Main.run(Main.java:164)
        at org.jruby.Main.run(Main.java:148)
        at org.jruby.Main.main(Main.java:128)
{code}",,,,,,,,
HBASE-5360,"I was testing the region_mover.rb script on a loaded hbase and noticed that it can hang (thus hanging graceful shutdown) if a region that it is attempting to move gets deleted (by a table delete operation).

Here's the start of the relevent stack dump
{code}
12/02/08 13:27:13 WARN client.HConnectionManager$HConnectionImplementation: Encountered problems when prefetch META table:
org.apache.hadoop.hbase.TableNotFoundException: Cannot find row in .META. for table: TestLoadAndVerify_1328735001040, row=TestLoadAnd\
Verify_1328735001040,yC^P\xD7\x945\xD4,99999999999999
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:136)
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:95)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.prefetchRegionCache(HConnectionManager.java:64\
9)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:703\
)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:594)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.relocateRegion(HConnectionManager.java:565)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionLocation(HConnectionManager.java:416)
        at org.apache.hadoop.hbase.client.ServerCallable.instantiateServer(ServerCallable.java:57)
        at org.apache.hadoop.hbase.client.ScannerCallable.instantiateServer(ScannerCallable.java:63)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionServerWithRetries(HConnectionManager.\
java:1018)
        at org.apache.hadoop.hbase.client.HTable$ClientScanner.nextScanner(HTable.java:1104)
        at org.apache.hadoop.hbase.client.HTable$ClientScanner.initialize(HTable.java:1027)
        at org.apache.hadoop.hbase.client.HTable.getScanner(HTable.java:535)
        at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.jruby.javasupport.JavaMethod.invokeDirectWithExceptionHandling(JavaMethod.java:525)
        at org.jruby.javasupport.JavaMethod.invokeDirect(JavaMethod.java:380)
        at org.jruby.java.invokers.InstanceMethodInvoker.call(InstanceMethodInvoker.java:58)
        at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:137)
        at usr.lib.hbase.bin.region_mover.method__7$RUBY$isSuccessfulScan(/usr/lib/hbase/bin/region_mover.rb:133)
        at usr$lib$hbase$bin$region_mover#method__7$RUBY$isSuccessfulScan.call(usr$lib$hbase$bin$region_mover#method__7$RUBY$isSucces\
sfulScan:65535)
        at usr$lib$hbase$bin$region_mover#method__7$RUBY$isSuccessfulScan.call(usr$lib$hbase$bin$region_mover#method__7$RUBY$isSucces\
sfulScan:65535)
        at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:171)
        at usr.lib.hbase.bin.region_mover.block_4$RUBY$__for__(/usr/lib/hbase/bin/region_mover.rb:326)
        at usr$lib$hbase$bin$region_mover#block_4$RUBY$__for__.call(usr$lib$hbase$bin$region_mover#block_4$RUBY$__for__:65535)
        at org.jruby.runtime.CompiledBlock.yield(CompiledBlock.java:133)
        at org.jruby.runtime.BlockBody.call(BlockBody.java:73)
        at org.jruby.runtime.Block.call(Block.java:89)
        at org.jruby.RubyProc.call(RubyProc.java:268)
        at org.jruby.RubyProc.call(RubyProc.java:228)
        at org.jruby.RubyProc$i$0$0$call.call(RubyProc$i$0$0$call.gen:65535)
        at org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:209)
        at org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:205)
        at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:137)
        at org.jruby.ast.CallOneArgNode.interpret(CallOneArgNode.java:57)
        at org.jruby.ast.NewlineNode.interpret(NewlineNode.java:103)
        at org.jruby.ast.WhileNode.interpret(WhileNode.java:131)
        at org.jruby.ast.NewlineNode.interpret(NewlineNode.java:103)
        at org.jruby.ast.BlockNode.interpret(BlockNode.java:71)
        at org.jruby.evaluator.ASTInterpreter.INTERPRET_METHOD(ASTInterpreter.java:74)
        at org.jruby.internal.runtime.methods.InterpretedMethod.call(InterpretedMethod.java:169)
        at org.jruby.internal.runtime.methods.DefaultMethod.call(DefaultMethod.java:171)
        at org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:272)
        at org.jruby.runtime.callsite.CachingCallSite.callBlock(CachingCallSite.java:114)
        at org.jruby.runtime.callsite.CachingCallSite.callIter(CachingCallSite.java:123)
        at usr.lib.hbase.bin.region_mover.chained_26_rescue_4$RUBY$SYNTHETICunloadRegions(/usr/lib/hbase/bin/region_mover.rb:319)
        at usr.lib.hbase.bin.region_mover.method__25$RUBY$unloadRegions(/usr/lib/hbase/bin/region_mover.rb:313)
        at usr$lib$hbase$bin$region_mover#method__25$RUBY$unloadRegions.call(usr$lib$hbase$bin$region_mover#method__25$RUBY$unloadRegions:65535)
        at usr$lib$hbase$bin$region_mover#method__25$RUBY$unloadRegions.call(usr$lib$hbase$bin$region_mover#method__25$RUBY$unloadRegions:65535)
        at org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:302)
        at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:173)
        at usr.lib.hbase.bin.region_mover.__file__(/usr/lib/hbase/bin/region_mover.rb:430)
        at usr.lib.hbase.bin.region_mover.load(/usr/lib/hbase/bin/region_mover.rb)
        at org.jruby.Ruby.runScript(Ruby.java:670)
        at org.jruby.Ruby.runNormally(Ruby.java:574)
        at org.jruby.Ruby.runFromMain(Ruby.java:423)
        at org.jruby.Main.doRunFromMain(Main.java:278)
        at org.jruby.Main.internalRun(Main.java:198)
        at org.jruby.Main.run(Main.java:164)
        at org.jruby.Main.run(Main.java:148)
        at org.jruby.Main.main(Main.java:128)
{code}",,,1,,,,,
HBASE-5369,"I was testing the region_mover.rb script on a loaded hbase and noticed that it can hang (thus hanging graceful shutdown) if a region that it is attempting to move gets deleted (by a table delete operation).

Here's the start of the relevent stack dump
{code}
12/02/08 13:27:13 WARN client.HConnectionManager$HConnectionImplementation: Encountered problems when prefetch META table:
org.apache.hadoop.hbase.TableNotFoundException: Cannot find row in .META. for table: TestLoadAndVerify_1328735001040, row=TestLoadAnd\
Verify_1328735001040,yC^P\xD7\x945\xD4,99999999999999
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:136)
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:95)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.prefetchRegionCache(HConnectionManager.java:64\
9)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:703\
)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:594)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.relocateRegion(HConnectionManager.java:565)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionLocation(HConnectionManager.java:416)
        at org.apache.hadoop.hbase.client.ServerCallable.instantiateServer(ServerCallable.java:57)
        at org.apache.hadoop.hbase.client.ScannerCallable.instantiateServer(ScannerCallable.java:63)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionServerWithRetries(HConnectionManager.\
java:1018)
        at org.apache.hadoop.hbase.client.HTable$ClientScanner.nextScanner(HTable.java:1104)
        at org.apache.hadoop.hbase.client.HTable$ClientScanner.initialize(HTable.java:1027)
        at org.apache.hadoop.hbase.client.HTable.getScanner(HTable.java:535)
        at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.jruby.javasupport.JavaMethod.invokeDirectWithExceptionHandling(JavaMethod.java:525)
        at org.jruby.javasupport.JavaMethod.invokeDirect(JavaMethod.java:380)
        at org.jruby.java.invokers.InstanceMethodInvoker.call(InstanceMethodInvoker.java:58)
        at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:137)
        at usr.lib.hbase.bin.region_mover.method__7$RUBY$isSuccessfulScan(/usr/lib/hbase/bin/region_mover.rb:133)
        at usr$lib$hbase$bin$region_mover#method__7$RUBY$isSuccessfulScan.call(usr$lib$hbase$bin$region_mover#method__7$RUBY$isSucces\
sfulScan:65535)
        at usr$lib$hbase$bin$region_mover#method__7$RUBY$isSuccessfulScan.call(usr$lib$hbase$bin$region_mover#method__7$RUBY$isSucces\
sfulScan:65535)
        at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:171)
        at usr.lib.hbase.bin.region_mover.block_4$RUBY$__for__(/usr/lib/hbase/bin/region_mover.rb:326)
        at usr$lib$hbase$bin$region_mover#block_4$RUBY$__for__.call(usr$lib$hbase$bin$region_mover#block_4$RUBY$__for__:65535)
        at org.jruby.runtime.CompiledBlock.yield(CompiledBlock.java:133)
        at org.jruby.runtime.BlockBody.call(BlockBody.java:73)
        at org.jruby.runtime.Block.call(Block.java:89)
        at org.jruby.RubyProc.call(RubyProc.java:268)
        at org.jruby.RubyProc.call(RubyProc.java:228)
        at org.jruby.RubyProc$i$0$0$call.call(RubyProc$i$0$0$call.gen:65535)
        at org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:209)
        at org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:205)
        at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:137)
        at org.jruby.ast.CallOneArgNode.interpret(CallOneArgNode.java:57)
        at org.jruby.ast.NewlineNode.interpret(NewlineNode.java:103)
        at org.jruby.ast.WhileNode.interpret(WhileNode.java:131)
        at org.jruby.ast.NewlineNode.interpret(NewlineNode.java:103)
        at org.jruby.ast.BlockNode.interpret(BlockNode.java:71)
        at org.jruby.evaluator.ASTInterpreter.INTERPRET_METHOD(ASTInterpreter.java:74)
        at org.jruby.internal.runtime.methods.InterpretedMethod.call(InterpretedMethod.java:169)
        at org.jruby.internal.runtime.methods.DefaultMethod.call(DefaultMethod.java:171)
        at org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:272)
        at org.jruby.runtime.callsite.CachingCallSite.callBlock(CachingCallSite.java:114)
        at org.jruby.runtime.callsite.CachingCallSite.callIter(CachingCallSite.java:123)
        at usr.lib.hbase.bin.region_mover.chained_26_rescue_4$RUBY$SYNTHETICunloadRegions(/usr/lib/hbase/bin/region_mover.rb:319)
        at usr.lib.hbase.bin.region_mover.method__25$RUBY$unloadRegions(/usr/lib/hbase/bin/region_mover.rb:313)
        at usr$lib$hbase$bin$region_mover#method__25$RUBY$unloadRegions.call(usr$lib$hbase$bin$region_mover#method__25$RUBY$unloadRegions:65535)
        at usr$lib$hbase$bin$region_mover#method__25$RUBY$unloadRegions.call(usr$lib$hbase$bin$region_mover#method__25$RUBY$unloadRegions:65535)
        at org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:302)
        at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:173)
        at usr.lib.hbase.bin.region_mover.__file__(/usr/lib/hbase/bin/region_mover.rb:430)
        at usr.lib.hbase.bin.region_mover.load(/usr/lib/hbase/bin/region_mover.rb)
        at org.jruby.Ruby.runScript(Ruby.java:670)
        at org.jruby.Ruby.runNormally(Ruby.java:574)
        at org.jruby.Ruby.runFromMain(Ruby.java:423)
        at org.jruby.Main.doRunFromMain(Main.java:278)
        at org.jruby.Main.internalRun(Main.java:198)
        at org.jruby.Main.run(Main.java:164)
        at org.jruby.Main.run(Main.java:148)
        at org.jruby.Main.main(Main.java:128)
{code}",,,1,,,,,
HBASE-5371,"We need to introduce something like AccessControllerProtocol.checkPermissions(Permission[] permissions) API, so that clients can check access rights before carrying out the operations. We need this kind of operation for HCATALOG-245, which introduces authorization providers for hbase over hcat. We cannot use getUserPermissions() since it requires ADMIN permissions on the global/table level.",1,,,,,,,
HBASE-5385,"Deleting the table or a column does not cascade to the stored permissions at the acl table. We should also remove those permissions, otherwise, it can be a security leak, where freshly created tables contain permissions from previous same-named tables. We might also want to ensure, upon table creation, that no entries are already stored at the acl table. ",1,,,,,,,
HBASE-5387,"We need to to reuse compression streams in HFileBlock.Writer instead of allocating them every time. The motivation is that when using Java's built-in implementation of Gzip, we allocate a new GZIPOutputStream object and an associated native data structure every time we create a compression stream. The native data structure is only deallocated in the finalizer. This is one suspected cause of recent TestHFileBlock failures on Hadoop QA: https://builds.apache.org/job/HBase-TRUNK/2658/testReport/org.apache.hadoop.hbase.io.hfile/TestHFileBlock/testPreviousOffset_1_/.
",,,1,,,,,
HBASE-5388,"We need to to reuse compression streams in HFileBlock.Writer instead of allocating them every time. The motivation is that when using Java's built-in implementation of Gzip, we allocate a new GZIPOutputStream object and an associated native data structure every time we create a compression stream. The native data structure is only deallocated in the finalizer. This is one suspected cause of recent TestHFileBlock failures on Hadoop QA: https://builds.apache.org/job/HBase-TRUNK/2658/testReport/org.apache.hadoop.hbase.io.hfile/TestHFileBlock/testPreviousOffset_1_/.
",,,1,,,,,
HBASE-5393,"We need to to reuse compression streams in HFileBlock.Writer instead of allocating them every time. The motivation is that when using Java's built-in implementation of Gzip, we allocate a new GZIPOutputStream object and an associated native data structure every time we create a compression stream. The native data structure is only deallocated in the finalizer. This is one suspected cause of recent TestHFileBlock failures on Hadoop QA: https://builds.apache.org/job/HBase-TRUNK/2658/testReport/org.apache.hadoop.hbase.io.hfile/TestHFileBlock/testPreviousOffset_1_/.
",,,1,,,,,
HBASE-5396,"The regions plan to open on this server while ServerShutdownHandler is handling, just be removed from AM.regionPlans, and only left to TimeoutMonitor handle these regions. This need to optimize.",,,1,,,,,
HBASE-5399,"The regions plan to open on this server while ServerShutdownHandler is handling, just be removed from AM.regionPlans, and only left to TimeoutMonitor handle these regions. This need to optimize.",,,1,,,,,
HBASE-5416,"The regions plan to open on this server while ServerShutdownHandler is handling, just be removed from AM.regionPlans, and only left to TimeoutMonitor handle these regions. This need to optimize.",,,1,,,,,
HBASE-5422,"In our produce environment
We find a lot of timeout on RIT when cluster up, there are about 7w regions in the cluster( 25 regionservers ).

First, we could see the following log:(See the region 33cf229845b1009aa8a3f7b0f85c9bd0)
master's log
2012-02-13 18:07:41,409 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x348f4a94723da5 Async create of unassigned node for 33cf229845b1009aa8a3f7b0f85c9bd0 with OFFLINE state 
2012-02-13 18:07:42,560 DEBUG org.apache.hadoop.hbase.master.AssignmentManager$CreateUnassignedAsyncCallback: rs=item_20120208,\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0. state=OFFLINE, ts=1329127661409, server=r03f11025.yh.aliyun.com,60020,1329127549907 
2012-02-13 18:07:42,996 DEBUG org.apache.hadoop.hbase.master.AssignmentManager$ExistsUnassignedAsyncCallback: rs=item_20120208,\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0. state=OFFLINE, ts=1329127661409 
2012-02-13 18:10:48,072 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out: item_20120208,\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0. state=PENDING_OPEN, ts=1329127662996
2012-02-13 18:10:48,072 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been PENDING_OPEN for too long, reassigning region=item_20120208,\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0. 
2012-02-13 18:11:16,744 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENED, server=r03f11025.yh.aliyun.com,60020,1329127549907, region=33cf229845b1009aa8a3f7b0f85c9bd0 
2012-02-13 18:38:07,310 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Handling OPENED event for 33cf229845b1009aa8a3f7b0f85c9bd0; deleting unassigned node 
2012-02-13 18:38:07,310 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x348f4a94723da5 Deleting existing unassigned node for 33cf229845b1009aa8a3f7b0f85c9bd0 that is in expected state RS_ZK_REGION_OPENED 
2012-02-13 18:38:07,314 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x348f4a94723da5 Successfully deleted unassigned node for region 33cf229845b1009aa8a3f7b0f85c9bd0 in expected state RS_ZK_REGION_OPENED 
2012-02-13 18:38:07,573 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Opened region item_20120208,\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0. on r03f11025.yh.aliyun.com,60020,1329127549907 
2012-02-13 18:50:54,428 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: No previous transition plan was found (or we are ignoring an existing plan) for item_20120208,\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0. so generated a random one; hri=item_20120208,\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0., src=, dest=r01b05043.yh.aliyun.com,60020,1329127549041; 29 (online=29, exclude=null) available servers 
2012-02-13 18:50:54,428 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region item_20120208,\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0. to r01b05043.yh.aliyun.com,60020,1329127549041 
2012-02-13 19:31:50,514 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out: item_20120208,\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0. state=PENDING_OPEN, ts=1329132528086 
2012-02-13 19:31:50,514 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been PENDING_OPEN for too long, reassigning region=item_20120208,\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0. 



Regionserver's log
2012-02-13 18:07:43,537 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Received request to open region: item_20120208,\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0. 
2012-02-13 18:11:16,560 DEBUG org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Processing open of item_20120208,\x009,1328794343859.33cf229845b1009aa8a3f7b0f85c9bd0. 





Through the RS's log, we could find it is larger than 3mins from receive openRegion request to start processing openRegion, causing timeout on RIT in master for the region.

Let's see the code of StartupBulkAssigner, we could find regionPlans are not added when assigning regions, therefore, when one region opened, it will not updateTimers of other regions whose destination is the same.",,,,,,,,
HBASE-5423,"If closeRegion throws any exception (It would be caused by FS ) when RS is aborting, 
RS will block forever on waitOnAllRegionsToClose().",,,,,,,,
HBASE-5437,"3.facebook.com,60020,1329865516120: Initialization of RS failed.  Hence aborting RS.
java.lang.ClassCastException: $Proxy9 cannot be cast to org.apache.hadoop.hbase.thrift.generated.Hbase$Iface
at org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.newInstance(HbaseHandlerMetricsProxy.java:47)
at org.apache.hadoop.hbase.thrift.ThriftServerRunner.<init>(ThriftServerRunner.java:239)
at org.apache.hadoop.hbase.regionserver.HRegionThriftServer.<init>(HRegionThriftServer.java:74)
at org.apache.hadoop.hbase.regionserver.HRegionServer.initializeThreads(HRegionServer.java:646)
at org.apache.hadoop.hbase.regionserver.HRegionServer.preRegistrationInitialization(HRegionServer.java:546)
at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:658)
at java.lang.Thread.run(Thread.java:662)
2012-02-21 15:05:18,749 FATAL org.apache.hadoop.h
",,,,,,,,
HBASE-5441,"This happens because the master is not started when ThriftServerRunner tries to create an HBaseAdmin.

org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hbase.ipc.ServerNotRunningYetException: Server is not running yet
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1333)

        at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:899)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:150)
        at $Proxy8.getProtocolVersion(Unknown Source)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine.getProxy(WritableRpcEngine.java:183)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:303)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:280)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:332)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getMaster(HConnectionManager.java:649)
        at org.apache.hadoop.hbase.client.HBaseAdmin.<init>(HBaseAdmin.java:108)
        at org.apache.hadoop.hbase.thrift.ThriftServerRunner$HBaseHandler.<init>(ThriftServerRunner.java:516)
        at org.apache.hadoop.hbase.regionserver.HRegionThriftServer$HBaseHandlerRegion.<init>(HRegionThriftServer.java:104)
        at org.apache.hadoop.hbase.regionserver.HRegionThriftServer.<init>(HRegionThriftServer.java:74)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.initializeThreads(HRegionServer.java:646)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.preRegistrationInitialization(HRegionServer.java:546)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:658)
        at java.lang.Thread.run(Thread.java:662)
2012-02-21 16:38:18,223 INFO org.apache.hadoop.hba",,,,,,,,
HBASE-5458,"I've seen some occasional NullPointerExceptions in ZlibFactory.isNativeZlibLoaded(conf) during region server startups and the completebulkload process.  This is being caused by a null configuration getting passed to the isNativeZlibLoaded method.  I think this happens when 2 or more threads call the CompressionTest.testCompression method at once.  If the GZ algorithm has not been tested yet both threads could continue on and attempt to load the compressor.  For GZ the getCodec method is not thread safe which could lead to one thread getting a reference to a GzipCodec that has a null configuration.
{code}
current:
      DefaultCodec getCodec(Configuration conf) {
        if (codec == null) {
          codec = new GzipCodec();
          codec.setConf(new Configuration(conf));
        }

        return codec;
      }
{code}
one possible fix would be something like this:
{code}
      DefaultCodec getCodec(Configuration conf) {
        if (codec == null) {
          GzipCodec gzip = new GzipCodec();
          gzip.setConf(new Configuration(conf));
          codec = gzip;
        }

        return codec;
      }
{code}
But that may not be totally safe without some synchronization.  An upstream fix in CompressionTest could also prevent multi thread access to GZ.getCodec(conf)

exceptions:
12/02/21 16:11:56 ERROR handler.OpenRegionHandler: Failed open of region=all-monthly,,1326263896983.bf574519a95263ec23a2bad9f5b8cbf4.
java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.hbase.util.CompressionTest.testCompression(CompressionTest.java:89)
        at org.apache.hadoop.hbase.regionserver.HRegion.checkCompressionCodecs(HRegion.java:2670)
        at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:2659)
        at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:2647)
        at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.openRegion(OpenRegionHandler.java:312)
        at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:99)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:158)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.io.compress.zlib.ZlibFactory.isNativeZlibLoaded(ZlibFactory.java:63)
        at org.apache.hadoop.io.compress.GzipCodec.getCompressorType(GzipCodec.java:166)
        at org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:100)
        at org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:112)
        at org.apache.hadoop.hbase.io.hfile.Compression$Algorithm.getCompressor(Compression.java:236)
        at org.apache.hadoop.hbase.util.CompressionTest.testCompression(CompressionTest.java:84)
        ... 9 more

Caused by: java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.hbase.util.CompressionTest.testCompression(CompressionTest.java:89)
        at org.apache.hadoop.hbase.io.hfile.HFile$Reader.readTrailer(HFile.java:890)
        at org.apache.hadoop.hbase.io.hfile.HFile$Reader.loadFileInfo(HFile.java:819)
        at org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.groupOrSplit(LoadIncrementalHFiles.java:405)
        at org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles$2.call(LoadIncrementalHFiles.java:323)
        at org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles$2.call(LoadIncrementalHFiles.java:321)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.io.compress.zlib.ZlibFactory.isNativeZlibLoaded(ZlibFactory.java:63)
        at org.apache.hadoop.io.compress.GzipCodec.getCompressorType(GzipCodec.java:166)
        at org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:100)
        at org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:112)
        at org.apache.hadoop.hbase.io.hfile.Compression$Algorithm.getCompressor(Compression.java:236)
        at org.apache.hadoop.hbase.util.CompressionTest.testCompression(CompressionTest.java:84)
        ... 10 more
",,,,,,,,
HBASE-5466,"Having upgraded to CDH3U3 version of hbase we found we had a zookeeper connection leak, tracking it down we found that closing the connection will only close the zookeeper connection if all calls to get the connection have been closed, there is incCount and decCount in the HConnection class,

When a table is opened it makes a call to the metascanner class which opens a connection to the meta table, this table never gets closed.

This caused the count in the HConnection class to never return to zero meaning that the zookeeper connection will not close when we close all the tables or calling
HConnectionManager.deleteConnection(config, true);	


",,,,,,,,
HBASE-5470,"DataBlockEncodingTool was fixed as part of porting data block encoding (HBASE-4218) to 89-fb (https://reviews.facebook.net/rHBASEEIGHTNINEFBBRANCH1245291, https://reviews.facebook.net/D1659). The bug appeared when using GZ as baseline compression codec but not loading native Hadoop libraries, in which case the compressor instance would be null. The purpose of this JIRA is to bring the trunk version of DataBlockEncodingTool to parity with the 89-fb version, and further improvements to the tool will be made separately.
",,,1,,,,,
HBASE-5472,"I have some HFiles for two column families 'y','z', but I specified a target table that only has CF 'y'.
I see the following repeated forever.
...
12/02/23 22:57:37 WARN mapreduce.LoadIncrementalHFiles: Attempt to bulk load region containing  into table z with files [family:y path:hdfs://bunnypig:9000/bulk/z2/y/bd6f1c3cc8b443fc9e9e5fddcdaa3b09, family:z path:hdfs://bunnypig:9000/bulk/z2/z/38f12fdbb7de40e8bf0e6489ef34365d] failed.  This is recoverable and they will be retried.
12/02/23 22:57:37 DEBUG client.MetaScanner: Scanning .META. starting at row=z,,00000000000000 for max=2147483647 rows using org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@7b7a4989
12/02/23 22:57:37 INFO mapreduce.LoadIncrementalHFiles: Split occured while grouping HFiles, retry attempt 1596 with 2 files remaining to group or split
12/02/23 22:57:37 INFO mapreduce.LoadIncrementalHFiles: Trying to load hfile=hdfs://bunnypig:9000/bulk/z2/y/bd6f1c3cc8b443fc9e9e5fddcdaa3b09 first=r last=r
12/02/23 22:57:37 INFO mapreduce.LoadIncrementalHFiles: Trying to load hfile=hdfs://bunnypig:9000/bulk/z2/z/38f12fdbb7de40e8bf0e6489ef34365d first=r last=r
12/02/23 22:57:37 DEBUG mapreduce.LoadIncrementalHFiles: Going to connect to server region=z,,1330066309814.d5fa76a38c9565f614755e34eacf8316., hostname=localhost, port=60020 for row 
...",,,,,,,,
HBASE-5479,"DataBlockEncodingTool was fixed as part of porting data block encoding (HBASE-4218) to 89-fb (https://reviews.facebook.net/rHBASEEIGHTNINEFBBRANCH1245291, https://reviews.facebook.net/D1659). The bug appeared when using GZ as baseline compression codec but not loading native Hadoop libraries, in which case the compressor instance would be null. The purpose of this JIRA is to bring the trunk version of DataBlockEncodingTool to parity with the 89-fb version, and further improvements to the tool will be made separately.
",,,1,,,,,
HBASE-5481,"If a host gets decommissioned and its hostname no longer resolves, and it was previously hosting ROOT or META, HBase won't be able to start up.  This easily happens when moving across networks (e.g. developing HBase on a laptop), but can also happen during cluster-wide maintenances where HBase is shut down, then one or more nodes get decommissioned such that their hostnames no longer resolve.

{code}
2012-02-26 20:05:48,339 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region -ROOT-,,0.70236052 to nowwhat.tsunanet.net,54092,1330315542087
[...]
2012-02-26 20:05:48,456 INFO org.apache.hadoop.hbase.regionserver.HRegion: Onlined -ROOT-,,0.70236052; next sequenceid=268
2012-02-26 20:05:48,456 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:54092-0x135bcfbb0580001 Attempting to transition node 70236052/-ROOT- from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2012-02-26 20:05:48,458 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:54092-0x135bcfbb0580001 Successfully transitioned node 70236052 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2012-02-26 20:05:48,459 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=nowwhat.tsunanet.net,54092,1330315542087, region=70236052/-ROOT-
2012-02-26 20:05:48,459 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Post open deploy tasks for region=-ROOT-,,0.70236052, daughter=false
2012-02-26 20:05:48,460 INFO org.apache.hadoop.hbase.catalog.RootLocationEditor: Setting ROOT region location in ZooKeeper as nowwhat.tsunanet.net,54092,1330315542087
2012-02-26 20:05:48,466 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Done with post open deploy task for region=-ROOT-,,0.70236052, daughter=false
2012-02-26 20:05:48,466 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:54092-0x135bcfbb0580001 Attempting to transition node 70236052/-ROOT- from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2012-02-26 20:05:48,468 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:54092-0x135bcfbb0580001 Successfully transitioned node 70236052 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2012-02-26 20:05:48,468 DEBUG org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: region transitioned to opened in zookeeper: {NAME => '-ROOT-,,0', STARTKEY => '', ENDKEY => '', ENCODED => 70236052,}, server: nowwhat.tsunanet.net,54092,1330315542087
2012-02-26 20:05:48,468 DEBUG org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Opened -ROOT-,,0.70236052 on server:nowwhat.tsunanet.net,54092,1330315542087
2012-02-26 20:05:48,468 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENED, server=nowwhat.tsunanet.net,54092,1330315542087, region=70236052/-ROOT-
2012-02-26 20:05:48,470 INFO org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Handling OPENED event for -ROOT-,,0.70236052 from nowwhat.tsunanet.net,54092,1330315542087; deleting unassigned node
2012-02-26 20:05:48,470 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:54081-0x135bcfbb0580000 Deleting existing unassigned node for 70236052 that is in expected state RS_ZK_REGION_OPENED
2012-02-26 20:05:48,472 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: The znode of region -ROOT-,,0.70236052 has been deleted.
2012-02-26 20:05:48,472 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:54081-0x135bcfbb0580000 Successfully deleted unassigned node for region 70236052 in expected state RS_ZK_REGION_OPENED
2012-02-26 20:05:48,472 INFO org.apache.hadoop.hbase.master.AssignmentManager: The master has opened the region -ROOT-,,0.70236052 that was online on nowwhat.tsunanet.net,54092,1330315542087
2012-02-26 20:05:48,473 INFO org.apache.hadoop.hbase.master.HMaster: -ROOT- assigned=1, rit=false, location=nowwhat.tsunanet.net,54092,1330315542087
2012-02-26 20:05:48,486 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation: Lookedup root region location, connection=org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@16d0a6a3; serverName=nowwhat.tsunanet.net,54092,1330315542087
2012-02-26 20:05:48,488 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation: Lookedup root region location, connection=org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@16d0a6a3; serverName=nowwhat.tsunanet.net,54092,1330315542087
2012-02-26 20:05:48,620 FATAL org.apache.hadoop.hbase.master.HMaster: Master server abort: loaded coprocessors are: []
2012-02-26 20:05:48,621 FATAL org.apache.hadoop.hbase.master.HMaster: Unhandled exception. Starting shutdown.
java.net.UnknownHostException: unknown host: h-27-1.sfo.stumble.net
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.<init>(HBaseClient.java:227)
        at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:1016)
        at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:878)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:150)
        at $Proxy12.getProtocolVersion(Unknown Source)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine.getProxy(WritableRpcEngine.java:183)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:303)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:280)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:332)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.waitForProxy(HBaseRPC.java:236)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:1278)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:1235)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:1222)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.getCachedConnection(CatalogTracker.java:564)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.getMetaServerConnection(CatalogTracker.java:422)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMeta(CatalogTracker.java:478)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMetaServerConnection(CatalogTracker.java:503)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.verifyMetaRegionLocation(CatalogTracker.java:674)
        at org.apache.hadoop.hbase.master.HMaster.assignRootAndMeta(HMaster.java:575)
        at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:491)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:326)
        at org.apache.hadoop.hbase.master.HMasterCommandLine$LocalHMaster.run(HMasterCommandLine.java:218)
        at java.lang.Thread.run(Thread.java:680)
2012-02-26 20:05:48,626 INFO org.apache.hadoop.hbase.master.HMaster: Aborting
{code}",,,,,,,,
HBASE-5482,"There are possibility of 2 problems
-> When we populate regionsToMove while iterating the serverinfo in descending manner there is a chance that the same region can be added twice.
Because in the first loop we do a randomization of the regions.
Where as when we get we have neededRegions!= 0 we just get the region in the index and add it again . This may lead to have same region in the regionsToMove list.
-> Another problem is 
when the problem in the first point happens then there is a chance that
the regionToMove can have the same src and destination and the same region can be picked every 5 mins.
{code}
for(Map.Entry<HServerInfo, List<HRegionInfo>> server :
        serversByLoad.descendingMap().entrySet()) {
        BalanceInfo balanceInfo = serverBalanceInfo.get(server.getKey());
        int idx =
          balanceInfo == null ? 0 : balanceInfo.getNextRegionForUnload();
        if (idx >= server.getValue().size()) break;
        HRegionInfo region = server.getValue().get(idx);
        if (region.isMetaRegion()) continue; // Don't move meta regions.
        regionsToMove.add(new RegionPlan(region, server.getKey(), null));
        if(--neededRegions == 0) {
          // No more regions needed, done shedding
          break;
        }
      }
{code}
If i have meta and root in the top two loaded region server(totally 3 RS), we just skip the regions in those region server and populate the region from the least loaded RS.
Then in the next loop we iterate from the least loaded server and populate the destination as also the same server.
This is leading to a condition where every 5 min balancing happens and also the server is same for src and dest.",,,1,,,,,
HBASE-5494,"Design doc: https://cwiki.apache.org/confluence/display/HCATALOG/HBase+Secure+Bulk+Load
Short summary:
Security as it stands does not cover the bulkLoadHFiles() feature. Users calling this method will bypass ACLs. Also loading is made more cumbersome in a secure setting because of hdfs privileges. bulkLoadHFiles() moves the data from user's directory to the hbase directory, which would require certain write access privileges set.
Our solution is to create a coprocessor which makes use of AuthManager to verify if a user has write access to the table. If so, launches a MR job as the hbase user to do the importing (ie rewrite from text to hfiles). One tricky part this job will have to do is impersonate the calling user when reading the input files. We can do this by expecting the user to pass an hdfs delegation token as part of the secureBulkLoad() coprocessor call and extend an inputformat to make use of that token. The output is written to a temporary directory accessible only by hbase and then bulkloadHFiles() is called.",1,,,,,,,
HBASE-5498,"Design doc: https://cwiki.apache.org/confluence/display/HCATALOG/HBase+Secure+Bulk+Load
Short summary:
Security as it stands does not cover the bulkLoadHFiles() feature. Users calling this method will bypass ACLs. Also loading is made more cumbersome in a secure setting because of hdfs privileges. bulkLoadHFiles() moves the data from user's directory to the hbase directory, which would require certain write access privileges set.
Our solution is to create a coprocessor which makes use of AuthManager to verify if a user has write access to the table. If so, launches a MR job as the hbase user to do the importing (ie rewrite from text to hfiles). One tricky part this job will have to do is impersonate the calling user when reading the input files. We can do this by expecting the user to pass an hdfs delegation token as part of the secureBulkLoad() coprocessor call and extend an inputformat to make use of that token. The output is written to a temporary directory accessible only by hbase and then bulkloadHFiles() is called.",1,,,,,,,
HBASE-5532,"We found error log (NullPointerException) below on our online cluster:

2012-03-05 00:17:09,592 ERROR org.apache.hadoop.hbase.regionserver.HRegionServer$MajorCompactionChecker: Caught exception
java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.Store.isMajorCompaction(Store.java:878)
        at org.apache.hadoop.hbase.regionserver.Store.isMajorCompaction(Store.java:857)
        at org.apache.hadoop.hbase.regionserver.HRegion.isMajorCompaction(HRegion.java:3017)
        at org.apache.hadoop.hbase.regionserver.HRegionServer$MajorCompactionChecker.chore(HRegionServer.java:1172)
        at org.apache.hadoop.hbase.Chore.run(Chore.java:66)

After Check the code we found although it already check whether store files has null reader at the begin of the function(isMajorCompaction), but it still has some possibility the reader is closed before it return(eg mini compaction). So we need to check store file reader before we use it to avoid this NPE


",,,,,,,,
HBASE-5545,"Scenario:
------------
1. File is created 
2. But while writing data, all datanodes might have crashed. So writing data will fail.
3. Now even if close is called in finally block, close also will fail and throw the Exception because writing data failed.
4. After this if RS try to create the same file again, then AlreadyBeingCreatedException will come.

Suggestion to handle this scenario.
---------------------------
1. Check for the existence of the file, if exists delete the file and create new file. 

Here delete call for the file will not check whether the file is open or closed.

Overwrite Option:
----------------
1. Overwrite option will be applicable if you are trying to overwrite a closed file.
2. If the file is not closed, then even with overwrite option Same AlreadyBeingCreatedException will be thrown.
This is the expected behaviour to avoid the Multiple clients writing to same file.


Region server logs:

org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: failed to create file /hbase/test1/12c01902324218d14b17a5880f24f64b/.tmp/.regioninfo for DFSClient_hb_rs_158-1-131-48,20020,1331107668635_1331107669061_-252463556_25 on client 158.1.132.19 because current leaseholder is trying to recreate file.
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:1570)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1440)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1382)
at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:658)
at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:547)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1137)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1133)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:396)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1131)

at org.apache.hadoop.ipc.Client.call(Client.java:961)
at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:245)
at $Proxy6.create(Unknown Source)
at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at $Proxy6.create(Unknown Source)
at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3643)
at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:778)
at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:364)
at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:630)
at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:611)
at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:518)
at org.apache.hadoop.hbase.regionserver.HRegion.checkRegioninfoOnFilesystem(HRegion.java:424)
at org.apache.hadoop.hbase.regionserver.HRegion.initialize(HRegion.java:340)
at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:2672)
at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:2658)
at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.openRegion(OpenRegionHandler.java:330)
at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:116)
at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:158)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:662)
[2012-03-07 20:51:45,858] [WARN ] [RS_OPEN_REGION-158-1-131-48,20020,1331107668635-23] [com.huawei.isap.ump.ha.client.RPCRetryAndSwitchInvoker 131] Retrying the method call: public abstract void org.apache.hadoop.hdfs.protocol.ClientProtocol.create(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,boolean,boolean,short,long) throws java.io.IOException with arguments of length: 7. The exisiting ActiveServerConnection is:
ActiveServerConnectionInfo:
Metadata:158-1-131-48/158.1.132.19:9000
Version:145720623220907

[2012-03-07 20:51:45,872] [DEBUG] [RS_OPEN_REGION-158-1-131-48,20020,1331107668635-20] [org.apache.hadoop.hbase.zookeeper.ZKAssign 849] regionserver:20020-0x135ec32b39e0002-0x135ec32b39e0002 Successfully transitioned node 91bf3e6f8adb2e4b335f061036353126 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
[2012-03-07 20:51:45,873] [DEBUG] [RS_OPEN_REGION-158-1-131-48,20020,1331107668635-20] [org.apache.hadoop.hbase.regionserver.HRegion 2649] Opening region: REGION => {NAME => 'test1,00088613810,1331112369360.91bf3e6f8adb2e4b335f061036353126.', STARTKEY => '00088613810', ENDKEY => '00088613815', ENCODED => 91bf3e6f8adb2e4b335f061036353126, TABLE => {{NAME => 'test1', FAMILIES => [{NAME => 'value', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION => 'GZ', TTL => '86400', BLOCKSIZE => '655360', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}
[2012-03-07 20:51:45,873] [DEBUG] [RS_OPEN_REGION-158-1-131-48,20020,1331107668635-20] [org.apache.hadoop.hbase.regionserver.HRegion 316] Instantiated test1,00088613810,1331112369360.91bf3e6f8adb2e4b335f061036353126.
[2012-03-07 20:51:45,874] [ERROR] [RS_OPEN_REGION-158-1-131-48,20020,1331107668635-20] [ 

",,,,,,,,
HBASE-5547,"There is a retry mechanism in RecoverableZooKeeper, but when the session expires, the whole ZooKeeperWatcher is recreated, hence the retry mechanism does not work in this case. This is why a sleep is needed in TestZooKeeper#testMasterSessionExpired: we need to wait for ZooKeeperWatcher to be recreated before using the connection.

This can happen in real life, it can happen when:
- master & zookeeper starts
- zookeeper connection is cut
- master enters the retry loop
- in the meantime the session expires
- the network comes back, the session is recreated
- the retries continues, but on the wrong object, hence fails.
",,,,,,,,
HBASE-5549,"There is a retry mechanism in RecoverableZooKeeper, but when the session expires, the whole ZooKeeperWatcher is recreated, hence the retry mechanism does not work in this case. This is why a sleep is needed in TestZooKeeper#testMasterSessionExpired: we need to wait for ZooKeeperWatcher to be recreated before using the connection.

This can happen in real life, it can happen when:
- master & zookeeper starts
- zookeeper connection is cut
- master enters the retry loop
- in the meantime the session expires
- the network comes back, the session is recreated
- the retries continues, but on the wrong object, hence fails.
",,,,,,,,
HBASE-5563,"In the one region multi assigned case,  we could find that two regions have the same table name, same startKey, same endKey, and different regionId, so these two regions are same in TreeMap but different in HashMap.",,,,,,,,
HBASE-5564,"Duplicate records are getting discarded when duplicate records exists in same input file and more specifically if they exists in same split.
Duplicate records are considered if the records are from diffrent different splits.

Version under test: HBase 0.92",,,,,,,,
HBASE-5568,"We could call HRegion#flushcache() concurrently now through HRegionServer#splitRegion or HRegionServer#flushRegion by HBaseAdmin.
However, we find if HRegion#internalFlushcache() is called concurrently by multi thread, HRegion.memstoreSize will be calculated wrong.
At the end of HRegion#internalFlushcache(), we will do this.addAndGetGlobalMemstoreSize(-flushsize), but the flushsize may not the actual memsize which flushed to hdfs. It cause HRegion.memstoreSize is negative and prevent next flush if we close this region.

Logs in RS for region e9d827913a056e696c39bc569ea3

2012-03-11 16:31:36,690 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memstore flush for writetest1,,1331454657410.e9d827913a056e696c39bc569ea3
f99f., current region memstore size 128.0m
2012-03-11 16:31:37,999 INFO org.apache.hadoop.hbase.regionserver.Store: Added hdfs://dw74.kgb.sqa.cm4:9700/hbase-func1/writetest1/e9d827913a056e696c39bc569e
a3f99f/cf1/8162481165586107427, entries=153106, sequenceid=619316544, memsize=59.6m, filesize=31.2m
2012-03-11 16:31:38,830 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memstore flush for writetest1,,1331454657410.e9d827913a056e696c39bc569ea3
f99f., current region memstore size 134.8m
2012-03-11 16:31:39,458 INFO org.apache.hadoop.hbase.regionserver.Store: Added hdfs://dw74.kgb.sqa.cm4:9700/hbase-func1/writetest1/e9d827913a056e696c39bc569e
a3f99f/cf2/3425971951499794221, entries=230183, sequenceid=619316544, memsize=68.5m, filesize=26.6m
2012-03-11 16:31:39,459 INFO org.apache.hadoop.hbase.regionserver.HRegion: Finished memstore flush of ~128.1m for region writetest1,,1331454657410.e9d827913a
056e696c39bc569ea3f99f. in 2769ms, sequenceid=619316544, compaction requested=false
2012-03-11 16:31:39,459 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memstore flush for writetest1,,1331454657410.e9d827913a056e696c39bc569ea3
f99f., current region memstore size 6.8m
2012-03-11 16:31:39,529 INFO org.apache.hadoop.hbase.regionserver.Store: Added hdfs://dw74.kgb.sqa.cm4:9700/hbase-func1/writetest1/e9d827913a056e696c39bc569e
a3f99f/cf1/1811012969998104626, entries=8002, sequenceid=619332759, memsize=3.1m, filesize=1.6m
2012-03-11 16:31:39,640 INFO org.apache.hadoop.hbase.regionserver.Store: Added hdfs://dw74.kgb.sqa.cm4:9700/hbase-func1/writetest1/e9d827913a056e696c39bc569e
a3f99f/cf2/770333473623552048, entries=12231, sequenceid=619332759, memsize=3.6m, filesize=1.4m
2012-03-11 16:31:39,641 INFO org.apache.hadoop.hbase.regionserver.HRegion: Finished memstore flush of ~134.8m for region writetest1,,1331454657410.e9d827913a
056e696c39bc569ea3f99f. in 811ms, sequenceid=619332759, compaction requested=true
2012-03-11 16:31:39,707 INFO org.apache.hadoop.hbase.regionserver.Store: Added hdfs://dw74.kgb.sqa.cm4:9700/hbase-func1/writetest1/e9d827913a056e696c39bc569e
a3f99f/cf1/5656568849587368557, entries=119, sequenceid=619332979, memsize=47.4k, filesize=25.6k
2012-03-11 16:31:39,775 INFO org.apache.hadoop.hbase.regionserver.Store: Added hdfs://dw74.kgb.sqa.cm4:9700/hbase-func1/writetest1/e9d827913a056e696c39bc569e
a3f99f/cf2/794343845650987521, entries=157, sequenceid=619332979, memsize=47.8k, filesize=19.3k
2012-03-11 16:31:39,777 INFO org.apache.hadoop.hbase.regionserver.HRegion: Finished memstore flush of ~6.8m for region writetest1,,1331454657410.e9d827913a05
6e696c39bc569ea3f99f. in 318ms, sequenceid=619332979, compaction requested=true",,,,,,,,
HBASE-5571,"If we restart master when it is disabling one table, the table will be disabling forever.

In current logic, Region CLOSE RPC will always returned NotServingRegionException because RS has already closed the region before we restart master. So table will be disabling forever because the region will in RIT all along.

In another case, when AssignmentManager#rebuildUserRegions(), it will put parent regions to AssignmentManager.regions, so we can't close these parent regions until it is purged by CatalogJanitor if we execute disabling the table.",,1,,,,,,
HBASE-5572,"If we restart master when it is disabling one table, the table will be disabling forever.

In current logic, Region CLOSE RPC will always returned NotServingRegionException because RS has already closed the region before we restart master. So table will be disabling forever because the region will in RIT all along.

In another case, when AssignmentManager#rebuildUserRegions(), it will put parent regions to AssignmentManager.regions, so we can't close these parent regions until it is purged by CatalogJanitor if we execute disabling the table.",,,1,,,,,
HBASE-5578,"The regeionserver log:
2012-03-11 11:55:37,808 FATAL org.apache.hadoop.hbase.regionserver.HRegionServer: ABORTING region server data3,60020,1331286604591: Unhandled exception: null
java.lang.NullPointerException
	at org.apache.hadoop.hbase.regionserver.Store.getTotalStaticIndexSize(Store.java:1788)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.createRegionLoad(HRegionServer.java:994)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.buildServerLoad(HRegionServer.java:800)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.tryRegionServerReport(HRegionServer.java:776)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:678)
	at java.lang.Thread.run(Thread.java:662)
2012-03-11 11:55:37,808 FATAL org.apache.hadoop.hbase.regionserver.HRegionServer: RegionServer abort: loaded coprocessors are: []
2012-03-11 11:55:37,808 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Dump of metrics: requestsPerSecond=1687, numberOfOnlineRegions=37, numberOfStores=37, numberOfStorefiles=144, storefileIndexSizeMB=2, rootIndexSizeKB=2362, totalStaticIndexSizeKB=229808, totalStaticBloomSizeKB=2166296, memstoreSizeMB=2854, readRequestsCount=1352673, writeRequestsCount=113137586, compactionQueueSize=8, flushQueueSize=3, usedHeapMB=7359, maxHeapMB=12999, blockCacheSizeMB=32.31, blockCacheFreeMB=3867.52, blockCacheCount=38, blockCacheHitCount=87713, blockCacheMissCount=22144560, blockCacheEvictedCount=122, blockCacheHitRatio=0%, blockCacheHitCachingRatio=99%, hdfsBlocksLocalityIndex=100
2012-03-11 11:55:37,992 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: STOPPED: Unhandled exception: null",,,,,,,,
HBASE-5583,"-> Create table using splitkeys
-> MAster goes down before all regions are added to meta
-> On master restart the table is again enabled but with less number of regions than specified in splitkeys

Anyway client will get an exception if i had called sync ",,,,,,,,
HBASE-5586,"This is from 0.92.1-ish:

{noformat}
2012-03-15 09:52:16,589 ERROR
org.apache.hadoop.hbase.replication.regionserver.ReplicationSource:
Unexpected exception in ReplicationSource, currentPath=null
java.lang.NullPointerException
       at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.chooseSinks(ReplicationSource.java:223)
       at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.connectToPeers(ReplicationSource.java:442)
       at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:246)
{noformat}

I wanted to add a replication stream to a cluster that wasn't existing yet so that the logs would be buffered until then. This should just be treated as if there was no region servers.",,,,,,,,
HBASE-5599,"This is from 0.92.1-ish:

{noformat}
2012-03-15 09:52:16,589 ERROR
org.apache.hadoop.hbase.replication.regionserver.ReplicationSource:
Unexpected exception in ReplicationSource, currentPath=null
java.lang.NullPointerException
       at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.chooseSinks(ReplicationSource.java:223)
       at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.connectToPeers(ReplicationSource.java:442)
       at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:246)
{noformat}

I wanted to add a replication stream to a cluster that wasn't existing yet so that the logs would be buffered until then. This should just be treated as if there was no region servers.",,,,,,,,
HBASE-5603,"Due to bugfix ZOOKEEPER-1059 (ZK 3.4.0+), the rolling-restart.sh script will hang when attempting to make sure the /hbase/master znode is deleted.

Here's the code
{code}
# make sure the master znode has been deleted before continuing
    zparent=`$bin/hbase org.apache.hadoop.hbase.util.HBaseConfTool zookeeper.znode.parent`
    if [ ""$zparent"" == ""null"" ]; then zparent=""/hbase""; fi
    zmaster=`$bin/hbase org.apache.hadoop.hbase.util.HBaseConfTool zookeeper.znode.master`
    if [ ""$zmaster"" == ""null"" ]; then zmaster=""master""; fi
    zmaster=$zparent/$zmaster
    echo -n ""Waiting for Master ZNode ${zmaster} to expire""
    while bin/hbase zkcli stat $zmaster >/dev/null 2>&1; do
      echo -n "".""
      sleep 1
    done
    echo #force a newline
{code}

Prior to ZOOKEEPER-1059, stat on a null znode would NPE and cause zkcli to exit with retcode 1.  Afterwards, the null is caught, zkcli will exit with 0 in the case where the znode is present and in the case where it does not exist.
",,,,,,,,
HBASE-5604,"1. One rs died, the servershutdownhandler found it out and started the distributed log splitting;
2. All tasks are failed due to ZK connection lost, so the all the tasks were deleted asynchronously;
3. Servershutdownhandler retried the log splitting;
4. The asynchronously deletion in step 2 finally happened for new task
5. This made the SplitLogManger in hanging state.

This leads to .META. region not assigened for long time


{noformat}
hbase-root-master-HOST-192-168-47-204.log.2012-03-14""(55413,79):2012-03-14 19:28:47,932 DEBUG org.apache.hadoop.hbase.master.SplitLogManager: put up splitlog task at znode /hbase/splitlog/hdfs%3A%2F%2F192.168.47.205%3A9000%2Fhbase%2F.logs%2Flinux-114.site%2C60020%2C1331720381665-splitting%2Flinux-114.site%252C60020%252C1331720381665.1331752316170
hbase-root-master-HOST-192-168-47-204.log.2012-03-14""(89303,79):2012-03-14 19:34:32,387 DEBUG org.apache.hadoop.hbase.master.SplitLogManager: put up splitlog task at znode /hbase/splitlog/hdfs%3A%2F%2F192.168.47.205%3A9000%2Fhbase%2F.logs%2Flinux-114.site%2C60020%2C1331720381665-splitting%2Flinux-114.site%252C60020%252C1331720381665.1331752316170
{noformat}

{noformat}
hbase-root-master-HOST-192-168-47-204.log.2012-03-14""(80417,99):2012-03-14 19:34:31,196 DEBUG org.apache.hadoop.hbase.master.SplitLogManager$DeleteAsyncCallback: deleted /hbase/splitlog/hdfs%3A%2F%2F192.168.47.205%3A9000%2Fhbase%2F.logs%2Flinux-114.site%2C60020%2C1331720381665-splitting%2Flinux-114.site%252C60020%252C1331720381665.1331752316170
hbase-root-master-HOST-192-168-47-204.log.2012-03-14""(89456,99):2012-03-14 19:34:32,497 DEBUG org.apache.hadoop.hbase.master.SplitLogManager$DeleteAsyncCallback: deleted /hbase/splitlog/hdfs%3A%2F%2F192.168.47.205%3A9000%2Fhbase%2F.logs%2Flinux-114.site%2C60020%2C1331720381665-splitting%2Flinux-114.site%252C60020%252C1331720381665.1331752316170
{noformat}
",,,1,,,,,
HBASE-5606,"1. One rs died, the servershutdownhandler found it out and started the distributed log splitting;
2. All tasks are failed due to ZK connection lost, so the all the tasks were deleted asynchronously;
3. Servershutdownhandler retried the log splitting;
4. The asynchronously deletion in step 2 finally happened for new task
5. This made the SplitLogManger in hanging state.

This leads to .META. region not assigened for long time


{noformat}
hbase-root-master-HOST-192-168-47-204.log.2012-03-14""(55413,79):2012-03-14 19:28:47,932 DEBUG org.apache.hadoop.hbase.master.SplitLogManager: put up splitlog task at znode /hbase/splitlog/hdfs%3A%2F%2F192.168.47.205%3A9000%2Fhbase%2F.logs%2Flinux-114.site%2C60020%2C1331720381665-splitting%2Flinux-114.site%252C60020%252C1331720381665.1331752316170
hbase-root-master-HOST-192-168-47-204.log.2012-03-14""(89303,79):2012-03-14 19:34:32,387 DEBUG org.apache.hadoop.hbase.master.SplitLogManager: put up splitlog task at znode /hbase/splitlog/hdfs%3A%2F%2F192.168.47.205%3A9000%2Fhbase%2F.logs%2Flinux-114.site%2C60020%2C1331720381665-splitting%2Flinux-114.site%252C60020%252C1331720381665.1331752316170
{noformat}

{noformat}
hbase-root-master-HOST-192-168-47-204.log.2012-03-14""(80417,99):2012-03-14 19:34:31,196 DEBUG org.apache.hadoop.hbase.master.SplitLogManager$DeleteAsyncCallback: deleted /hbase/splitlog/hdfs%3A%2F%2F192.168.47.205%3A9000%2Fhbase%2F.logs%2Flinux-114.site%2C60020%2C1331720381665-splitting%2Flinux-114.site%252C60020%252C1331720381665.1331752316170
hbase-root-master-HOST-192-168-47-204.log.2012-03-14""(89456,99):2012-03-14 19:34:32,497 DEBUG org.apache.hadoop.hbase.master.SplitLogManager$DeleteAsyncCallback: deleted /hbase/splitlog/hdfs%3A%2F%2F192.168.47.205%3A9000%2Fhbase%2F.logs%2Flinux-114.site%2C60020%2C1331720381665-splitting%2Flinux-114.site%252C60020%252C1331720381665.1331752316170
{noformat}
",,,,,,,,
HBASE-5611,"This bug is rather easy to get if the {{TimeoutMonitor}} is on, else I think it's still possible to hit it if a region fails to open for more obscure reasons like HDFS errors.

Consider a region that just went through distributed splitting and that's now being opened by a new RS. The first thing it does is to read the recovery files and put the edits in the {{MemStores}}. If this process takes a long time, the master will move that region away. At that point the edits are still accounted for in the global {{MemStore}} size but they are dropped when the {{HRegion}} gets cleaned up. It's completely invisible until the {{MemStoreFlusher}} needs to force flush a region and that none of them have edits:

{noformat}
2012-03-21 00:33:39,303 DEBUG org.apache.hadoop.hbase.regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=5.9g
2012-03-21 00:33:39,303 ERROR org.apache.hadoop.hbase.regionserver.MemStoreFlusher: Cache flusher failed for entry null
java.lang.IllegalStateException
        at com.google.common.base.Preconditions.checkState(Preconditions.java:129)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushOneForGlobalPressure(MemStoreFlusher.java:199)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.run(MemStoreFlusher.java:223)
        at java.lang.Thread.run(Thread.java:662)
{noformat}

The {{null}} here is a region. In my case I had so many edits in the {{MemStore}} during recovery that I'm over the low barrier although in fact I'm at 0. It happened yesterday and it still printing this out.

To fix this we need to be able to decrease the global {{MemStore}} size when the region can't open.",,1,,,,,,
HBASE-5615,"the master never do balance becauseof when master do rebuildUserRegions()it will add the parent region into  AssignmentManager#servers,
if balancer let the parent region to move,the parent will in RIT forever.thus balance will never be executed.",,,,,,,,
HBASE-5616,"When doing a ycsb test with a large number of handlers (regionserver.handler.count=60), I get the following exceptions:

{code}
Caused by: org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.NullPointerException
	at org.apache.hadoop.io.SequenceFile$Writer.getLength(SequenceFile.java:1099)
	at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.getLength(SequenceFileLogWriter.java:314)
	at org.apache.hadoop.hbase.regionserver.wal.HLog.syncer(HLog.java:1291)
	at org.apache.hadoop.hbase.regionserver.wal.HLog.sync(HLog.java:1388)
	at org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchPut(HRegion.java:2192)
	at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1985)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.java:3400)
	at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:366)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1351)

	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:920)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:152)
	at $Proxy1.multi(Unknown Source)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$3$1.call(HConnectionManager.java:1691)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$3$1.call(HConnectionManager.java:1689)
	at org.apache.hadoop.hbase.client.ServerCallable.withoutRetries(ServerCallable.java:214)
{code}

and 
{code}
	java.lang.NullPointerException
		at org.apache.hadoop.io.SequenceFile$Writer.checkAndWriteSync(SequenceFile.java:1026)
		at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:1068)
		at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:1035)
		at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.append(SequenceFileLogWriter.java:279)
		at org.apache.hadoop.hbase.regionserver.wal.HLog$LogSyncer.hlogFlush(HLog.java:1237)
		at org.apache.hadoop.hbase.regionserver.wal.HLog.syncer(HLog.java:1271)
		at org.apache.hadoop.hbase.regionserver.wal.HLog.sync(HLog.java:1391)
		at org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchPut(HRegion.java:2192)
		at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1985)
		at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.java:3400)
		at sun.reflect.GeneratedMethodAccessor33.invoke(Unknown Source)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
		at java.lang.reflect.Method.invoke(Method.java:597)
		at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:366)
		at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1351)
{code}

It seems the root cause of the issue is that we open a new log writer and close the old one at HLog#rollWriter() holding the updateLock, but the other threads doing syncer() calls
{code} 
logSyncerThread.hlogFlush(this.writer);
{code}
without holding the updateLock. LogSyncer only synchronizes against concurrent appends and flush(), but not on the passed writer, which can be closed already by rollWriter(). In this case, since SequenceFile#Writer.close() sets it's out field as null, we get the NPE. 
",,,,,,,,
HBASE-5617,"When doing a ycsb test with a large number of handlers (regionserver.handler.count=60), I get the following exceptions:

{code}
Caused by: org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.NullPointerException
	at org.apache.hadoop.io.SequenceFile$Writer.getLength(SequenceFile.java:1099)
	at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.getLength(SequenceFileLogWriter.java:314)
	at org.apache.hadoop.hbase.regionserver.wal.HLog.syncer(HLog.java:1291)
	at org.apache.hadoop.hbase.regionserver.wal.HLog.sync(HLog.java:1388)
	at org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchPut(HRegion.java:2192)
	at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1985)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.java:3400)
	at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:366)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1351)

	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:920)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:152)
	at $Proxy1.multi(Unknown Source)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$3$1.call(HConnectionManager.java:1691)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$3$1.call(HConnectionManager.java:1689)
	at org.apache.hadoop.hbase.client.ServerCallable.withoutRetries(ServerCallable.java:214)
{code}

and 
{code}
	java.lang.NullPointerException
		at org.apache.hadoop.io.SequenceFile$Writer.checkAndWriteSync(SequenceFile.java:1026)
		at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:1068)
		at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:1035)
		at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.append(SequenceFileLogWriter.java:279)
		at org.apache.hadoop.hbase.regionserver.wal.HLog$LogSyncer.hlogFlush(HLog.java:1237)
		at org.apache.hadoop.hbase.regionserver.wal.HLog.syncer(HLog.java:1271)
		at org.apache.hadoop.hbase.regionserver.wal.HLog.sync(HLog.java:1391)
		at org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchPut(HRegion.java:2192)
		at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1985)
		at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.java:3400)
		at sun.reflect.GeneratedMethodAccessor33.invoke(Unknown Source)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
		at java.lang.reflect.Method.invoke(Method.java:597)
		at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:366)
		at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1351)
{code}

It seems the root cause of the issue is that we open a new log writer and close the old one at HLog#rollWriter() holding the updateLock, but the other threads doing syncer() calls
{code} 
logSyncerThread.hlogFlush(this.writer);
{code}
without holding the updateLock. LogSyncer only synchronizes against concurrent appends and flush(), but not on the passed writer, which can be closed already by rollWriter(). In this case, since SequenceFile#Writer.close() sets it's out field as null, we get the NPE. 
",,,1,,,,,
HBASE-5623,"When doing a ycsb test with a large number of handlers (regionserver.handler.count=60), I get the following exceptions:

{code}
Caused by: org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.NullPointerException
	at org.apache.hadoop.io.SequenceFile$Writer.getLength(SequenceFile.java:1099)
	at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.getLength(SequenceFileLogWriter.java:314)
	at org.apache.hadoop.hbase.regionserver.wal.HLog.syncer(HLog.java:1291)
	at org.apache.hadoop.hbase.regionserver.wal.HLog.sync(HLog.java:1388)
	at org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchPut(HRegion.java:2192)
	at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1985)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.java:3400)
	at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:366)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1351)

	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:920)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:152)
	at $Proxy1.multi(Unknown Source)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$3$1.call(HConnectionManager.java:1691)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$3$1.call(HConnectionManager.java:1689)
	at org.apache.hadoop.hbase.client.ServerCallable.withoutRetries(ServerCallable.java:214)
{code}

and 
{code}
	java.lang.NullPointerException
		at org.apache.hadoop.io.SequenceFile$Writer.checkAndWriteSync(SequenceFile.java:1026)
		at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:1068)
		at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:1035)
		at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.append(SequenceFileLogWriter.java:279)
		at org.apache.hadoop.hbase.regionserver.wal.HLog$LogSyncer.hlogFlush(HLog.java:1237)
		at org.apache.hadoop.hbase.regionserver.wal.HLog.syncer(HLog.java:1271)
		at org.apache.hadoop.hbase.regionserver.wal.HLog.sync(HLog.java:1391)
		at org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchPut(HRegion.java:2192)
		at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1985)
		at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.java:3400)
		at sun.reflect.GeneratedMethodAccessor33.invoke(Unknown Source)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
		at java.lang.reflect.Method.invoke(Method.java:597)
		at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:366)
		at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1351)
{code}

It seems the root cause of the issue is that we open a new log writer and close the old one at HLog#rollWriter() holding the updateLock, but the other threads doing syncer() calls
{code} 
logSyncerThread.hlogFlush(this.writer);
{code}
without holding the updateLock. LogSyncer only synchronizes against concurrent appends and flush(), but not on the passed writer, which can be closed already by rollWriter(). In this case, since SequenceFile#Writer.close() sets it's out field as null, we get the NPE. 
",,,,,,,,
HBASE-5625,"When doing a ycsb test with a large number of handlers (regionserver.handler.count=60), I get the following exceptions:

{code}
Caused by: org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.NullPointerException
	at org.apache.hadoop.io.SequenceFile$Writer.getLength(SequenceFile.java:1099)
	at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.getLength(SequenceFileLogWriter.java:314)
	at org.apache.hadoop.hbase.regionserver.wal.HLog.syncer(HLog.java:1291)
	at org.apache.hadoop.hbase.regionserver.wal.HLog.sync(HLog.java:1388)
	at org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchPut(HRegion.java:2192)
	at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1985)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.java:3400)
	at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:366)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1351)

	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:920)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:152)
	at $Proxy1.multi(Unknown Source)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$3$1.call(HConnectionManager.java:1691)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$3$1.call(HConnectionManager.java:1689)
	at org.apache.hadoop.hbase.client.ServerCallable.withoutRetries(ServerCallable.java:214)
{code}

and 
{code}
	java.lang.NullPointerException
		at org.apache.hadoop.io.SequenceFile$Writer.checkAndWriteSync(SequenceFile.java:1026)
		at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:1068)
		at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:1035)
		at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.append(SequenceFileLogWriter.java:279)
		at org.apache.hadoop.hbase.regionserver.wal.HLog$LogSyncer.hlogFlush(HLog.java:1237)
		at org.apache.hadoop.hbase.regionserver.wal.HLog.syncer(HLog.java:1271)
		at org.apache.hadoop.hbase.regionserver.wal.HLog.sync(HLog.java:1391)
		at org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchPut(HRegion.java:2192)
		at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1985)
		at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.java:3400)
		at sun.reflect.GeneratedMethodAccessor33.invoke(Unknown Source)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
		at java.lang.reflect.Method.invoke(Method.java:597)
		at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:366)
		at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1351)
{code}

It seems the root cause of the issue is that we open a new log writer and close the old one at HLog#rollWriter() holding the updateLock, but the other threads doing syncer() calls
{code} 
logSyncerThread.hlogFlush(this.writer);
{code}
without holding the updateLock. LogSyncer only synchronizes against concurrent appends and flush(), but not on the passed writer, which can be closed already by rollWriter(). In this case, since SequenceFile#Writer.close() sets it's out field as null, we get the NPE. 
",,,1,,,,,
HBASE-5633,"If zoo.cfg contains server.* (""server.0=server0:2888:3888\n"") and cluster.distributed property (in hbase-site.xml) is empty we get an NPE in parseZooCfg().

The easy way to reproduce the bug is running org.apache.hbase.zookeeper.TestHQuorumPeer with hbase-site.xml containing:
{code}
<property>
  <name>hbase.cluster.distributed</name>
  <value></value>
</property>
{code}",,1,,,,,,
HBASE-5635,"During the hlog split operation if all the zookeepers are down ,then the paths will be returned as null and the splitworker thread wil be exited
Now this regionserver wil not be able to acquire any other tasks since the splitworker thread is exited
Please find the attached code for more details
{code}
private List<String> getTaskList() {
    for (int i = 0; i < zkretries; i++) {
      try {
        return (ZKUtil.listChildrenAndWatchForNewChildren(this.watcher,
            this.watcher.splitLogZNode));
      } catch (KeeperException e) {
        LOG.warn(""Could not get children of znode "" +
            this.watcher.splitLogZNode, e);
        try {
          Thread.sleep(1000);
        } catch (InterruptedException e1) {
          LOG.warn(""Interrupted while trying to get task list ..."", e1);
          Thread.currentThread().interrupt();
          return null;
        }
      }
    }
{code}

in the org.apache.hadoop.hbase.regionserver.SplitLogWorker 


 

",,,,,,,,
HBASE-5656,"LoadIncrementalHFiles doesn't set compression when creating the the table.

This can be detected from the files within each family dir. ",,,1,,,,,
HBASE-5665,"Repeated splits on large tables (2 consecutive would suffice) will essentially ""break"" the table (and the cluster), unrecoverable.
The regionserver doing the split dies and the master will get into an infinite loop trying to assign regions that seem to have the files missing from HDFS.

The table can be disabled once. upon trying to re-enable it, it will remain in an intermediary state forever.

I was able to reproduce this on a smaller table consistently.

{code}
hbase(main):030:0> (0..10000).each{|x| put 't1', ""#{x}"", 'f1:t', 'dd'}
hbase(main):030:0> (0..1000).each{|x| split 't1', ""#{x*10}""}
{code}

Running overlapping splits in parallel (e.g. ""#{x*10+1}"", ""#{x*10+2}""... ) will reproduce the issue almost instantly and consistently. 

{code}
2012-03-28 10:57:16,320 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Offlined parent region t1,,1332957435767.2fb0473f4e71339e88dab0ee0d4dffa1. in META
2012-03-28 10:57:16,321 DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitThread: Split requested for t1,5,1332957435767.648d30de55a5cec6fc2f56dcb3c7eee1..  compaction_queue=(0:1), split_queue=10
2012-03-28 10:57:16,343 INFO org.apache.hadoop.hbase.regionserver.SplitRequest: Running rollback/cleanup of failed split of t1,,1332957435767.2fb0473f4e71339e88dab0ee0d4dffa1.; Failed ld2,60020,1332957343833-daughterOpener=2469c5650ea2aeed631eb85d3cdc3124
java.io.IOException: Failed ld2,60020,1332957343833-daughterOpener=2469c5650ea2aeed631eb85d3cdc3124
        at org.apache.hadoop.hbase.regionserver.SplitTransaction.openDaughters(SplitTransaction.java:363)
        at org.apache.hadoop.hbase.regionserver.SplitTransaction.execute(SplitTransaction.java:451)
        at org.apache.hadoop.hbase.regionserver.SplitRequest.run(SplitRequest.java:67)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.FileNotFoundException: File does not exist: /hbase/t1/589c44cabba419c6ad8c9b427e5894e3.2fb0473f4e71339e88dab0ee0d4dffa1/f1/d62a852c25ad44e09518e102ca557237
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:1822)
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.<init>(DFSClient.java:1813)
        at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:544)
        at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:187)
        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:456)
        at org.apache.hadoop.hbase.io.hfile.HFile.createReader(HFile.java:341)
        at org.apache.hadoop.hbase.regionserver.StoreFile$Reader.<init>(StoreFile.java:1008)
        at org.apache.hadoop.hbase.io.HalfStoreFileReader.<init>(HalfStoreFileReader.java:65)
        at org.apache.hadoop.hbase.regionserver.StoreFile.open(StoreFile.java:467)
        at org.apache.hadoop.hbase.regionserver.StoreFile.createReader(StoreFile.java:548)
        at org.apache.hadoop.hbase.regionserver.Store.loadStoreFiles(Store.java:284)
        at org.apache.hadoop.hbase.regionserver.Store.<init>(Store.java:221)
        at org.apache.hadoop.hbase.regionserver.HRegion.instantiateHStore(HRegion.java:2511)
        at org.apache.hadoop.hbase.regionserver.HRegion.initialize(HRegion.java:450)
        at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:3229)
        at org.apache.hadoop.hbase.regionserver.SplitTransaction.openDaughterRegion(SplitTransaction.java:504)
        at org.apache.hadoop.hbase.regionserver.SplitTransaction$DaughterOpener.run(SplitTransaction.java:484)
        ... 1 more
2012-03-28 10:57:16,345 FATAL org.apache.hadoop.hbase.regionserver.HRegionServer: ABORTING region server ld2,60020,1332957343833: Abort; we got an error after point-of-no-return
{code}

http://hastebin.com/diqinibajo.avrasm

later edit:

(I'm using the last 4 characters from each string)
Region 94e3 has storefile 7237
Region 94e3 gets splited in daughters a: ffa1 and b: eee1
Daughter region ffa1 get's splitted in daughters a: 3124 and b: dc77
ffa1 has a reference: 7237.94e3 for it's store file
when ffa1 gets splited it will create another reference: 7237.94e3.ffa1
when SplitTransaction will execute() it will try to open that (openDaughters above) and it will match it from left to right [storefile].[region] 
{code}
""^([0-9a-f]+)(?:\\.(.+))?$""
{code}
and will attempt to go to /hbase/t1/[region] which resolves to 
/hbase/t1/94e3.ffa1/f1/7237 - which obviously doesn't exist and will fail. 

This seems like a design problem: we should either stop from splitting if the path is reference or be able to recursively resolve reference paths (e.g. parse right to left 7237.94e3.ffa1 -> [7237.94e3].ffa1 -> open /hbase/t1/ffa1/f1/7237.94e3 -> [7237].94e3 -> open /hbase/t1/94e3/7237)

",,,,,,,,
HBASE-5684,"Let's see the following scenario:
1.Region is on the server A
2.put KV(r1->v1) to the region
3.move region from server A to server B
4.put KV(r2->v2) to the region
5.move region from server B to server A
6.put KV(r3->v3) to the region
7.kill -9 server B and start it
8.kill -9 server A and start it 
9.scan the region, we could only get two KV(r1->v1,r2->v2), the third KV(r3->v3) is lost.

Let's analyse the upper scenario from the code:
1.the edit logs of KV(r1->v1) and KV(r3->v3) are both recorded in the same hlog file on server A.
2.when we split server B's hlog file in the process of ServerShutdownHandler, we create one RecoveredEdits file f1 for the region.
2.when we split server A's hlog file in the process of ServerShutdownHandler, we create another RecoveredEdits file f2 for the region.
3.however, RecoveredEdits file f2 will be skiped when initializing region
HRegion#replayRecoveredEditsIfAny
{code}
 for (Path edits: files) {
      if (edits == null || !this.fs.exists(edits)) {
        LOG.warn(""Null or non-existent edits file: "" + edits);
        continue;
      }
      if (isZeroLengthThenDelete(this.fs, edits)) continue;

      if (checkSafeToSkip) {
        Path higher = files.higher(edits);
        long maxSeqId = Long.MAX_VALUE;
        if (higher != null) {
          // Edit file name pattern, HLog.EDITFILES_NAME_PATTERN: ""-?[0-9]+""
          String fileName = higher.getName();
          maxSeqId = Math.abs(Long.parseLong(fileName));
        }
        if (maxSeqId <= minSeqId) {
          String msg = ""Maximum possible sequenceid for this log is "" + maxSeqId
              + "", skipped the whole file, path="" + edits;
          LOG.debug(msg);
          continue;
        } else {
          checkSafeToSkip = false;
        }
      }
{code}

 
",,,,,,,,
HBASE-5689,"Let's see the following scenario:
1.Region is on the server A
2.put KV(r1->v1) to the region
3.move region from server A to server B
4.put KV(r2->v2) to the region
5.move region from server B to server A
6.put KV(r3->v3) to the region
7.kill -9 server B and start it
8.kill -9 server A and start it 
9.scan the region, we could only get two KV(r1->v1,r2->v2), the third KV(r3->v3) is lost.

Let's analyse the upper scenario from the code:
1.the edit logs of KV(r1->v1) and KV(r3->v3) are both recorded in the same hlog file on server A.
2.when we split server B's hlog file in the process of ServerShutdownHandler, we create one RecoveredEdits file f1 for the region.
2.when we split server A's hlog file in the process of ServerShutdownHandler, we create another RecoveredEdits file f2 for the region.
3.however, RecoveredEdits file f2 will be skiped when initializing region
HRegion#replayRecoveredEditsIfAny
{code}
 for (Path edits: files) {
      if (edits == null || !this.fs.exists(edits)) {
        LOG.warn(""Null or non-existent edits file: "" + edits);
        continue;
      }
      if (isZeroLengthThenDelete(this.fs, edits)) continue;

      if (checkSafeToSkip) {
        Path higher = files.higher(edits);
        long maxSeqId = Long.MAX_VALUE;
        if (higher != null) {
          // Edit file name pattern, HLog.EDITFILES_NAME_PATTERN: ""-?[0-9]+""
          String fileName = higher.getName();
          maxSeqId = Math.abs(Long.parseLong(fileName));
        }
        if (maxSeqId <= minSeqId) {
          String msg = ""Maximum possible sequenceid for this log is "" + maxSeqId
              + "", skipped the whole file, path="" + edits;
          LOG.debug(msg);
          continue;
        } else {
          checkSafeToSkip = false;
        }
      }
{code}

 
",,,,,,,,
HBASE-5694,"The getRowsWithColumnsTs() method in the Thrift interface only applies the timestamp if columns are explicitly specified. However, this method also allows for columns to be unspecified (this is even used internally to implement e.g. getRows()). The cause of the bug is a minor scoping issue: the time range is set inside a wrong if statement.",,,,,,,,
HBASE-5699,"The getRowsWithColumnsTs() method in the Thrift interface only applies the timestamp if columns are explicitly specified. However, this method also allows for columns to be unspecified (this is even used internally to implement e.g. getRows()). The cause of the bug is a minor scoping issue: the time range is set inside a wrong if statement.",,,1,,,,,
HBASE-5712,"The getRowsWithColumnsTs() method in the Thrift interface only applies the timestamp if columns are explicitly specified. However, this method also allows for columns to be unspecified (this is even used internally to implement e.g. getRows()). The cause of the bug is a minor scoping issue: the time range is set inside a wrong if statement.",,,1,,,,,
HBASE-5719,"{code}
List<String> nodes =
      ZKUtil.listChildrenAndWatchForNewChildren(zkw, baseNode);
    List<NodeAndData> newNodes = new ArrayList<NodeAndData>();
    for (String node: nodes) {
      String nodePath = ZKUtil.joinZNode(baseNode, node);
      byte [] data = ZKUtil.getDataAndWatch(zkw, nodePath);
      newNodes.add(new NodeAndData(nodePath, data));
    }
{code}

The above code can throw NPE when listChildrenAndWatchForNewChildren returns null.",,,,,,,,
HBASE-5722,"{code}
List<String> nodes =
      ZKUtil.listChildrenAndWatchForNewChildren(zkw, baseNode);
    List<NodeAndData> newNodes = new ArrayList<NodeAndData>();
    for (String node: nodes) {
      String nodePath = ZKUtil.joinZNode(baseNode, node);
      byte [] data = ZKUtil.getDataAndWatch(zkw, nodePath);
      newNodes.add(new NodeAndData(nodePath, data));
    }
{code}

The above code can throw NPE when listChildrenAndWatchForNewChildren returns null.",,1,,,,,,
HBASE-5733,"Found while going through the code...
AssignmentManager#processDeadServersAndRegionsInTransition can fail with NPE as this is directly iterating the nodes from listChildrenAndWatchForNewChildren with-out checking for null.

Here also we need to handle with  null  check like other places.",,,,,,,,
HBASE-5757,"Prior to HBASE-4196 there was different handling of IOExceptions thrown from scanner in mapred and mapreduce API. The patch to HBASE-4196 unified this handling so that if exception is caught a reconnect is attempted (without bothering the mapred client). After that, HBASE-4269 changed this behavior back, but in both mapred and mapreduce APIs. The question is, is there any reason not to handle all errors that the input format can handle? In other words, why not try to reissue the request after *any* IOException? I see the following disadvantages of current approach
 * the client may see exceptions like LeaseException and ScannerTimeoutException if he fails to process all fetched data in timeout
 * to avoid ScannerTimeoutException the client must raise hbase.regionserver.lease.period
 * timeouts for tasks is aready configured in mapred.task.timeout, so this seems to me a bit redundant, because typically one needs to update both these parameters
 * I don't see any possibility to get rid of LeaseException (this is configured on server side)

I think all of these issues would be gone, if the DoNotRetryIOException would not be rethrown. -On the other hand, handling errors in InputFormat has disadvantage, that it may hide from the user some inefficiency. Eg. if I have very big scanner.caching, and I manage to process only a few rows in timeout, I will end up with single row being fetched many times (and will not be explicitly notified about this). Could we solve this problem by adding some counter to the InputFormat?-",,,,,,,,
HBASE-5776,"The getRowsWithColumnsTs() method in the Thrift interface only applies the timestamp if columns are explicitly specified. However, this method also allows for columns to be unspecified (this is even used internally to implement e.g. getRows()). The cause of the bug is a minor scoping issue: the time range is set inside a wrong if statement.",,,1,,,,,
HBASE-5778,"The getRowsWithColumnsTs() method in the Thrift interface only applies the timestamp if columns are explicitly specified. However, this method also allows for columns to be unspecified (this is even used internally to implement e.g. getRows()). The cause of the bug is a minor scoping issue: the time range is set inside a wrong if statement.",,,1,,,,,
HBASE-5780,"Secure RegionServers sometimes fail to start with the following backtrace:

2012-03-22 17:20:16,737 FATAL org.apache.hadoop.hbase.regionserver.HRegionServer: ABORTING region server centos60-20.ent.cloudera.com,60020,1332462015929: Unexpected exception during initialization, aborting
org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /hbase/shutdown
at org.apache.zookeeper.KeeperException.create(KeeperException.java:113)
at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1131)
at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.getData(RecoverableZooKeeper.java:295)
at org.apache.hadoop.hbase.zookeeper.ZKUtil.getDataInternal(ZKUtil.java:518)
at org.apache.hadoop.hbase.zookeeper.ZKUtil.getDataAndWatch(ZKUtil.java:494)
at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.start(ZooKeeperNodeTracker.java:77)
at org.apache.hadoop.hbase.regionserver.HRegionServer.initializeZooKeeper(HRegionServer.java:569)
at org.apache.hadoop.hbase.regionserver.HRegionServer.preRegistrationInitialization(HRegionServer.java:532)
at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:634)
at java.lang.Thread.run(Thread.java:662)",,,,,,,,
HBASE-5782,"Secure RegionServers sometimes fail to start with the following backtrace:

2012-03-22 17:20:16,737 FATAL org.apache.hadoop.hbase.regionserver.HRegionServer: ABORTING region server centos60-20.ent.cloudera.com,60020,1332462015929: Unexpected exception during initialization, aborting
org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /hbase/shutdown
at org.apache.zookeeper.KeeperException.create(KeeperException.java:113)
at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1131)
at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.getData(RecoverableZooKeeper.java:295)
at org.apache.hadoop.hbase.zookeeper.ZKUtil.getDataInternal(ZKUtil.java:518)
at org.apache.hadoop.hbase.zookeeper.ZKUtil.getDataAndWatch(ZKUtil.java:494)
at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.start(ZooKeeperNodeTracker.java:77)
at org.apache.hadoop.hbase.regionserver.HRegionServer.initializeZooKeeper(HRegionServer.java:569)
at org.apache.hadoop.hbase.regionserver.HRegionServer.preRegistrationInitialization(HRegionServer.java:532)
at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:634)
at java.lang.Thread.run(Thread.java:662)",,,,,,,,
HBASE-5806,"This issue is raised to solve issues that comes out of partial region split happened and the region node in the ZK which is in RS_ZK_REGION_SPLITTING and RS_ZK_REGION_SPLIT is not yet processed.
This also tries to address HBASE-5615.",,,,,,,,
HBASE-5809,"This issue is raised to solve issues that comes out of partial region split happened and the region node in the ZK which is in RS_ZK_REGION_SPLITTING and RS_ZK_REGION_SPLIT is not yet processed.
This also tries to address HBASE-5615.",,,,,,,,
HBASE-5816,"The first assign thread exits with success after updating the RegionState to PENDING_OPEN, while the second assign follows immediately into ""assign"" and fails the RegionState check in setOfflineInZooKeeper(). This causes the master to abort.

In the below case, the two concurrent assigns occurred when AM tried to assign a region to a dying/dead RS, and meanwhile the ShutdownServerHandler tried to assign this region (from the region plan) spontaneously.
{code}
2012-04-17 05:44:57,648 INFO org.apache.hadoop.hbase.master.HMaster: balance hri=TABLE_ORDER_CUSTOMER,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b., src=hadoop05.sh.intel.com,60020,1334544902186, dest=xmlqa-clv16.sh.intel.com,60020,1334612497253
2012-04-17 05:44:57,648 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Starting unassignment of region TABLE_ORDER_CUSTOMER,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b. (offlining)
2012-04-17 05:44:57,648 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Sent CLOSE to serverName=hadoop05.sh.intel.com,60020,1334544902186, load=(requests=0, regions=0, usedHeap=0, maxHeap=0) for region TABLE_ORDER_CUSTOMER,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b.
2012-04-17 05:44:57,666 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling new unassigned node: /hbase/unassigned/fe38fe31caf40b6e607a3e6bbed6404b (region=TABLE_ORDER_CUSTOMER,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b., server=hadoop05.sh.intel.com,60020,1334544902186, state=RS_ZK_REGION_CLOSING)
2012-04-17 05:52:58,984 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=TABLE_ORDER_CUSTOMER,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b. state=CLOSED, ts=1334612697672, server=hadoop05.sh.intel.com,60020,1334544902186
2012-04-17 05:52:58,984 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x236b912e9b3000e Creating (or updating) unassigned node for fe38fe31caf40b6e607a3e6bbed6404b with OFFLINE state
2012-04-17 05:52:59,096 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Using pre-existing plan for region TABLE_ORDER_CUSTOMER,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b.; plan=hri=TABLE_ORDER_CUSTOMER,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b., src=hadoop05.sh.intel.com,60020,1334544902186, dest=xmlqa-clv16.sh.intel.com,60020,1334612497253
2012-04-17 05:52:59,096 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region TABLE_ORDER_CUSTOMER,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b. to xmlqa-clv16.sh.intel.com,60020,1334612497253
2012-04-17 05:54:19,159 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=TABLE_ORDER_CUSTOMER,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b. state=PENDING_OPEN, ts=1334613179096, server=xmlqa-clv16.sh.intel.com,60020,1334612497253
2012-04-17 05:54:59,033 WARN org.apache.hadoop.hbase.master.AssignmentManager: Failed assignment of TABLE_ORDER_CUSTOMER,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b. to serverName=xmlqa-clv16.sh.intel.com,60020,1334612497253, load=(requests=0, regions=0, usedHeap=0, maxHeap=0), trying to assign elsewhere instead; retry=0
java.net.SocketTimeoutException: Call to /10.239.47.87:60020 failed on socket timeout exception: java.net.SocketTimeoutException: 120000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.239.47.89:41302 remote=/10.239.47.87:60020]
        at org.apache.hadoop.hbase.ipc.HBaseClient.wrapException(HBaseClient.java:805)
        at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:778)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:283)
        at $Proxy7.openRegion(Unknown Source)
        at org.apache.hadoop.hbase.master.ServerManager.sendRegionOpen(ServerManager.java:573)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1127)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:912)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:892)
        at org.apache.hadoop.hbase.master.handler.ClosedRegionHandler.process(ClosedRegionHandler.java:92)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.net.SocketTimeoutException: 120000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.239.47.89:41302 remote=/10.239.47.87:60020]
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)
        at java.io.FilterInputStream.read(FilterInputStream.java:116)
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection$PingInputStream.read(HBaseClient.java:301)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:237)
        at java.io.DataInputStream.readInt(DataInputStream.java:370)
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.receiveResponse(HBaseClient.java:541)
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.run(HBaseClient.java:479)
2012-04-17 05:54:59,035 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: No previous transition plan was found (or we are ignoring an existing plan) for TABLE_ORDER_CUSTOMER,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b. so generated a random one; hri=TABLE_ORDER_CUSTOMER,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b., src=, dest=hadoop06.sh.intel.com,60020,1334544901894; 7 (online=7, exclude=serverName=xmlqa-clv16.sh.intel.com,60020,1334612497253, load=(requests=0, regions=0, usedHeap=0, maxHeap=0)) available servers
2012-04-17 05:54:59,035 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x236b912e9b3000e Creating (or updating) unassigned node for fe38fe31caf40b6e607a3e6bbed6404b with OFFLINE state
2012-04-17 05:54:59,045 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Using pre-existing plan for region TABLE_ORDER_CUSTOMER,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b.; plan=hri=TABLE_ORDER_CUSTOMER,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b., src=, dest=hadoop06.sh.intel.com,60020,1334544901894
2012-04-17 05:54:59,045 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region TABLE_ORDER_CUSTOMER,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b. to hadoop06.sh.intel.com,60020,1334544901894
2012-04-17 05:54:59,046 FATAL org.apache.hadoop.hbase.master.HMaster: Unexpected state trying to OFFLINE; TABLE_ORDER_CUSTOMER,,1334017820846.fe38fe31caf40b6e607a3e6bbed6404b. state=PENDING_OPEN, ts=1334613299045, server=hadoop06.sh.intel.com,60020,1334544901894
java.lang.IllegalStateException
        at org.apache.hadoop.hbase.master.AssignmentManager.setOfflineInZooKeeper(AssignmentManager.java:1167)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1107)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:912)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:892)
        at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:259)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
2012-04-17 05:54:59,047 INFO org.apache.hadoop.hbase.master.HMaster: Aborting
{code}",,,,,,,,
HBASE-5829,"There are occurrences in AM where this.servers is not kept consistent with this.regions. This might cause balancer to offline a region from the RS that already returned NotServingRegionException at a previous offline attempt.

In AssignmentManager.unassign(HRegionInfo, boolean)
    try {
      // TODO: We should consider making this look more like it does for the
      // region open where we catch all throwables and never abort
      if (serverManager.sendRegionClose(server, state.getRegion(),
        versionOfClosingNode)) {
        LOG.debug(""Sent CLOSE to "" + server + "" for region "" +
          region.getRegionNameAsString());
        return;
      }
      // This never happens. Currently regionserver close always return true.
      LOG.warn(""Server "" + server + "" region CLOSE RPC returned false for "" +
        region.getRegionNameAsString());
    } catch (NotServingRegionException nsre) {
      LOG.info(""Server "" + server + "" returned "" + nsre + "" for "" +
        region.getRegionNameAsString());
      // Presume that master has stale data.  Presume remote side just split.
      // Presume that the split message when it comes in will fix up the master's
      // in memory cluster state.
    } catch (Throwable t) {
      if (t instanceof RemoteException) {
        t = ((RemoteException)t).unwrapRemoteException();
        if (t instanceof NotServingRegionException) {
          if (checkIfRegionBelongsToDisabling(region)) {
            // Remove from the regionsinTransition map
            LOG.info(""While trying to recover the table ""
                + region.getTableNameAsString()
                + "" to DISABLED state the region "" + region
                + "" was offlined but the table was in DISABLING state"");
            synchronized (this.regionsInTransition) {
              this.regionsInTransition.remove(region.getEncodedName());
            }
            // Remove from the regionsMap
            synchronized (this.regions) {
              this.regions.remove(region);
            }
            deleteClosingOrClosedNode(region);
          }
        }
        // RS is already processing this region, only need to update the timestamp
        if (t instanceof RegionAlreadyInTransitionException) {
          LOG.debug(""update "" + state + "" the timestamp."");
          state.update(state.getState());
        }
      }

In AssignmentManager.assign(HRegionInfo, RegionState, boolean, boolean, boolean)
          synchronized (this.regions) {
            this.regions.put(plan.getRegionInfo(), plan.getDestination());
          }
",,,,,,,,
HBASE-5835,"Currently, if hbck attempts to close a region and catches a NotServerRegionException, hbck may hang outputting a stack trace.  Since the goal is to close the region at a particular server, and since it is not serving the region, the region is closed, and we should just warn and eat this exception.

{code}
Exception in thread ""main"" org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hbase.NotServingRegionException: Received close for <regionid> but we are not serving it
at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2162)
at sun.reflect.GeneratedMethodAccessor36.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:570)
at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1039)

at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:771)
at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:257)
at $Proxy5.closeRegion(Unknown Source)
at org.apache.hadoop.hbase.util.HBaseFsckRepair.closeRegionSilentlyAndWait(HBaseFsckRepair.java:165)
at org.apache.hadoop.hbase.util.HBaseFsck.closeRegion(HBaseFsck.java:1185)
at org.apache.hadoop.hbase.util.HBaseFsck.checkRegionConsistency(HBaseFsck.java:1302)
at org.apache.hadoop.hbase.util.HBaseFsck.checkAndFixConsistency(HBaseFsck.java:1065)
at org.apache.hadoop.hbase.util.HBaseFsck.onlineConsistencyRepair(HBaseFsck.java:351)
at org.apache.hadoop.hbase.util.HBaseFsck.onlineHbck(HBaseFsck.java:370)
at org.apache.hadoop.hbase.util.HBaseFsck.main(HBaseFsck.java:3001)
{code}",,,,,,,,
HBASE-5837,"When using the hbase shell to manipulate meta entries, one is allowed to 'delete' malformed rows (entries with less than 2 ascii 44 ',' chars).  When this happens HBase servers may go down and the cluster will not be restartable without manual intervention.  

The delete results in a durable malformed rowkey in .META.'s memstore, .META.'s HLog, and eventually .META.'s HFiles.  Subsequent scans to meta (such as when a HMaster starts) fail in the scanner because the comparator fails.  In the case of an HMaster startup, it causes an abort that kills the HMaster process.


{code}
12/04/18 22:07:34 FATAL master.HMaster: Unhandled exception. Starting shutdown.
org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.IllegalArgumentException: No 44 in <blah,1334744821162.81f2df35c332dd2d3bb966fb5b419568.>, length=47, offset=54
at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:990)
at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:979)
at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1894)
at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1834)
at sun.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:570)
at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1039)
Caused by: java.lang.IllegalArgumentException: No 44 in <blah,1334744821162.81f2df35c332dd2d3bb966fb5b419568.>, length=47, offset=54
at org.apache.hadoop.hbase.KeyValue.getRequiredDelimiterInReverse(KeyValue.java:1300)
at org.apache.hadoop.hbase.KeyValue$MetaKeyComparator.compareRows(KeyValue.java:1846)
at org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.match(ScanQueryMatcher.java:130)
at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:257)
at org.apache.hadoop.hbase.regionserver.KeyValueHeap.next(KeyValueHeap.java:114)
at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.nextInternal(HRegion.java:2435)
at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.next(HRegion.java:2391)
at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.next(HRegion.java:2408)
at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1870)
... 6 more

at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:771)
at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:257)
at $Proxy9.next(Unknown Source)
at org.apache.hadoop.hbase.catalog.MetaReader.fullScan(MetaReader.java:264)
at org.apache.hadoop.hbase.catalog.MetaReader.fullScan(MetaReader.java:237)
at org.apache.hadoop.hbase.catalog.MetaReader.fullScanOfResults(MetaReader.java:220)
at org.apache.hadoop.hbase.master.AssignmentManager.rebuildUserRegions(AssignmentManager.java:1580)
at org.apache.hadoop.hbase.master.AssignmentManager.processFailover(AssignmentManager.java:221)
at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:422)
at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:295)
12/04/18 22:07:34 INFO master.HMaster: Aborting 
{code}",,1,,,,,,
HBASE-5848,"A coworker of mine just had this scenario. It does not make sense the EMPTY_START_ROW as splitKey (since the region with the empty start key is implicit), but it should not cause the HMaster to abort.
The abort happens because it tries to bulk assign the same region twice and then runs into race conditions with ZK.

The same would (presumably) happen when two identical split keys are passed, but the client blocks that. The simplest solution here is to also block passed null or EMPTY_START_ROW as split key by the client.",,1,,,,,,
HBASE-5849,"When launching a fresh new cluster, the master has to be started first, which might create race conditions for starting master and rs at the same time. 

Master startup code is smt like this: 
 - establish zk connection
 - create root znodes in zk (/hbase)
 - create ephemeral node for master /hbase/master, 

 Region server start up code is smt like this: 
 - establish zk connection
 - check whether the root znode (/hbase) is there. If not, shutdown. 
 - wait for the master to create znodes /hbase/master

So, the problem is on the very first launch of the cluster, RS aborts to start since /hbase znode might not have been created yet (only the master creates it if needed). Since /hbase/ is not deleted on cluster shutdown, on subsequent cluster starts, it does not matter which order the servers are started. So this affects only first launchs. ",,1,,,,,,
HBASE-5875,"If on master restart it finds the ROOT/META to be in RIT state, master tries to assign the ROOT region through ProcessRIT.

Master will trigger the assignment and next will try to verify the Root Region Location.
Root region location verification is done seeing if the RS has the region in its online list.
If the master triggered assignment has not yet been completed in RS then the verify root region location will fail.
Because it failed 
{code}
splitLogAndExpireIfOnline(currentRootServer);
{code}
we do split log and also remove the server from online server list. Ideally here there is nothing to do in splitlog as no region server was restarted.

So master, though the server is online, master just invalidates the region server.
In a special case, if i have only one RS then my cluster will become non operative.
",,,,,,,,
HBASE-5877,"When using the hbase shell to manipulate meta entries, one is allowed to 'delete' malformed rows (entries with less than 2 ascii 44 ',' chars).  When this happens HBase servers may go down and the cluster will not be restartable without manual intervention.  

The delete results in a durable malformed rowkey in .META.'s memstore, .META.'s HLog, and eventually .META.'s HFiles.  Subsequent scans to meta (such as when a HMaster starts) fail in the scanner because the comparator fails.  In the case of an HMaster startup, it causes an abort that kills the HMaster process.


{code}
12/04/18 22:07:34 FATAL master.HMaster: Unhandled exception. Starting shutdown.
org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.IllegalArgumentException: No 44 in <blah,1334744821162.81f2df35c332dd2d3bb966fb5b419568.>, length=47, offset=54
at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:990)
at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:979)
at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1894)
at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1834)
at sun.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:570)
at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1039)
Caused by: java.lang.IllegalArgumentException: No 44 in <blah,1334744821162.81f2df35c332dd2d3bb966fb5b419568.>, length=47, offset=54
at org.apache.hadoop.hbase.KeyValue.getRequiredDelimiterInReverse(KeyValue.java:1300)
at org.apache.hadoop.hbase.KeyValue$MetaKeyComparator.compareRows(KeyValue.java:1846)
at org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.match(ScanQueryMatcher.java:130)
at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:257)
at org.apache.hadoop.hbase.regionserver.KeyValueHeap.next(KeyValueHeap.java:114)
at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.nextInternal(HRegion.java:2435)
at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.next(HRegion.java:2391)
at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.next(HRegion.java:2408)
at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1870)
... 6 more

at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:771)
at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:257)
at $Proxy9.next(Unknown Source)
at org.apache.hadoop.hbase.catalog.MetaReader.fullScan(MetaReader.java:264)
at org.apache.hadoop.hbase.catalog.MetaReader.fullScan(MetaReader.java:237)
at org.apache.hadoop.hbase.catalog.MetaReader.fullScanOfResults(MetaReader.java:220)
at org.apache.hadoop.hbase.master.AssignmentManager.rebuildUserRegions(AssignmentManager.java:1580)
at org.apache.hadoop.hbase.master.AssignmentManager.processFailover(AssignmentManager.java:221)
at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:422)
at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:295)
12/04/18 22:07:34 INFO master.HMaster: Aborting 
{code}",,1,,,,,,
HBASE-5883,"The active master node network was down for some time (This node contains Master,DN,ZK,RS). Here backup node got 
notification, and started to became active. Immedietly backup node got aborted with the below exception.

{noformat}
2012-04-09 10:42:24,270 INFO org.apache.hadoop.hbase.master.SplitLogManager: finished splitting (more than or equal to) 861248320 bytes in 4 log files in [hdfs://192.168.47.205:9000/hbase/.logs/HOST-192-168-47-202,60020,1333715537172-splitting] in 26374ms
2012-04-09 10:42:24,316 FATAL org.apache.hadoop.hbase.master.HMaster: Master server abort: loaded coprocessors are: []
2012-04-09 10:42:24,333 FATAL org.apache.hadoop.hbase.master.HMaster: Unhandled exception. Starting shutdown.
java.io.IOException: java.net.ConnectException: Connection refused
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupIOstreams(HBaseClient.java:375)
	at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:1045)
	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:897)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:150)
	at $Proxy13.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine.getProxy(WritableRpcEngine.java:183)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:303)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:280)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:332)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.waitForProxy(HBaseRPC.java:236)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:1276)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:1233)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:1220)
	at org.apache.hadoop.hbase.catalog.CatalogTracker.getCachedConnection(CatalogTracker.java:569)
	at org.apache.hadoop.hbase.catalog.CatalogTracker.getRootServerConnection(CatalogTracker.java:369)
	at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForRootServerConnection(CatalogTracker.java:353)
	at org.apache.hadoop.hbase.catalog.CatalogTracker.verifyRootRegionLocation(CatalogTracker.java:660)
	at org.apache.hadoop.hbase.master.HMaster.assignRootAndMeta(HMaster.java:616)
	at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:540)
	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:363)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:488)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupConnection(HBaseClient.java:328)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupIOstreams(HBaseClient.java:362)
	... 20 more
2012-04-09 10:42:24,336 INFO org.apache.hadoop.hbase.master.HMaster: Aborting
2012-04-09 10:42:24,336 DEBUG org.apache.hadoop.hbase.master.HMaster: Stopping service threads
{noformat}",,,,,,,,
HBASE-5889,"I was running an insert workload against trunk under oprofile and saw that a significant portion of CPU usage was going to calling the ""prePut"" coprocessor hook inside doMiniBatchPut, even though I don't have any coprocessors installed. I ran a million-row insert and collected CPU time spent in the RS after commenting out the preput hook, and found CPU usage reduced by 33%.",,,1,,,,,
HBASE-5897,"I was running an insert workload against trunk under oprofile and saw that a significant portion of CPU usage was going to calling the ""prePut"" coprocessor hook inside doMiniBatchPut, even though I don't have any coprocessors installed. I ran a million-row insert and collected CPU time spent in the RS after commenting out the preput hook, and found CPU usage reduced by 33%.",,,1,,,,,
HBASE-5898,"I was running an insert workload against trunk under oprofile and saw that a significant portion of CPU usage was going to calling the ""prePut"" coprocessor hook inside doMiniBatchPut, even though I don't have any coprocessors installed. I ran a million-row insert and collected CPU time spent in the RS after commenting out the preput hook, and found CPU usage reduced by 33%.",,,1,,,,,
HBASE-5904,"When using the hbase shell to manipulate meta entries, one is allowed to 'delete' malformed rows (entries with less than 2 ascii 44 ',' chars).  When this happens HBase servers may go down and the cluster will not be restartable without manual intervention.  

The delete results in a durable malformed rowkey in .META.'s memstore, .META.'s HLog, and eventually .META.'s HFiles.  Subsequent scans to meta (such as when a HMaster starts) fail in the scanner because the comparator fails.  In the case of an HMaster startup, it causes an abort that kills the HMaster process.


{code}
12/04/18 22:07:34 FATAL master.HMaster: Unhandled exception. Starting shutdown.
org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.IllegalArgumentException: No 44 in <blah,1334744821162.81f2df35c332dd2d3bb966fb5b419568.>, length=47, offset=54
at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:990)
at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:979)
at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1894)
at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1834)
at sun.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:570)
at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1039)
Caused by: java.lang.IllegalArgumentException: No 44 in <blah,1334744821162.81f2df35c332dd2d3bb966fb5b419568.>, length=47, offset=54
at org.apache.hadoop.hbase.KeyValue.getRequiredDelimiterInReverse(KeyValue.java:1300)
at org.apache.hadoop.hbase.KeyValue$MetaKeyComparator.compareRows(KeyValue.java:1846)
at org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.match(ScanQueryMatcher.java:130)
at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:257)
at org.apache.hadoop.hbase.regionserver.KeyValueHeap.next(KeyValueHeap.java:114)
at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.nextInternal(HRegion.java:2435)
at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.next(HRegion.java:2391)
at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.next(HRegion.java:2408)
at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1870)
... 6 more

at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:771)
at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:257)
at $Proxy9.next(Unknown Source)
at org.apache.hadoop.hbase.catalog.MetaReader.fullScan(MetaReader.java:264)
at org.apache.hadoop.hbase.catalog.MetaReader.fullScan(MetaReader.java:237)
at org.apache.hadoop.hbase.catalog.MetaReader.fullScanOfResults(MetaReader.java:220)
at org.apache.hadoop.hbase.master.AssignmentManager.rebuildUserRegions(AssignmentManager.java:1580)
at org.apache.hadoop.hbase.master.AssignmentManager.processFailover(AssignmentManager.java:221)
at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:422)
at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:295)
12/04/18 22:07:34 INFO master.HMaster: Aborting 
{code}",,1,,,,,,
HBASE-5916,"Consider a case where my master is getting restarted.  RS that was alive when the master restart started, gets restarted before the master initializes the ServerShutDownHandler.
{code}
serverShutdownHandlerEnabled = true;
{code}

In this case when the RS tries to register with the master, the master will try to expire the server but the server cannot be expired as still the serverShutdownHandler is not enabled.

This case may happen when i have only one RS gets restarted or all the RS gets restarted at the same time.(before assignRootandMeta).
{code}
LOG.info(message);
      if (existingServer.getStartcode() < serverName.getStartcode()) {
        LOG.info(""Triggering server recovery; existingServer "" +
          existingServer + "" looks stale, new server:"" + serverName);
        expireServer(existingServer);
      }
{code}
If another RS is brought up then the cluster comes back to normalcy.

May be a very corner case.



",,1,,,,,,
HBASE-5918,"When master is initializing, if root server died between assign root and assign meta, master will block at 
HMaster#assignRootAndMeta:{code}assignmentManager.assignMeta();
this.catalogTracker.waitForMeta();{code}
because ServerShutdownHandler is disabled,

So we should enable ServerShutdownHandler after called assignmentManager.assignMeta();",,,,,,,,
HBASE-5920,"There seem to be some tuning settings in which manually triggered major compactions will do nothing, including loggic

From Store.java in the function
  List<StoreFile> compactSelection(List<StoreFile> candidates)

When a user manually triggers a compaction, this follows the same logic as a normal compaction check.  when a user manually triggers a major compaction, something similar happens.  Putting this all together:

1. If a user triggers a major compaction, this is checked against a max files threshold (hbase.hstore.compaction.max). If the number of storefiles to compact is > max files, then we downgrade to a minor compaction
2. If we are in a minor compaction, we do the following checks:
   a. If the file is less than a minimum size (hbase.hstore.compaction.min.size) we automatically include it
   b. Otherwise, we check how the size compares to the next largest size.  based on hbase.hstore.compaction.ratio.  
  c. If the number of files included is less than a minimum count (hbase.hstore.compaction.min) then don't compact.
In many of the exit strategies, we aren't seeing an error message.
The net-net of this is that if we have a mix of very large and very small files, we may end up having too many files to do a major compact, but too few files to do a minor compact.

I'm trying to go through and see if I'm understanding things correctly, but this seems like the bug

To put it another way
2012-05-02 20:09:36,389 DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitThread: Large Compaction requested: regionName=str,44594594594594592,1334939064521.f7aed25b55d4d7988af763bede9ce74e., store
Name=c, fileCount=15, fileSize=1.5g (20.2k, 362.5m, 155.3k, 3.0m, 30.7k, 361.2m, 6.9m, 4.7m, 14.7k, 363.4m, 30.9m, 3.2m, 7.3k, 362.9m, 23.5m), priority=-9, time=3175046817624398; Because: Recursive enqueue; compaction_queue=(59:0), split_queue=0

When we had a minimum compaction size of 128M, and default settings for hbase.hstore.compaction.min,hbase.hstore.compaction.max,hbase.hstore.compaction.ratio, we were not getting a compaction to run even if we ran
major_compact 'str,44594594594594592,1334939064521.f7aed25b55d4d7988af763bede9ce74e.' from the ruby shell.  Note that we had many tiny regions (20k, 155k, 3m, 30k,..) and several large regions (362.5m,361.2m,363.4m,362.9m).  I think the bimodal nature of the sizes prevented us from doing a compaction.
I'm not 100% sure where this errored out because when I manually triggered a compaction, I did not see
'      // if we don't have enough files to compact, just wait             
      if (filesToCompact.size() < this.minFilesToCompact) {              
        if (LOG.isDebugEnabled()) {                                      
          LOG.debug(""Skipped compaction of "" + this.storeNameStr         
            + "".  Only "" + (end - start) + "" file(s) of size ""           
            + StringUtils.humanReadableInt(totalSize)                    
            + "" have met compaction criteria."");                         
        }                                                                
' 
being printed in the logs (and I know DEBUG logging was enabled because I saw this elsewhere).  

I'd be happy with better error messages when we decide not to compact for user enabled compactions.
I'd also like to see some override that says ""user triggered major compaction always occurs"", but maybe that's a bad idea for other reasons.
",,,1,,,,,
HBASE-5927,"A possible exception: If the related regionserver was just killed(But HMaster has not perceived that), then we will get a local exception ""Connection reset by peer"". If this region belongs to a disabling table. what will happen?

ServerShutdownHandler will remove this region from AM#regions. So this region is still existing in RIT. TimeoutMonitor will take care of it after it got timeout. Then invoke unassign again. Since this region has been removed from AM#regions, it will return directly due to the below code:
{code}
    synchronized (this.regions) {
      // Check if this region is currently assigned
      if (!regions.containsKey(region)) {
        LOG.debug(""Attempted to unassign region "" +
          region.getRegionNameAsString() + "" but it is not "" +
          ""currently assigned anywhere"");
        return;
      }
    }
{code}
Then it leads to an end-less loop.
",,,,,,,,
HBASE-5930,"There seem to be some tuning settings in which manually triggered major compactions will do nothing, including loggic

From Store.java in the function
  List<StoreFile> compactSelection(List<StoreFile> candidates)

When a user manually triggers a compaction, this follows the same logic as a normal compaction check.  when a user manually triggers a major compaction, something similar happens.  Putting this all together:

1. If a user triggers a major compaction, this is checked against a max files threshold (hbase.hstore.compaction.max). If the number of storefiles to compact is > max files, then we downgrade to a minor compaction
2. If we are in a minor compaction, we do the following checks:
   a. If the file is less than a minimum size (hbase.hstore.compaction.min.size) we automatically include it
   b. Otherwise, we check how the size compares to the next largest size.  based on hbase.hstore.compaction.ratio.  
  c. If the number of files included is less than a minimum count (hbase.hstore.compaction.min) then don't compact.
In many of the exit strategies, we aren't seeing an error message.
The net-net of this is that if we have a mix of very large and very small files, we may end up having too many files to do a major compact, but too few files to do a minor compact.

I'm trying to go through and see if I'm understanding things correctly, but this seems like the bug

To put it another way
2012-05-02 20:09:36,389 DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitThread: Large Compaction requested: regionName=str,44594594594594592,1334939064521.f7aed25b55d4d7988af763bede9ce74e., store
Name=c, fileCount=15, fileSize=1.5g (20.2k, 362.5m, 155.3k, 3.0m, 30.7k, 361.2m, 6.9m, 4.7m, 14.7k, 363.4m, 30.9m, 3.2m, 7.3k, 362.9m, 23.5m), priority=-9, time=3175046817624398; Because: Recursive enqueue; compaction_queue=(59:0), split_queue=0

When we had a minimum compaction size of 128M, and default settings for hbase.hstore.compaction.min,hbase.hstore.compaction.max,hbase.hstore.compaction.ratio, we were not getting a compaction to run even if we ran
major_compact 'str,44594594594594592,1334939064521.f7aed25b55d4d7988af763bede9ce74e.' from the ruby shell.  Note that we had many tiny regions (20k, 155k, 3m, 30k,..) and several large regions (362.5m,361.2m,363.4m,362.9m).  I think the bimodal nature of the sizes prevented us from doing a compaction.
I'm not 100% sure where this errored out because when I manually triggered a compaction, I did not see
'      // if we don't have enough files to compact, just wait             
      if (filesToCompact.size() < this.minFilesToCompact) {              
        if (LOG.isDebugEnabled()) {                                      
          LOG.debug(""Skipped compaction of "" + this.storeNameStr         
            + "".  Only "" + (end - start) + "" file(s) of size ""           
            + StringUtils.humanReadableInt(totalSize)                    
            + "" have met compaction criteria."");                         
        }                                                                
' 
being printed in the logs (and I know DEBUG logging was enabled because I saw this elsewhere).  

I'd be happy with better error messages when we decide not to compact for user enabled compactions.
I'd also like to see some override that says ""user triggered major compaction always occurs"", but maybe that's a bad idea for other reasons.
",,1,1,,,,,
HBASE-5942,"HConnnectionManager.getRegionServerWithRetries:
{code}
          return callable.call();
        } catch (Throwable t) {
          callable.shouldRetry(t);
{code}
shouldRetry relies on the proper startTime and endTime to calculate the timeout value. However, callable.afterCall() is called in finally block. Thus callable.callTimeout will be set to negative value in callable.shouldRetry().
",,,,,,,,
HBASE-5945,"There seem to be some tuning settings in which manually triggered major compactions will do nothing, including loggic

From Store.java in the function
  List<StoreFile> compactSelection(List<StoreFile> candidates)

When a user manually triggers a compaction, this follows the same logic as a normal compaction check.  when a user manually triggers a major compaction, something similar happens.  Putting this all together:

1. If a user triggers a major compaction, this is checked against a max files threshold (hbase.hstore.compaction.max). If the number of storefiles to compact is > max files, then we downgrade to a minor compaction
2. If we are in a minor compaction, we do the following checks:
   a. If the file is less than a minimum size (hbase.hstore.compaction.min.size) we automatically include it
   b. Otherwise, we check how the size compares to the next largest size.  based on hbase.hstore.compaction.ratio.  
  c. If the number of files included is less than a minimum count (hbase.hstore.compaction.min) then don't compact.
In many of the exit strategies, we aren't seeing an error message.
The net-net of this is that if we have a mix of very large and very small files, we may end up having too many files to do a major compact, but too few files to do a minor compact.

I'm trying to go through and see if I'm understanding things correctly, but this seems like the bug

To put it another way
2012-05-02 20:09:36,389 DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitThread: Large Compaction requested: regionName=str,44594594594594592,1334939064521.f7aed25b55d4d7988af763bede9ce74e., store
Name=c, fileCount=15, fileSize=1.5g (20.2k, 362.5m, 155.3k, 3.0m, 30.7k, 361.2m, 6.9m, 4.7m, 14.7k, 363.4m, 30.9m, 3.2m, 7.3k, 362.9m, 23.5m), priority=-9, time=3175046817624398; Because: Recursive enqueue; compaction_queue=(59:0), split_queue=0

When we had a minimum compaction size of 128M, and default settings for hbase.hstore.compaction.min,hbase.hstore.compaction.max,hbase.hstore.compaction.ratio, we were not getting a compaction to run even if we ran
major_compact 'str,44594594594594592,1334939064521.f7aed25b55d4d7988af763bede9ce74e.' from the ruby shell.  Note that we had many tiny regions (20k, 155k, 3m, 30k,..) and several large regions (362.5m,361.2m,363.4m,362.9m).  I think the bimodal nature of the sizes prevented us from doing a compaction.
I'm not 100% sure where this errored out because when I manually triggered a compaction, I did not see
'      // if we don't have enough files to compact, just wait             
      if (filesToCompact.size() < this.minFilesToCompact) {              
        if (LOG.isDebugEnabled()) {                                      
          LOG.debug(""Skipped compaction of "" + this.storeNameStr         
            + "".  Only "" + (end - start) + "" file(s) of size ""           
            + StringUtils.humanReadableInt(totalSize)                    
            + "" have met compaction criteria."");                         
        }                                                                
' 
being printed in the logs (and I know DEBUG logging was enabled because I saw this elsewhere).  

I'd be happy with better error messages when we decide not to compact for user enabled compactions.
I'd also like to see some override that says ""user triggered major compaction always occurs"", but maybe that's a bad idea for other reasons.
",,,1,,,,,
HBASE-5952,"Since the hardcoded default flush size is 64MB, but the default in hbase-default.xml is 128MB, if the client does set it to 64MB,
the actual flush size will be 128MB instead, due to the way HRegion get the flush size. We can change how HRegion get the flush size,
but it is clean and simple to sync up the defaults.",,1,,,,,,
HBASE-5959,"There seem to be some tuning settings in which manually triggered major compactions will do nothing, including loggic

From Store.java in the function
  List<StoreFile> compactSelection(List<StoreFile> candidates)

When a user manually triggers a compaction, this follows the same logic as a normal compaction check.  when a user manually triggers a major compaction, something similar happens.  Putting this all together:

1. If a user triggers a major compaction, this is checked against a max files threshold (hbase.hstore.compaction.max). If the number of storefiles to compact is > max files, then we downgrade to a minor compaction
2. If we are in a minor compaction, we do the following checks:
   a. If the file is less than a minimum size (hbase.hstore.compaction.min.size) we automatically include it
   b. Otherwise, we check how the size compares to the next largest size.  based on hbase.hstore.compaction.ratio.  
  c. If the number of files included is less than a minimum count (hbase.hstore.compaction.min) then don't compact.
In many of the exit strategies, we aren't seeing an error message.
The net-net of this is that if we have a mix of very large and very small files, we may end up having too many files to do a major compact, but too few files to do a minor compact.

I'm trying to go through and see if I'm understanding things correctly, but this seems like the bug

To put it another way
2012-05-02 20:09:36,389 DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitThread: Large Compaction requested: regionName=str,44594594594594592,1334939064521.f7aed25b55d4d7988af763bede9ce74e., store
Name=c, fileCount=15, fileSize=1.5g (20.2k, 362.5m, 155.3k, 3.0m, 30.7k, 361.2m, 6.9m, 4.7m, 14.7k, 363.4m, 30.9m, 3.2m, 7.3k, 362.9m, 23.5m), priority=-9, time=3175046817624398; Because: Recursive enqueue; compaction_queue=(59:0), split_queue=0

When we had a minimum compaction size of 128M, and default settings for hbase.hstore.compaction.min,hbase.hstore.compaction.max,hbase.hstore.compaction.ratio, we were not getting a compaction to run even if we ran
major_compact 'str,44594594594594592,1334939064521.f7aed25b55d4d7988af763bede9ce74e.' from the ruby shell.  Note that we had many tiny regions (20k, 155k, 3m, 30k,..) and several large regions (362.5m,361.2m,363.4m,362.9m).  I think the bimodal nature of the sizes prevented us from doing a compaction.
I'm not 100% sure where this errored out because when I manually triggered a compaction, I did not see
'      // if we don't have enough files to compact, just wait             
      if (filesToCompact.size() < this.minFilesToCompact) {              
        if (LOG.isDebugEnabled()) {                                      
          LOG.debug(""Skipped compaction of "" + this.storeNameStr         
            + "".  Only "" + (end - start) + "" file(s) of size ""           
            + StringUtils.humanReadableInt(totalSize)                    
            + "" have met compaction criteria."");                         
        }                                                                
' 
being printed in the logs (and I know DEBUG logging was enabled because I saw this elsewhere).  

I'd be happy with better error messages when we decide not to compact for user enabled compactions.
I'd also like to see some override that says ""user triggered major compaction always occurs"", but maybe that's a bad idea for other reasons.
",,1,1,,,,,
HBASE-5964,"I've been seeing this with Hadoop 2.0.0-alpha-SNAPSHOT and HBase 0.94.0-SNAPSHOT:

{noformat}
2012-05-08 15:18:00,692 FATAL [RegionServer:0;acer.localdomain,48307,1336515479011] regionserver.HRegionServer(1674): ABORTING region server acer.localdomain,48307,1336515479011: Unhandled exception: No FileSystem for scheme: hdfs
java.io.IOException: No FileSystem for scheme: hdfs
	at org.apache.hadoop.hbase.fs.HFileSystem.newInstanceFileSystem(HFileSystem.java:146)
	at org.apache.hadoop.hbase.fs.HFileSystem.<init>(HFileSystem.java:75)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.handleReportForDutyResponse(HRegionServer.java:973)
	at org.apache.hadoop.hbase.MiniHBaseCluster$MiniHBaseClusterRegionServer.handleReportForDutyResponse(MiniHBaseCluster.java:110)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:671)
	at org.apache.hadoop.hbase.MiniHBaseCluster$MiniHBaseClusterRegionServer.runRegionServer(MiniHBaseCluster.java:136)
	at org.apache.hadoop.hbase.MiniHBaseCluster$MiniHBaseClusterRegionServer.access$000(MiniHBaseCluster.java:89)
	at org.apache.hadoop.hbase.MiniHBaseCluster$MiniHBaseClusterRegionServer$1.run(MiniHBaseCluster.java:120)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:357)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1212)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at org.apache.hadoop.hbase.util.Methods.call(Methods.java:37)
	at org.apache.hadoop.hbase.security.User.call(User.java:586)
	at org.apache.hadoop.hbase.security.User.access$700(User.java:50)
	at org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:426)
	at org.apache.hadoop.hbase.MiniHBaseCluster$MiniHBaseClusterRegionServer.run(MiniHBaseCluster.java:118)
	at java.lang.Thread.run(Thread.java:679)
{noformat}

Not sure precisely when it started. First I thought it might be due to HBASE-5955 but I cherry picked that change over from trunk. Then I got HBASE-5963 out of the way.  ",,1,,,,,,
HBASE-5967,"I saw this error in the master log:

Caused by: java.lang.IllegalArgumentException: Method org.apache.hadoop.hbase.master.MXBean.getRegionServers has parameter or return type that cannot be translated into an open type
    at com.sun.jmx.mbeanserver.ConvertingMethod.from(ConvertingMethod.java:32)
    at com.sun.jmx.mbeanserver.MXBeanIntrospector.mFrom(MXBeanIntrospector.java:63)
    at com.sun.jmx.mbeanserver.MXBeanIntrospector.mFrom(MXBeanIntrospector.java:33)
    at com.sun.jmx.mbeanserver.MBeanAnalyzer.initMaps(MBeanAnalyzer.java:118)
    at com.sun.jmx.mbeanserver.MBeanAnalyzer.<init>(MBeanAnalyzer.java:99)
    ... 14 more
Caused by: javax.management.openmbean.OpenDataException: Cannot convert type: java.util.Map<java.lang.String, org.apache.hadoop.hbase.ServerLoad>
    at com.sun.jmx.mbeanserver.OpenConverter.openDataException(OpenConverter.jav

",,,,,,,,
HBASE-5970,"There seem to be some tuning settings in which manually triggered major compactions will do nothing, including loggic

From Store.java in the function
  List<StoreFile> compactSelection(List<StoreFile> candidates)

When a user manually triggers a compaction, this follows the same logic as a normal compaction check.  when a user manually triggers a major compaction, something similar happens.  Putting this all together:

1. If a user triggers a major compaction, this is checked against a max files threshold (hbase.hstore.compaction.max). If the number of storefiles to compact is > max files, then we downgrade to a minor compaction
2. If we are in a minor compaction, we do the following checks:
   a. If the file is less than a minimum size (hbase.hstore.compaction.min.size) we automatically include it
   b. Otherwise, we check how the size compares to the next largest size.  based on hbase.hstore.compaction.ratio.  
  c. If the number of files included is less than a minimum count (hbase.hstore.compaction.min) then don't compact.
In many of the exit strategies, we aren't seeing an error message.
The net-net of this is that if we have a mix of very large and very small files, we may end up having too many files to do a major compact, but too few files to do a minor compact.

I'm trying to go through and see if I'm understanding things correctly, but this seems like the bug

To put it another way
2012-05-02 20:09:36,389 DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitThread: Large Compaction requested: regionName=str,44594594594594592,1334939064521.f7aed25b55d4d7988af763bede9ce74e., store
Name=c, fileCount=15, fileSize=1.5g (20.2k, 362.5m, 155.3k, 3.0m, 30.7k, 361.2m, 6.9m, 4.7m, 14.7k, 363.4m, 30.9m, 3.2m, 7.3k, 362.9m, 23.5m), priority=-9, time=3175046817624398; Because: Recursive enqueue; compaction_queue=(59:0), split_queue=0

When we had a minimum compaction size of 128M, and default settings for hbase.hstore.compaction.min,hbase.hstore.compaction.max,hbase.hstore.compaction.ratio, we were not getting a compaction to run even if we ran
major_compact 'str,44594594594594592,1334939064521.f7aed25b55d4d7988af763bede9ce74e.' from the ruby shell.  Note that we had many tiny regions (20k, 155k, 3m, 30k,..) and several large regions (362.5m,361.2m,363.4m,362.9m).  I think the bimodal nature of the sizes prevented us from doing a compaction.
I'm not 100% sure where this errored out because when I manually triggered a compaction, I did not see
'      // if we don't have enough files to compact, just wait             
      if (filesToCompact.size() < this.minFilesToCompact) {              
        if (LOG.isDebugEnabled()) {                                      
          LOG.debug(""Skipped compaction of "" + this.storeNameStr         
            + "".  Only "" + (end - start) + "" file(s) of size ""           
            + StringUtils.humanReadableInt(totalSize)                    
            + "" have met compaction criteria."");                         
        }                                                                
' 
being printed in the logs (and I know DEBUG logging was enabled because I saw this elsewhere).  

I'd be happy with better error messages when we decide not to compact for user enabled compactions.
I'd also like to see some override that says ""user triggered major compaction always occurs"", but maybe that's a bad idea for other reasons.
",,,1,,,,,
HBASE-5973,"I'm seeing the following behavior:
- set RPC timeout to a short value
- call next() for some batch of rows, big enough so the client times out before the result is returned
- the HConnectionManager stuff will retry the next() call to the same server. At this point, one of two things can happen: 1) the previous next() call will still be processing, in which case you get a LeaseException, because it was removed from the map during the processing, or 2) the next() call will succeed but skip the prior batch of rows.",,1,,,,,,
HBASE-5974,"I'm seeing the following behavior:
- set RPC timeout to a short value
- call next() for some batch of rows, big enough so the client times out before the result is returned
- the HConnectionManager stuff will retry the next() call to the same server. At this point, one of two things can happen: 1) the previous next() call will still be processing, in which case you get a LeaseException, because it was removed from the map during the processing, or 2) the next() call will succeed but skip the prior batch of rows.",,1,,,,,,
HBASE-5979,"There seem to be some tuning settings in which manually triggered major compactions will do nothing, including loggic

From Store.java in the function
  List<StoreFile> compactSelection(List<StoreFile> candidates)

When a user manually triggers a compaction, this follows the same logic as a normal compaction check.  when a user manually triggers a major compaction, something similar happens.  Putting this all together:

1. If a user triggers a major compaction, this is checked against a max files threshold (hbase.hstore.compaction.max). If the number of storefiles to compact is > max files, then we downgrade to a minor compaction
2. If we are in a minor compaction, we do the following checks:
   a. If the file is less than a minimum size (hbase.hstore.compaction.min.size) we automatically include it
   b. Otherwise, we check how the size compares to the next largest size.  based on hbase.hstore.compaction.ratio.  
  c. If the number of files included is less than a minimum count (hbase.hstore.compaction.min) then don't compact.
In many of the exit strategies, we aren't seeing an error message.
The net-net of this is that if we have a mix of very large and very small files, we may end up having too many files to do a major compact, but too few files to do a minor compact.

I'm trying to go through and see if I'm understanding things correctly, but this seems like the bug

To put it another way
2012-05-02 20:09:36,389 DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitThread: Large Compaction requested: regionName=str,44594594594594592,1334939064521.f7aed25b55d4d7988af763bede9ce74e., store
Name=c, fileCount=15, fileSize=1.5g (20.2k, 362.5m, 155.3k, 3.0m, 30.7k, 361.2m, 6.9m, 4.7m, 14.7k, 363.4m, 30.9m, 3.2m, 7.3k, 362.9m, 23.5m), priority=-9, time=3175046817624398; Because: Recursive enqueue; compaction_queue=(59:0), split_queue=0

When we had a minimum compaction size of 128M, and default settings for hbase.hstore.compaction.min,hbase.hstore.compaction.max,hbase.hstore.compaction.ratio, we were not getting a compaction to run even if we ran
major_compact 'str,44594594594594592,1334939064521.f7aed25b55d4d7988af763bede9ce74e.' from the ruby shell.  Note that we had many tiny regions (20k, 155k, 3m, 30k,..) and several large regions (362.5m,361.2m,363.4m,362.9m).  I think the bimodal nature of the sizes prevented us from doing a compaction.
I'm not 100% sure where this errored out because when I manually triggered a compaction, I did not see
'      // if we don't have enough files to compact, just wait             
      if (filesToCompact.size() < this.minFilesToCompact) {              
        if (LOG.isDebugEnabled()) {                                      
          LOG.debug(""Skipped compaction of "" + this.storeNameStr         
            + "".  Only "" + (end - start) + "" file(s) of size ""           
            + StringUtils.humanReadableInt(totalSize)                    
            + "" have met compaction criteria."");                         
        }                                                                
' 
being printed in the logs (and I know DEBUG logging was enabled because I saw this elsewhere).  

I'd be happy with better error messages when we decide not to compact for user enabled compactions.
I'd also like to see some override that says ""user triggered major compaction always occurs"", but maybe that's a bad idea for other reasons.
",,,1,,,,,
HBASE-5986,"We found this issue when running large scale ingestion tests for HBASE-5754. The problem is that the .META. table updates are not atomic while splitting a region. In SplitTransaction, there is a time lap between the marking the parent offline, and adding of daughters to the META table. This can result in clients using MetaScanner, of HTable.getStartEndKeys (used by the TableInputFormat) missing regions which are made just offline, but the daughters are not added yet. 

This is also related to HBASE-4335. ",,,,,,,,
HBASE-5987,"There seem to be some tuning settings in which manually triggered major compactions will do nothing, including loggic

From Store.java in the function
  List<StoreFile> compactSelection(List<StoreFile> candidates)

When a user manually triggers a compaction, this follows the same logic as a normal compaction check.  when a user manually triggers a major compaction, something similar happens.  Putting this all together:

1. If a user triggers a major compaction, this is checked against a max files threshold (hbase.hstore.compaction.max). If the number of storefiles to compact is > max files, then we downgrade to a minor compaction
2. If we are in a minor compaction, we do the following checks:
   a. If the file is less than a minimum size (hbase.hstore.compaction.min.size) we automatically include it
   b. Otherwise, we check how the size compares to the next largest size.  based on hbase.hstore.compaction.ratio.  
  c. If the number of files included is less than a minimum count (hbase.hstore.compaction.min) then don't compact.
In many of the exit strategies, we aren't seeing an error message.
The net-net of this is that if we have a mix of very large and very small files, we may end up having too many files to do a major compact, but too few files to do a minor compact.

I'm trying to go through and see if I'm understanding things correctly, but this seems like the bug

To put it another way
2012-05-02 20:09:36,389 DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitThread: Large Compaction requested: regionName=str,44594594594594592,1334939064521.f7aed25b55d4d7988af763bede9ce74e., store
Name=c, fileCount=15, fileSize=1.5g (20.2k, 362.5m, 155.3k, 3.0m, 30.7k, 361.2m, 6.9m, 4.7m, 14.7k, 363.4m, 30.9m, 3.2m, 7.3k, 362.9m, 23.5m), priority=-9, time=3175046817624398; Because: Recursive enqueue; compaction_queue=(59:0), split_queue=0

When we had a minimum compaction size of 128M, and default settings for hbase.hstore.compaction.min,hbase.hstore.compaction.max,hbase.hstore.compaction.ratio, we were not getting a compaction to run even if we ran
major_compact 'str,44594594594594592,1334939064521.f7aed25b55d4d7988af763bede9ce74e.' from the ruby shell.  Note that we had many tiny regions (20k, 155k, 3m, 30k,..) and several large regions (362.5m,361.2m,363.4m,362.9m).  I think the bimodal nature of the sizes prevented us from doing a compaction.
I'm not 100% sure where this errored out because when I manually triggered a compaction, I did not see
'      // if we don't have enough files to compact, just wait             
      if (filesToCompact.size() < this.minFilesToCompact) {              
        if (LOG.isDebugEnabled()) {                                      
          LOG.debug(""Skipped compaction of "" + this.storeNameStr         
            + "".  Only "" + (end - start) + "" file(s) of size ""           
            + StringUtils.humanReadableInt(totalSize)                    
            + "" have met compaction criteria."");                         
        }                                                                
' 
being printed in the logs (and I know DEBUG logging was enabled because I saw this elsewhere).  

I'd be happy with better error messages when we decide not to compact for user enabled compactions.
I'd also like to see some override that says ""user triggered major compaction always occurs"", but maybe that's a bad idea for other reasons.
",,,1,,,,,
HBASE-5997,"Pls refer to the comment
https://issues.apache.org/jira/browse/HBASE-5922?focusedCommentId=13269346&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13269346.
Raised this issue to solve that comment. Just incase we don't forget it.",,,,,,,,
HBASE-5998,"There seem to be some tuning settings in which manually triggered major compactions will do nothing, including loggic

From Store.java in the function
  List<StoreFile> compactSelection(List<StoreFile> candidates)

When a user manually triggers a compaction, this follows the same logic as a normal compaction check.  when a user manually triggers a major compaction, something similar happens.  Putting this all together:

1. If a user triggers a major compaction, this is checked against a max files threshold (hbase.hstore.compaction.max). If the number of storefiles to compact is > max files, then we downgrade to a minor compaction
2. If we are in a minor compaction, we do the following checks:
   a. If the file is less than a minimum size (hbase.hstore.compaction.min.size) we automatically include it
   b. Otherwise, we check how the size compares to the next largest size.  based on hbase.hstore.compaction.ratio.  
  c. If the number of files included is less than a minimum count (hbase.hstore.compaction.min) then don't compact.
In many of the exit strategies, we aren't seeing an error message.
The net-net of this is that if we have a mix of very large and very small files, we may end up having too many files to do a major compact, but too few files to do a minor compact.

I'm trying to go through and see if I'm understanding things correctly, but this seems like the bug

To put it another way
2012-05-02 20:09:36,389 DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitThread: Large Compaction requested: regionName=str,44594594594594592,1334939064521.f7aed25b55d4d7988af763bede9ce74e., store
Name=c, fileCount=15, fileSize=1.5g (20.2k, 362.5m, 155.3k, 3.0m, 30.7k, 361.2m, 6.9m, 4.7m, 14.7k, 363.4m, 30.9m, 3.2m, 7.3k, 362.9m, 23.5m), priority=-9, time=3175046817624398; Because: Recursive enqueue; compaction_queue=(59:0), split_queue=0

When we had a minimum compaction size of 128M, and default settings for hbase.hstore.compaction.min,hbase.hstore.compaction.max,hbase.hstore.compaction.ratio, we were not getting a compaction to run even if we ran
major_compact 'str,44594594594594592,1334939064521.f7aed25b55d4d7988af763bede9ce74e.' from the ruby shell.  Note that we had many tiny regions (20k, 155k, 3m, 30k,..) and several large regions (362.5m,361.2m,363.4m,362.9m).  I think the bimodal nature of the sizes prevented us from doing a compaction.
I'm not 100% sure where this errored out because when I manually triggered a compaction, I did not see
'      // if we don't have enough files to compact, just wait             
      if (filesToCompact.size() < this.minFilesToCompact) {              
        if (LOG.isDebugEnabled()) {                                      
          LOG.debug(""Skipped compaction of "" + this.storeNameStr         
            + "".  Only "" + (end - start) + "" file(s) of size ""           
            + StringUtils.humanReadableInt(totalSize)                    
            + "" have met compaction criteria."");                         
        }                                                                
' 
being printed in the logs (and I know DEBUG logging was enabled because I saw this elsewhere).  

I'd be happy with better error messages when we decide not to compact for user enabled compactions.
I'd also like to see some override that says ""user triggered major compaction always occurs"", but maybe that's a bad idea for other reasons.
",,,1,,,,,
HBASE-6002,In HLogSplitter.splitLogFileToTemp-Reader(in) is not closed and in finally block in loop while closing the writers(wap.w) if any exception comes other writers won't close.,,,,,,,,
HBASE-6007,"Making the getTableRegions Thrift API method handle TableNotFoundException and return an empty list in that case. Without this the behavior is dependent on whether an HTable object is present in the thread-local cache in case a table was deleted.
",,,,,,,,
HBASE-6012,"Since HBASE-5914, we using bulk assign for SSH

But in the bulk assign case if we get an ALREADY_OPENED case there is no one to clear the znode created by bulk assign. 


Another thing, when RS opening a list of regions, if one region is already in transition, it will throw RegionAlreadyInTransitionException and stop opening other regions.",,,,,,,,
HBASE-6014,"There seem to be some tuning settings in which manually triggered major compactions will do nothing, including loggic

From Store.java in the function
  List<StoreFile> compactSelection(List<StoreFile> candidates)

When a user manually triggers a compaction, this follows the same logic as a normal compaction check.  when a user manually triggers a major compaction, something similar happens.  Putting this all together:

1. If a user triggers a major compaction, this is checked against a max files threshold (hbase.hstore.compaction.max). If the number of storefiles to compact is > max files, then we downgrade to a minor compaction
2. If we are in a minor compaction, we do the following checks:
   a. If the file is less than a minimum size (hbase.hstore.compaction.min.size) we automatically include it
   b. Otherwise, we check how the size compares to the next largest size.  based on hbase.hstore.compaction.ratio.  
  c. If the number of files included is less than a minimum count (hbase.hstore.compaction.min) then don't compact.
In many of the exit strategies, we aren't seeing an error message.
The net-net of this is that if we have a mix of very large and very small files, we may end up having too many files to do a major compact, but too few files to do a minor compact.

I'm trying to go through and see if I'm understanding things correctly, but this seems like the bug

To put it another way
2012-05-02 20:09:36,389 DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitThread: Large Compaction requested: regionName=str,44594594594594592,1334939064521.f7aed25b55d4d7988af763bede9ce74e., store
Name=c, fileCount=15, fileSize=1.5g (20.2k, 362.5m, 155.3k, 3.0m, 30.7k, 361.2m, 6.9m, 4.7m, 14.7k, 363.4m, 30.9m, 3.2m, 7.3k, 362.9m, 23.5m), priority=-9, time=3175046817624398; Because: Recursive enqueue; compaction_queue=(59:0), split_queue=0

When we had a minimum compaction size of 128M, and default settings for hbase.hstore.compaction.min,hbase.hstore.compaction.max,hbase.hstore.compaction.ratio, we were not getting a compaction to run even if we ran
major_compact 'str,44594594594594592,1334939064521.f7aed25b55d4d7988af763bede9ce74e.' from the ruby shell.  Note that we had many tiny regions (20k, 155k, 3m, 30k,..) and several large regions (362.5m,361.2m,363.4m,362.9m).  I think the bimodal nature of the sizes prevented us from doing a compaction.
I'm not 100% sure where this errored out because when I manually triggered a compaction, I did not see
'      // if we don't have enough files to compact, just wait             
      if (filesToCompact.size() < this.minFilesToCompact) {              
        if (LOG.isDebugEnabled()) {                                      
          LOG.debug(""Skipped compaction of "" + this.storeNameStr         
            + "".  Only "" + (end - start) + "" file(s) of size ""           
            + StringUtils.humanReadableInt(totalSize)                    
            + "" have met compaction criteria."");                         
        }                                                                
' 
being printed in the logs (and I know DEBUG logging was enabled because I saw this elsewhere).  

I'd be happy with better error messages when we decide not to compact for user enabled compactions.
I'd also like to see some override that says ""user triggered major compaction always occurs"", but maybe that's a bad idea for other reasons.
",,,1,,,,,
HBASE-6016,"{code}
   * @return Returns true if specified region should be assigned, false if not.
   * @throws IOException
   */
  public static boolean processDeadRegion(HRegionInfo hri, Result result,
      AssignmentManager assignmentManager, CatalogTracker catalogTracker)
{code}

For the disabling region, I think we needn't assign it , and processDeadRegion could return false.",,1,,,,,,
HBASE-6018,"On a long running job 0.94.0rc3 cluster, we get to a point where hbck consistently encounters this error and fails:

{code}
Exception in thread ""main"" java.util.concurrent.RejectedExecutionException
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:1768)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
	at org.apache.hadoop.hbase.util.HBaseFsck.loadHdfsRegionInfos(HBaseFsck.java:633)
	at org.apache.hadoop.hbase.util.HBaseFsck.onlineConsistencyRepair(HBaseFsck.java:354)
	at org.apache.hadoop.hbase.util.HBaseFsck.onlineHbck(HBaseFsck.java:382)
	at org.apache.hadoop.hbase.util.HBaseFsck.main(HBaseFsck.java:3120)
{code}
",,,,,,,,
HBASE-6023,"A pretty trivial change, we log failed authentication attempts at WARN level, as does Hadoop, but log successful authentication at TRACE level, where Hadoop instead logs it at INFO level.",1,,,,,,,
HBASE-6031,"Following is the thread dump.
{code}
""1997531088@qtp-716941846-5"" prio=10 tid=0x00007f7c5820c800 nid=0xe1b in Object.wait() [0x00007f7c56ae8000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	at org.mortbay.io.nio.SelectChannelEndPoint.blockWritable(SelectChannelEndPoint.java:279)
	- locked <0x00007f7cfe0616d0> (a org.mortbay.jetty.nio.SelectChannelConnector$ConnectorEndPoint)
	at org.mortbay.jetty.AbstractGenerator$Output.blockForOutput(AbstractGenerator.java:545)
	at org.mortbay.jetty.AbstractGenerator$Output.write(AbstractGenerator.java:639)
	at org.mortbay.jetty.AbstractGenerator$Output.write(AbstractGenerator.java:580)
	at java.io.ByteArrayOutputStream.writeTo(ByteArrayOutputStream.java:109)
	- locked <0x00007f7cfe74d758> (a org.mortbay.util.ByteArrayOutputStream2)
	at org.mortbay.jetty.AbstractGenerator$OutputWriter.write(AbstractGenerator.java:904)
	at java.io.Writer.write(Writer.java:96)
	- locked <0x00007f7cfca02fc0> (a org.mortbay.jetty.HttpConnection$OutputWriter)
	at java.io.PrintWriter.write(PrintWriter.java:361)
	- locked <0x00007f7cfca02fc0> (a org.mortbay.jetty.HttpConnection$OutputWriter)
	at org.jamon.escaping.HtmlEscaping.write(HtmlEscaping.java:43)
	at org.jamon.escaping.AbstractCharacterEscaping.write(AbstractCharacterEscaping.java:35)
	at org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmplImpl.renderNoFlush(RSStatusTmplImpl.java:222)
	at org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.renderNoFlush(RSStatusTmpl.java:180)
	at org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.render(RSStatusTmpl.java:171)
	at org.apache.hadoop.hbase.regionserver.RSStatusServlet.doGet(RSStatusServlet.java:48)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
	at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:932)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)

""1374615312@qtp-716941846-3"" prio=10 tid=0x00007f7c58214800 nid=0xc42 in Object.wait() [0x00007f7c55bd9000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	at org.mortbay.io.nio.SelectChannelEndPoint.blockWritable(SelectChannelEndPoint.java:279)
	- locked <0x00007f7cfdbb6cc8> (a org.mortbay.jetty.nio.SelectChannelConnector$ConnectorEndPoint)
	at org.mortbay.jetty.AbstractGenerator$Output.blockForOutput(AbstractGenerator.java:545)
	at org.mortbay.jetty.AbstractGenerator$Output.write(AbstractGenerator.java:639)
	at org.mortbay.jetty.AbstractGenerator$Output.write(AbstractGenerator.java:580)
	at java.io.ByteArrayOutputStream.writeTo(ByteArrayOutputStream.java:109)
	- locked <0x00007f7cfe7dbda0> (a org.mortbay.util.ByteArrayOutputStream2)
	at org.mortbay.jetty.AbstractGenerator$OutputWriter.write(AbstractGenerator.java:904)
	at java.io.Writer.write(Writer.java:96)
	- locked <0x00007f7cfe30ebe0> (a org.mortbay.jetty.HttpConnection$OutputWriter)
	at java.io.PrintWriter.write(PrintWriter.java:361)
	- locked <0x00007f7cfe30ebe0> (a org.mortbay.jetty.HttpConnection$OutputWriter)
	at org.jamon.escaping.HtmlEscaping.write(HtmlEscaping.java:43)
	at org.jamon.escaping.AbstractCharacterEscaping.write(AbstractCharacterEscaping.java:35)
	at org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmplImpl.renderNoFlush(RSStatusTmplImpl.java:222)
	at org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.renderNoFlush(RSStatusTmpl.java:180)
	at org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.render(RSStatusTmpl.java:171)
	at org.apache.hadoop.hbase.regionserver.RSStatusServlet.doGet(RSStatusServlet.java:48)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
	at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:932)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)

""615294984@qtp-716941846-2"" prio=10 tid=0x00007f7c58086000 nid=0x907 in Object.wait() [0x00007f7c57ffd000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	at org.mortbay.io.nio.SelectChannelEndPoint.blockWritable(SelectChannelEndPoint.java:279)
	- locked <0x00007f7d6383cd40> (a org.mortbay.jetty.nio.SelectChannelConnector$ConnectorEndPoint)
	at org.mortbay.jetty.AbstractGenerator$Output.blockForOutput(AbstractGenerator.java:545)
	at org.mortbay.jetty.AbstractGenerator$Output.write(AbstractGenerator.java:639)
	at org.mortbay.jetty.AbstractGenerator$Output.write(AbstractGenerator.java:580)
	at java.io.ByteArrayOutputStream.writeTo(ByteArrayOutputStream.java:109)
	- locked <0x00007f7d116bbf90> (a org.mortbay.util.ByteArrayOutputStream2)
	at org.mortbay.jetty.AbstractGenerator$OutputWriter.write(AbstractGenerator.java:904)
	at java.io.Writer.write(Writer.java:96)
	- locked <0x00007f7d54b19ea0> (a org.mortbay.jetty.HttpConnection$OutputWriter)
	at java.io.PrintWriter.write(PrintWriter.java:361)
	- locked <0x00007f7d54b19ea0> (a org.mortbay.jetty.HttpConnection$OutputWriter)
	at org.jamon.escaping.HtmlEscaping.write(HtmlEscaping.java:43)
	at org.jamon.escaping.AbstractCharacterEscaping.write(AbstractCharacterEscaping.java:35)
	at org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmplImpl.renderNoFlush(RSStatusTmplImpl.java:222)
	at org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.renderNoFlush(RSStatusTmpl.java:180)
	at org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.render(RSStatusTmpl.java:171)
	at org.apache.hadoop.hbase.regionserver.RSStatusServlet.doGet(RSStatusServlet.java:48)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
	at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:932)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)

""regionserver60020.decayingSampleTick.1"" daemon prio=10 tid=0x0000000040b2b800 nid=0x6f1a waiting on condition [0x00007f7c697a5000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00007f7c9eb40cb0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:198)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1963)
	at java.util.concurrent.DelayQueue.take(DelayQueue.java:164)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:583)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:576)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
	at java.lang.Thread.run(Thread.java:619)

""LeaseRenewer:root@10.18.40.217:9000"" daemon prio=10 tid=0x00007f7c6c770000 nid=0x6f18 waiting on condition [0x00007f7c699a7000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:432)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:69)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:299)
	at java.lang.Thread.run(Thread.java:619)
{code}",,,,,,,,
HBASE-6032,"There seem to be some tuning settings in which manually triggered major compactions will do nothing, including loggic

From Store.java in the function
  List<StoreFile> compactSelection(List<StoreFile> candidates)

When a user manually triggers a compaction, this follows the same logic as a normal compaction check.  when a user manually triggers a major compaction, something similar happens.  Putting this all together:

1. If a user triggers a major compaction, this is checked against a max files threshold (hbase.hstore.compaction.max). If the number of storefiles to compact is > max files, then we downgrade to a minor compaction
2. If we are in a minor compaction, we do the following checks:
   a. If the file is less than a minimum size (hbase.hstore.compaction.min.size) we automatically include it
   b. Otherwise, we check how the size compares to the next largest size.  based on hbase.hstore.compaction.ratio.  
  c. If the number of files included is less than a minimum count (hbase.hstore.compaction.min) then don't compact.
In many of the exit strategies, we aren't seeing an error message.
The net-net of this is that if we have a mix of very large and very small files, we may end up having too many files to do a major compact, but too few files to do a minor compact.

I'm trying to go through and see if I'm understanding things correctly, but this seems like the bug

To put it another way
2012-05-02 20:09:36,389 DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitThread: Large Compaction requested: regionName=str,44594594594594592,1334939064521.f7aed25b55d4d7988af763bede9ce74e., store
Name=c, fileCount=15, fileSize=1.5g (20.2k, 362.5m, 155.3k, 3.0m, 30.7k, 361.2m, 6.9m, 4.7m, 14.7k, 363.4m, 30.9m, 3.2m, 7.3k, 362.9m, 23.5m), priority=-9, time=3175046817624398; Because: Recursive enqueue; compaction_queue=(59:0), split_queue=0

When we had a minimum compaction size of 128M, and default settings for hbase.hstore.compaction.min,hbase.hstore.compaction.max,hbase.hstore.compaction.ratio, we were not getting a compaction to run even if we ran
major_compact 'str,44594594594594592,1334939064521.f7aed25b55d4d7988af763bede9ce74e.' from the ruby shell.  Note that we had many tiny regions (20k, 155k, 3m, 30k,..) and several large regions (362.5m,361.2m,363.4m,362.9m).  I think the bimodal nature of the sizes prevented us from doing a compaction.
I'm not 100% sure where this errored out because when I manually triggered a compaction, I did not see
'      // if we don't have enough files to compact, just wait             
      if (filesToCompact.size() < this.minFilesToCompact) {              
        if (LOG.isDebugEnabled()) {                                      
          LOG.debug(""Skipped compaction of "" + this.storeNameStr         
            + "".  Only "" + (end - start) + "" file(s) of size ""           
            + StringUtils.humanReadableInt(totalSize)                    
            + "" have met compaction criteria."");                         
        }                                                                
' 
being printed in the logs (and I know DEBUG logging was enabled because I saw this elsewhere).  

I'd be happy with better error messages when we decide not to compact for user enabled compactions.
I'd also like to see some override that says ""user triggered major compaction always occurs"", but maybe that's a bad idea for other reasons.
",,,1,,,,,
HBASE-6035,"bin/hbase-jruby region_status.rb prints some weired numbers

1047/2 .. the denominator is 2, which should have been the total number of regions in the region.

It turns out that this was because we were using both:

scan.setFilter(FirstKeyOnlyFilter.new)
and
scan.addColumn INFO, REGION_INFO

it worked fine when there was no column before info:regioninfo.

But, after adding info:favourednodes we were no longer getting info:regioninfo.

Removing the scan.setFilter fixes the problem.",,,,,,,,
HBASE-6040,"There seem to be some tuning settings in which manually triggered major compactions will do nothing, including loggic

From Store.java in the function
  List<StoreFile> compactSelection(List<StoreFile> candidates)

When a user manually triggers a compaction, this follows the same logic as a normal compaction check.  when a user manually triggers a major compaction, something similar happens.  Putting this all together:

1. If a user triggers a major compaction, this is checked against a max files threshold (hbase.hstore.compaction.max). If the number of storefiles to compact is > max files, then we downgrade to a minor compaction
2. If we are in a minor compaction, we do the following checks:
   a. If the file is less than a minimum size (hbase.hstore.compaction.min.size) we automatically include it
   b. Otherwise, we check how the size compares to the next largest size.  based on hbase.hstore.compaction.ratio.  
  c. If the number of files included is less than a minimum count (hbase.hstore.compaction.min) then don't compact.
In many of the exit strategies, we aren't seeing an error message.
The net-net of this is that if we have a mix of very large and very small files, we may end up having too many files to do a major compact, but too few files to do a minor compact.

I'm trying to go through and see if I'm understanding things correctly, but this seems like the bug

To put it another way
2012-05-02 20:09:36,389 DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitThread: Large Compaction requested: regionName=str,44594594594594592,1334939064521.f7aed25b55d4d7988af763bede9ce74e., store
Name=c, fileCount=15, fileSize=1.5g (20.2k, 362.5m, 155.3k, 3.0m, 30.7k, 361.2m, 6.9m, 4.7m, 14.7k, 363.4m, 30.9m, 3.2m, 7.3k, 362.9m, 23.5m), priority=-9, time=3175046817624398; Because: Recursive enqueue; compaction_queue=(59:0), split_queue=0

When we had a minimum compaction size of 128M, and default settings for hbase.hstore.compaction.min,hbase.hstore.compaction.max,hbase.hstore.compaction.ratio, we were not getting a compaction to run even if we ran
major_compact 'str,44594594594594592,1334939064521.f7aed25b55d4d7988af763bede9ce74e.' from the ruby shell.  Note that we had many tiny regions (20k, 155k, 3m, 30k,..) and several large regions (362.5m,361.2m,363.4m,362.9m).  I think the bimodal nature of the sizes prevented us from doing a compaction.
I'm not 100% sure where this errored out because when I manually triggered a compaction, I did not see
'      // if we don't have enough files to compact, just wait             
      if (filesToCompact.size() < this.minFilesToCompact) {              
        if (LOG.isDebugEnabled()) {                                      
          LOG.debug(""Skipped compaction of "" + this.storeNameStr         
            + "".  Only "" + (end - start) + "" file(s) of size ""           
            + StringUtils.humanReadableInt(totalSize)                    
            + "" have met compaction criteria."");                         
        }                                                                
' 
being printed in the logs (and I know DEBUG logging was enabled because I saw this elsewhere).  

I'd be happy with better error messages when we decide not to compact for user enabled compactions.
I'd also like to see some override that says ""user triggered major compaction always occurs"", but maybe that's a bad idea for other reasons.
",,,1,,,,,
HBASE-6041,"This is 0.90 only.

2012-05-04 14:27:57,913 FATAL org.apache.hadoop.hbase.master.HMaster: Unhandled exception. Starting shutdown.
java.lang.NullPointerException
	at org.apache.hadoop.hbase.master.AssignmentManager.regionOnline(AssignmentManager.java:731)
	at org.apache.hadoop.hbase.master.AssignmentManager.processFailover(AssignmentManager.java:215)
	at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:419)
	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:293)
2012-05-04 14:27:57,914 INFO org.apache.hadoop.hbase.master.HMaster: Aborting
2012-05-04 14:27:57,915 INFO org.apache.hadoop.ipc.HBaseServer: Stopping server on 1433",,,,,,,,
HBASE-6043,"There seem to be some tuning settings in which manually triggered major compactions will do nothing, including loggic

From Store.java in the function
  List<StoreFile> compactSelection(List<StoreFile> candidates)

When a user manually triggers a compaction, this follows the same logic as a normal compaction check.  when a user manually triggers a major compaction, something similar happens.  Putting this all together:

1. If a user triggers a major compaction, this is checked against a max files threshold (hbase.hstore.compaction.max). If the number of storefiles to compact is > max files, then we downgrade to a minor compaction
2. If we are in a minor compaction, we do the following checks:
   a. If the file is less than a minimum size (hbase.hstore.compaction.min.size) we automatically include it
   b. Otherwise, we check how the size compares to the next largest size.  based on hbase.hstore.compaction.ratio.  
  c. If the number of files included is less than a minimum count (hbase.hstore.compaction.min) then don't compact.
In many of the exit strategies, we aren't seeing an error message.
The net-net of this is that if we have a mix of very large and very small files, we may end up having too many files to do a major compact, but too few files to do a minor compact.

I'm trying to go through and see if I'm understanding things correctly, but this seems like the bug

To put it another way
2012-05-02 20:09:36,389 DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitThread: Large Compaction requested: regionName=str,44594594594594592,1334939064521.f7aed25b55d4d7988af763bede9ce74e., store
Name=c, fileCount=15, fileSize=1.5g (20.2k, 362.5m, 155.3k, 3.0m, 30.7k, 361.2m, 6.9m, 4.7m, 14.7k, 363.4m, 30.9m, 3.2m, 7.3k, 362.9m, 23.5m), priority=-9, time=3175046817624398; Because: Recursive enqueue; compaction_queue=(59:0), split_queue=0

When we had a minimum compaction size of 128M, and default settings for hbase.hstore.compaction.min,hbase.hstore.compaction.max,hbase.hstore.compaction.ratio, we were not getting a compaction to run even if we ran
major_compact 'str,44594594594594592,1334939064521.f7aed25b55d4d7988af763bede9ce74e.' from the ruby shell.  Note that we had many tiny regions (20k, 155k, 3m, 30k,..) and several large regions (362.5m,361.2m,363.4m,362.9m).  I think the bimodal nature of the sizes prevented us from doing a compaction.
I'm not 100% sure where this errored out because when I manually triggered a compaction, I did not see
'      // if we don't have enough files to compact, just wait             
      if (filesToCompact.size() < this.minFilesToCompact) {              
        if (LOG.isDebugEnabled()) {                                      
          LOG.debug(""Skipped compaction of "" + this.storeNameStr         
            + "".  Only "" + (end - start) + "" file(s) of size ""           
            + StringUtils.humanReadableInt(totalSize)                    
            + "" have met compaction criteria."");                         
        }                                                                
' 
being printed in the logs (and I know DEBUG logging was enabled because I saw this elsewhere).  

I'd be happy with better error messages when we decide not to compact for user enabled compactions.
I'd also like to see some override that says ""user triggered major compaction always occurs"", but maybe that's a bad idea for other reasons.
",,,1,,,,,
HBASE-6046,"1> ZK Session timeout in the hmaster leads to bulk assignment though all the RSs are online.
2> While doing bulk assignment, if the master again goes down & restart(or backup comes up) all the node created in the ZK will now be tried to reassign to the new RSs. This is leading to double assignment.

we had 2800 regions, among this 1900 region got double assignment, taking the region count to 4700. ",,,,,,,,
HBASE-6047,"the public method 'has(byte [] family, byte [] qualifier)' internally invoked the private method 'has(byte [] family, byte [] qualifier, long ts, byte [] value, boolean ignoreTS, boolean ignoreValue)' with 'value=new byte[0], ignoreTS=true, ignoreValue=true', but there's a logical error in the body, it'll enter the block
{code}
else if (ignoreValue) {
      for (KeyValue kv: list) {
        if (Arrays.equals(kv.getFamily(), family) && Arrays.equals(kv.getQualifier(), qualifier)
            && kv.getTimestamp() == ts) {
          return true;
        }
      }
    }
{code}
the expression 'kv.getTimestamp() == ts' in the if conditions should only exist when 'ignoreTS=false', otherwise, the following code will return false!
{code}
Put put = new Put(Bytes.toBytes(""row-01""));
put.add(Bytes.toBytes(""family-01""), Bytes.toBytes(""qualifier-01""),
				1234567L, Bytes.toBytes(""value-01""));
System.out.println(put.has(Bytes.toBytes(""family-01""),
				Bytes.toBytes(""qualifier-01"")));
{code}",,,,,,,,
HBASE-6048,"Table Scan is failing if offheap cache enabled.

{noformat}
2012-05-18 20:03:38,446 DEBUG org.apache.hadoop.hbase.io.hfile.HFileWriterV2: Initialized with CacheConfig:enabled [cacheDataOnRead=true] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheCompressed=false]
2012-05-18 20:03:38,446 INFO org.apache.hadoop.hbase.regionserver.StoreFile: Delete Family Bloom filter type for hdfs://10.18.40.217:9000/hbase/ufdr/1d4656fd417a07c9171a38b8f4d08510/.tmp/03742024b28f443bb63cfc338d4ca422: CompoundBloomFilterWriter
2012-05-18 20:04:25,576 DEBUG org.apache.hadoop.hbase.io.hfile.LruBlockCache: Block cache LRU eviction started; Attempting to free 120.57 MB of total=1020.57 MB
2012-05-18 20:04:25,655 DEBUG org.apache.hadoop.hbase.io.hfile.LruBlockCache: Block cache LRU eviction completed; freed=120.82 MB, total=907.89 MB, single=1012.11 MB, multi=6.12 MB, memory=0 KB
2012-05-18 20:04:25,733 ERROR org.apache.hadoop.hbase.regionserver.HRegionServer: Failed openScanner
java.lang.IllegalStateException: Schema metrics requested before table/CF name initialization: {""tableName"":""null"",""cfName"":""null""}
	at org.apache.hadoop.hbase.regionserver.metrics.SchemaConfigured.getSchemaMetrics(SchemaConfigured.java:182)
	at org.apache.hadoop.hbase.io.hfile.LruBlockCache.updateSizeMetrics(LruBlockCache.java:310)
	at org.apache.hadoop.hbase.io.hfile.LruBlockCache.cacheBlock(LruBlockCache.java:274)
	at org.apache.hadoop.hbase.io.hfile.LruBlockCache.cacheBlock(LruBlockCache.java:293)
	at org.apache.hadoop.hbase.io.hfile.DoubleBlockCache.getBlock(DoubleBlockCache.java:102)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2.readBlock(HFileReaderV2.java:296)
	at org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader.seekToDataBlock(HFileBlockIndex.java:213)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.seekTo(HFileReaderV2.java:455)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.seekTo(HFileReaderV2.java:475)
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seekAtOrAfter(StoreFileScanner.java:226)
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seek(StoreFileScanner.java:145)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.<init>(StoreScanner.java:130)
	at org.apache.hadoop.hbase.regionserver.Store.getScanner(Store.java:2001)
	at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.<init>(HRegion.java:3274)
	at org.apache.hadoop.hbase.regionserver.HRegion.instantiateRegionScanner(HRegion.java:1604)
	at org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:1596)
	at org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:1572)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.openScanner(HRegionServer.java:2310)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1376)
2012-05-18 20:04:25,828 ERROR org.apache.hadoop.hbase.regionserver.HRegionServer: Failed openScanner
{noformat}",,1,,,,,,
HBASE-6050,"The scenario is like this
-> A region is getting splitted.
-> The master is still not processed the split .
-> Region server goes down.
-> Split log manager starts splitting the logs and creates the recovered.edits in the splitlog path.
-> CJ starts and deletes the entry from META and also just completes the deletion of the region dir.
-> in hlogSplitter on final step we rename the recovered.edits to come under the regiondir.
There if the regiondir doesnot exist we tend to create and then add the recovered.edits.

Because of this HBCK thinks it to be an orphan region because we have the regiondir but with no regioninfo.
Ideally cluster is fine but we it is misleading.
{code}
        } else {
          Path dstdir = dst.getParent();
          if (!fs.exists(dstdir)) {
            if (!fs.mkdirs(dstdir)) LOG.warn(""mkdir failed on "" + dstdir);
          }
        }
        fs.rename(src, dst);
        LOG.debug("" moved "" + src + "" => "" + dst);
      } else {
        LOG.debug(""Could not move recovered edits from "" + src +
            "" as it doesn't exist"");
      }
    }
    archiveLogs(null, corruptedLogs, processedLogs,
        oldLogDir, fs, conf);
{code}",,1,,,,,,
HBASE-6059,"When we replay recovered edits, we used the minSeqId of Store, It may cause deleted data appeared again.

Let's see how it happens. Suppose the region with two families(cf1,cf2)

1.put one data to the region (put r1,cf1:q1,v1)

2.move the region from server A to server B.

3.delete the data put by step 1(delete r1)

4.flush this region.

5.make major compaction for this region

6.move the region from server B to server A.

7.Abort server A

8.After the region is online, we could get the deleted data(r1,cf1:q1,v1)
(When we replay recovered edits, we used the minSeqId of Store, because cf2 has no store files, so its seqId is 0, so the edit log of put data will be replayed to the region)


",,,,,,,,
HBASE-6060,"we have seen a pattern in tests, that the regions are stuck in OPENING state for a very long time when the region server who is opening the region fails. My understanding of the process: 
 
 - master calls rs to open the region. If rs is offline, a new plan is generated (a new rs is chosen). RegionState is set to PENDING_OPEN (only in master memory, zk still shows OFFLINE). See HRegionServer.openRegion(), HMaster.assign()
 - RegionServer, starts opening a region, changes the state in znode. But that znode is not ephemeral. (see ZkAssign)
 - Rs transitions zk node from OFFLINE to OPENING. See OpenRegionHandler.process()
 - rs then opens the region, and changes znode from OPENING to OPENED
 - when rs is killed between OPENING and OPENED states, then zk shows OPENING state, and the master just waits for rs to change the region state, but since rs is down, that wont happen. 
 - There is a AssignmentManager.TimeoutMonitor, which does exactly guard against these kind of conditions. It periodically checks (every 10 sec by default) the regions in transition to see whether they timedout (hbase.master.assignment.timeoutmonitor.timeout). Default timeout is 30 min, which explains what you and I are seeing. 
 - ServerShutdownHandler in Master does not reassign regions in OPENING state, although it handles other states. 

Lowering that threshold from the configuration is one option, but still I think we can do better. 

Will investigate more. ",,1,1,,,,,
HBASE-6065,"After completing flush region, we will append a log edit in the hlog file through HLog#completeCacheFlush.

{code}
public void completeCacheFlush(final byte [] encodedRegionName,
      final byte [] tableName, final long logSeqId, final boolean isMetaRegion)
{
...
HLogKey key = makeKey(encodedRegionName, tableName, logSeqId,
            System.currentTimeMillis(), HConstants.DEFAULT_CLUSTER_ID);
...
}
{code}



when we make the hlog key, we use the seqId from the parameter, and it is generated by HLog#startCacheFlush,
Here, we may append a lower seq id edit than the last edit in the hlog file.

If it is the last edit log in the file, it may cause data loss.
because 
{code}

HRegion#replayRecoveredEditsIfAny{
...
maxSeqId = Math.abs(Long.parseLong(fileName));
      if (maxSeqId <= minSeqId) {
        String msg = ""Maximum sequenceid for this log is "" + maxSeqId
            + "" and minimum sequenceid for the region is "" + minSeqId
            + "", skipped the whole file, path="" + edits;
        LOG.debug(msg);
        continue;
      }
...
}
{code}

We may skip the splitted log file, because we use the lase edit's seq id as its file name, and consider this seqId as the max seq id in this log file.",,,,,,,,
HBASE-6067,"While running Hive(0.9.0) query over HBase(0.94.0) with hive-hbase-handler, there always throws a Null Pointer Exception on Scanner object. Since the TableInputFormatBase#createRecordReader() missed the initialization of TableRecordReader object. The scanner will be null in that case. This issue causes Hive query fails.",,1,,,,,,
HBASE-6068,"In case of secure cluster, we allow the HBase clients to read the zk nodes by providing the global read permissions to all for certain nodes. These nodes are the master address znode, root server znode and the clusterId znode. In ZKUtil.createACL() , we can see these node names are specially handled.
But there are some other client side admin APIs which makes a read call into the zookeeper from the client. This include the isTableEnabled() call (May be some other. I have seen this).  Here the client directly reads a node in the zookeeper ( node created for this table ) and the data is matched to know whether this is enabled or not.
Now in secure cluster case any client can read zookeeper nodes which it needs for its normal operation like the master address and root server address.  But what if the client calls this API? [isTableEnaled () ].",1,,,,,,,
HBASE-6069,"While running Hive(0.9.0) query over HBase(0.94.0) with hive-hbase-handler, there always throws a Null Pointer Exception on Scanner object. Since the TableInputFormatBase#createRecordReader() missed the initialization of TableRecordReader object. The scanner will be null in that case. This issue causes Hive query fails.",,,,,,,,
HBASE-6070,"We tried to address the problems in Master restart and RS restart while SPLIT region is in progress as part of HBASE-5806.
While doing some more we found still there is one race condition.
-> Split has just started and the znode is in RS_SPLIT state.
-> RS goes down.
-> First call back for SSH comes.
-> As part of the fix for HBASE-5806 SSH knows that some region is in RIT.
-> But now nodeDeleted event comes for the SPLIt node and there we try to delete the RIT.
-> After this we try to see in the SSH whether any node is in RIT.  As we dont find the region in RIT the region is never assigned.

When we fixed HBASE-5806 step 6 happened first and then step 5 happened.  So we missed it.  Now we found that. Will come up with a patch shortly.",,,,,,,,
HBASE-6085,"I often hit this problem where the client request is just ignored* by the server.
The logs at the server doesn't reflect anything about the client, and then it does process the request in the next trial.",1,,,,,,,
HBASE-6088,"Region splitting not happened for long time due to ZK exception while creating RS_ZK_SPLITTING node

{noformat}
2012-05-24 01:45:41,363 INFO org.apache.zookeeper.ClientCnxn: Client session timed out, have not heard from server in 26668ms for sessionid 0x1377a75f41d0012, closing socket connection and attempting reconnect
2012-05-24 01:45:41,464 WARN org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper exception: org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/unassigned/bd1079bf948c672e493432020dc0e144
{noformat}

{noformat}
2012-05-24 01:45:43,300 DEBUG org.apache.hadoop.hbase.regionserver.wal.HLog: cleanupCurrentWriter  waiting for transactions to get synced  total 189377 synced till here 189365
2012-05-24 01:45:48,474 INFO org.apache.hadoop.hbase.regionserver.SplitRequest: Running rollback/cleanup of failed split of ufdr,011365398471659,1337823505339.bd1079bf948c672e493432020dc0e144.; Failed setting SPLITTING znode on ufdr,011365398471659,1337823505339.bd1079bf948c672e493432020dc0e144.
java.io.IOException: Failed setting SPLITTING znode on ufdr,011365398471659,1337823505339.bd1079bf948c672e493432020dc0e144.
	at org.apache.hadoop.hbase.regionserver.SplitTransaction.createDaughters(SplitTransaction.java:242)
	at org.apache.hadoop.hbase.regionserver.SplitTransaction.execute(SplitTransaction.java:450)
	at org.apache.hadoop.hbase.regionserver.SplitRequest.run(SplitRequest.java:67)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.zookeeper.KeeperException$BadVersionException: KeeperErrorCode = BadVersion for /hbase/unassigned/bd1079bf948c672e493432020dc0e144
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:115)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1246)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.setData(RecoverableZooKeeper.java:321)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.setData(ZKUtil.java:659)
	at org.apache.hadoop.hbase.zookeeper.ZKAssign.transitionNode(ZKAssign.java:811)
	at org.apache.hadoop.hbase.zookeeper.ZKAssign.transitionNode(ZKAssign.java:747)
	at org.apache.hadoop.hbase.regionserver.SplitTransaction.transitionNodeSplitting(SplitTransaction.java:919)
	at org.apache.hadoop.hbase.regionserver.SplitTransaction.createNodeSplitting(SplitTransaction.java:869)
	at org.apache.hadoop.hbase.regionserver.SplitTransaction.createDaughters(SplitTransaction.java:239)
	... 5 more
2012-05-24 01:45:48,476 INFO org.apache.hadoop.hbase.regionserver.SplitRequest: Successful rollback of failed split of ufdr,011365398471659,1337823505339.bd1079bf948c672e493432020dc0e144.
{noformat}


{noformat}
2012-05-24 01:47:28,141 ERROR org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper: Node /hbase/unassigned/bd1079bf948c672e493432020dc0e144 already exists and this is not a retry
2012-05-24 01:47:28,142 INFO org.apache.hadoop.hbase.regionserver.SplitRequest: Running rollback/cleanup of failed split of ufdr,011365398471659,1337823505339.bd1079bf948c672e493432020dc0e144.; Failed create of ephemeral /hbase/unassigned/bd1079bf948c672e493432020dc0e144
java.io.IOException: Failed create of ephemeral /hbase/unassigned/bd1079bf948c672e493432020dc0e144
	at org.apache.hadoop.hbase.regionserver.SplitTransaction.createNodeSplitting(SplitTransaction.java:865)
	at org.apache.hadoop.hbase.regionserver.SplitTransaction.createDaughters(SplitTransaction.java:239)
	at org.apache.hadoop.hbase.regionserver.SplitTransaction.execute(SplitTransaction.java:450)
	at org.apache.hadoop.hbase.regionserver.SplitRequest.run(SplitRequest.java:67)
{noformat}

Due to the above exception, region splitting was failing contineously more than 5hrs",,,,,,,,
HBASE-6089,AM.regions map is parallely accessed in SSH and Master initialization leading to ConcurrentModificationException.,,,,,,,,
HBASE-6103,"The EXEC action currently exists as only a placeholder in access control. It should really be used to enforce access to coprocessor endpoint RPC calls, which are currently unrestricted.
How the ACLs to support this would be modeled deserves some discussion:
Should access be scoped to a specific table and CoprocessorProtocol extension?
Should it be possible to grant access to a CoprocessorProtocol implementation globally (regardless of table)?
Are per-method restrictions necessary?
Should we expose hooks available to endpoint implementors so that they could additionally apply their own permission checks? Some CP endpoints may want to require READ permissions, others may want to enforce WRITE, or READ + WRITE.
To apply these kinds of checks we would also have to extend the RegionObserver interface to provide hooks wrapping HRegion.exec().",1,,,,,,,
HBASE-6104,"The EXEC action currently exists as only a placeholder in access control. It should really be used to enforce access to coprocessor endpoint RPC calls, which are currently unrestricted.
How the ACLs to support this would be modeled deserves some discussion:
Should access be scoped to a specific table and CoprocessorProtocol extension?
Should it be possible to grant access to a CoprocessorProtocol implementation globally (regardless of table)?
Are per-method restrictions necessary?
Should we expose hooks available to endpoint implementors so that they could additionally apply their own permission checks? Some CP endpoints may want to require READ permissions, others may want to enforce WRITE, or READ + WRITE.
To apply these kinds of checks we would also have to extend the RegionObserver interface to provide hooks wrapping HRegion.exec().",1,,,,,,,
HBASE-6107,"Sometimes, master web UI shows the distributed log splitting is going on, waiting for one last task to be done.  However, in ZK, there is no task under /hbase/splitlog at all.",,,,,,,,
HBASE-6109,"Sometimes, master web UI shows the distributed log splitting is going on, waiting for one last task to be done.  However, in ZK, there is no task under /hbase/splitlog at all.",,,1,,,,,
HBASE-6115,"Lets suppose we have two region servers RS1 and RS2.
If region server (RS1) having root and meta regions went down, master will assign them to another region server RS2. At that time recieved NullPointerException.
{code}
2012-05-04 20:19:52,912 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation: Looked up root region location, connection=org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@25de152f; serverName=
2012-05-04 20:19:52,914 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation: Looked up root region location, connection=org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@25de152f; serverName=
2012-05-04 20:19:52,916 WARN org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Exception running postOpenDeployTasks; region=1028785192
java.lang.NullPointerException
at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.processBatchCallback(HConnectionManager.java:1483)
at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.processBatch(HConnectionManager.java:1367)
at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:945)
at org.apache.hadoop.hbase.client.HTable.doPut(HTable.java:801)
at org.apache.hadoop.hbase.client.HTable.put(HTable.java:776)
at org.apache.hadoop.hbase.catalog.MetaEditor.put(MetaEditor.java:98)
at org.apache.hadoop.hbase.catalog.MetaEditor.putToCatalogTable(MetaEditor.java:88)
at org.apache.hadoop.hbase.catalog.MetaEditor.updateLocation(MetaEditor.java:259)
at org.apache.hadoop.hbase.catalog.MetaEditor.updateMetaLocation(MetaEditor.java:221)
at org.apache.hadoop.hbase.regionserver.HRegionServer.postOpenDeployTasks(HRegionServer.java:1625)
at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler$PostOpenDeployTasksThread.run(OpenRegionHandler.java:241)
2012-05-04 20:19:52,920 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Closing .META.,,1.1028785192: disabling compactions & flushes
2012-05-04 20:19:52,920 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Updates disabled for region .META.,,1.1028785192

{code}",,,,,,,,
HBASE-6122,"-> Active master gets ZK expiry exception.
-> Backup master becomes active.
-> The previous active master retries and becomes the back up master.
Now when the new active master goes down and the current back up master comes up, it goes down again with the",,,,,,,,
HBASE-6134,"Sometimes, master web UI shows the distributed log splitting is going on, waiting for one last task to be done.  However, in ZK, there is no task under /hbase/splitlog at all.",,,1,,,,,
HBASE-6147,"We are facing few issues in the master restart and SSH going in parallel.
Chunhui also suggested that we need to rework on this part.  This JIRA is aimed at solving all such possibilities of region assignment inconsistency",,,,,,,,
HBASE-6153,"I had a RS crash with the following:

2012-05-31 18:34:42,534 DEBUG org.apache.hadoop.hbase.regionserver.Store: Renaming flushed file at hdfs://ip-10-140-14-134.ec2.internal:8020/apps/hbase/data/TestLoadAndVerify_1338488017181/8974506aa04c5a04e5cc23c11de0039d/.tmp/294a7a31f04949b8bf07682a43157b35 to hdfs://ip-10-140-14-134.ec2.internal:8020/apps/hbase/data/TestLoadAndVerify_1338488017181/8974506aa04c5a04e5cc23c11de0039d/f1/294a7a31f04949b8bf07682a43157b35
2012-05-31 18:34:42,536 WARN org.apache.hadoop.hbase.regionserver.Store: Unable to rename hdfs://ip-10-140-14-134.ec2.internal:8020/apps/hbase/data/TestLoadAndVerify_1338488017181/8974506aa04c5a04e5cc23c11de0039d/.tmp/294a7a31f04949b8bf07682a43157b35 to hdfs://ip-10-140-14-134.ec2.internal:8020/apps/hbase/data/TestLoadAndVerify_1338488017181/8974506aa04c5a04e5cc23c11de0039d/f1/294a7a31f04949b8bf07682a43157b35
2012-05-31 18:34:42,541 FATAL org.apache.hadoop.hbase.regionserver.HRegionServer: ABORTING region server ip-10-68-7-146.ec2.internal,60020,1338343120038: Replay of HLog required. Forcing server shutdown
org.apache.hadoop.hbase.DroppedSnapshotException: region: TestLoadAndVerify_1338488017181,\x15\xD9\x01\x00\x00\x00\x00\x00/000087_0,1338491364569.8974506aa04c5a04e5cc23c11de0039d.
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1288)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1172)
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:1114)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:400)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:374)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.run(MemStoreFlusher.java:243)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.FileNotFoundException: File does not exist: /apps/hbase/data/TestLoadAndVerify_1338488017181/8974506aa04c5a04e5cc23c11de0039d/f1/294a7a31f04949b8bf07682a43157b35
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:1901)
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.<init>(DFSClient.java:1892)
        at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:636)
        at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:154)
        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:427)
        at org.apache.hadoop.hbase.io.hfile.HFile.createReader(HFile.java:387)
        at org.apache.hadoop.hbase.regionserver.StoreFile$Reader.<init>(StoreFile.java:1008)
        at org.apache.hadoop.hbase.regionserver.StoreFile.open(StoreFile.java:470)
        at org.apache.hadoop.hbase.regionserver.StoreFile.createReader(StoreFile.java:548)
        at org.apache.hadoop.hbase.regionserver.Store.internalFlushCache(Store.java:595)


On the NameNode logs:
2012-05-31 18:34:42,588 WARN org.apache.hadoop.hdfs.StateChange: DIR* FSDirectory.unprotectedRenameTo: failed to rename /apps/hbase/data/TestLoadAndVerify_1338488017181/8974506aa04c5a04e5cc23c11de0039d/.tmp/294a7a31f04949b8bf07682a43157b35 to /apps/hbase/data/TestLoadAndVerify_1338488017181/8974506aa04c5a04e5cc23c11de0039d/f1/294a7a31f04949b8bf07682a43157b35 because destination's parent does not exist


I haven't looked deeply yet but I guess it is a race of some sort.
",,,,,,,,
HBASE-6158,"If a table is creates with either 'merges' or 'splits' as one of the Column Family name it can never be flushed to the disk even though the table creation (and data population) succeeds.

The reason for this is that these two are used as temporary directory names inside the region folder or merge and splits respectively and hence conflicts with the directories created for CF with same name.

A simple fix would be to uses "".merges' and "".splits"" as the working folder (patch attached). This will also be consistent with other work folder names. An alternate fix would be to declare these words (and other similar) as reserve words and throw exception when they are used. However, I do find the alternate approach as unnecessarily restrictive.",,1,,,,,,
HBASE-6160,"HBASE-5986 fixed and issue, where the client sees the META entry for the parent, but not the children. However, after the fix, we have seen the following issue in tests: 

Region A is split to -> B, C
Region B is split to -> D, E

After some time, META entry for B is deleted since it is not needed anymore, but META entry for Region A stays in META (C still refers it). In this case, the client throws RegionOfflineException for B. ",,,,,,,,
HBASE-6165,When restarting a large set of regions on a reasonably small cluster the replication from another cluster tied up every xceiver meaning nothing could be onlined.,,,,,,,,
HBASE-6195,"There are two problems in increment() now:
First:
I see that the timestamp(the variable now) in HRegion's Increment() is generated before got the rowLock, so when there are multi-thread increment the same row, although it generate earlier, it may got the lock later. Because increment just store one version, so till now, the result will still be right.

When the region is flushing, these increment will read the kv from snapshot and memstore with whose timestamp is larger, and write it back to memstore. If the snapshot's timestamp larger than the memstore, the increment will got the old data and then do the increment, it's wrong.

Secondly:
Also there is a risk in increment. Because it writes the memstore first and then HLog, so if it writes HLog failed, the client will also read the incremented value.",,,,,,,,
HBASE-6197,"Like the HBASE-6195, when flushing the append thread will read out the old value for the larger timestamp in snapshot and smaller timestamp in memstore.

We Should make the first-in-thread generates the smaller timestamp.",,1,,,,,,
HBASE-6200,"As reported by Desert Rose on IRC and on the ML, {{Result}} has a weird behavior when some families share the same prefix. He posted a link to his code to show how it fails, http://pastebin.com/7TBA1XGh

Basically {{KeyComparator.compareWithoutRow}} doesn't differentiate families and qualifiers so ""f:a"" is said to be bigger than ""f1:"", which is false. Then what happens is that the KVs are returned in the right order from the RS but then doing {{Result.binarySearch}} it uses {{KeyComparator.compareWithoutRow}} which has a different sorting so the end result is undetermined.

I added some debug and I can see that the data is returned in the right order but {{Arrays.binarySearch}} returned the wrong KV, which is then verified agains the passed family and qualifier which fails so null is returned.

I don't know how frequent it is for users to have families with the same prefix, but those that do have that and that use those families at the same time will have big correctness issues. This is why I mark this as a blocker.",,1,,,,,,
HBASE-6202,"APIs provided in AccessController are authorized against global-admin permissions. Instead we need to check for table-admin level permissions.
Edit: Append operation also has no authorization check. We can update it together.",,,,,,,,
HBASE-6207,"APIs provided in AccessController are authorized against global-admin permissions. Instead we need to check for table-admin level permissions.
Edit: Append operation also has no authorization check. We can update it together.",,,1,,,,,
HBASE-6205,"APIs provided in AccessController are authorized against global-admin permissions. Instead we need to check for table-admin level permissions.
Edit: Append operation also has no authorization check. We can update it together.",,,,,,,,
HBASE-6209,"APIs provided in AccessController are authorized against global-admin permissions. Instead we need to check for table-admin level permissions.
Edit: Append operation also has no authorization check. We can update it together.",1,,,,,,,
HBASE-6217,"APIs provided in AccessController are authorized against global-admin permissions. Instead we need to check for table-admin level permissions.
Edit: Append operation also has no authorization check. We can update it together.",,,1,,,,,
HBASE-6222,"Saw an interesting article: http://www.fiercegovernmentit.com/story/sasc-accumulo-language-pro-open-source-say-proponents/2012-06-14
""The Senate Armed Services Committee version of the fiscal 2013 national defense authorization act (S. 3254) would require DoD agencies to foreswear the Accumulo NoSQL database after Sept. 30, 2013, unless the DoD CIO certifies that there exists either no viable commercial open source database with security features comparable to [Accumulo] (such as the HBase or Cassandra databases)...""
Not sure what a 'commercial open source database' is, and I'm not sure whats going on in the article, but tra-la-la'ing, if we had per-KeyValue 'security' like Accumulo's, we might put ourselves in the running for federal contributions?",1,,,,,,,
HBASE-6227,"In AssignmentManager#processDeadServersAndRegionsInTransition, if servershutdownhandler is processing and master consider it cluster startup, master will assign all user regions, however, servershutdownhandler has not completed splitting logs.

Let me describe it in more detail.

Suppose there are two regionservers A1 and B1, A1 carried META and ROOT

1.master restart and completed assignRootAndMeta

2.A1 and B1 are both restarted, new regionservers are A2 and B2

3.SSH which processed for A1 completed assigning ROOT and META

4.master do rebuilding user regions and no regions added to master's region list

5.master consider it as a cluster startup, and assign all user regions

6.SSH which processed for B1 start to split B1's logs

7.All regions' data carried by B1 would loss.",,,,,,,,
HBASE-6228,"First, how fixup daughters twice happen?
1.we will fixupDaughters at the last of HMaster#finishInitialization
2.ServerShutdownHandler will fixupDaughters when reassigning region through ServerShutdownHandler#processDeadRegion

When fixupDaughters, we will added daughters to .META., but it coudn't prevent the above case, because FindDaughterVisitor.

The detail is as the following:
Suppose region A is a splitted parent region, and its daughter region B is missing

1.First, ServerShutdownHander thread fixup daughter, so add daughter region B to .META. with serverName=null, and assign the daughter.

2.Then, Master's initialization thread will also find the daughter region B is missing and assign it. It is because FindDaughterVisitor consider daughter is missing if its serverName=null",,,1,,,,,
HBASE-6229,In case of assign from EnableTableHandler table state is ENABLING. Any how EnableTableHandler will set ENABLED after assigning all the the table regions. If we try to set to ENABLED directly then client api may think ENABLE table is completed. When we have a case like all the regions are added directly into META and we call assignRegion then we need to make the table ENABLED.  Hence in such case the table will not be in ENABLING or ENABLED state.,,,,,,,,
HBASE-6236,"While building the .META. and \-ROOT\- from FS data alone (HBASE-4377), hbck tries to move the existing .META. and \-ROOT\- directories to a backup folder.

This backup folder is created at the same level as the base HBase folder (e.g. /hbase-xxxxxx if the base HBase folder is '/hbase').

In a federated HDFS like ViewFS and other similar FS implementations, it is not possible to rename files/directories across namespace volumes (ViewFS guide section 3.5) and as a result hbck crashes.

A solution to this problem is to create the backup directory under the folder where HBase base folder has been mounted. This ensures that source and destination of rename operation are on the same namespace volume.

Patch for 0.94 and trunk is attached for review. The patch modifies the location of the backup directory from '/hbase-xxxxxxx' to '/hbase/.hbcktmp-xxxxxxx'",,1,,,,,,
HBASE-6238,User is not able to perform authorized operations on Meta.,1,,,,,,,
HBASE-6240,"I found this issue trying to run YCSB on 0.94, I don't think it exists on any other branch. I believe that this was introduced in HBASE-5058 ""Allow HBaseAdmin to use an existing connection"".

The issue is that in HCM.getMaster it does this recipe:

 # Check if the master is null and runs (if so, return)
 # Grab a lock on masterLock
 # nullify this.master
 # try to get a new master

The issue happens at 3, it should re-run 1 since while you're waiting on the lock someone else could have already fixed it for you. What happens right now is that the threads are all able to set the master to null before others are able to get out of getMaster and it's a complete mess.

Figuring it out took me some time because it doesn't manifest itself right away, silent retries are done in the background. Basically the first clue was this:

{noformat}
Error doing get: org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=10, exceptions:
Tue Jun 19 23:40:46 UTC 2012, org.apache.hadoop.hbase.client.HTable$3@571a4bd4, java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@2eb0a3f5 closed
Tue Jun 19 23:40:47 UTC 2012, org.apache.hadoop.hbase.client.HTable$3@571a4bd4, java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@2eb0a3f5 closed
Tue Jun 19 23:40:48 UTC 2012, org.apache.hadoop.hbase.client.HTable$3@571a4bd4, java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@2eb0a3f5 closed
Tue Jun 19 23:40:49 UTC 2012, org.apache.hadoop.hbase.client.HTable$3@571a4bd4, java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@2eb0a3f5 closed
Tue Jun 19 23:40:51 UTC 2012, org.apache.hadoop.hbase.client.HTable$3@571a4bd4, java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@2eb0a3f5 closed
Tue Jun 19 23:40:53 UTC 2012, org.apache.hadoop.hbase.client.HTable$3@571a4bd4, java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@2eb0a3f5 closed
Tue Jun 19 23:40:57 UTC 2012, org.apache.hadoop.hbase.client.HTable$3@571a4bd4, java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@2eb0a3f5 closed
Tue Jun 19 23:41:01 UTC 2012, org.apache.hadoop.hbase.client.HTable$3@571a4bd4, java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@2eb0a3f5 closed
Tue Jun 19 23:41:09 UTC 2012, org.apache.hadoop.hbase.client.HTable$3@571a4bd4, java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@2eb0a3f5 closed
Tue Jun 19 23:41:25 UTC 2012, org.apache.hadoop.hbase.client.HTable$3@571a4bd4, java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@2eb0a3f5 closed
{noformat}

This was caused by the little dance up in HBaseAdmin where it deletes ""stale"" connections... which are not stale at all.",,,,,,,,
HBASE-6246,"{code}
    if (destServerName == null || destServerName.length == 0) {
      LOG.info(""Passed destination servername is null/empty so "" +
        ""choosing a server at random"");
      this.assignmentManager.clearRegionPlan(hri);
      // Unassign will reassign it elsewhere choosing random server.
      this.assignmentManager.unassign(hri);
{code}
I think we should go through security to see if there is sufficient permissions to do this operation?",1,,,,,,,
HBASE-6248,"InfoServer.getWebAppsPath() unsafely assumes that any instance of the string ""master"" in the full path to hbase-webapps can be truncated. This breaks in the case where hbase is installed in a directory such as ""/my/hbasemaster/"".

The result is that Jetty initialization will fail since the master determines an incorrect path to hbase-webapps. The master still runs but the web UI returns 503.

I have a patch for this problem that I'll upload soon.",,1,,,,,,
HBASE-6253,"Currently HTableDescriptor.isLegalTableName API doesn't check for the acl table name, due to this user can able to disable/enable/drop/create the acl table. ",1,,,,,,,
HBASE-6254,"Execution of Deletes constructed with thousands of calls to Delete.deleteColumn(family, qualifier) are very expensive and slow.

On our (quiet) cluster, a Delete w/ 20k qualifiers took about 13s to complete (as measured by client).

When 10 such Deletes were sent to the cluster via HTable.delete(List<Delete>), one of RegionServers ended up w/ 5 of the requests and became 100% CPU utilized for about 1 hour.

This lead to the client timing out after 20min (2min x 10 retries).  In one case, the client was able to fill the RPC callqueue and received the following error:
{code}
  Failed all from region=<region>,hostname=<host>, port=<port> java.util.concurrent.ExecutionException: java.io.IOException: Call queue is full, is ipc.server.max.callqueue.size too small?
{code}
Based on feedback (http://search-hadoop.com/m/yITsc1WcDWP), I switched to Delete.deleteColumn(family, qual, timestamp) where timestamp came from KeyValue retrieved from scan based on domain objects.  This version of the delete ran in about 500ms.

User group thread titled ""RS unresponsive after series of deletes"" has related logs and stacktraces.  

Link to thread: http://search-hadoop.com/m/RmIyr1WcDWP

Here is the stack dump of region server: http://pastebin.com/8y5x4xU7",,,1,,,,,
HBASE-6260,"Execution of Deletes constructed with thousands of calls to Delete.deleteColumn(family, qualifier) are very expensive and slow.

On our (quiet) cluster, a Delete w/ 20k qualifiers took about 13s to complete (as measured by client).

When 10 such Deletes were sent to the cluster via HTable.delete(List<Delete>), one of RegionServers ended up w/ 5 of the requests and became 100% CPU utilized for about 1 hour.

This lead to the client timing out after 20min (2min x 10 retries).  In one case, the client was able to fill the RPC callqueue and received the following error:
{code}
  Failed all from region=<region>,hostname=<host>, port=<port> java.util.concurrent.ExecutionException: java.io.IOException: Call queue is full, is ipc.server.max.callqueue.size too small?
{code}
Based on feedback (http://search-hadoop.com/m/yITsc1WcDWP), I switched to Delete.deleteColumn(family, qual, timestamp) where timestamp came from KeyValue retrieved from scan based on domain objects.  This version of the delete ran in about 500ms.

User group thread titled ""RS unresponsive after series of deletes"" has related logs and stacktraces.  

Link to thread: http://search-hadoop.com/m/RmIyr1WcDWP

Here is the stack dump of region server: http://pastebin.com/8y5x4xU7",,,1,,,,,
HBASE-6261,"Execution of Deletes constructed with thousands of calls to Delete.deleteColumn(family, qualifier) are very expensive and slow.

On our (quiet) cluster, a Delete w/ 20k qualifiers took about 13s to complete (as measured by client).

When 10 such Deletes were sent to the cluster via HTable.delete(List<Delete>), one of RegionServers ended up w/ 5 of the requests and became 100% CPU utilized for about 1 hour.

This lead to the client timing out after 20min (2min x 10 retries).  In one case, the client was able to fill the RPC callqueue and received the following error:
{code}
  Failed all from region=<region>,hostname=<host>, port=<port> java.util.concurrent.ExecutionException: java.io.IOException: Call queue is full, is ipc.server.max.callqueue.size too small?
{code}
Based on feedback (http://search-hadoop.com/m/yITsc1WcDWP), I switched to Delete.deleteColumn(family, qual, timestamp) where timestamp came from KeyValue retrieved from scan based on domain objects.  This version of the delete ran in about 500ms.

User group thread titled ""RS unresponsive after series of deletes"" has related logs and stacktraces.  

Link to thread: http://search-hadoop.com/m/RmIyr1WcDWP

Here is the stack dump of region server: http://pastebin.com/8y5x4xU7",,,1,,,,,
HBASE-6265,"There is an issue when you call getTimestamp() on any KV handed into a Coprocessor's prePut(). It initializes the internal ""timestampCache"" variable. 

When you then pass it to the normal processing, the region server sets the time to the server time in case you have left it unset from the client side (updateLatestStamp() call). 

The TimeRangeTracker then calls getTimestamp() later on to see if it has to include the KV, but instead of getting the proper time it sees the cached timestamp from the prePut() call.",,,,,,,,
HBASE-6269,"When I fix the bug HBASE-6195, there is happened to find sometimes the test case will fail, https://builds.apache.org/job/HBase-0.94/259/.

If there are two Put/Increment with same row, family, qualifier, timestamp and different memstoreTS, after each Put/Increment, we do a memstore flush. So there will be two StoreFile with same KeyValue(except memstoreTS and SequenceId).

When I got the row, I always got the old records, the test case like this:
{code}
  public void testPutWithMemStoreFlush() throws Exception {
    Configuration conf = HBaseConfiguration.create();
    String method = ""testPutWithMemStoreFlush"";
    byte[] tableName = Bytes.toBytes(method);
    byte[] family = Bytes.toBytes(""family"");;
    byte[] qualifier = Bytes.toBytes(""qualifier"");
    byte[] row = Bytes.toBytes(""putRow"");
    byte[] value = null;
    this.region = initHRegion(tableName, method, conf, family);
    Put put = null;
    Get get = null;
    List<KeyValue> kvs = null;
    Result res = null;
    
    put = new Put(row);
    value = Bytes.toBytes(""value0"");
    put.add(family, qualifier, 1234567l, value);
    region.put(put);
    System.out.print(""get value before flush after put value0 : "");
    get = new Get(row);
    get.addColumn(family, qualifier);
    get.setMaxVersions();
    res = this.region.get(get, null);
    kvs = res.getColumn(family, qualifier);
    for (int i = 0; i < kvs.size(); i++) {
      System.out.println(Bytes.toString(kvs.get(i).getValue()));
    }

    region.flushcache();
    
    System.out.print(""get value after flush after put value0 : "");
    get = new Get(row);
    get.addColumn(family, qualifier);
    get.setMaxVersions();
    res = this.region.get(get, null);
    kvs = res.getColumn(family, qualifier);
    for (int i = 0; i < kvs.size(); i++) {
      System.out.println(Bytes.toString(kvs.get(i).getValue()));
    }
    
    put = new Put(row);
    value = Bytes.toBytes(""value1"");
    put.add(family, qualifier, 1234567l, value);
    region.put(put);
    System.out.print(""get value before flush after put value1 : "");
    get = new Get(row);
    get.addColumn(family, qualifier);
    get.setMaxVersions();
    res = this.region.get(get, null);
    kvs = res.getColumn(family, qualifier);
    for (int i = 0; i < kvs.size(); i++) {
      System.out.println(Bytes.toString(kvs.get(i).getValue()));
    }
    region.flushcache();
    System.out.print(""get value after flush after put value1 : "");
    get = new Get(row);
    get.addColumn(family, qualifier);
    get.setMaxVersions();
    res = this.region.get(get, null);
    kvs = res.getColumn(family, qualifier);
    for (int i = 0; i < kvs.size(); i++) {
      System.out.println(Bytes.toString(kvs.get(i).getValue()));
    }
    
    put = new Put(row);
    value = Bytes.toBytes(""value2"");
    put.add(family, qualifier, 1234567l, value);
    region.put(put);
    System.out.print(""get value before flush after put value2 : "");
    get = new Get(row);
    get.addColumn(family, qualifier);
    get.setMaxVersions();
    res = this.region.get(get, null);
    kvs = res.getColumn(family, qualifier);
    for (int i = 0; i < kvs.size(); i++) {
      System.out.println(Bytes.toString(kvs.get(i).getValue()));
    }
    region.flushcache();
    System.out.print(""get value after flush after put value2 : "");
    get = new Get(row);
    get.addColumn(family, qualifier);
    get.setMaxVersions();
    res = this.region.get(get, null);
    kvs = res.getColumn(family, qualifier);
    for (int i = 0; i < kvs.size(); i++) {
      System.out.println(Bytes.toString(kvs.get(i).getValue()));
    } 
  }
{code}
and the result print as followed:
{code}
get value before flush after put value0 : value0
get value after flush after put value0 : value0
get value before flush after put value1 : value1
get value after flush after put value1 : value0
get value before flush after put value2 : value2
get value after flush after put value2 : value0
{code}

I analyze the code for StoreFileScanner with lazy seek, the StoreFileScanners are sorted by SequenceId, so the latest StoreFile is on the top KeyValueHeap, and the KeyValue for latest StoreFile will comapre to the second latest StoreFile, but the second latest StoreFile generated the fake row for same row, family, qualifier excepts the timestamp( maximum), memstoreTS(0). And the latest KeyValue recognized as not latest than the second latest.",,,,,,,,
HBASE-6272,"AssignmentManger stores region state related information in several places: regionsInTransition, regions (region info to server name map), and servers (server name to region info set map).  However the access to these places is not coordinated properly.  It leads to inconsistent in-memory region state information.  Sometimes, some region could even be offline, and not in transition.",,,,,,,,
HBASE-6281,"Currently during clean cluster start up if there are tables in DISABLING state, we do bulk assignment through assignAllUserRegions() and after region is OPENED in RS, master checks if the table is in DISBALING/DISABLED state (in Am.regionOnline) and again calls unassign.  This roundtrip can be avoided even before calling assignment.
This JIRA is to address the above scenario.",,,1,,,,,
HBASE-6283,"Currently during clean cluster start up if there are tables in DISABLING state, we do bulk assignment through assignAllUserRegions() and after region is OPENED in RS, master checks if the table is in DISBALING/DISABLED state (in Am.regionOnline) and again calls unassign.  This roundtrip can be avoided even before calling assignment.
This JIRA is to address the above scenario.",,,1,,,,,
HBASE-6284,"From Anoop under thread 'Can there be a doMiniBatchDelete in HRegion':

The HTable#delete(List<Delete>) groups the Deletes for the same RS and make one n/w call only. But within the RS, there will be N number of delete calls on the region one by one. This will include N number of HLog write and sync. If this also can be grouped can we get better performance for the multi row delete.

I have made the new miniBatchDelete () and made the HTable#delete(List<Delete>) to call this new batch delete.
Just tested initially with the one node cluster.  In that itself I am getting a performance boost which is very much promising.
Only one CF and qualifier.
10K total rows delete with a batch of 100 deletes. Only deletes happening on the table from one thread.
With the new way the net time taken is reduced by more than 1/10
Will test in a 4 node cluster also. I think it will worth doing this change.",,,1,,,,,
HBASE-6287,"HBASE-5941 has been integrated to 0.89-fb

This JIRA ports it to HBase trunk",,,1,,,,,
HBASE-6289,"The ROOT RS has some network problem and its ZK node expires first, which kicks off the ServerShutdownHandler. it calls verifyAndAssignRoot() to try to re-assign ROOT. At that time, the RS is actually still working and passes the verifyRootRegionLocation() check, so the ROOT region is skipped from re-assignment.
{code}
  private void verifyAndAssignRoot()
  throws InterruptedException, IOException, KeeperException {
    long timeout = this.server.getConfiguration().
      getLong(""hbase.catalog.verification.timeout"", 1000);
    if (!this.server.getCatalogTracker().verifyRootRegionLocation(timeout)) {
      this.services.getAssignmentManager().assignRoot();
    }
  }
{code}
After a few moments, this RS encounters DFS write problem and decides to abort. The RS then soon gets restarted from commandline, and constantly report:
{code}
2012-06-27 23:13:08,627 DEBUG org.apache.hadoop.hbase.regionserver.HRegionServer: NotServingRegionException; Region is not online: -ROOT-,,0
2012-06-27 23:13:08,627 DEBUG org.apache.hadoop.hbase.regionserver.HRegionServer: NotServingRegionException; Region is not online: -ROOT-,,0
2012-06-27 23:13:08,628 DEBUG org.apache.hadoop.hbase.regionserver.HRegionServer: NotServingRegionException; Region is not online: -ROOT-,,0
2012-06-27 23:13:08,628 DEBUG org.apache.hadoop.hbase.regionserver.HRegionServer: NotServingRegionException; Region is not online: -ROOT-,,0
2012-06-27 23:13:08,630 DEBUG org.apache.hadoop.hbase.regionserver.HRegionServer: NotServingRegionException; Region is not online: -ROOT-,,0
{code}",,,,,,,,
HBASE-6292,"When client sends compact command to rs, the rs just create a CompactionRequest, and then put it into the thread pool to process the CompactionRequest. And when the region do the compact, it uses the rs's ugi to process the compact, so the compact can successfully done.
Example:
user ""mapred"" do not have permission ""Admin"",
hbase(main):001:0> user_permission 'Security'
User                                Table,Family,Qualifier:Permission                                                                      
 mapred                             Security,f1,c1: [Permission: actions=READ,WRITE] 

hbase(main):004:0> put 'Security', 'r6', 'f1:c1', 'v9'
0 row(s) in 0.0590 seconds

hbase(main):005:0> put 'Security', 'r6', 'f1:c1', 'v10'
0 row(s) in 0.0040 seconds

hbase(main):006:0> compact 'Security'
0 row(s) in 0.0260 seconds
Maybe we can add permission check in the preCompactSelection() ?",1,,,,,,,
HBASE-6293,"When master starts up and tries to do splitlog, in case of any error we try to do that infinitely in a loop until it succeeds.
But now if we get a shutdown call, inside SplitLogManager
{code}
          if (stopper.isStopped()) {
            LOG.warn(""Stopped while waiting for log splits to be completed"");
            return;
          }
{code}
Here we know that the master has stopped.  As the task may not be completed now
{code}
 if (batch.done != batch.installed) {
      batch.isDead = true;
      tot_mgr_log_split_batch_err.incrementAndGet();
      LOG.warn(""error while splitting logs in "" + logDirs +
      "" installed = "" + batch.installed + "" but only "" + batch.done + "" done"");
      throw new IOException(""error or interrupt while splitting logs in ""
          + logDirs + "" Task = "" + batch);
    }
{code} 
we throw an exception.  In MasterFileSystem.splitLogAfterStartup() we don't check if the master is stopped and we try continously. 
",,,,,,,,
HBASE-6294,"1. HMaster tries to assign a region to an RS.
2. HMaster creates a RegionState for this region and puts it into regionsInTransition.
3. In the first assign attempt, HMaster calls RS.openRegion(). The RS receives the open region request and starts to proceed, with success eventually. However, due to network problems, HMaster fails to receive the response for the openRegion() call, and the call times out.
4. HMaster attemps to assign for a second time, choosing another RS. 
5. But since the HMaster's OpenedRegionHandler has been triggered by the region open of the previous RS, and the RegionState has already been removed from regionsInTransition, HMaster finds invalid and ignores the unassigned ZK node ""RS_ZK_REGION_OPENING"" updated by the second attempt.
6. The unassigned ZK node stays and a later unassign fails coz RS_ZK_REGION_CLOSING cannot be created.
{code}
2012-06-29 07:03:38,870 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Using pre-existing plan for region CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568.; plan=hri=CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568., src=swbss-hadoop-004,60020,1340890123243, dest=swbss-hadoop-006,60020,1340890678078
2012-06-29 07:03:38,870 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. to swbss-hadoop-006,60020,1340890678078
2012-06-29 07:03:38,870 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=M_ZK_REGION_OFFLINE, server=swbss-hadoop-002:60000, region=b713fd655fa02395496c5a6e39ddf568
2012-06-29 07:06:28,882 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=swbss-hadoop-006,60020,1340890678078, region=b713fd655fa02395496c5a6e39ddf568
2012-06-29 07:06:32,291 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=swbss-hadoop-006,60020,1340890678078, region=b713fd655fa02395496c5a6e39ddf568
2012-06-29 07:06:32,299 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENED, server=swbss-hadoop-006,60020,1340890678078, region=b713fd655fa02395496c5a6e39ddf568
2012-06-29 07:06:32,299 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Handling OPENED event for CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. from serverName=swbss-hadoop-006,60020,1340890678078, load=(requests=518945, regions=575, usedHeap=15282, maxHeap=31301); deleting unassigned node
2012-06-29 07:06:32,299 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x2377fee2ae80007 Deleting existing unassigned node for b713fd655fa02395496c5a6e39ddf568 that is in expected state RS_ZK_REGION_OPENED
2012-06-29 07:06:32,301 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x2377fee2ae80007 Successfully deleted unassigned node for region b713fd655fa02395496c5a6e39ddf568 in expected state RS_ZK_REGION_OPENED
2012-06-29 07:06:32,301 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: The master has opened the region CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. that was online on serverName=swbss-hadoop-006,60020,1340890678078, load=(requests=518945, regions=575, usedHeap=15282, maxHeap=31301)
2012-06-29 07:07:41,140 WARN org.apache.hadoop.hbase.master.AssignmentManager: Failed assignment of CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. to serverName=swbss-hadoop-006,60020,1340890678078, load=(requests=0, regions=575, usedHeap=0, maxHeap=0), trying to assign elsewhere instead; retry=0
java.net.SocketTimeoutException: Call to /172.16.0.6:60020 failed on socket timeout exception: java.net.SocketTimeoutException: 120000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.16.0.2:53765 remote=/172.16.0.6:60020]
        at org.apache.hadoop.hbase.ipc.HBaseClient.wrapException(HBaseClient.java:805)
        at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:778)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:283)
        at $Proxy8.openRegion(Unknown Source)
        at org.apache.hadoop.hbase.master.ServerManager.sendRegionOpen(ServerManager.java:573)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1127)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:912)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:892)
        at org.apache.hadoop.hbase.master.handler.ClosedRegionHandler.process(ClosedRegionHandler.java:92)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.net.SocketTimeoutException: 120000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.16.0.2:53765 remote=/172.16.0.6:60020]
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)
        at java.io.FilterInputStream.read(FilterInputStream.java:116)
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection$PingInputStream.read(HBaseClient.java:301)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:237)
        at java.io.DataInputStream.readInt(DataInputStream.java:370)
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.receiveResponse(HBaseClient.java:541)
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.run(HBaseClient.java:479)
2012-06-29 07:07:41,142 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: No previous transition plan was found (or we are ignoring an existing plan) for CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. so generated a random one; hri=CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568., src=, dest=swbss-hadoop-164,60020,1340888346294; 15 (online=15, exclude=serverName=swbss-hadoop-006,60020,1340890678078, load=(requests=0, regions=575, usedHeap=0, maxHeap=0)) available servers
2012-06-29 07:07:41,142 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x2377fee2ae80007 Creating (or updating) unassigned node for b713fd655fa02395496c5a6e39ddf568 with OFFLINE state
2012-06-29 07:07:41,145 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Using pre-existing plan for region CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568.; plan=hri=CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568., src=, dest=swbss-hadoop-164,60020,1340888346294
2012-06-29 07:07:41,145 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. to swbss-hadoop-164,60020,1340888346294
2012-06-29 07:07:41,149 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=swbss-hadoop-164,60020,1340888346294, region=b713fd655fa02395496c5a6e39ddf568
2012-06-29 07:07:41,150 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received OPENING for region b713fd655fa02395496c5a6e39ddf568 from server swbss-hadoop-164,60020,1340888346294 but region was in  the state null and not in expected PENDING_OPEN or OPENING states
2012-06-29 07:07:41,296 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=swbss-hadoop-164,60020,1340888346294, region=b713fd655fa02395496c5a6e39ddf568
2012-06-29 07:07:41,296 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received OPENING for region b713fd655fa02395496c5a6e39ddf568 from server swbss-hadoop-164,60020,1340888346294 but region was in  the state null and not in expected PENDING_OPEN or OPENING states
2012-06-29 07:07:41,302 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENED, server=swbss-hadoop-164,60020,1340888346294, region=b713fd655fa02395496c5a6e39ddf568
2012-06-29 07:07:41,302 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received OPENED for region b713fd655fa02395496c5a6e39ddf568 from server swbss-hadoop-164,60020,1340888346294 but region was in  the state null and not in expected PENDING_OPEN or OPENING states
2012-06-29 07:08:38,872 INFO org.apache.hadoop.hbase.master.HMaster: balance hri=CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568., src=swbss-hadoop-006,60020,1340890678078, dest=swbss-hadoop-008,60020,1340891085175
2012-06-29 07:08:38,872 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Starting unassignment of region CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. (offlining)
2012-06-29 07:08:47,875 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Sent CLOSE to serverName=swbss-hadoop-006,60020,1340890678078, load=(requests=0, regions=0, usedHeap=0, maxHeap=0) for region CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568.
...
2012-06-29 08:04:37,681 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. state=PENDING_CLOSE, ts=1340926468331, server=null
2012-06-29 08:04:37,681 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been PENDING_CLOSE for too long, running forced unassign again on region=CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568.
2012-06-29 08:04:47,681 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. state=PENDING_CLOSE, ts=1340926468331, server=null
2012-06-29 08:04:47,682 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been PENDING_CLOSE for too long, running forced unassign again on region=CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568.
{code}

",,,,,,,,
HBASE-6295,"HBASE-5941 has been integrated to 0.89-fb

This JIRA ports it to HBase trunk",,,1,,,,,
HBASE-6299,"1. HMaster tries to assign a region to an RS.
2. HMaster creates a RegionState for this region and puts it into regionsInTransition.
3. In the first assign attempt, HMaster calls RS.openRegion(). The RS receives the open region request and starts to proceed, with success eventually. However, due to network problems, HMaster fails to receive the response for the openRegion() call, and the call times out.
4. HMaster attemps to assign for a second time, choosing another RS. 
5. But since the HMaster's OpenedRegionHandler has been triggered by the region open of the previous RS, and the RegionState has already been removed from regionsInTransition, HMaster finds invalid and ignores the unassigned ZK node ""RS_ZK_REGION_OPENING"" updated by the second attempt.
6. The unassigned ZK node stays and a later unassign fails coz RS_ZK_REGION_CLOSING cannot be created.
{code}
2012-06-29 07:03:38,870 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Using pre-existing plan for region CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568.; plan=hri=CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568., src=swbss-hadoop-004,60020,1340890123243, dest=swbss-hadoop-006,60020,1340890678078
2012-06-29 07:03:38,870 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. to swbss-hadoop-006,60020,1340890678078
2012-06-29 07:03:38,870 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=M_ZK_REGION_OFFLINE, server=swbss-hadoop-002:60000, region=b713fd655fa02395496c5a6e39ddf568
2012-06-29 07:06:28,882 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=swbss-hadoop-006,60020,1340890678078, region=b713fd655fa02395496c5a6e39ddf568
2012-06-29 07:06:32,291 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=swbss-hadoop-006,60020,1340890678078, region=b713fd655fa02395496c5a6e39ddf568
2012-06-29 07:06:32,299 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENED, server=swbss-hadoop-006,60020,1340890678078, region=b713fd655fa02395496c5a6e39ddf568
2012-06-29 07:06:32,299 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Handling OPENED event for CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. from serverName=swbss-hadoop-006,60020,1340890678078, load=(requests=518945, regions=575, usedHeap=15282, maxHeap=31301); deleting unassigned node
2012-06-29 07:06:32,299 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x2377fee2ae80007 Deleting existing unassigned node for b713fd655fa02395496c5a6e39ddf568 that is in expected state RS_ZK_REGION_OPENED
2012-06-29 07:06:32,301 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x2377fee2ae80007 Successfully deleted unassigned node for region b713fd655fa02395496c5a6e39ddf568 in expected state RS_ZK_REGION_OPENED
2012-06-29 07:06:32,301 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: The master has opened the region CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. that was online on serverName=swbss-hadoop-006,60020,1340890678078, load=(requests=518945, regions=575, usedHeap=15282, maxHeap=31301)
2012-06-29 07:07:41,140 WARN org.apache.hadoop.hbase.master.AssignmentManager: Failed assignment of CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. to serverName=swbss-hadoop-006,60020,1340890678078, load=(requests=0, regions=575, usedHeap=0, maxHeap=0), trying to assign elsewhere instead; retry=0
java.net.SocketTimeoutException: Call to /172.16.0.6:60020 failed on socket timeout exception: java.net.SocketTimeoutException: 120000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.16.0.2:53765 remote=/172.16.0.6:60020]
        at org.apache.hadoop.hbase.ipc.HBaseClient.wrapException(HBaseClient.java:805)
        at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:778)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:283)
        at $Proxy8.openRegion(Unknown Source)
        at org.apache.hadoop.hbase.master.ServerManager.sendRegionOpen(ServerManager.java:573)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1127)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:912)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:892)
        at org.apache.hadoop.hbase.master.handler.ClosedRegionHandler.process(ClosedRegionHandler.java:92)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:162)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.net.SocketTimeoutException: 120000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.16.0.2:53765 remote=/172.16.0.6:60020]
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)
        at java.io.FilterInputStream.read(FilterInputStream.java:116)
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection$PingInputStream.read(HBaseClient.java:301)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:237)
        at java.io.DataInputStream.readInt(DataInputStream.java:370)
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.receiveResponse(HBaseClient.java:541)
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.run(HBaseClient.java:479)
2012-06-29 07:07:41,142 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: No previous transition plan was found (or we are ignoring an existing plan) for CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. so generated a random one; hri=CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568., src=, dest=swbss-hadoop-164,60020,1340888346294; 15 (online=15, exclude=serverName=swbss-hadoop-006,60020,1340890678078, load=(requests=0, regions=575, usedHeap=0, maxHeap=0)) available servers
2012-06-29 07:07:41,142 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x2377fee2ae80007 Creating (or updating) unassigned node for b713fd655fa02395496c5a6e39ddf568 with OFFLINE state
2012-06-29 07:07:41,145 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Using pre-existing plan for region CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568.; plan=hri=CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568., src=, dest=swbss-hadoop-164,60020,1340888346294
2012-06-29 07:07:41,145 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. to swbss-hadoop-164,60020,1340888346294
2012-06-29 07:07:41,149 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=swbss-hadoop-164,60020,1340888346294, region=b713fd655fa02395496c5a6e39ddf568
2012-06-29 07:07:41,150 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received OPENING for region b713fd655fa02395496c5a6e39ddf568 from server swbss-hadoop-164,60020,1340888346294 but region was in  the state null and not in expected PENDING_OPEN or OPENING states
2012-06-29 07:07:41,296 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=swbss-hadoop-164,60020,1340888346294, region=b713fd655fa02395496c5a6e39ddf568
2012-06-29 07:07:41,296 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received OPENING for region b713fd655fa02395496c5a6e39ddf568 from server swbss-hadoop-164,60020,1340888346294 but region was in  the state null and not in expected PENDING_OPEN or OPENING states
2012-06-29 07:07:41,302 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENED, server=swbss-hadoop-164,60020,1340888346294, region=b713fd655fa02395496c5a6e39ddf568
2012-06-29 07:07:41,302 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received OPENED for region b713fd655fa02395496c5a6e39ddf568 from server swbss-hadoop-164,60020,1340888346294 but region was in  the state null and not in expected PENDING_OPEN or OPENING states
2012-06-29 07:08:38,872 INFO org.apache.hadoop.hbase.master.HMaster: balance hri=CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568., src=swbss-hadoop-006,60020,1340890678078, dest=swbss-hadoop-008,60020,1340891085175
2012-06-29 07:08:38,872 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Starting unassignment of region CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. (offlining)
2012-06-29 07:08:47,875 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Sent CLOSE to serverName=swbss-hadoop-006,60020,1340890678078, load=(requests=0, regions=0, usedHeap=0, maxHeap=0) for region CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568.
...
2012-06-29 08:04:37,681 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. state=PENDING_CLOSE, ts=1340926468331, server=null
2012-06-29 08:04:37,681 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been PENDING_CLOSE for too long, running forced unassign again on region=CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568.
2012-06-29 08:04:47,681 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. state=PENDING_CLOSE, ts=1340926468331, server=null
2012-06-29 08:04:47,682 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been PENDING_CLOSE for too long, running forced unassign again on region=CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568.
{code}

",,,,,,,,
HBASE-6300,"When RS updates an unassigned ZK node to RS_ZK_REGION_OPENED, it will most probably proceed to update the region location in META. This would cause inconsistency between the region's location in HMaster and that in META. Not deleting this ZK node would also make further region transitions fail with ZK exception ""node already exists"".
So the master should either abort or fix this inconsistency.
{code}
        case RS_ZK_REGION_OPENED:
          hri = checkIfInFailover(regionState, encodedName, regionName);
          if (hri != null) {
            regionState = new RegionState(hri, RegionState.State.OPEN, createTime, sn);
            regionsInTransition.put(encodedName, regionState);
            new OpenedRegionHandler(master, this, regionState.getRegion(), sn, expectedVersion).process();
            failoverProcessedRegions.put(encodedName, hri);
            break;
          }
          // Should see OPENED after OPENING but possible after PENDING_OPEN
          if (regionState == null ||
              (!regionState.isPendingOpen() && !regionState.isOpening())) {
            LOG.warn(""Received OPENED for region "" +
                prettyPrintedRegionName +
                "" from server "" + sn + "" but region was in "" +
                "" the state "" + regionState + "" and not "" +
                ""in expected PENDING_OPEN or OPENING states"");
            return;
          }
          // Handle OPENED by removing from transition and deleted zk node
          regionState.update(RegionState.State.OPEN, createTime, sn);
          this.executorService.submit(
            new OpenedRegionHandler(master, this, regionState.getRegion(), sn, expectedVersion));
          break;
{code}

Error logs:
{code}
2012-06-29 07:07:41,149 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=swbss-hadoop-164,60020,1340888346294, region=b713fd655fa02395496c5a6e39ddf568
2012-06-29 07:07:41,150 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received OPENING for region b713fd655fa02395496c5a6e39ddf568 from server swbss-hadoop-164,60020,1340888346294 but region was in  the state null and not in expected PENDING_OPEN or OPENING states
2012-06-29 07:07:41,296 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=swbss-hadoop-164,60020,1340888346294, region=b713fd655fa02395496c5a6e39ddf568
2012-06-29 07:07:41,296 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received OPENING for region b713fd655fa02395496c5a6e39ddf568 from server swbss-hadoop-164,60020,1340888346294 but region was in  the state null and not in expected PENDING_OPEN or OPENING states
2012-06-29 07:07:41,302 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENED, server=swbss-hadoop-164,60020,1340888346294, region=b713fd655fa02395496c5a6e39ddf568
2012-06-29 07:07:41,302 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received OPENED for region b713fd655fa02395496c5a6e39ddf568 from server swbss-hadoop-164,60020,1340888346294 but region was in  the state null and not in expected PENDING_OPEN or OPENING states
2012-06-29 07:08:38,872 INFO org.apache.hadoop.hbase.master.HMaster: balance hri=CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568., src=swbss-hadoop-006,60020,1340890678078, dest=swbss-hadoop-008,60020,1340891085175
2012-06-29 07:08:38,872 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Starting unassignment of region CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. (offlining)
2012-06-29 07:08:47,875 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Sent CLOSE to serverName=swbss-hadoop-006,60020,1340890678078, load=(requests=0, regions=0, usedHeap=0, maxHeap=0) for region CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568.
2012-06-29 08:04:37,681 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. state=PENDING_CLOSE, ts=1340926468331, server=null
2012-06-29 08:04:37,681 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been PENDING_CLOSE for too long, running forced unassign again on region=CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568.
2012-06-29 08:04:47,681 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. state=PENDING_CLOSE, ts=1340926468331, server=null
2012-06-29 08:04:47,682 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been PENDING_CLOSE for too long, running forced unassign again on region=CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568.
2012-06-29 08:04:57,681 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. state=PENDING_CLOSE, ts=1340926468331, server=null
{code}
",,,,,,,,
HBASE-6308,"When RS updates an unassigned ZK node to RS_ZK_REGION_OPENED, it will most probably proceed to update the region location in META. This would cause inconsistency between the region's location in HMaster and that in META. Not deleting this ZK node would also make further region transitions fail with ZK exception ""node already exists"".
So the master should either abort or fix this inconsistency.
{code}
        case RS_ZK_REGION_OPENED:
          hri = checkIfInFailover(regionState, encodedName, regionName);
          if (hri != null) {
            regionState = new RegionState(hri, RegionState.State.OPEN, createTime, sn);
            regionsInTransition.put(encodedName, regionState);
            new OpenedRegionHandler(master, this, regionState.getRegion(), sn, expectedVersion).process();
            failoverProcessedRegions.put(encodedName, hri);
            break;
          }
          // Should see OPENED after OPENING but possible after PENDING_OPEN
          if (regionState == null ||
              (!regionState.isPendingOpen() && !regionState.isOpening())) {
            LOG.warn(""Received OPENED for region "" +
                prettyPrintedRegionName +
                "" from server "" + sn + "" but region was in "" +
                "" the state "" + regionState + "" and not "" +
                ""in expected PENDING_OPEN or OPENING states"");
            return;
          }
          // Handle OPENED by removing from transition and deleted zk node
          regionState.update(RegionState.State.OPEN, createTime, sn);
          this.executorService.submit(
            new OpenedRegionHandler(master, this, regionState.getRegion(), sn, expectedVersion));
          break;
{code}

Error logs:
{code}
2012-06-29 07:07:41,149 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=swbss-hadoop-164,60020,1340888346294, region=b713fd655fa02395496c5a6e39ddf568
2012-06-29 07:07:41,150 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received OPENING for region b713fd655fa02395496c5a6e39ddf568 from server swbss-hadoop-164,60020,1340888346294 but region was in  the state null and not in expected PENDING_OPEN or OPENING states
2012-06-29 07:07:41,296 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=swbss-hadoop-164,60020,1340888346294, region=b713fd655fa02395496c5a6e39ddf568
2012-06-29 07:07:41,296 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received OPENING for region b713fd655fa02395496c5a6e39ddf568 from server swbss-hadoop-164,60020,1340888346294 but region was in  the state null and not in expected PENDING_OPEN or OPENING states
2012-06-29 07:07:41,302 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENED, server=swbss-hadoop-164,60020,1340888346294, region=b713fd655fa02395496c5a6e39ddf568
2012-06-29 07:07:41,302 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received OPENED for region b713fd655fa02395496c5a6e39ddf568 from server swbss-hadoop-164,60020,1340888346294 but region was in  the state null and not in expected PENDING_OPEN or OPENING states
2012-06-29 07:08:38,872 INFO org.apache.hadoop.hbase.master.HMaster: balance hri=CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568., src=swbss-hadoop-006,60020,1340890678078, dest=swbss-hadoop-008,60020,1340891085175
2012-06-29 07:08:38,872 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Starting unassignment of region CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. (offlining)
2012-06-29 07:08:47,875 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Sent CLOSE to serverName=swbss-hadoop-006,60020,1340890678078, load=(requests=0, regions=0, usedHeap=0, maxHeap=0) for region CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568.
2012-06-29 08:04:37,681 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. state=PENDING_CLOSE, ts=1340926468331, server=null
2012-06-29 08:04:37,681 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been PENDING_CLOSE for too long, running forced unassign again on region=CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568.
2012-06-29 08:04:47,681 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. state=PENDING_CLOSE, ts=1340926468331, server=null
2012-06-29 08:04:47,682 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been PENDING_CLOSE for too long, running forced unassign again on region=CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568.
2012-06-29 08:04:57,681 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. state=PENDING_CLOSE, ts=1340926468331, server=null
{code}
",,,,,,,,
HBASE-6309,"When RS updates an unassigned ZK node to RS_ZK_REGION_OPENED, it will most probably proceed to update the region location in META. This would cause inconsistency between the region's location in HMaster and that in META. Not deleting this ZK node would also make further region transitions fail with ZK exception ""node already exists"".
So the master should either abort or fix this inconsistency.
{code}
        case RS_ZK_REGION_OPENED:
          hri = checkIfInFailover(regionState, encodedName, regionName);
          if (hri != null) {
            regionState = new RegionState(hri, RegionState.State.OPEN, createTime, sn);
            regionsInTransition.put(encodedName, regionState);
            new OpenedRegionHandler(master, this, regionState.getRegion(), sn, expectedVersion).process();
            failoverProcessedRegions.put(encodedName, hri);
            break;
          }
          // Should see OPENED after OPENING but possible after PENDING_OPEN
          if (regionState == null ||
              (!regionState.isPendingOpen() && !regionState.isOpening())) {
            LOG.warn(""Received OPENED for region "" +
                prettyPrintedRegionName +
                "" from server "" + sn + "" but region was in "" +
                "" the state "" + regionState + "" and not "" +
                ""in expected PENDING_OPEN or OPENING states"");
            return;
          }
          // Handle OPENED by removing from transition and deleted zk node
          regionState.update(RegionState.State.OPEN, createTime, sn);
          this.executorService.submit(
            new OpenedRegionHandler(master, this, regionState.getRegion(), sn, expectedVersion));
          break;
{code}

Error logs:
{code}
2012-06-29 07:07:41,149 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=swbss-hadoop-164,60020,1340888346294, region=b713fd655fa02395496c5a6e39ddf568
2012-06-29 07:07:41,150 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received OPENING for region b713fd655fa02395496c5a6e39ddf568 from server swbss-hadoop-164,60020,1340888346294 but region was in  the state null and not in expected PENDING_OPEN or OPENING states
2012-06-29 07:07:41,296 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=swbss-hadoop-164,60020,1340888346294, region=b713fd655fa02395496c5a6e39ddf568
2012-06-29 07:07:41,296 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received OPENING for region b713fd655fa02395496c5a6e39ddf568 from server swbss-hadoop-164,60020,1340888346294 but region was in  the state null and not in expected PENDING_OPEN or OPENING states
2012-06-29 07:07:41,302 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENED, server=swbss-hadoop-164,60020,1340888346294, region=b713fd655fa02395496c5a6e39ddf568
2012-06-29 07:07:41,302 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received OPENED for region b713fd655fa02395496c5a6e39ddf568 from server swbss-hadoop-164,60020,1340888346294 but region was in  the state null and not in expected PENDING_OPEN or OPENING states
2012-06-29 07:08:38,872 INFO org.apache.hadoop.hbase.master.HMaster: balance hri=CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568., src=swbss-hadoop-006,60020,1340890678078, dest=swbss-hadoop-008,60020,1340891085175
2012-06-29 07:08:38,872 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Starting unassignment of region CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. (offlining)
2012-06-29 07:08:47,875 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Sent CLOSE to serverName=swbss-hadoop-006,60020,1340890678078, load=(requests=0, regions=0, usedHeap=0, maxHeap=0) for region CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568.
2012-06-29 08:04:37,681 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. state=PENDING_CLOSE, ts=1340926468331, server=null
2012-06-29 08:04:37,681 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been PENDING_CLOSE for too long, running forced unassign again on region=CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568.
2012-06-29 08:04:47,681 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. state=PENDING_CLOSE, ts=1340926468331, server=null
2012-06-29 08:04:47,682 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been PENDING_CLOSE for too long, running forced unassign again on region=CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568.
2012-06-29 08:04:57,681 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  CDR_STATS_TRAFFIC,13184390567|20120508|17||2|3|913,1337256975556.b713fd655fa02395496c5a6e39ddf568. state=PENDING_CLOSE, ts=1340926468331, server=null
{code}
",,,,,,,,
HBASE-6311,"It is a big problem we found in 0.94, and you could reproduce the problem in Trunk using the test case I uploaded.

When we do compaction, we will use region.getSmallestReadPoint() to keep MVCC for opened scanners;
However,It will make data mistake after majorCompaction because we will skip delete type KV but keep the put type kv in the compacted storefile.

The following is the reason from code:
In StoreFileScanner, enforceMVCC is false when compaction, so we could read the delete type KV,
However, we will skip this delete type KV in ScanQueryMatcher because following code

{code}
if (kv.isDelete())
{
...
 if (includeDeleteMarker
            && kv.getMemstoreTS() <= maxReadPointToTrackVersions) {
          System.out.println(""add deletes,maxReadPointToTrackVersions=""
              + maxReadPointToTrackVersions);
          this.deletes.add(bytes, offset, qualLength, timestamp, type);
        }
...
}
{code}

Here maxReadPointToTrackVersions = region.getSmallestReadPoint();
and kv.getMemstoreTS() > maxReadPointToTrackVersions 
So we won't add this to DeleteTracker.

Why test case passed if remove the line MultiVersionConsistencyControl.setThreadReadPoint(smallestReadPoint);

Because in the StoreFileScanner#skipKVsNewerThanReadpoint
{code}
if (cur.getMemstoreTS() <= readPoint) {
      cur.setMemstoreTS(0);
    }
{code}
So if we remove the line MultiVersionConsistencyControl.setThreadReadPoint(smallestReadPoint);
Here readPoint is LONG.MAX_VALUE, we will set memStore ts as 0, so we will add it to DeleteTracker in ScanQueryMatcher 


Solution:
We use smallestReadPoint of region when compaction to keep MVCC for OPENED scanner, So we should retain delete type kv in output in the case(Already deleted KV is retained in output to make old opened scanner could read this KV) even if it is a majorcompaction.",,,,,,,,
HBASE-6312,"If the call first remove from the calls, when some exception happened in reading from the DataInputStream, the call will not be notified, cause the client hangs.
",,1,1,,,,,
HBASE-6313,"If the call first remove from the calls, when some exception happened in reading from the DataInputStream, the call will not be notified, cause the client hangs.
",,,,,,,,
HBASE-6314,"In case of unauthenticated users in secure hbase, hbase shell does a connection retry at two levels:
a) HConnection: It retries hbase.client.retries.number times in the getMaster()
b) HBaseAdmin: it again retries hbase.client.retries.number times in its ctr
So, hbase shell retries square number of times of the configured setting. We can make it failfast (no retries) in case the user is not authenticated (no valid kerberos credentials).",1,,,,,,,
HBASE-6317,"If we have a  table in partially enabled state (ENABLING) then on HMaster restart we treat it as a clean cluster start up and do a bulk assign.  Currently in 0.94 bulk assign will not handle ALREADY_OPENED scenarios and it leads to region assignment problems.  Analysing more on this we found that we have better way to handle these scenarios.
{code}
if (false == checkIfRegionBelongsToDisabled(regionInfo)
            && false == checkIfRegionsBelongsToEnabling(regionInfo)) {
          synchronized (this.regions) {
            regions.put(regionInfo, regionLocation);
            addToServers(regionLocation, regionInfo);
          }
{code}
We dont add to regions map so that enable table handler can handle it.  But as nothing is added to regions map we think it as a clean cluster start up.
Will come up with a patch tomorrow.",,,,,,,,
HBASE-6318,"In playing with 0.96 code on a live cluster, found this issue:

2012-07-03 12:13:32,572 ERROR org.apache.hadoop.hbase.regionserver.SplitLogWorker: unexpected error
java.util.ConcurrentModificationException
        at java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1100)
        at java.util.TreeMap$ValueIterator.next(TreeMap.java:1145)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$OutputSink.closeLogWriters(HLogSplitter.java:1330)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$OutputSink.finishWritingAndClose(HLogSplitter.java:1221)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFileToTemp(HLogSplitter.java:441)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFileToTemp(HLogSplitter.java:369)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker$1.exec(SplitLogWorker.java:113)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.grabTask(SplitLogWorker.java:276)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.taskLoop(SplitLogWorker.java:197)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.run(SplitLogWorker.java:164)
        at java.lang.Thread.run(Thread.java:662)
2012-07-03 12:13:32,575 INFO org.apache.hadoop.hbase.regionserver.SplitLogWorker: SplitLogWorker ****.cloudera.com,57020,1341335300238 exiting",,,,,,,,
HBASE-6319,In a few places in the ReplicationSource code calls terminate on itself which is a problem since in terminate() we wait on that thread to die.,,,,,,,,
HBASE-6322,"From a mailing list question:

While generating some load against a library that makes extensive use of HTablePool in 0.92, I noticed that the largest heap consumer was java.lang.ref.Finalizer.  Digging in, I discovered that HTablePool's internal PooledHTable extends HTable, which instantiates a ThreadPoolExecutor and supporting objects every time a pooled HTable is retrieved.  Since ThreadPoolExecutor has a finalizer, it and its dependencies can't get garbage collected until the finalizer runs.  The result is by using HTablePool, we're creating a ton of objects to be finalized that are stuck on the heap longer than they should be, creating our largest source of pressure on the garbage collector.  It looks like this will also be a problem in 0.94 and trunk.

The easy fix is just to have PooledHTable implement HTableInterface (rather than subclass HTable), but this does break a unit test that explicitly checks that PooledHTable implements HTable -- I can only assume this test is there for some historical passivity reason.",,,,,,,,
HBASE-6325,"Yet another bug found during the leap second madness, it's possible to miss the registration of new region servers so that in ReplicationSourceManager.init we start the failover of a live and replicating region server. I don't think there's data loss but the RS that's being failed over will die on:

{noformat}
2012-07-01 06:25:15,604 FATAL org.apache.hadoop.hbase.regionserver.HRegionServer: ABORTING region server sv4r23s48,10304,1341112194623: Writing replication status
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /hbase/replication/rs/sv4r23s48,10304,1341112194623/4/sv4r23s48%2C10304%2C1341112194623.1341112195369
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1246)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.setData(RecoverableZooKeeper.java:372)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.setData(ZKUtil.java:655)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.setData(ZKUtil.java:697)
        at org.apache.hadoop.hbase.replication.ReplicationZookeeper.writeReplicationStatus(ReplicationZookeeper.java:470)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.logPositionAndCleanOldLogs(ReplicationSourceManager.java:154)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.shipEdits(ReplicationSource.java:607)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:368)
{noformat}

It seems to me that just refreshing {{otherRegionServers}} after getting the list of {{currentReplicators}} would be enough to fix this.",,,,,,,,
HBASE-6326,"While testing client timeouts when the HBase is not available we found that even with aggressive settings, it takes the client 10 minutes or more to finally receive an exception.
Part of this is due to nested nested retry loops in locateRegion.

locateRegion will first try to locate the table in meta (which is retried), then it will try to locate the meta table is root (which is also retried).
So for each retry of the meta lookup we retry the root lookup as well.

I have have that avoids locateRegion retrying if it is called from code that already has a retry loop.",,,1,,,,,
HBASE-6327,"As HBASE-4010 discussed, the HLog can be null.

We have meet createTable failed because the no use hlog.

When createHReagion, the HLog.LogSyncer is run sync(), in under layer it call the DFSClient.DFSOutputStream.sync(). 

Then the hlog.closeAndDelete() was calledfirstly the HLog.close() will interrupt the LogSyncer, and interrupt DFSClient.DFSOutputStream.sync().The DFSClient.DFSOutputStream will store the exception and throw it when we called DFSClient.close(). 

The HLog.close() call the writer.close()/DFSClient.close() after interrupt the LogSyncer. And there is no catch exception for the close().

So the Master throw exception to the client. There is no need to throw this exception, further?the hlog is no use.

Our cluster is 0.90, the logs is attached, after ""closing hlog writer"", there is no log for the createTable().

The trunk and 0.92, 0.94, we used just one hlog, and if the exception happends, the client will got createTable failed, but indeed ,we expect all the regions for the table can also be assigned.

I will give the patch for this later.",,,,,,,,
HBASE-6329,"We found this issue in 0.94, first let me describe the case?Stop META rs when split is in progress

1.Stopping META rs(Server A).
2.The main thread of rs close ZK and delete ephemeral node of the rs.
3.SplitTransaction is retring MetaEditor.addDaughter
4.Master's ServerShutdownHandler process the above dead META server
5.Master fixup daughter and assign the daughter
6.The daughter is opened on another server(Server B)
7.Server A's splitTransaction successfully add the daughter to .META. with serverName=Server A
8.Now, in the .META., daughter's region location is Server A but it is onlined on Server B
9.Restart Master, and master will assign the daughter again.


Attaching the logs, daughter region 80f999ea84cb259e20e9a228546f6c8a

Master log:
2012-07-04 13:45:56,493 INFO org.apache.hadoop.hbase.master.handler.ServerShutdownHandler: Splitting logs for dw93.kgb.sqa.cm4,60020,1341378224464
2012-07-04 13:45:58,983 INFO org.apache.hadoop.hbase.master.handler.ServerShutdownHandler: Fixup; missing daughter writetest,JC\xCA\xC8\xCF<Q\xC49>OH\xCEV\xCC\xC2\xB5\xC2@\xD4,1341380730558.80f999ea84cb259e20e9a228546f6c8a. 
2012-07-04 13:45:58,985 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Added daughter writetest,JC\xCA\xC8\xCF<Q\xC49>OH\xCEV\xCC\xC2\xB5\xC2@\xD4,1341380730558.80f999ea84cb259e20e9a228546f6c8a., serverName=null 
2012-07-04 13:45:58,988 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region writetest,JC\xCA\xC8\xCF<Q\xC49>OH\xCEV\xCC\xC2\xB5\xC2@\xD4,1341380730558.80f999ea84cb259e20e9a228546f6c8a. to dw88.kgb.sqa.cm4,60020,1341379188777 
2012-07-04 13:46:00,201 INFO org.apache.hadoop.hbase.master.AssignmentManager: The master has opened the region writetest,JC\xCA\xC8\xCF<Q\xC49>OH\xCEV\xCC\xC2\xB5\xC2@\xD4,1341380730558.80f999ea84cb259e20e9a228546f6c8a. that was online on dw88.kgb.sqa.cm4,60020,1341379188777 

Master log after restart:
2012-07-04 14:27:05,824 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x136187d60e34644 Creating (or updating) unassigned node for 80f999ea84cb259e20e9a228546f6c8a with OFFLINE state 
2012-07-04 14:27:05,851 INFO org.apache.hadoop.hbase.master.AssignmentManager: Processing region writetest,JC\xCA\xC8\xCF<Q\xC49>OH\xCEV\xCC\xC2\xB5\xC2@\xD4,1341380730558.80f999ea84cb259e20e9a228546f6c8a. in state M_ZK_REGION_OFFLINE 
2012-07-04 14:27:05,854 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region writetest,JC\xCA\xC8\xCF<Q\xC49>OH\xCEV\xCC\xC2\xB5\xC2@\xD4,1341380730558.80f999ea84cb259e20e9a228546f6c8a. to dw93.kgb.sqa.cm4,60020,1341380812020 
2012-07-04 14:27:06,051 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENED, server=dw93.kgb.sqa.cm4,60020,1341380812020, region=80f999ea84cb259e20e9a228546f6c8a 



Regionserver(META rs) log:
2012-07-04 13:45:56,491 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: stopping server dw93.kgb.sqa.cm4,60020,1341378224464; zookeeper connection c
losed.
2012-07-04 13:46:11,951 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Added daughter writetest,JC\xCA\xC8\xCF<Q\xC49>OH\xCEV\xCC\xC2\xB5\xC2@\xD4,1341380730558.80f999ea84cb259e20e9a228546f6c8a., serverName=dw93.kgb.sqa.cm4,60020,1341378224464 
2012-07-04 13:46:11,952 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Done with post open deploy task for region=writetest,JC\xCA\xC8\xCF<Q\xC49>OH\xCEV\xCC\xC2\xB5\xC2@\xD4,1341380730558.80f999ea84cb259e20e9a228546f6c8a., daughter=true 


",,,,,,,,
HBASE-6335,"How happen?

If server A is down, and it has three log files, all the data is from one region.
File 1: kv01 kv02 kv03
File 2: kv04 kv05 kv06
File 3: kv07 kv08 kv09
Here,kv01 means, its log seqID is 01

Case:Switch to maste-local-log-splitting from distributed-log-splitting

1.Master find serverA is down, and start to split its log files using split-log-splitting.

2.Successfully split log file2, and move it to oldLogs, and generate one edit file named 06 in region recover.edits dir.

3.Master restart, and change the log-splitting policy to maste-local-log-splitting , and start to split file 1, file 3

4.Successfully split log file1 and file3, and generate one edit file named 09 in region recover.edits dir.

5.Region replay edits from edit file 06 and 09, Region's seqID is 06 after it replay edits from 06, and when replaying edit from 09, it will skip kv01,kv02,kv03, So these data loss.


As the above case, if we switch  to distributed-log-splitting from maste-local-log-splitting, it could also cause data loss


Should we fix this bug or avoid the case? I'm not sure...",,,,,,,,
HBASE-6347,"table.jsp does not use a lookup method on {{CatalogTracker}} that does not force a refresh of the cache, thus it can get a stale location if -ROOT- or .META. moved and the master hasn't tried to access them yet.

Should just be a matter of using waitForRoot/Meta.",,1,,,,,,
HBASE-6354,Failed distributed log splitting MonitoredTask is stuck on the master web UI since it is not aborted.,,,,,,,,
HBASE-6357,Failed distributed log splitting MonitoredTask is stuck on the master web UI since it is not aborted.,,,,,,,,
HBASE-6358,"Bulk loading hfiles that don't live on the same filesystem as HBase can cause problems for subtle reasons.

In Store.bulkLoadHFile(), the regionserver will copy the source hfile to its own filesystem if it's not already there. Since this can take a long time for large hfiles, it's likely that the client will timeout and retry. When the client retries repeatedly, there may be several bulkload operations in flight for the same hfile, causing lots of unnecessary IO and tying up handler threads. This can seriously impact performance. In my case, the cluster became unusable and the regionservers had to be kill -9'ed.

Possible solutions:
 # Require that hfiles already be on the same filesystem as HBase in order for bulkloading to succeed. The copy could be handled by LoadIncrementalHFiles before the regionserver is called.
 # Others? I'm not familiar with Hadoop IPC so there may be tricks to extend the timeout or something else.

I'm willing to write a patch but I'd appreciate recommendations on how to proceed.",,,1,,,,,
HBASE-6359,"When the same KeyValue object is used multiple times for deserialization using readFields, some methods may return incorrect values. Here is a sequence of operations that will reproduce the problem:

 # A KeyValue is created whose key has length 10. The private field keyLength is initialized to 0.
 # KeyValue.getKeyLength() is called. This reads the key length 10 from the backing array and caches it in keyLength.
 # KeyValue.readFields() is called to deserialize a new value. The keyLength field is not cleared and keeps its value of 10, even though this value is probably incorrect.
 # If getKeyLength() is called, the value 10 will be returned.

For example, in a reducer with Iterable<KeyValue>, all values after the first one from the iterable are likely to return incorrect values from getKeyLength().

The solution is to clear all memoized values in KeyValue.readFields(). I'll write a patch for this soon.",,,,,,,,
HBASE-6364,"When a server host with a Region Server holding the .META. table is powered down on a live cluster, while the HBase cluster itself detects and reassigns the .META. table, connected HBase Client's take an excessively long time to detect this and re-discover the reassigned .META. 

Workaround: Decrease the ipc.socket.timeout on HBase Client side to a low  value (default is 20s leading to 35 minute recovery time; we were able to get acceptable results with 100ms getting a 3 minute recovery) 

This was found during some hardware failure testing scenarios. 

Test Case:
1) Apply load via client app on HBase cluster for several minutes
2) Power down the region server holding the .META. server (i.e. power off ... and keep it off)
3) Measure how long it takes for cluster to reassign META table and for client threads to re-lookup and re-orient to the lesser cluster (minus the RS and DN on that host).

Observation:
1) Client threads spike up to maxThreads size ... and take over 35 mins to recover (i.e. for the thread count to go back to normal) - no client calls are serviced - they just back up on a synchronized method (see #2 below)

2) All the client app threads queue up behind the oahh.ipc.HBaseClient#setupIOStreams method http://tinyurl.com/7js53dj

After taking several thread dumps we found that the thread within this synchronized method was blocked on  NetUtils.connect(this.socket, remoteId.getAddress(), getSocketTimeout(conf));

The client thread that gets the synchronized lock would try to connect to the dead RS (till socket times out after 20s), retries, and then the next thread gets in and so forth in a serial manner.

Workaround:
-------------------
Default ipc.socket.timeout is set to 20s. We dropped this to a low number (1000 ms,  100 ms, etc) on the client side hbase-site.xml. With this setting, the client threads recovered in a couple of minutes by failing fast and re-discovering the .META. table on a reassigned RS.

Assumption: This ipc.socket.timeout is only ever used during the initial ""HConnection"" setup via the NetUtils.connect and should only ever be used when connectivity to a region server is lost and needs to be re-established. i.e it does not affect the normal ""RPC"" actiivity as this is just the connect timeout.
During RS GC periods, any _new_ clients trying to connect will fail and will require .META. table re-lookups.

This above timeout workaround is only for the HBase client side.",,1,,,,,,
HBASE-6370,"When a server host with a Region Server holding the .META. table is powered down on a live cluster, while the HBase cluster itself detects and reassigns the .META. table, connected HBase Client's take an excessively long time to detect this and re-discover the reassigned .META. 

Workaround: Decrease the ipc.socket.timeout on HBase Client side to a low  value (default is 20s leading to 35 minute recovery time; we were able to get acceptable results with 100ms getting a 3 minute recovery) 

This was found during some hardware failure testing scenarios. 

Test Case:
1) Apply load via client app on HBase cluster for several minutes
2) Power down the region server holding the .META. server (i.e. power off ... and keep it off)
3) Measure how long it takes for cluster to reassign META table and for client threads to re-lookup and re-orient to the lesser cluster (minus the RS and DN on that host).

Observation:
1) Client threads spike up to maxThreads size ... and take over 35 mins to recover (i.e. for the thread count to go back to normal) - no client calls are serviced - they just back up on a synchronized method (see #2 below)

2) All the client app threads queue up behind the oahh.ipc.HBaseClient#setupIOStreams method http://tinyurl.com/7js53dj

After taking several thread dumps we found that the thread within this synchronized method was blocked on  NetUtils.connect(this.socket, remoteId.getAddress(), getSocketTimeout(conf));

The client thread that gets the synchronized lock would try to connect to the dead RS (till socket times out after 20s), retries, and then the next thread gets in and so forth in a serial manner.

Workaround:
-------------------
Default ipc.socket.timeout is set to 20s. We dropped this to a low number (1000 ms,  100 ms, etc) on the client side hbase-site.xml. With this setting, the client threads recovered in a couple of minutes by failing fast and re-discovering the .META. table on a reassigned RS.

Assumption: This ipc.socket.timeout is only ever used during the initial ""HConnection"" setup via the NetUtils.connect and should only ever be used when connectivity to a region server is lost and needs to be re-established. i.e it does not affect the normal ""RPC"" actiivity as this is just the connect timeout.
During RS GC periods, any _new_ clients trying to connect will fail and will require .META. table re-lookups.

This above timeout workaround is only for the HBase client side.",,1,1,,,,,
HBASE-6371,"When a server host with a Region Server holding the .META. table is powered down on a live cluster, while the HBase cluster itself detects and reassigns the .META. table, connected HBase Client's take an excessively long time to detect this and re-discover the reassigned .META. 

Workaround: Decrease the ipc.socket.timeout on HBase Client side to a low  value (default is 20s leading to 35 minute recovery time; we were able to get acceptable results with 100ms getting a 3 minute recovery) 

This was found during some hardware failure testing scenarios. 

Test Case:
1) Apply load via client app on HBase cluster for several minutes
2) Power down the region server holding the .META. server (i.e. power off ... and keep it off)
3) Measure how long it takes for cluster to reassign META table and for client threads to re-lookup and re-orient to the lesser cluster (minus the RS and DN on that host).

Observation:
1) Client threads spike up to maxThreads size ... and take over 35 mins to recover (i.e. for the thread count to go back to normal) - no client calls are serviced - they just back up on a synchronized method (see #2 below)

2) All the client app threads queue up behind the oahh.ipc.HBaseClient#setupIOStreams method http://tinyurl.com/7js53dj

After taking several thread dumps we found that the thread within this synchronized method was blocked on  NetUtils.connect(this.socket, remoteId.getAddress(), getSocketTimeout(conf));

The client thread that gets the synchronized lock would try to connect to the dead RS (till socket times out after 20s), retries, and then the next thread gets in and so forth in a serial manner.

Workaround:
-------------------
Default ipc.socket.timeout is set to 20s. We dropped this to a low number (1000 ms,  100 ms, etc) on the client side hbase-site.xml. With this setting, the client threads recovered in a couple of minutes by failing fast and re-discovering the .META. table on a reassigned RS.

Assumption: This ipc.socket.timeout is only ever used during the initial ""HConnection"" setup via the NetUtils.connect and should only ever be used when connectivity to a region server is lost and needs to be re-established. i.e it does not affect the normal ""RPC"" actiivity as this is just the connect timeout.
During RS GC periods, any _new_ clients trying to connect will fail and will require .META. table re-lookups.

This above timeout workaround is only for the HBase client side.",,,1,,,,,
HBASE-6375,"While investigating an Out of Memory issue, I had an interesting observation where the master tries to assign all regions to a single region server even though 7 other had already registered with it.

As the cluster had MSLAB enabled, this resulted in OOM on the RS when it tired to open all of them.

*From master's log (edited for brevity):*
{quote}
55,468Waitingonregionserver(s)tocheckin
56,968Waitingonregionserver(s)tocheckin
58,468Waitingonregionserver(s)tocheckin
59,968Waitingonregionserver(s)tocheckin
01,242Registeringserver=srv109.datacenter,60020,1338673920529,regionCount=0,userLoad=false
01,469Waitingonregionserver(s)counttosettle;currently=1
02,969Finishedwaitingforregionservercounttosettle;count=1,sleptFor=46500
02,969Exitingwaitonregionserver(s)tocheckin;count=1,stopped=false,countofregionsoutoncluster=0
03,010Processingregion\-ROOT\-,,0.70236052instateM_ZK_REGION_OFFLINE
03,220\-ROOT\-assigned=0,rit=true,location=srv109.datacenter:60020
03,221Processingregion.META.,,1.1028785192instateM_ZK_REGION_OFFLINE
03,336DetectedcompletedassignmentofMETA,notifyingcatalogtracker
03,350.META.assigned=0,rit=true,location=srv109.datacenter:60020
03,350Masterstartupproceeding:clusterstartup
04,006Registeringserver=srv111.datacenter,60020,1338673923399,regionCount=0,userLoad=false
04,012Registeringserver=srv113.datacenter,60020,1338673923532,regionCount=0,userLoad=false
04,269Registeringserver=srv115.datacenter,60020,1338673923471,regionCount=0,userLoad=false
04,363Registeringserver=srv117.datacenter,60020,1338673923928,regionCount=0,userLoad=false
04,599Registeringserver=srv127.datacenter,60020,1338673924067,regionCount=0,userLoad=false
04,606Registeringserver=srv119.datacenter,60020,1338673923953,regionCount=0,userLoad=false
04,804Registeringserver=srv129.datacenter,60020,1338673924339,regionCount=0,userLoad=false
05,126Bulkassigning1252region(s)across1server(s),retainAssignment=true
05,546hd109.datacenter,60020,1338673920529unassignedznodes=207of
{quote}

*A peek at AssignmentManager code offer some explanation:*
{code}
  public void assignAllUserRegions() throws IOException, InterruptedException {
    // Get all available servers
    List<HServerInfo> servers = serverManager.getOnlineServersList();

    // Scan META for all user regions, skipping any disabled tables
    Map<HRegionInfo,HServerAddress> allRegions =
      MetaReader.fullScan(catalogTracker, this.zkTable.getDisabledTables(), true);
    if (allRegions == null || allRegions.isEmpty()) return;

    // Determine what type of assignment to do on startup
    boolean retainAssignment = master.getConfiguration().
      getBoolean(""hbase.master.startup.retainassign"", true);

    Map<HServerInfo, List<HRegionInfo>> bulkPlan = null;
    if (retainAssignment) {
      // Reuse existing assignment info
      bulkPlan = LoadBalancer.retainAssignment(allRegions, servers);
    } else {
      // assign regions in round-robin fashion
      bulkPlan = LoadBalancer.roundRobinAssignment(new ArrayList<HRegionInfo>(allRegions.keySet()), servers);
    }
    LOG.info(""Bulk assigning "" + allRegions.size() + "" region(s) across "" +
      servers.size() + "" server(s), retainAssignment="" + retainAssignment);
    ...
{code}

In the function assignAllUserRegions(), listed above, AM fetches the server list from ServerManager long before it actually use it to create assignment plan.

In between these, it performs a full scan of META to create an assignment map of regions. So even if additional RSes have registered in the meantime (as happened in this case), AM still has the old list of just one server.

This code snippet is from 0.90.6 but the same issue exists in 0.92, 0.94 and trunk. Since MSLAB is enabled by default in 0.92 onwards, any large cluster can hit this issue upon cluster start-up when the following sequence holds true.

# Master start long before the RSes (by default this long ~= 4.5 seconds)
# All the RSes start togather but one wins the race of registering with Master by few seconds.

I am attaching a patch for the trunk which moves the code which fetches the RS list form the beginning of the function to where it is first use.

Apart from this change, one other HBase setting that now becomes important is ""hbase.master.wait.on.regionservers.mintostart"" due to MSLAB being enabled by default.

In large clusters which keeps it enabled now must modify ""hbase.master.wait.on.regionservers.mintostart"" to a suitable number than the default of 1 to ensure that the master waits for a quorum of RSes which are sufficient to open all the regions among themselves. I'll create a separate JIRA for the documentation change.",,,1,,,,,
HBASE-6380,"After bulkloading some HFiles into the Table, we found the force-split didn't work because of the MidKey == NULL. Only if we re-booted the HBase service, the force-split can work normally. ",,1,,,,,,
HBASE-6383,"While investigating an Out of Memory issue, I had an interesting observation where the master tries to assign all regions to a single region server even though 7 other had already registered with it.

As the cluster had MSLAB enabled, this resulted in OOM on the RS when it tired to open all of them.

*From master's log (edited for brevity):*
{quote}
55,468Waitingonregionserver(s)tocheckin
56,968Waitingonregionserver(s)tocheckin
58,468Waitingonregionserver(s)tocheckin
59,968Waitingonregionserver(s)tocheckin
01,242Registeringserver=srv109.datacenter,60020,1338673920529,regionCount=0,userLoad=false
01,469Waitingonregionserver(s)counttosettle;currently=1
02,969Finishedwaitingforregionservercounttosettle;count=1,sleptFor=46500
02,969Exitingwaitonregionserver(s)tocheckin;count=1,stopped=false,countofregionsoutoncluster=0
03,010Processingregion\-ROOT\-,,0.70236052instateM_ZK_REGION_OFFLINE
03,220\-ROOT\-assigned=0,rit=true,location=srv109.datacenter:60020
03,221Processingregion.META.,,1.1028785192instateM_ZK_REGION_OFFLINE
03,336DetectedcompletedassignmentofMETA,notifyingcatalogtracker
03,350.META.assigned=0,rit=true,location=srv109.datacenter:60020
03,350Masterstartupproceeding:clusterstartup
04,006Registeringserver=srv111.datacenter,60020,1338673923399,regionCount=0,userLoad=false
04,012Registeringserver=srv113.datacenter,60020,1338673923532,regionCount=0,userLoad=false
04,269Registeringserver=srv115.datacenter,60020,1338673923471,regionCount=0,userLoad=false
04,363Registeringserver=srv117.datacenter,60020,1338673923928,regionCount=0,userLoad=false
04,599Registeringserver=srv127.datacenter,60020,1338673924067,regionCount=0,userLoad=false
04,606Registeringserver=srv119.datacenter,60020,1338673923953,regionCount=0,userLoad=false
04,804Registeringserver=srv129.datacenter,60020,1338673924339,regionCount=0,userLoad=false
05,126Bulkassigning1252region(s)across1server(s),retainAssignment=true
05,546hd109.datacenter,60020,1338673920529unassignedznodes=207of
{quote}

*A peek at AssignmentManager code offer some explanation:*
{code}
  public void assignAllUserRegions() throws IOException, InterruptedException {
    // Get all available servers
    List<HServerInfo> servers = serverManager.getOnlineServersList();

    // Scan META for all user regions, skipping any disabled tables
    Map<HRegionInfo,HServerAddress> allRegions =
      MetaReader.fullScan(catalogTracker, this.zkTable.getDisabledTables(), true);
    if (allRegions == null || allRegions.isEmpty()) return;

    // Determine what type of assignment to do on startup
    boolean retainAssignment = master.getConfiguration().
      getBoolean(""hbase.master.startup.retainassign"", true);

    Map<HServerInfo, List<HRegionInfo>> bulkPlan = null;
    if (retainAssignment) {
      // Reuse existing assignment info
      bulkPlan = LoadBalancer.retainAssignment(allRegions, servers);
    } else {
      // assign regions in round-robin fashion
      bulkPlan = LoadBalancer.roundRobinAssignment(new ArrayList<HRegionInfo>(allRegions.keySet()), servers);
    }
    LOG.info(""Bulk assigning "" + allRegions.size() + "" region(s) across "" +
      servers.size() + "" server(s), retainAssignment="" + retainAssignment);
    ...
{code}

In the function assignAllUserRegions(), listed above, AM fetches the server list from ServerManager long before it actually use it to create assignment plan.

In between these, it performs a full scan of META to create an assignment map of regions. So even if additional RSes have registered in the meantime (as happened in this case), AM still has the old list of just one server.

This code snippet is from 0.90.6 but the same issue exists in 0.92, 0.94 and trunk. Since MSLAB is enabled by default in 0.92 onwards, any large cluster can hit this issue upon cluster start-up when the following sequence holds true.

# Master start long before the RSes (by default this long ~= 4.5 seconds)
# All the RSes start togather but one wins the race of registering with Master by few seconds.

I am attaching a patch for the trunk which moves the code which fetches the RS list form the beginning of the function to where it is first use.

Apart from this change, one other HBase setting that now becomes important is ""hbase.master.wait.on.regionservers.mintostart"" due to MSLAB being enabled by default.

In large clusters which keeps it enabled now must modify ""hbase.master.wait.on.regionservers.mintostart"" to a suitable number than the default of 1 to ensure that the master waits for a quorum of RSes which are sufficient to open all the regions among themselves. I'll create a separate JIRA for the documentation change.",,,1,,,,,
HBASE-6386,"The code related to this issue is in AccessController.java:permissionGranted().
When creating audit logs, that method will do one of the following:
grant access, create audit log with table name only
deny access because of table permission, create audit log with table name only
deny access because of column family / qualifier permission, create audit log with specific family / qualifier
So, in the case where more than one column family and/or qualifier are in the same request, there will be a loss of information. Even in the case where only one column family and/or qualifier is involved, information may be lost.
It would be better if this behavior consistently included all the information in the request; regardless of access being granted or denied, and regardless which permission caused the denial, the column family and qualifier info should be part of the audit log message.",1,,,,,,,
HBASE-6388,"During a controlled shutdown, Regionserver deletes HLogs even if HRegion.close() fails. We should not be doing this.

",,,,,,,,
HBASE-6389,"Continuing from HBASE-6375.

It seems I was mistaken in my assumption that changing the value of ""hbase.master.wait.on.regionservers.mintostart"" to a sufficient number (from default of 1) can help prevent assignment of all regions to one (or a small number of) region server(s).

While this was the case in 0.90.x and 0.92.x, the behavior has changed in 0.94.0 onwards to address HBASE-4993.

From 0.94.0 onwards, Master will proceed immediately after the timeout has lapsed, even if ""hbase.master.wait.on.regionservers.mintostart"" has not reached.

Reading the current conditions of waitForRegionServers() clarifies it

{code:title=ServerManager.java (trunk rev:1360470)}
....
581	  /**
582	   * Wait for the region servers to report in.
583	   * We will wait until one of this condition is met:
584	   *  - the master is stopped
585	   *  - the 'hbase.master.wait.on.regionservers.timeout' is reached
586	   *  - the 'hbase.master.wait.on.regionservers.maxtostart' number of
587	   *    region servers is reached
588	   *  - the 'hbase.master.wait.on.regionservers.mintostart' is reached AND
589	   *   there have been no new region server in for
590	   *      'hbase.master.wait.on.regionservers.interval' time
591	   *
592	   * @throws InterruptedException
593	   */
594	  public void waitForRegionServers(MonitoredTask status)
595	  throws InterruptedException {
....
....
612	    while (
613	      !this.master.isStopped() &&
614	        slept < timeout &&
615	        count < maxToStart &&
616	        (lastCountChange+interval > now || count < minToStart)
617	      ){
....
{code}

So with the current conditions, the wait will end as soon as timeout is reached even lesser number of RS have checked-in with the Master and the master will proceed with the region assignment among these RSes alone.

As mentioned in -[HBASE-4993|https://issues.apache.org/jira/browse/HBASE-4993?focusedCommentId=13237196#comment-13237196]-, and I concur, this could have disastrous effect in large cluster especially now that MSLAB is turned on.

To enforce the required quorum as specified by ""hbase.master.wait.on.regionservers.mintostart"" irrespective of timeout, these conditions need to be modified as following

{code:title=ServerManager.java}
..
  /**
   * Wait for the region servers to report in.
   * We will wait until one of this condition is met:
   *  - the master is stopped
   *  - the 'hbase.master.wait.on.regionservers.maxtostart' number of
   *    region servers is reached
   *  - the 'hbase.master.wait.on.regionservers.mintostart' is reached AND
   *   there have been no new region server in for
   *      'hbase.master.wait.on.regionservers.interval' time AND
   *   the 'hbase.master.wait.on.regionservers.timeout' is reached
   *
   * @throws InterruptedException
   */
  public void waitForRegionServers(MonitoredTask status)
..
..
    int minToStart = this.master.getConfiguration().
    getInt(""hbase.master.wait.on.regionservers.mintostart"", 1);
    int maxToStart = this.master.getConfiguration().
    getInt(""hbase.master.wait.on.regionservers.maxtostart"", Integer.MAX_VALUE);
    if (maxToStart < minToStart) {
      maxToStart = minToStart;
    }
..
..
    while (
      !this.master.isStopped() &&
        count < maxToStart &&
        (lastCountChange+interval > now || timeout > slept || count < minToStart)
      ){
..
{code}",,1,,,,,,
HBASE-6391,"The Scenario can be reproduce below.
Enabling an table, some region is online on regionserver,some are still being processed.
And restart the master.

when master failover:
        // Region is being served and on an active server
        // add only if region not in disabled and enabling table
        if (false == checkIfRegionBelongsToDisabled(regionInfo)
            && false == checkIfRegionsBelongsToEnabling(regionInfo)) {
          regions.put(regionInfo, regionLocation);
          addToServers(regionLocation, regionInfo);
        }

the opened region will not add to the Regions in master.
and in the following recoverTableInEnablingState,the region will be assigned again.
that will lead to the cluster inconsistent
",,,,,,,,
HBASE-6392,"Before sidelining a big overlap region, hbck tries to close it and offline it at first.  However, sometimes, it throws NotServingRegion or UnknownRegionException.
It could be because the region is not open/assigned at all, or some other issue.
We should figure out why and fix it.

By the way, it's better to print out in the log the command line to bulk load back sidelined regions, if any. ",,,,,,,,
HBASE-6393,"Currently, AccessControler takes care of both generating audit events (by performing access checks) and storing them (by creating a log message and writing it to the AUDITLOG logger).
This makes the logging system the only way to catch audit events. It means that if someone wants to do something fancier (like writing these records to a database somewhere), they need to hack through the logging system, and parse the messages generated by AccessController, which is not optimal.
The attached patch decouples generation and storage by introducing a new interface, used by AccessController, to log the audit events. The current, log-based storage is kept in place so that current users won't be affected by the change.
I'm filing this as an RFC at this point, so the patch is not totally clean; it's on top of HBase 0.92 (which is easier for me to test) and doesn't have any unit tests, for starters. But the changes should be very similar on trunk - I don't remember changes in this particular area of the code between those versions.",1,,,,,,,
HBASE-6416,"This is what I'm getting for leftover data that has no .regioninfo

First:

{quote}
12/07/17 23:13:37 WARN util.HBaseFsck: Failed to read .regioninfo file for region null
java.io.FileNotFoundException: File does not exist: /hbase/stumble_info_urlid_user/bd5f6cfed674389b4d7b8c1be227cb46/.regioninfo
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:1822)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.<init>(DFSClient.java:1813)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:544)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:187)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:456)
	at org.apache.hadoop.hbase.util.HBaseFsck.loadHdfsRegioninfo(HBaseFsck.java:611)
	at org.apache.hadoop.hbase.util.HBaseFsck.access$2200(HBaseFsck.java:140)
	at org.apache.hadoop.hbase.util.HBaseFsck$WorkItemHdfsRegionInfo.call(HBaseFsck.java:2882)
	at org.apache.hadoop.hbase.util.HBaseFsck$WorkItemHdfsRegionInfo.call(HBaseFsck.java:2866)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{quote}

Then it hangs on:

{quote}
12/07/17 23:13:39 INFO util.HBaseFsck: Attempting to handle orphan hdfs dir: hdfs://sfor3s24:10101/hbase/stumble_info_urlid_user/bd5f6cfed674389b4d7b8c1be227cb46
12/07/17 23:13:39 INFO util.HBaseFsck: checking orphan for table null
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.hadoop.hbase.util.HBaseFsck$TableInfo.access$100(HBaseFsck.java:1634)
	at org.apache.hadoop.hbase.util.HBaseFsck.adoptHdfsOrphan(HBaseFsck.java:435)
	at org.apache.hadoop.hbase.util.HBaseFsck.adoptHdfsOrphans(HBaseFsck.java:408)
	at org.apache.hadoop.hbase.util.HBaseFsck.restoreHdfsIntegrity(HBaseFsck.java:529)
	at org.apache.hadoop.hbase.util.HBaseFsck.offlineHdfsIntegrityRepair(HBaseFsck.java:313)
	at org.apache.hadoop.hbase.util.HBaseFsck.onlineHbck(HBaseFsck.java:386)
	at org.apache.hadoop.hbase.util.HBaseFsck.main(HBaseFsck.java:3227)
{quote}

The NPE is sent by:

{code}
Preconditions.checkNotNull(""Table "" + tableName + ""' not present!"", tableInfo);
{code}

I wonder why the condition checking was added if we don't handle it... In any case hbck dies but it hangs because there are some non-daemon hanging around.", ,,,,,,,
HBASE-6420,"Currently, in closing a HLog, logSyncerThread is interrupted. logSyncer could be in the middle to sync the writer.  We should avoid interrupting the sync.", ,,,,,,,
HBASE-6423,"We have a big data use case where we turn off WAL and have a ton of reads and writes. We found that:

1. flushing a memstore takes a while (GZIP compression)
2. incoming writes cause the new memstore to grow in an unbounded fashion
3. this triggers blocking memstore updates
4. in turn, this causes all the RPC handler threads to block on writes to that memstore
5. we are not able to read during this time as RPC handlers are blocked

At a higher level, we should not hold up the RPC threads while blocking updates, and we should build in some sort of rate control.", ,1,,,,,,
HBASE-6427,"Currently if we scan with bot limit and a Filter with filterRow(List<KeyValue>) implemented, an  IncompatibleFilterException will be thrown. The same exception should also be thrown if the filer has its filterRow() implemented.

", ,,1,,,,,
HBASE-6429,"Currently if we scan with bot limit and a Filter with filterRow(List<KeyValue>) implemented, an  IncompatibleFilterException will be thrown. The same exception should also be thrown if the filer has its filterRow() implemented.

", ,,,,,,,
HBASE-6431,"Some of the constructors for FilterList set the internal list of filters to list types which don't support the add operation. As a result 

FilterList(final List<Filter> rowFilters)
FilterList(final Filter... rowFilters)
FilterList(final Operator operator, final List<Filter> rowFilters)
FilterList(final Operator operator, final Filter... rowFilters)

may init private List<Filter> filters = new ArrayList<Filter>(); incorrectly.", ,,,,,,,
HBASE-6432,"ClusterId is normally set into the passed conf during instantiation of an HTable class. In the case of a HRegionServer this is bypassed and set to ""default"" since getMaster() since it uses HBaseRPC to create the proxy directly and bypasses the class which retrieves and sets the correct clusterId. 

This becomes a problem with clients (ie within a coprocessor) using delegation tokens for authentication. Since the token's service will be the correct clusterId and while the TokenSelector is looking for one with service ""default"".", ,1,,,,,,
HBASE-6433,"Seeing some of the recent issues in region assignment, RegionAlreadyInTransitionException is one reason after which the region assignment may or may not happen(in the sense we need to wait for the TM to assign).
In HBASE-6317 we got one problem due to RegionAlreadyInTransitionException on master restart.
Consider the following case, due to some reason like master restart or external assign call, we try to assign a region that is already getting opened in a RS.
Now the next call to assign has already changed the state of the znode and so the current assign that is going on the RS is affected and it fails.  The second assignment that started also fails getting RAITE exception.  Finally both assignments not carrying on.  Idea is to find whether any such RAITE exception can be retried or not.
Here again we have following cases like where
-> The znode is yet to transitioned from OFFLINE to OPENING in RS
-> RS may be in the step of openRegion.
-> RS may be trying to transition OPENING to OPENED.
-> RS is yet to add to online regions in the RS side.

Here in openRegion() and updateMeta() any failures we are moving the znode to FAILED_OPEN.  So in these cases getting an RAITE should be ok.  But in other cases the assignment is stopped.
The idea is to just add the current state of the region assignment in the RIT map in the RS side and using that info we can determine whether the assignment can be retried or not on getting an RAITE.

Considering the current work going on in AM, pls do share if this is needed atleast in the 0.92/0.94 versions?  ", ,,1,,,,,
HBASE-6435,"Seeing some of the recent issues in region assignment, RegionAlreadyInTransitionException is one reason after which the region assignment may or may not happen(in the sense we need to wait for the TM to assign).
In HBASE-6317 we got one problem due to RegionAlreadyInTransitionException on master restart.
Consider the following case, due to some reason like master restart or external assign call, we try to assign a region that is already getting opened in a RS.
Now the next call to assign has already changed the state of the znode and so the current assign that is going on the RS is affected and it fails.  The second assignment that started also fails getting RAITE exception.  Finally both assignments not carrying on.  Idea is to find whether any such RAITE exception can be retried or not.
Here again we have following cases like where
-> The znode is yet to transitioned from OFFLINE to OPENING in RS
-> RS may be in the step of openRegion.
-> RS may be trying to transition OPENING to OPENED.
-> RS is yet to add to online regions in the RS side.

Here in openRegion() and updateMeta() any failures we are moving the znode to FAILED_OPEN.  So in these cases getting an RAITE should be ok.  But in other cases the assignment is stopped.
The idea is to just add the current state of the region assignment in the RIT map in the RS side and using that info we can determine whether the assignment can be retried or not on getting an RAITE.

Considering the current work going on in AM, pls do share if this is needed atleast in the 0.92/0.94 versions?  ", ,,1,,,,,
HBASE-6438,"Seeing some of the recent issues in region assignment, RegionAlreadyInTransitionException is one reason after which the region assignment may or may not happen(in the sense we need to wait for the TM to assign).
In HBASE-6317 we got one problem due to RegionAlreadyInTransitionException on master restart.
Consider the following case, due to some reason like master restart or external assign call, we try to assign a region that is already getting opened in a RS.
Now the next call to assign has already changed the state of the znode and so the current assign that is going on the RS is affected and it fails.  The second assignment that started also fails getting RAITE exception.  Finally both assignments not carrying on.  Idea is to find whether any such RAITE exception can be retried or not.
Here again we have following cases like where
-> The znode is yet to transitioned from OFFLINE to OPENING in RS
-> RS may be in the step of openRegion.
-> RS may be trying to transition OPENING to OPENED.
-> RS is yet to add to online regions in the RS side.

Here in openRegion() and updateMeta() any failures we are moving the znode to FAILED_OPEN.  So in these cases getting an RAITE should be ok.  But in other cases the assignment is stopped.
The idea is to just add the current state of the region assignment in the RIT map in the RS side and using that info we can determine whether the assignment can be retried or not on getting an RAITE.

Considering the current work going on in AM, pls do share if this is needed atleast in the 0.92/0.94 versions?  ", ,,,,,,,
HBASE-6441,"FSUtils.getRootDir() just takes a configuration object, which is used to:
1) Get the name of the root directory
2) Create a filesystem (based on the configured scheme)
3) Qualify the root onto the filesystem

However, the FileSystem from the master filesystem won't generate the correctly qualified root directory under hadoop-2.0 (though it works fine on hadoop-1.0). Seems to be an issue with the configuration parameters.", ,1,,,,,,
HBASE-6455,"I was the running PerformanceEvaluation test with a modified job configuration where the job output path is created before the splits and that unmasked the issue because the the PeInputFormat.getSplits() function expects to find only files under the input path.

The attached patch addresses both the issues.

# Creates separate input and output path rooted under a single folder e.g. ""<user_home>/performance_evaluation/<yyyyMMddHHmmss>/inputs"" and  ""<user_home>/performance_evaluation/<yyyyMMddHHmmss>/outputs"", and
# The PeInputFormat.getSplits(), now skips any folder found under the input path and process only files.", ,,1,,,,,
HBASE-6457,, ,,,,,,,
HBASE-6459,"I was the running PerformanceEvaluation test with a modified job configuration where the job output path is created before the splits and that unmasked the issue because the the PeInputFormat.getSplits() function expects to find only files under the input path.

The attached patch addresses both the issues.

# Creates separate input and output path rooted under a single folder e.g. ""<user_home>/performance_evaluation/<yyyyMMddHHmmss>/inputs"" and  ""<user_home>/performance_evaluation/<yyyyMMddHHmmss>/outputs"", and
# The PeInputFormat.getSplits(), now skips any folder found under the input path and process only files.", ,,1,,,,,
HBASE-6461,"Spun up a new dfs on hadoop-0.20.2-cdh3u3
Started hbase
started running loadtest tool.
killed rs and dn holding root with killall -9 java on server sv4r27s44 at about 2012-07-25 22:40:00

After things stabilize Root is in a bad state. Ran hbck and got:
Exception in thread ""main"" org.apache.hadoop.hbase.client.NoServerForRegionException: No server address listed in -ROOT- for region .META.,,1.1028785192 containing row 
at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:1016)
at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:841)
at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:810)
at org.apache.hadoop.hbase.client.HTable.finishSetup(HTable.java:232)
at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:172)
at org.apache.hadoop.hbase.util.HBaseFsck.connect(HBaseFsck.java:241)
at org.apache.hadoop.hbase.util.HBaseFsck.main(HBaseFsck.java:3236)



hbase(main):001:0> scan '-ROOT-'
ROW                                           COLUMN+CELL                                                                                                                       
12/07/25 22:43:18 INFO security.UserGroupInformation: JAAS Configuration already set up for Hadoop, not re-installing.
 .META.,,1                                    column=info:regioninfo, timestamp=1343255838525, value={NAME => '.META.,,1', STARTKEY => '', ENDKEY => '', ENCODED => 1028785192,}
 .META.,,1                                    column=info:v, timestamp=1343255838525, value=\x00\x00                                                                            
1 row(s) in 0.5930 seconds


Here's the master log: https://gist.github.com/3179194

I tried the same thing with 0.92.1 and I was able to get into a similar situation, so I don't think this is anything new. ", ,,,,,,,
HBASE-6465,"Through the master and regionserver log,I find load balancer repeatedly
close and open region in the same regionserver(period in
hbase.balancer.period ).
Does this is a bug in load balancer and how can I dig into or avoid this?


the hbase and hadoop version is
HBase Version0.94.0, r1332822Hadoop Version0.20.2-cdh3u1,
rbdafb1dbffd0d5f2fbc6ee022e1c8df6500fd638
the following is a detail log about the same region
trackurl_status_list,zO6u4o8,1342291884831.93caf5147d40f5dd4625e160e1b7e956,
and it repeats again and again.:
2012-07-16 00:12:49,843 INFO org.apache.hadoop.hbase.master.HMaster: balance
hri=trackurl_status_list,zO6u4o8,1342291884831.93caf5147d40f5dd4625e160e1b7e956.,
src=192.168.1.2,60020,1342017399608, dest=192.168.1.2,60020,1342002082592
2012-07-16 00:12:49,843 DEBUG
org.apache.hadoop.hbase.master.AssignmentManager: Starting unassignment of
region
trackurl_status_list,zO6u4o8,1342291884831.93caf5147d40f5dd4625e160e1b7e956.
(offlining)
2012-07-16 00:12:49,843 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign:
master:60000-0x4384d0a47f40068 Creating unassigned node for
93caf5147d40f5dd4625e160e1b7e956 in a CLOSING state
2012-07-16 00:12:49,845 DEBUG
org.apache.hadoop.hbase.master.AssignmentManager: Sent CLOSE to
192.168.1.2,60020,1342017399608 for region
trackurl_status_list,zO6u4o8,1342291884831.93caf5147d40f5dd4625e160e1b7e956.
2012-07-16 00:12:50,555 DEBUG
org.apache.hadoop.hbase.master.AssignmentManager: Handling
transition=RS_ZK_REGION_CLOSED, server=192.168.1.2,60020,1342017399608,
region=93caf5147d40f5dd4625e160e1b7e956
2012-07-16 00:12:50,555 DEBUG
org.apache.hadoop.hbase.master.handler.ClosedRegionHandler: Handling CLOSED
event for 93caf5147d40f5dd4625e160e1b7e956
2012-07-16 00:12:50,555 DEBUG
org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE;
was=trackurl_status_list,zO6u4o8,1342291884831.93caf5147d40f5dd4625e160e1b7e956.
state=CLOSED, ts=1342368770556, server=192.168.1.2,60020,1342017399608
2012-07-16 00:12:50,555 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign:
master:60000-0x4384d0a47f40068 Creating (or updating) unassigned node for
93caf5147d40f5dd4625e160e1b7e956 with OFFLINE state
2012-07-16 00:12:50,558 DEBUG
org.apache.hadoop.hbase.master.AssignmentManager: Handling
transition=M_ZK_REGION_OFFLINE, server=10.75.18.34,60000,1342017369575,
region=93caf5147d40f5dd4625e160e1b7e956
2012-07-16 00:12:50,558 DEBUG
org.apache.hadoop.hbase.master.AssignmentManager: Found an existing plan
for
trackurl_status_list,zO6u4o8,1342291884831.93caf5147d40f5dd4625e160e1b7e956.
destination server is 192.168.1.2,60020,1342002082592
2012-07-16 00:12:50,558 DEBUG
org.apache.hadoop.hbase.master.AssignmentManager: Using pre-existing plan
for region
trackurl_status_list,zO6u4o8,1342291884831.93caf5147d40f5dd4625e160e1b7e956.;
plan=hri=trackurl_status_list,zO6u4o8,1342291884831.93caf5147d40f5dd4625e160e1b7e956.,
src=192.168.1.2,60020,1342017399608, dest=192.168.1.2,60020,1342002082592
2012-07-16 00:12:50,558 DEBUG
org.apache.hadoop.hbase.master.AssignmentManager: Assigning region
trackurl_status_list,zO6u4o8,1342291884831.93caf5147d40f5dd4625e160e1b7e956.
to 192.168.1.2,60020,1342002082592
2012-07-16 00:12:50,574 DEBUG
org.apache.hadoop.hbase.master.AssignmentManager: Handling
transition=RS_ZK_REGION_OPENING, server=192.168.1.2,60020,1342017399608,
region=93caf5147d40f5dd4625e160e1b7e956
2012-07-16 00:12:50,635 DEBUG
org.apache.hadoop.hbase.master.AssignmentManager: Handling
transition=RS_ZK_REGION_OPENING, server=192.168.1.2,60020,1342017399608,
region=93caf5147d40f5dd4625e160e1b7e956
2012-07-16 00:12:50,639 DEBUG
org.apache.hadoop.hbase.master.AssignmentManager: Handling
transition=RS_ZK_REGION_OPENED, server=192.168.1.2,60020,1342017399608,
region=93caf5147d40f5dd4625e160e1b7e956
2012-07-16 00:12:50,639 DEBUG
org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Handling OPENED
event for
trackurl_status_list,zO6u4o8,1342291884831.93caf5147d40f5dd4625e160e1b7e956.
from 192.168.1.2,60020,1342017399608; deleting unassigned node
2012-07-16 00:12:50,640 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign:
master:60000-0x4384d0a47f40068 Deleting existing unassigned node for
93caf5147d40f5dd4625e160e1b7e956 that is in expected state
RS_ZK_REGION_OPENED
2012-07-16 00:12:50,641 DEBUG
org.apache.hadoop.hbase.master.AssignmentManager: The znode of region
trackurl_status_list,zO6u4o8,1342291884831.93caf5147d40f5dd4625e160e1b7e956.
has been deleted.
2012-07-16 00:12:50,641 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign:
master:60000-0x4384d0a47f40068 Successfully deleted unassigned node for
region 93caf5147d40f5dd4625e160e1b7e956 in expected state
RS_ZK_REGION_OPENED
2012-07-16 00:12:50,641 INFO
org.apache.hadoop.hbase.master.AssignmentManager: The master has opened the
region
trackurl_status_list,zO6u4o8,1342291884831.93caf5147d40f5dd4625e160e1b7e956.
that was online on 192.168.1.2,60020,1342017399608
2012-07-16 00:17:49,870 INFO org.apache.hadoop.hbase.master.HMaster: balance
hri=trackurl_status_list,zO6u4o8,1342291884831.93caf5147d40f5dd4625e160e1b7e956.,
src=192.168.1.2,60020,1342017399608, dest=192.168.1.2,60020,1342002082592
2012-07-16 00:17:49,870 DEBUG
org.apache.hadoop.hbase.master.AssignmentManager: Starting unassignment of
region
trackurl_status_list,zO6u4o8,1342291884831.93caf5147d40f5dd4625e160e1b7e956.
(offlining)
2012-07-16 00:17:49,870 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign:
master:60000-0x4384d0a47f40068 Creating unassigned node for
93caf5147d40f5dd4625e160e1b7e956 in a CLOSING state
2012-07-16 00:17:49,872 DEBUG
org.apache.hadoop.hbase.master.AssignmentManager: Sent CLOSE to
192.168.1.2,60020,1342017399608 for region
trackurl_status_list,zO6u4o8,1342291884831.93caf5147d40f5dd4625e160e1b7e956.
2012-07-16 00:17:50,464 DEBUG
org.apache.hadoop.hbase.master.AssignmentManager: Handling
transition=RS_ZK_REGION_CLOSED, server=192.168.1.2,60020,1342017399608,
region=93caf5147d40f5dd4625e160e1b7e956
2012-07-16 00:17:50,464 DEBUG
org.apache.hadoop.hbase.master.handler.ClosedRegionHandler: Handling CLOSED
event for 93caf5147d40f5dd4625e160e1b7e956
2012-07-16 00:17:50,464 DEBUG
org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE;
was=trackurl_status_list,zO6u4o8,1342291884831.93caf5147d40f5dd4625e160e1b7e956.
state=CLOSED, ts=1342369070465, server=192.168.1.2,60020,1342017399608
2012-07-16 00:17:50,464 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign:
master:60000-0x4384d0a47f40068 Creating (or updating) unassigned node for
93caf5147d40f5dd4625e160e1b7e956 with OFFLINE state
2012-07-16 00:17:50,467 DEBUG
org.apache.hadoop.hbase.master.AssignmentManager: Handling
transition=M_ZK_REGION_OFFLINE, server=10.75.18.34,60000,1342017369575,
region=93caf5147d40f5dd4625e160e1b7e956
2012-07-16 00:17:50,467 DEBUG
org.apache.hadoop.hbase.master.AssignmentManager: Found an existing plan
for
trackurl_status_list,zO6u4o8,1342291884831.93caf5147d40f5dd4625e160e1b7e956.
destination server is 192.168.1.2,60020,1342002082592
2012-07-16 00:17:50,467 DEBUG
org.apache.hadoop.hbase.master.AssignmentManager: Using pre-existing plan
for region
trackurl_status_list,zO6u4o8,1342291884831.93caf5147d40f5dd4625e160e1b7e956.;
plan=hri=trackurl_status_list,zO6u4o8,1342291884831.93caf5147d40f5dd4625e160e1b7e956.,
src=192.168.1.2,60020,1342017399608, dest=192.168.1.2,60020,1342002082592
2012-07-16 00:17:50,467 DEBUG
org.apache.hadoop.hbase.master.AssignmentManager: Assigning region
trackurl_status_list,zO6u4o8,1342291884831.93caf5147d40f5dd4625e160e1b7e956.
to 192.168.1.2,60020,1342002082592
2012-07-16 00:17:50,509 DEBUG
org.apache.hadoop.hbase.master.AssignmentManager: Handling
transition=RS_ZK_REGION_OPENING, server=192.168.1.2,60020,1342017399608,
region=93caf5147d40f5dd4625e160e1b7e956
2012-07-16 00:17:50,761 DEBUG
org.apache.hadoop.hbase.master.AssignmentManager: Handling
transition=RS_ZK_REGION_OPENING, server=192.168.1.2,60020,1342017399608,
region=93caf5147d40f5dd4625e160e1b7e956
2012-07-16 00:17:50,774 DEBUG
org.apache.hadoop.hbase.master.AssignmentManager: Handling
transition=RS_ZK_REGION_OPENED, server=192.168.1.2,60020,1342017399608,
region=93caf5147d40f5dd4625e160e1b7e956
2012-07-16 00:17:50,774 DEBUG
org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Handling OPENED
event for
trackurl_status_list,zO6u4o8,1342291884831.93caf5147d40f5dd4625e160e1b7e956.
from 192.168.1.2,60020,1342017399608; deleting unassigned node
2012-07-16 00:17:50,774 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign:
master:60000-0x4384d0a47f40068 Deleting existing unassigned node for
93caf5147d40f5dd4625e160e1b7e956 that is in expected state
RS_ZK_REGION_OPENED
2012-07-16 00:17:50,775 DEBUG
org.apache.hadoop.hbase.master.AssignmentManager: The znode of region
trackurl_status_list,zO6u4o8,1342291884831.93caf5147d40f5dd4625e160e1b7e956.
has been deleted.
2012-07-16 00:17:50,775 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign:
master:60000-0x4384d0a47f40068 Successfully deleted unassigned node for
region 93caf5147d40f5dd4625e160e1b7e956 in expected state
RS_ZK_REGION_OPENED
2012-07-16 00:17:50,775 INFO
org.apache.hadoop.hbase.master.AssignmentManager: The master has opened the
region
trackurl_status_list,zO6u4o8,1342291884831.93caf5147d40f5dd4625e160e1b7e956.
that was online on 192.168.1.2,60020,1342017399608
2012-07-16 00:22:49,916 INFO org.apache.hadoop.hbase.master.HMaster: balance
hri=trackurl_status_list,zO6u4o8,1342291884831.93caf5147d40f5dd4625e160e1b7e956.,
src=192.168.1.2,60020,1342017399608, dest=192.168.1.2,60020,1342002082592", ,,,,,,,
HBASE-6466,"I was the running PerformanceEvaluation test with a modified job configuration where the job output path is created before the splits and that unmasked the issue because the the PeInputFormat.getSplits() function expects to find only files under the input path.

The attached patch addresses both the issues.

# Creates separate input and output path rooted under a single folder e.g. ""<user_home>/performance_evaluation/<yyyyMMddHHmmss>/inputs"" and  ""<user_home>/performance_evaluation/<yyyyMMddHHmmss>/outputs"", and
# The PeInputFormat.getSplits(), now skips any folder found under the input path and process only files.", ,,1,,,,,
HBASE-6468,"The RowCounter use FirstKeyOnlyFilter regardless of whether or not the
command line argument specified a column family (or family:qualifier).
In case when no qualifier was specified as argument, the scan will
give correct result. However in the other case the scan instance may
have been set with columns other than the very first column in the
row, causing scan to get nothing as the FirstKeyOnlyFilter removes
everything else.

https://issues.apache.org/jira/browse/HBASE-6042 is related. ", ,1,,,,,,
HBASE-6469,"In Enable/DisableTableHandler code, if something goes wrong in handling, the table state in zk is left as ENABLING / DISABLING. After that we cannot force any more action from the API or CLI, and the only recovery path is restarting the master. 

{code}
    if (done) {
      // Flip the table to enabled.
      this.assignmentManager.getZKTable().setEnabledTable(
        this.tableNameStr);
      LOG.info(""Table '"" + this.tableNameStr
      + ""' was successfully enabled. Status: done="" + done);
    } else {
      LOG.warn(""Table '"" + this.tableNameStr
      + ""' wasn't successfully enabled. Status: done="" + done);
    }
{code}

Here, if done is false, the table state is not changed. There is also no way to set skipTableStateCheck from cli / api. 

We have run into this issue a couple of times before. ", ,,,,,,,
HBASE-6471,"I was the running PerformanceEvaluation test with a modified job configuration where the job output path is created before the splits and that unmasked the issue because the the PeInputFormat.getSplits() function expects to find only files under the input path.

The attached patch addresses both the issues.

# Creates separate input and output path rooted under a single folder e.g. ""<user_home>/performance_evaluation/<yyyyMMddHHmmss>/inputs"" and  ""<user_home>/performance_evaluation/<yyyyMMddHHmmss>/outputs"", and
# The PeInputFormat.getSplits(), now skips any folder found under the input path and process only files.", ,,1,,,,,
HBASE-6473,"consider such Scenario:

we have a table called T1, which has 1 regions: A 

1. move A from rs1 to rs2,and A is now closed
2. disable T1,
3. delete  T1.
when we disable T1, disable handler will just set the zk to disabled and A will still be assigned. when Ais opened, A in transition will be clean out. 

At that time, Deletetable found it is safe to delete all regions and table in meta and fs , it will also delete the zk node of T1.
{code}
while (System.currentTimeMillis() < done) {
        AssignmentManager.RegionState rs = am.isRegionInTransition(region);
        if (rs == null) break;
        Threads.sleep(waitingTimeForEvents);
        LOG.debug(""Waiting on  region to clear regions in transition; "" + rs);
      }
      if (am.isRegionInTransition(region) != null) {
        throw new IOException(""Waited hbase.master.wait.on.region ("" +
          waitTime + ""ms) for region to leave region "" +
          region.getRegionNameAsString() + "" in transitions"");
      }
{code}
however A is still being unassigned, when it finished closed the A,it finds that the disabled state in zk is deleted, and then A will be assigned again.
", ,,,,,,,
HBASE-6478,"When hudson runs for HBASE-6459, it encounters a failed testcase in org.apache.hadoop.hbase.coprocessor.TestClassLoading.testClassLoadingFromLibDirInJar. The link is https://builds.apache.org/job/PreCommit-HBASE-Build/2455/testReport/org.apache.hadoop.hbase.coprocessor/TestClassLoading/testClassLoadingFromLibDirInJar/

I check the log, and find that the function waitTableAvailable will only check the meta table, when rs open the region and update the metalocation in meta, it may not be added to the onlineregions in rs.

for (HRegion region:
        hbase.getRegionServer(0).getOnlineRegionsLocalContext()) {

this Loop will ship, and found1 will be false altogether.
that's why the testcase failed.

So maybe we can  hbave some strictly check when table is created
", ,,,,,,,
HBASE-6479,"If the hfile's version is 1 now, when splitting, two daughters would loadBloomfilter concurrently in the open progress. Because their META block is the same one(parent's META block),  the following expection would be thrown when doing HFileReaderV1#getMetaBlock
{code}
java.io.IOException: Failed null-daughterOpener=af73f8c9a9b409531ac211a9a7f92eba
	at org.apache.hadoop.hbase.regionserver.SplitTransaction.openDaughters(SplitTransaction.java:367)
	at org.apache.hadoop.hbase.regionserver.SplitTransaction.execute(SplitTransaction.java:453)
	at org.apache.hadoop.hbase.regionserver.TestSplitTransaction.testWholesomeSplit(TestSplitTransaction.java:225)
	at org.apache.hadoop.hbase.regionserver.TestSplitTransaction.testWholesomeSplitWithHFileV1(TestSplitTransaction.java:203)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:47)
	at org.junit.rules.RunRules.evaluate(RunRules.java:18)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:49)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)
Caused by: java.io.IOException: java.io.IOException: java.lang.RuntimeException: Cached an already cached block
	at org.apache.hadoop.hbase.regionserver.HRegion.initializeRegionInternals(HRegion.java:540)
	at org.apache.hadoop.hbase.regionserver.HRegion.initialize(HRegion.java:463)
	at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:3784)
	at org.apache.hadoop.hbase.regionserver.SplitTransaction.openDaughterRegion(SplitTransaction.java:506)
	at org.apache.hadoop.hbase.regionserver.SplitTransaction$DaughterOpener.run(SplitTransaction.java:486)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: java.lang.RuntimeException: Cached an already cached block
	at org.apache.hadoop.hbase.regionserver.Store.loadStoreFiles(Store.java:424)
	at org.apache.hadoop.hbase.regionserver.Store.<init>(Store.java:271)
	at org.apache.hadoop.hbase.regionserver.HRegion.instantiateHStore(HRegion.java:2918)
	at org.apache.hadoop.hbase.regionserver.HRegion$2.call(HRegion.java:516)
	at org.apache.hadoop.hbase.regionserver.HRegion$2.call(HRegion.java:1)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	... 1 more
Caused by: java.lang.RuntimeException: Cached an already cached block
	at org.apache.hadoop.hbase.io.hfile.LruBlockCache.cacheBlock(LruBlockCache.java:271)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV1.getMetaBlock(HFileReaderV1.java:258)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV1.getGeneralBloomFilterMetadata(HFileReaderV1.java:689)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Reader.loadBloomfilter(StoreFile.java:1564)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Reader.access$1(StoreFile.java:1558)
	at org.apache.hadoop.hbase.regionserver.StoreFile.open(StoreFile.java:571)
	at org.apache.hadoop.hbase.regionserver.StoreFile.createReader(StoreFile.java:606)
	at org.apache.hadoop.hbase.regionserver.Store$1.call(Store.java:395)
	at org.apache.hadoop.hbase.regionserver.Store$1.call(Store.java:1)
	... 8 more

{code}

We could reproduce the problem through the attached test patch,

It would happen when cluster upgrading from 0.90.x to 0.94.x or 0.92.x


", ,,,,,,,
HBASE-6488,"In IPv6, an address may have a zone-index, which is specified with a percent, eg: ...%0.  This looks like a format string, and thus in a part of the code which uses the hostname as a prefix to another string which is interpreted with String.format, you end up with an exception:


2012-07-31 18:21:39,848 FATAL org.apache.hadoop.hbase.master.HMaster:
Unhandled exception. Starting shutdown.
java.util.UnknownFormatConversionException: Conversion = '0'
        at java.util.Formatter.checkText(Formatter.java:2503)
        at java.util.Formatter.parse(Formatter.java:2467)
        at java.util.Formatter.format(Formatter.java:2414)
        at java.util.Formatter.format(Formatter.java:2367)
        at java.lang.String.format(String.java:2769)
        at com.google.common.util.concurrent.ThreadFactoryBuilder.setNameFormat(ThreadFactoryBuilder.java:68)
        at org.apache.hadoop.hbase.executor.ExecutorService$Executor.<init>(ExecutorService.java:299)
        at org.apache.hadoop.hbase.executor.ExecutorService.startExecutorService(ExecutorService.java:185)
        at org.apache.hadoop.hbase.executor.ExecutorService.startExecutorService(ExecutorService.java:227)
        at org.apache.hadoop.hbase.master.HMaster.startServiceThreads(HMaster.java:821)
        at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:507)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:344)
        at org.apache.hadoop.hbase.master.HMasterCommandLine$LocalHMaster.run(HMasterCommandLine.java:220)
        at java.lang.Thread.run(Thread.java:680)
2012-07-31 18:21:39,908 INFO org.apache.hadoop.hbase.master.HMaster: Aborting
", ,1,,,,,,
HBASE-6490,"Currently, the HBaseAdmin has a constructor that takes an HConnection, but then immediately casts it to an HConnectionManager.HConnectionImplementation:
{code}
  public HBaseAdmin(HConnection connection)
      throws MasterNotRunningException, ZooKeeperConnectionException {
    this.conf = connection.getConfiguration();

    // We want the real class, without showing it our public interface,
    //  hence the cast.
    this.connection = (HConnectionManager.HConnectionImplementation)connection;
{code}

However, this breaks the explicit contract in the javadocs and makes it basically impossible to mock out the hbaseadmin. 

We need to either make the hbaseadmin use a basic HConnection and optimize for cases where its smarter or bring up the couple of methods in HConnectionManager.HConnectionImplementation to the HConnection interface.", ,,,,,,,
HBASE-6495,"Currently, the HBaseAdmin has a constructor that takes an HConnection, but then immediately casts it to an HConnectionManager.HConnectionImplementation:
{code}
  public HBaseAdmin(HConnection connection)
      throws MasterNotRunningException, ZooKeeperConnectionException {
    this.conf = connection.getConfiguration();

    // We want the real class, without showing it our public interface,
    //  hence the cast.
    this.connection = (HConnectionManager.HConnectionImplementation)connection;
{code}

However, this breaks the explicit contract in the javadocs and makes it basically impossible to mock out the hbaseadmin. 

We need to either make the hbaseadmin use a basic HConnection and optimize for cases where its smarter or bring up the couple of methods in HConnectionManager.HConnectionImplementation to the HConnection interface.", ,,,,,,,
HBASE-6496,Provide an example of a RegionServer that listens to a ZK node to learn about what set of KVs can safely be deleted during a compaction.,1,,,,,,,
HBASE-6499,,,,,,,,,
HBASE-6505,,1,,,,,,,
HBASE-6507,"HBaseFsck checks those missing .tableinfo files in loadHdfsRegionInfos() function. However, no IoException will be catched while .tableinfo is missing, since ""FSTableDescriptors.getTableDescriptor"" doesn't throw any IoException.", ,,,,,,,
HBASE-6508,"I was the running PerformanceEvaluation test with a modified job configuration where the job output path is created before the splits and that unmasked the issue because the the PeInputFormat.getSplits() function expects to find only files under the input path.

The attached patch addresses both the issues.

# Creates separate input and output path rooted under a single folder e.g. ""<user_home>/performance_evaluation/<yyyyMMddHHmmss>/inputs"" and  ""<user_home>/performance_evaluation/<yyyyMMddHHmmss>/outputs"", and
# The PeInputFormat.getSplits(), now skips any folder found under the input path and process only files.", ,,1,,,,,
HBASE-6509,"I was the running PerformanceEvaluation test with a modified job configuration where the job output path is created before the splits and that unmasked the issue because the the PeInputFormat.getSplits() function expects to find only files under the input path.

The attached patch addresses both the issues.

# Creates separate input and output path rooted under a single folder e.g. ""<user_home>/performance_evaluation/<yyyyMMddHHmmss>/inputs"" and  ""<user_home>/performance_evaluation/<yyyyMMddHHmmss>/outputs"", and
# The PeInputFormat.getSplits(), now skips any folder found under the input path and process only files.", ,,1,,,,,
HBASE-6516,"HBaseFsck checks those missing .tableinfo files in loadHdfsRegionInfos() function. However, no IoException will be catched while .tableinfo is missing, since ""FSTableDescriptors.getTableDescriptor"" doesn't throw any IoException.", ,1,,,,,,
HBASE-6520,"When use MemStoreLAB, the KeyValues will share the byte array allocated by the MemStoreLAB, all the KeyValues' ""bytes"" attributes are the same byte array. When use the functions such as Bytes.toLong(byte[] bytes, int offset):
{code}
  public static long toLong(byte[] bytes, int offset) {
    return toLong(bytes, offset, SIZEOF_LONG);
  }

  public static long toLong(byte[] bytes, int offset, final int length) {
    if (length != SIZEOF_LONG || offset + length > bytes.length) {
      throw explainWrongLengthOrOffset(bytes, offset, length, SIZEOF_LONG);
    }
    long l = 0;
    for(int i = offset; i < offset + length; i++) {
      l <<= 8;
      l ^= bytes[i] & 0xFF;
    }
    return l;
  }
{code}
If we do not put a long value to the KeyValue, and read it as a long value in HRegion.increment(),the check 
{code}
offset + length > bytes.length
{code}
will take no effects, because the bytes.length is not equal to keyLength+valueLength, indeed it is MemStoreLAB chunkSize which is default 2048 * 1024.



I will paste the patch later.", ,,,,,,,
HBASE-6522,"All the code for the TestSplitLogWorker, only the testAcquireTaskAtStartup waits 100ms, other testCase wait 1000ms. The 100ms is short and sometimes causes testAcquireTaskAtStartup failure.", ,,,,,,,
HBASE-6528,"All the code for the TestSplitLogWorker, only the testAcquireTaskAtStartup waits 100ms, other testCase wait 1000ms. The 100ms is short and sometimes causes testAcquireTaskAtStartup failure.", ,1,,,,,,
HBASE-6529,"With HFile v2 implementation in HBase 0.94 & 0.96, the region server will use HFileSystem as its {color:blue}fs{color}. When it performs bulk load in Store.bulkLoadHFile(), it checks if its {color:blue}fs{color} is the same as {color:blue}srcFs{color}, which however will be DistributedFileSystem. Consequently, it will always perform an extra copy of source files.", ,,1,,,,,
HBASE-6537,"Appear in 94. trunk is ok for the issue
Balancer will collect the regionplans to move(unassign and then assign).
before unassign, disable table appears, 
after close the region in rs, master will delete the znode, romove region from RIT,
and then clean the region from the online regions.

During romoving region from RIT and cleaning out the region from the online regions. 
balancer begins to unassign, it will get a NotServingRegionException and if the table is disabling, it will deal with the state in master and delete the znode . However the table is disabled now, so the RIT and znode will remain. TimeoutMonitor draws a blank on it.

It will hold back enabling the table or balancer unless restart
", ,,,,,,,
HBASE-6550,"With HFile v2 implementation in HBase 0.94 & 0.96, the region server will use HFileSystem as its {color:blue}fs{color}. When it performs bulk load in Store.bulkLoadHFile(), it checks if its {color:blue}fs{color} is the same as {color:blue}srcFs{color}, which however will be DistributedFileSystem. Consequently, it will always perform an extra copy of source files.", ,,1,,,,,
HBASE-6561,"This came from the mailing this:
We were able to replicate this behavior in a pseudo-distributed hbase
(hbase-0.94.1) environment. We wrote a test program that creates a test
table ""MyTestTable"" and populates it with random rows, then it creates a
row with 60,000 columns and repeatedly updates it. Each column has a 18
byte qualifier and a 50 byte value. In our tests, when we ran the
program, we usually never got beyond 15 updates before it would flush
for a really long time. The rows that are being updated are about 4MB
each (minues any hbase metadata).

It doesn't seem like it's caused by GC. I turned on gc logging, and
didn't see any long pauses. This is the gc log during the flush.
http://pastebin.com/vJKKXDx5

This is the regionserver log with debug on during the same flush
http://pastebin.com/Fh5213mg

This is the test program we wrote.
http://pastebin.com/aZ0k5tx2

You should be able to just compile it, and run it against a running
HBase cluster.
$ java TestTable

Carlos
", ,,,,,,,
HBASE-6562,"In internal tests at Salesforce we found that fake row keys sometimes are passed to filters (Filter.filterRowKey(...) specifically).

The KVs are eventually filtered by the StoreScanner/ScanQueryMatcher, but the row key is passed to filterRowKey in RegionScannImpl *before* that happens.", ,,,,,,,
HBASE-6564,"When a column family of a table is deleted, the HDFS space of the column family does not seem to be reclaimed even after a major compaction.", ,1,,,,,,
HBASE-6565,"I develop a coprocessor program ,but found some different results in repeated tests.for example,normally,the result's size is 10.but sometimes it appears 9.
I read the HTable.java code,found a TreeMap(thread-unsafe) be used in multithreading environment.It cause the bug happened", ,,,,,,,
HBASE-6576,"The function:
{code}
public void createTable(final HTableDescriptor desc, byte [][] splitKeys)
{code}

in HBaseAdmin is synchronous and returns once all the regions of the table are online, but does not wait for the table to be enabled, which is the last step of table creation (see CreateTableHandler).

This is confusing and leads to racy code because users do not realize that this is the case.  For example, I saw the following test failure in 0.92 when I ran:
mvn test -Dtest=org.apache.hadoop.hbase.client.TestAdmin#testEnableDisableAddColumnDeleteColumn 

{code}
Error Message

org.apache.hadoop.hbase.TableNotEnabledException: testMasterAdmin at org.apache.hadoop.hbase.master.handler.DisableTableHandler.<init>(DisableTableHandler.java:75) at org.apache.hadoop.hbase.master.HMaster.disableTable(HMaster.java:1154) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364) at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1336)

Stacktrace

org.apache.hadoop.hbase.TableNotEnabledException: org.apache.hadoop.hbase.TableNotEnabledException: testMasterAdmin
at org.apache.hadoop.hbase.master.handler.DisableTableHandler.<init>(DisableTableHandler.java:75)
at org.apache.hadoop.hbase.master.HMaster.disableTable(HMaster.java:1154)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364)
at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1336)
{code}

The issue is that code will create and table and immediately disable it in order to do some testing, for example, to test an operation that only works when the table is disabled.  If the table has not been enabled yet, they will get back a TableNotEnabledException.

The specific test above was fixed in HBASE-5206, but other examples exist in the code, for example the following:
{code}
hbase org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat newtable asdf14
{code}

The code in question is:
{code}
byte[] tname = args[1].getBytes();
HTable table = util.createTable(tname, FAMILIES);
HBaseAdmin admin = new HBaseAdmin(conf);
admin.disableTable(tname);
{code}

It would be better if createTable just waited until the table was enabled, or threw a TableNotEnabledException if it exhausted the configured number of retries.", ,,,,,,,
HBASE-6577,RegionScannerImpl.nextRow() is called when a filter filters the entire row. In that case we should seek to the next row rather then iterating over all versions of all columns to get there., ,,1,,,,,
HBASE-6580,RegionScannerImpl.nextRow() is called when a filter filters the entire row. In that case we should seek to the next row rather then iterating over all versions of all columns to get there., ,,1,,,,,
HBASE-6584,, ,1,,,,,,
HBASE-6585,"Currently, audit log messages contains the ""action"" for which access was checked; this is one of READ, WRITE, CREATE or ADMIN.
These give very little information to the person digging into the logs about what was done, though. You can't ask ""who deleted rows from table x?"", because ""delete"" is translated to a ""WRITE"" action.
It would be nice if the audit logs contained the higher-level operation, either replacing or in addition to the RWCA information.",1,,,,,,,
HBASE-6586,, ,1,,,,,,
HBASE-6587,"In the TimeoutMonitor, we would act on time out for the regions if (this.allRegionServersOffline && !noRSAvailable)
The code is as the following:
{code}
 if (regionState.getStamp() + timeout <= now ||
          (this.allRegionServersOffline && !noRSAvailable)) {
          //decide on action upon timeout or, if some RSs just came back online, we can start the
          // the assignment
          actOnTimeOut(regionState);
        }
{code}

But we found it exists a bug that it would act on time out for the region which was assigned just now , and cause assigning the region twice.


Master log for the region 277b9b6df6de2b9be1353b4fa25f4222:
{code}
2012-08-14 20:42:54,367 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Unable to determine a plan to assign .META.,,1.1028785192 state=OFFLINE, ts=1
344948174367, server=null
2012-08-14 20:44:31,640 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: No previous transition plan was found (or we are ignoring an existing plan) for writete
st,VHXYHJN0BL48HMR4DI1L,1344925649429.277b9b6df6de2b9be1353b4fa25f4222. so generated a random one; hri=writetest,VHXYHJN0BL48HMR4DI1L,1344925649429.277b9b6df6de2b9be13
53b4fa25f4222., src=, dest=dw92.kgb.sqa.cm4,60020,1344948267642; 1 (online=1, available=1) available servers
2012-08-14 20:44:31,640 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x438f53bbf9b0acd Creating (or updating) unassigned node for 277b9b6df6de2b9be13
53b4fa25f4222 with OFFLINE state
2012-08-14 20:44:31,643 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region writetest,VHXYHJN0BL48HMR4DI1L,1344925649429.277b9b6df6de2b9be1353b4fa
25f4222. to dw92.kgb.sqa.cm4,60020,1344948267642
2012-08-14 20:44:32,291 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=dw92.kgb.sqa.cm4,60020,1344948267642, 
region=277b9b6df6de2b9be1353b4fa25f4222
// ?2012-08-14 20:44:32,518 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out: writetest,VHXYHJN0BL48HMR4DI1L,1344925649429.277b9b6df
6de2b9be1353b4fa25f4222. state=OPENING, ts=1344948272279, server=dw92.kgb.sqa.cm4,60020,1344948267642
2012-08-14 20:44:32,518 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been OPENING for too long, reassigning region=writetest,VHXYHJN0BL48HMR4DI1L,
1344925649429.277b9b6df6de2b9be1353b4fa25f4222.
{code}", ,,,,,,,
HBASE-6588,"2012-08-15 19:23:36,178 DEBUG org.apache.hadoop.hbase.client.ClientScanner: Creating scanner over .META. starting at key 'test,,'
2012-08-15 19:23:36,178 DEBUG org.apache.hadoop.hbase.client.ClientScanner: Advancing internal scanner to startKey at 'test,,'
2012-08-15 19:24:09,180 DEBUG org.apache.hadoop.hbase.client.ClientScanner: Creating scanner over .META. starting at key ''
2012-08-15 19:24:09,180 DEBUG org.apache.hadoop.hbase.client.ClientScanner: Advancing internal scanner to startKey at ''
2012-08-15 19:24:09,183 DEBUG org.apache.hadoop.hbase.client.ClientScanner: Finished with scanning at {NAME => '.META.,,1', STARTKEY => '', ENDKEY => '', ENCODED => 1028785192,}
2012-08-15 19:24:09,183 DEBUG org.apache.hadoop.hbase.master.CatalogJanitor: Scanned 2 catalog row(s) and gc'd 0 unreferenced parent region(s)
2012-08-15 19:25:12,260 DEBUG org.apache.hadoop.hbase.master.handler.DeleteTableHandler: Deleting region test,,1345029764571.d1e24b251ca6286c840a9a5f571b7db1. from META and FS
2012-08-15 19:25:12,263 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted region test,,1345029764571.d1e24b251ca6286c840a9a5f571b7db1. from META
2012-08-15 19:25:12,265 INFO org.apache.hadoop.hbase.master.handler.EnableTableHandler: Attemping to enable the table test
2012-08-15 19:25:12,265 WARN org.apache.hadoop.hbase.zookeeper.ZKTable: Moving table test state to enabling but was not first in disabled state: null
2012-08-15 19:25:12,267 DEBUG org.apache.hadoop.hbase.client.ClientScanner: Creating scanner over .META. starting at key 'test,,'
2012-08-15 19:25:12,267 DEBUG org.apache.hadoop.hbase.client.ClientScanner: Advancing internal scanner to startKey at 'test,,'
2012-08-15 19:25:12,270 DEBUG org.apache.hadoop.hbase.client.ClientScanner: Finished with scanning at {NAME => '.META.,,1', STARTKEY => '', ENDKEY => '', ENCODED => 1028785192,}
2012-08-15 19:25:12,270 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event C_M_ENABLE_TABLE
java.lang.NullPointerException
        at org.apache.hadoop.hbase.master.handler.EnableTableHandler.handleEnableTable(EnableTableHandler.java:116)
        at org.apache.hadoop.hbase.master.handler.EnableTableHandler.process(EnableTableHandler.java:97)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:169)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)



table is disabled now, then we enable and delete the table at the same time. 

Since the thread num of MASTER_TABLE_OPERATIONS is 1 by default. The two operations are serial in master.Before deletetable deletes all the regions in meta, CreateTableHandler ships the check of tableExists,then it will block until deletetable finishs, then CreateTableHandler will set zk enabling, and find no data in meta:

 regionsInMeta = MetaReader.getTableRegions(this.ct, tableName, true);
 int countOfRegionsInTable = regionsInMeta.size();

npe will be throwed here. And we could not create the same table anymore.
", ,,,,,,,
HBASE-6589,"When using coprocessor with custom classes like LongColumnInterpreter(mine is MultiColumnSchema), the coprocessor can not work for hot deploy, if the custom classes do not deploy in the regionserver's classpath. Although the self-defined class is deployed in the regions' classpath through hdfs jar.

The exception threw at the regionserver's log:
{code}
2012-08-15 16:24:24,403 ERROR org.apache.hadoop.hbase.io.HbaseObjectWritable: Error in readFields
java.io.IOException: Can't find class com.taobao.hbase.coprocessor.MultiColumnSchema
        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:674)
        at org.apache.hadoop.hbase.client.coprocessor.Exec.readFields(Exec.java:114)
        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:682)
        at org.apache.hadoop.hbase.ipc.Invocation.readFields(Invocation.java:125)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.processData(HBaseServer.java:1292)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.readAndProcess(HBaseServer.java:1207)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.doRead(HBaseServer.java:735)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.doRunLoop(HBaseServer.java:524)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:499)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.ClassNotFoundException: com.taobao.hbase.coprocessor.MultiColumnSchema
        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:247)
        at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:943)
        at org.apache.hadoop.hbase.io.HbaseObjectWritable.getClassByName(HbaseObjectWritable.java:784)
        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:671)
        ... 11 more

{code} 

It is similar as HBASE-4946, but I do not know how to solve this bug.
If add these custom class to the RegionServer's classloader may fix it, but it is conflicted with HBASE-6308 to prevent dependency conflicts.
Does anyone have some idea?", ,1,,,,,,
HBASE-6597,"When regions are getting added and removed lots of cpu time can be used by jmx.  This is caused by sending jmx messages for every new metric that is added or removed.

Seeing jstacks like this:

""RMI TCP Connection(3)-10.4.19.33"" daemon prio=10 tid=0x00007f9d64b1d000 nid=0x353 runnable [0x00007f9d598d6000]
   java.lang.Thread.State: RUNNABLE
	at java.util.HashMap.put(HashMap.java:374)
	at org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase.createMBeanInfo(MetricsDynamicMBeanBase.java:103)
	at org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase.updateMbeanInfoIfMetricsListChanged(MetricsDynamicMBeanBase.java:75)
	at org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase.getAttribute(MetricsDynamicMBeanBase.java:133)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:666)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:638)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1404)
--
	at java.util.TimerThread.run(Timer.java:462)

""Timer thread for monitoring hbase"" daemon prio=10 tid=0x00007f9d648fe000 nid=0x2b5 runnable [0x00007f9d624c7000]
   java.lang.Thread.State: RUNNABLE
	at java.util.HashMap.put(HashMap.java:374)
	at org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase.createMBeanInfo(MetricsDynamicMBeanBase.java:103)
	at org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase.updateMbeanInfoIfMetricsListChanged(MetricsDynamicMBeanBase.java:75)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.regionserver.metrics.RegionServerDynamicMetrics.setNumericMetric(RegionServerDynamicMetrics.java:105)", ,,,,,,,
HBASE-6602,"When regions are getting added and removed lots of cpu time can be used by jmx.  This is caused by sending jmx messages for every new metric that is added or removed.

Seeing jstacks like this:

""RMI TCP Connection(3)-10.4.19.33"" daemon prio=10 tid=0x00007f9d64b1d000 nid=0x353 runnable [0x00007f9d598d6000]
   java.lang.Thread.State: RUNNABLE
	at java.util.HashMap.put(HashMap.java:374)
	at org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase.createMBeanInfo(MetricsDynamicMBeanBase.java:103)
	at org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase.updateMbeanInfoIfMetricsListChanged(MetricsDynamicMBeanBase.java:75)
	at org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase.getAttribute(MetricsDynamicMBeanBase.java:133)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:666)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:638)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1404)
--
	at java.util.TimerThread.run(Timer.java:462)

""Timer thread for monitoring hbase"" daemon prio=10 tid=0x00007f9d648fe000 nid=0x2b5 runnable [0x00007f9d624c7000]
   java.lang.Thread.State: RUNNABLE
	at java.util.HashMap.put(HashMap.java:374)
	at org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase.createMBeanInfo(MetricsDynamicMBeanBase.java:103)
	at org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase.updateMbeanInfoIfMetricsListChanged(MetricsDynamicMBeanBase.java:75)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.regionserver.metrics.RegionServerDynamicMetrics.setNumericMetric(RegionServerDynamicMetrics.java:105)", ,,1,,,,,
HBASE-6603,"Running an HBase scan load through the profiler revealed that RegionMetricsStorage.incrNumericMetric is called way too often.

It turns out that we make this call for *each* KV in StoreScanner.next(...).
Incrementing AtomicLong requires expensive memory barriers.

The observation here is that StoreScanner.next(...) can maintain a simple 
long in its internal loop and only update the metric upon exit. Thus the AtomicLong is not updated nearly as often.

That cuts about 10% runtime from scan only load (I'll quantify this better soon).", ,,1,,,,,
HBASE-6606,"Our nightlies discovered that the patch for HBASE-6160 did not actually fix the issue of ""META entries from daughters can be deleted before parent entries"". Instead of reopening the HBASE-6160, it is cleaner to track it here. 

The original issue is: 
{quote}
HBASE-5986 fixed and issue, where the client sees the META entry for the parent, but not the children. However, after the fix, we have seen the following issue in tests:
Region A is split to -> B, C
Region B is split to -> D, E
After some time, META entry for B is deleted since it is not needed anymore, but META entry for Region A stays in META (C still refers it). In this case, the client throws RegionOfflineException for B.
{quote}

The problem with the fix seems to be that we keep and compare HRegionInfo's in the HashSet at CatalogJanitor.java#scan(), but HRI that are compared are not equal.  
{code}
HashSet<HRegionInfo> parentNotCleaned = new HashSet<HRegionInfo>(); //regions whose parents are still around
      for (Map.Entry<HRegionInfo, Result> e : splitParents.entrySet()) {
        if (!parentNotCleaned.contains(e.getKey()) && cleanParent(e.getKey(), e.getValue())) {
          cleaned++;
        } else {
...
{code}
In the above case, Meta row for region A will contain a serialized version of B that is not offline. However Meta row for region B will contain a serialized version of B that is offline (MetaEditor.offlineParentInMeta() does that). So the deserialized version we put to HashSet and the deserialized version we query contains() from HashSet are different in the offline field, thus HRI.equals() fail. 

", ,,,,,,,
HBASE-6608,"Our nightlies discovered that the patch for HBASE-6160 did not actually fix the issue of ""META entries from daughters can be deleted before parent entries"". Instead of reopening the HBASE-6160, it is cleaner to track it here. 

The original issue is: 
{quote}
HBASE-5986 fixed and issue, where the client sees the META entry for the parent, but not the children. However, after the fix, we have seen the following issue in tests:
Region A is split to -> B, C
Region B is split to -> D, E
After some time, META entry for B is deleted since it is not needed anymore, but META entry for Region A stays in META (C still refers it). In this case, the client throws RegionOfflineException for B.
{quote}

The problem with the fix seems to be that we keep and compare HRegionInfo's in the HashSet at CatalogJanitor.java#scan(), but HRI that are compared are not equal.  
{code}
HashSet<HRegionInfo> parentNotCleaned = new HashSet<HRegionInfo>(); //regions whose parents are still around
      for (Map.Entry<HRegionInfo, Result> e : splitParents.entrySet()) {
        if (!parentNotCleaned.contains(e.getKey()) && cleanParent(e.getKey(), e.getValue())) {
          cleaned++;
        } else {
...
{code}
In the above case, Meta row for region A will contain a serialized version of B that is not offline. However Meta row for region B will contain a serialized version of B that is offline (MetaEditor.offlineParentInMeta() does that). So the deserialized version we put to HashSet and the deserialized version we query contains() from HashSet are different in the offline field, thus HRI.equals() fail. 

", ,,,,,,,
HBASE-6611,"In assigning a region, assignment manager forces the region state offline if it is not. This could cause double assignment, for example, if the region is already assigned and in the Open state, you should not just change it's state to Offline, and assign it again.

I think this could be the root cause for all double assignments IF the region state is reliable.

After this loophole is closed, TestHBaseFsck should come up a different way to create some assignment inconsistencies, for example, calling region server to open a region directly. ", ,,,,,,,
HBASE-6616,"java.lang.AssertionError
at org.junit.Assert.fail(Assert.java:92)
at org.junit.Assert.assertTrue(Assert.java:43)
at org.junit.Assert.assertTrue(Assert.java:54)
at org.apache.hadoop.hbase.ipc.TestDelayedRpc.testTooManyDelayedRpcs(TestDelayedRpc.java:146)

assertTrue(listAppender.getMessages().isEmpty());

listAppender.getMessages returned something like

[Starting Thread-17, Starting Thread-17, Starting Thread-17, Starting Thread-17, Starting Thread-17, Starting Thread-17, Starting Thread-17, Starting Thread-17, Starting Thread-17, Starting IPC Server listener on 41965, IPC Server Responder: starting, IPC Server listener on 41965: starting, IPC Server handler 0 on 41965: starting]

That comes from

HBaseServer.java, Reader class
LOG.info(""Starting "" + getName());
", ,,,,,,,
HBASE-6618,"Running an HBase scan load through the profiler revealed that RegionMetricsStorage.incrNumericMetric is called way too often.

It turns out that we make this call for *each* KV in StoreScanner.next(...).
Incrementing AtomicLong requires expensive memory barriers.

The observation here is that StoreScanner.next(...) can maintain a simple 
long in its internal loop and only update the metric upon exit. Thus the AtomicLong is not updated nearly as often.

That cuts about 10% runtime from scan only load (I'll quantify this better soon).", ,,1,,,,,
HBASE-6619,"While investigating perf of HBase, Michal noticed that we could cut about 5-40% (depending on number of threads) from the total get time in the RPC on the server side if we eliminated re-registering for interest ops.", ,,,,,,,
HBASE-6621,"Bytes.toInt shows up quite often in a profiler run.
It turns out that one source is HFileReaderV2$ScannerV2.getKeyValue().

Notice that we call the KeyValue(byte[], int) constructor, which forces the constructor to determine its size by reading some of the header information and calculate the size. In this case, however, we already know the size (from the call to readKeyValueLen), so we could just use that.

In the extreme case of 10000's of columns this noticeably reduces CPU. ", ,,1,,,,,
HBASE-6629,"Bytes.toInt shows up quite often in a profiler run.
It turns out that one source is HFileReaderV2$ScannerV2.getKeyValue().

Notice that we call the KeyValue(byte[], int) constructor, which forces the constructor to determine its size by reading some of the header information and calculate the size. In this case, however, we already know the size (from the call to readKeyValueLen), so we could just use that.

In the extreme case of 10000's of columns this noticeably reduces CPU. ", ,,1,,,,,
HBASE-6631,#502 and #498 0.92 builds have TestHMasterRPCException failing because of socket timeout when servernotrunning is expected.  Socket timeout is 100ms only., ,1,,,,,,
HBASE-6632,"I see that in 0.92 #502 and #501 that TestAdmin.testHundredsOfTable fails because socket times out after 1500ms.  I see in TestAdmin that before this test runs, testCreateTableRPCTimeOut sets the socket timeout to 1500 and then does not set it back.  Maybe the obnoxious testHundredsOfTable will pass more often if it has the default rpc timeout.", ,1,,,,,,
HBASE-6633,"The test class TestMasterZKSessionRecovery has been removed in trunk.  Its master tests were moved elsewhere or removed because useless (See nkeywal reasoning over in HBASE-5572 ""KeeperException.SessionExpiredException management could be improved in Master""; it was actually removed by HBASE-5549 ""Master can fail if ZooKeeper session expires"").

TestMasterZKSessionRecovery in 0.92 and 0.94 has an extra test that was not in trunk, the sporadically failing testRegionAssignmentAfterMasterRecoveryDueToZKExpiry.  This was added by ""HBASE-6046
Master retry on ZK session expiry causes inconsistent region assignments"".", ,,,,,,,
HBASE-6636,"The test class TestMasterZKSessionRecovery has been removed in trunk.  Its master tests were moved elsewhere or removed because useless (See nkeywal reasoning over in HBASE-5572 ""KeeperException.SessionExpiredException management could be improved in Master""; it was actually removed by HBASE-5549 ""Master can fail if ZooKeeper session expires"").

TestMasterZKSessionRecovery in 0.92 and 0.94 has an extra test that was not in trunk, the sporadically failing testRegionAssignmentAfterMasterRecoveryDueToZKExpiry.  This was added by ""HBASE-6046
Master retry on ZK session expiry causes inconsistent region assignments"".", ,,,,,,,
HBASE-6639,"There are some logics to call Class.newInstance() without catching Exception,
for example, in the method CoprocessorHost.loadInstance().

Class.newInstance() is declared to throw InstantiationException and IllegalAccessException but indeed the method can throw any checked exceptions without declaration.", ,,,,,,,
HBASE-6640,"Bytes.toInt shows up quite often in a profiler run.
It turns out that one source is HFileReaderV2$ScannerV2.getKeyValue().

Notice that we call the KeyValue(byte[], int) constructor, which forces the constructor to determine its size by reading some of the header information and calculate the size. In this case, however, we already know the size (from the call to readKeyValueLen), so we could just use that.

In the extreme case of 10000's of columns this noticeably reduces CPU. ", ,,1,,,,,
HBASE-6641,"when  write a row with wrong or unexist family into a table , we will get message below

org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 2000 actions: DoNotRetryIOException: 2000 times, servers with issues: dw82.kgb.sqa.cm4:64020, at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.processBatchCallback(HConnectionManager.java:1591)
at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.processBatch(HConnectionManager.java:1367)
at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:945)
at org.apache.hadoop.hbase.client.HTable.doPut(HTable.java:801)
at org.apache.hadoop.hbase.client.HTable.put(HTable.java:784)
at zookeeper.WriteMultiThread.doInsert(WriteToTable.java:101)
at zookeeper.WriteMultiThread.run(WriteToTable.java:80)


it not friendly to the client. Need to show the client more details about the exception.", ,1,,,,,,
HBASE-6644,"Bytes.toInt shows up quite often in a profiler run.
It turns out that one source is HFileReaderV2$ScannerV2.getKeyValue().

Notice that we call the KeyValue(byte[], int) constructor, which forces the constructor to determine its size by reading some of the header information and calculate the size. In this case, however, we already know the size (from the call to readKeyValueLen), so we could just use that.

In the extreme case of 10000's of columns this noticeably reduces CPU. ", ,,1,,,,,
HBASE-6647,"Since we upgraded to 0.94.1 from 0.92 I saw that our ICVs are about twice as slow as they were. jstack'ing I saw that most of the time we are waiting on sync()... but those tables have deferred log flush turned on so they shouldn't even be calling it.

HTD.isDeferredLogFlush is currently only called in the append() methods which are pretty much not in use anymore.", ,,1,,,,,
HBASE-6648,"I have seen a hudson run failing with TestMasterObserver.testRegionTransitionOperations failing. The logs (http://bit.ly/Q5PiVJ) seemed to indicate that {noformat} HMaster.move {noformat} was called on a region that was under transition, and the master's assignment manager at that time didn't have information about the region.", ,,,,,,,
HBASE-6649,"There are some operations in HTablePool accessing PoolMap in multiple places without any explicit synchronization. 

For example HTablePool.closeTablePool() calls PoolMap.values(), and calls PoolMap.remove(). If other threads add new instances to the pool in the middle of the calls, the newly added instances might be dropped. (HTablePool.closeTablePool() also has another problem that calling it by multiple threads causes accessing HTable by multiple threads.)

Moreover, PoolMap is not thread safe for the same reason.

For example PoolMap.put() calles ConcurrentMap.get() and calles ConcurrentMap.put(). If other threads add a new instance to the concurent map in the middle of the calls, the new instance might be dropped.

And also implementations of Pool have the same problems.
", ,,,,,,,
HBASE-6651,"There are some operations in HTablePool accessing PoolMap in multiple places without any explicit synchronization. 

For example HTablePool.closeTablePool() calls PoolMap.values(), and calls PoolMap.remove(). If other threads add new instances to the pool in the middle of the calls, the newly added instances might be dropped. (HTablePool.closeTablePool() also has another problem that calling it by multiple threads causes accessing HTable by multiple threads.)

Moreover, PoolMap is not thread safe for the same reason.

For example PoolMap.put() calles ConcurrentMap.get() and calles ConcurrentMap.put(). If other threads add a new instance to the concurent map in the middle of the calls, the new instance might be dropped.

And also implementations of Pool have the same problems.
", ,,,,,,,
HBASE-6652,"now our replication replicationQueueSizeCapacity is set to 64M and replicationQueueNbCapacity is set to 25000. So when a master cluster with many regionserver replicate to a small cluster ?Slave rpc queue will full and out of memory .


java.util.concurrent.ExecutionException: java.io.IOException: Call queue is full, is ipc.server.max.callqueue.size too small?
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.processBatchCallback(HConnectionManager.java:
1524)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.processBatch(HConnectionManager.java:1376)
        at org.apache.hadoop.hbase.client.HTable.batch(HTable.java:700)
        at org.apache.hadoop.hbase.client.HTablePool$PooledHTable.batch(HTablePool.java:361)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSink.batch(ReplicationSink.java:172)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSink.replicateEntries(ReplicationSink.java:129)
        at org.apache.hadoop.hbase.replication.regionserver.Replication.replicateLogEntries(Replication.java:139)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.replicateLogEntries(HRegionServer.java:4018)
        at sun.reflect.GeneratedMethodAccessor41.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:361)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1414)


", ,1,,,,,,
HBASE-6660,"Recently I upgraded three data centers to our own checkout of 0.92.2, last commit :

{noformat}
commit 5accb6a1be4776630126ac21d07adb652b74df95
Author: Zhihong Yu <tedyu@apache.org>
Date:   Mon Aug 20 18:19:45 2012 +0000
HBASE-6608 Fix for HBASE-6160, META entries from daughters can be deleted before parent entries, shouldn't compare HRegionInfo's (Enis)
{noformat}

Two upgrades went fine, upgrade to one data center failed. Failed in the sense that ROOT and META assignment took forever. Panic struck I restarted master and all region servers. I may have deleted zookeeper node /hbase/root-region-server as well, dont ask me why :-( 

After this I managed to get ROOT assigned. But META assignment got stuck again. 

The log is here : https://raw.github.com/gist/3455435/adebd118b47aa3d715201010aa09e5eb8930033c/npe_rs_0.92.2.log

Notice how region server was stuck in a loop of NPE (grep processBatchCallback). There is one more NPE related to zookeeper constructor. 
", ,,,,,,,
HBASE-6662,"Region server incorrectly reports its own address as Master's address while announcing successful connection to Master. 

Example: ine-51 is a RS connecting to master at ine-60
{noformat}
2012-08-22 20:16:02,427 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Attempting connect to Master server at ine-60.rfiserve.net,60000,1345680901361
2012-08-22 20:16:09,501 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Connected to master at ine-51.rfiserve.net/172.22.2.51:60020
{noformat}


Bug is introduced by a typo, the variable *isa* is declared both as a field in class and local variable in the method printing this line. 
{code}
LOG.info(""Connected to master at "" + isa);
{code}

", ,1,,,,,,
HBASE-6663,"On 0.94, HConnectionImplementation has code like this:

{code}
getZooKeeperWatcher();
String tableNameStr = Bytes.toString(tableName);
try {
  if (online) {
    return ZKTable.isEnabledTable(this.zooKeeper, tableNameStr);
  }
  return ZKTable.isDisabledTable(this.zooKeeper, tableNameStr);
{code}

The issue is that between the time that getZooKeeperWatcher is called and this.zooKeeper is used, this.zooKeeper can be set to null by another thread via resetZooKeeperTrackers.

The cleanest solution to me seems to be to cache the value.  I have a patch that does this and a test that fails without the patch and passes with it.

This issue doesn't appear on 0.96 because the zookeeper code has been separated in HBASE-5399.", ,,,,,,,
HBASE-6665,"split operation on ROOT table by specifying explicit split key as .META.
closing the ROOT region and taking long time to fail the split before rollback.
I think we can skip split for ROOT table as how we are doing for META region.", ,,1,,,,,
HBASE-6671,There a services such a oozie which perform actions in behalf of the user using proxy authentication. Retrieving delegation tokens should support this behavior. ,1,,,,,,,
HBASE-6679,"In our nightlies, we have seen RS aborts due to compaction and split racing. Original parent file gets deleted after the compaction, and hence, the daughters don't find the parent data file. The RS kills itself when this happens. Will attach a snippet of the relevant RS logs.", ,,,,,,,
HBASE-6685,"expected error: TSocket: Could not read 4 bytes from localhost:9090

12/06/25 13:48:21 ERROR server.TThreadPoolServer: Error occurred during processing of message. 
java.lang.NullPointerException 
at org.apache.hadoop.hbase.util.Bytes.getBytes(Bytes.java:765) 
at org.apache.hadoop.hbase.thrift.ThriftServer$HBaseHandler.mutateRowTs(ThriftServer.java:591) 
at org.apache.hadoop.hbase.thrift.ThriftServer$HBaseHandler.mutateRow(ThriftServer.java:576) 
at org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRow.getResult(Hbase.java:3630) 
at org.apache.hadoop.hbase.thrift.generated.Hbase$Processor$mutateRow.getResult(Hbase.java:3618) 
at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32) 
at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34) 
at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:176) 
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) 
at java.lang.Thread.run(Thread.java:662)", ,,,,,,,
HBASE-6692,"This got introduced as a side effect of HBASE-3313. Now it is not possible to describe '.META.' or '\-ROOT\-' tables from HBase shell.

{noformat}
hbase(main):002:0> describe '-ROOT-'
DESCRIPTION                                                                                                                                               ENABLED

ERROR: java.lang.IllegalArgumentException: Illegal first character <45> at 0. User-space table names can only start with 'word characters': i.e. [a-zA-Z_0-9]: -ROOT-

Here is some help for this command:
Describe the named table. For example:
  hbase> describe 't1'
{noformat}
", ,1,,,,,,
HBASE-6696,"You can have this if the port is used.
Testable by doing ""nc -l 60020"" before launching the test.
{noformat}
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.hbase.regionserver.TestPriorityRpc
-------------------------------------------------------------------------------
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.286 sec <<< FAILURE!
org.apache.hadoop.hbase.regionserver.TestPriorityRpc  Time elapsed: 0 sec  <<< ERROR!
java.lang.RuntimeException: Failed construction of Regionserver: class org.apache.hadoop.hbase.regionserver.HRegionServer
        at org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(HRegionServer.java:2376)
        at org.apache.hadoop.hbase.regionserver.TestPriorityRpc.onetimeSetup(TestPriorityRpc.java:53)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
        at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:53)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:123)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:104)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:164)
        at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:110)
        at org.apache.maven.surefire.booter.SurefireStarter.invokeProvider(SurefireStarter.java:175)
        at org.apache.maven.surefire.booter.SurefireStarter.runSuitesInProcessWhenForked(SurefireStarter.java:81)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:68)
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(HRegionServer.java:2374)
        ... 22 more
Caused by: java.net.BindException: Problem binding to localhost/127.0.0.1:60020 : Address already in use
        at org.apache.hadoop.hbase.ipc.HBaseServer.bind(HBaseServer.java:295)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.<init>(HBaseServer.java:510)
        at org.apache.hadoop.hbase.ipc.HBaseServer.<init>(HBaseServer.java:1922)
        at org.apache.hadoop.hbase.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:247)
        at org.apache.hadoop.hbase.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:85)
        at org.apache.hadoop.hbase.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:57)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.getServer(HBaseRPC.java:400)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.<init>(HRegionServer.java:516)
        ... 27 more
Caused by: java.net.BindException: Address already in use
        at sun.nio.ch.Net.bind(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:126)
        at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:59)
        at org.apache.hadoop.hbase.ipc.HBaseServer.bind(HBaseServer.java:293)
        ... 34 more
{noformat}", ,,,,,,,
HBASE-6697,"You can have this if the port is used.
Testable by doing ""nc -l 60020"" before launching the test.
{noformat}
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.hbase.regionserver.TestPriorityRpc
-------------------------------------------------------------------------------
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.286 sec <<< FAILURE!
org.apache.hadoop.hbase.regionserver.TestPriorityRpc  Time elapsed: 0 sec  <<< ERROR!
java.lang.RuntimeException: Failed construction of Regionserver: class org.apache.hadoop.hbase.regionserver.HRegionServer
        at org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(HRegionServer.java:2376)
        at org.apache.hadoop.hbase.regionserver.TestPriorityRpc.onetimeSetup(TestPriorityRpc.java:53)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
        at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:53)
        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:123)
        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:104)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:164)
        at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:110)
        at org.apache.maven.surefire.booter.SurefireStarter.invokeProvider(SurefireStarter.java:175)
        at org.apache.maven.surefire.booter.SurefireStarter.runSuitesInProcessWhenForked(SurefireStarter.java:81)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:68)
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(HRegionServer.java:2374)
        ... 22 more
Caused by: java.net.BindException: Problem binding to localhost/127.0.0.1:60020 : Address already in use
        at org.apache.hadoop.hbase.ipc.HBaseServer.bind(HBaseServer.java:295)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.<init>(HBaseServer.java:510)
        at org.apache.hadoop.hbase.ipc.HBaseServer.<init>(HBaseServer.java:1922)
        at org.apache.hadoop.hbase.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:247)
        at org.apache.hadoop.hbase.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:85)
        at org.apache.hadoop.hbase.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:57)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.getServer(HBaseRPC.java:400)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.<init>(HRegionServer.java:516)
        ... 27 more
Caused by: java.net.BindException: Address already in use
        at sun.nio.ch.Net.bind(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:126)
        at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:59)
        at org.apache.hadoop.hbase.ipc.HBaseServer.bind(HBaseServer.java:293)
        ... 34 more
{noformat}", ,1,,,,,,
HBASE-6700,"Please check code below

{code:title=ReplicationSourceManager.java|borderStyle=solid}
// NodeFailoverWorker class
public void run() {
{
    ...

      LOG.info(""Moving "" + rsZnode + ""'s hlogs to my queue"");
      SortedMap<String, SortedSet<String>> newQueues =
          zkHelper.copyQueuesFromRS(rsZnode);   // Node create here*
      zkHelper.deleteRsQueues(rsZnode); 
      if (newQueues == null || newQueues.size() == 0) {
        return;  
      }
    ...
}


  public void closeRecoveredQueue(ReplicationSourceInterface src) {
    LOG.info(""Done with the recovered queue "" + src.getPeerClusterZnode());
    this.oldsources.remove(src);
    this.zkHelper.deleteSource(src.getPeerClusterZnode(), false);  // Node delete here*
  }
{code} 

So from code we can see if newQueues == null or newQueues.size() == 0, Failover replication Source will never start and the failover zk node will never deleted.


eg below failover node will never be delete:

[zk: 10.232.98.77:2181(CONNECTED) 16] ls /hbase-test3-repl/replication/rs/dw93.kgb.sqa.cm4,60020,1346337383956/1-dw93.kgb.sqa.cm4,60020,1346309263932-dw91.kgb.sqa.cm4,60020,1346307150041-dw89.kgb.sqa.cm4,60020,1346307911711-dw93.kgb.sqa.cm4,60020,1346312019213-dw88.kgb.sqa.cm4,60020,1346311774939-dw89.kgb.sqa.cm4,60020,1346312314229-dw93.kgb.sqa.cm4,60020,1346312524307-dw88.kgb.sqa.cm4,60020,1346313203367-dw89.kgb.sqa.cm4,60020,1346313944402-dw88.kgb.sqa.cm4,60020,1346314214286-dw91.kgb.sqa.cm4,60020,1346315119613-dw93.kgb.sqa.cm4,60020,1346314186436-dw88.kgb.sqa.cm4,60020,1346315594396-dw89.kgb.sqa.cm4,60020,1346315909491-dw92.kgb.sqa.cm4,60020,1346315315634-dw89.kgb.sqa.cm4,60020,1346316742242-dw93.kgb.sqa.cm4,60020,1346317604055-dw92.kgb.sqa.cm4,60020,1346318098972-dw91.kgb.sqa.cm4,60020,1346317855650-dw93.kgb.sqa.cm4,60020,1346318532530-dw92.kgb.sqa.cm4,60020,1346318573238-dw89.kgb.sqa.cm4,60020,1346321299040-dw91.kgb.sqa.cm4,60020,1346321304393-dw92.kgb.sqa.cm4,60020,1346325755894-dw89.kgb.sqa.cm4,60020,1346326520895-dw91.kgb.sqa.cm4,60020,1346328246992-dw92.kgb.sqa.cm4,60020,1346327290653-dw93.kgb.sqa.cm4,60020,1346337303018-dw91.kgb.sqa.cm4,60020,1346337318929
[] // empty node will never be deleted
       

", ,,,,,,,
HBASE-6702,"split operation on ROOT table by specifying explicit split key as .META.
closing the ROOT region and taking long time to fail the split before rollback.
I think we can skip split for ROOT table as how we are doing for META region.", ,,1,,,,,
HBASE-6707,"https://builds.apache.org/job/HBase-TRUNK/3293/

Error Message

Archived HFiles (hdfs://localhost:59986/user/jenkins/hbase/.archive/otherTable/01ced3b55d7220a9c460273a4a57b198/fam) should have gotten deleted, but didn't, remaining files:[hdfs://localhost:59986/user/jenkins/hbase/.archive/otherTable/01ced3b55d7220a9c460273a4a57b198/fam/fc872572a1f5443eb55b6e2567cfeb1c]

Stacktrace

java.lang.AssertionError: Archived HFiles (hdfs://localhost:59986/user/jenkins/hbase/.archive/otherTable/01ced3b55d7220a9c460273a4a57b198/fam) should have gotten deleted, but didn't, remaining files:[hdfs://localhost:59986/user/jenkins/hbase/.archive/otherTable/01ced3b55d7220a9c460273a4a57b198/fam/fc872572a1f5443eb55b6e2567cfeb1c]
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.assertTrue(Assert.java:43)
	at org.junit.Assert.assertNull(Assert.java:551)
	at org.apache.hadoop.hbase.backup.example.TestZooKeeperTableArchiveClient.testMultipleTables(TestZooKeeperTableArchiveClient.java:291)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)", ,1,,,,,,
HBASE-6711,"In StoreScanner the number of results is limited to avoid OOMs.
However, this is done by first adding the KV into a local ArrayList and then copying the entries in this list to the final result list.

In turns out the this temporary list is only used to keep track of the size of the result set in this loop. Can use a simple int instead.", ,,1,,,,,
HBASE-6712,"When we stop the RS carrying ROOT/META, if it is in the splitting for some region, the whole stopping process may take 50 mins.
The reason is :
1.ROOT/META region is closed when stopping the regionserver
2.The Split Transaction failed updating META and it will retry
3.The retry num is 100, and the total time is about 50 mins as default;
This configuration is set by HConnectionManager#setServerSideHConnectionRetries

I think 50 mins is too long to acceptable, my suggested solution is closing MetaTable regions after the compact/split thread is closed", ,,,,,,,
HBASE-6713,"When we stop the RS carrying ROOT/META, if it is in the splitting for some region, the whole stopping process may take 50 mins.
The reason is :
1.ROOT/META region is closed when stopping the regionserver
2.The Split Transaction failed updating META and it will retry
3.The retry num is 100, and the total time is about 50 mins as default;
This configuration is set by HConnectionManager#setServerSideHConnectionRetries

I think 50 mins is too long to acceptable, my suggested solution is closing MetaTable regions after the compact/split thread is closed", ,,1,,,,,
HBASE-6719,"Please Take a look below code

{code:title=ReplicationSource.java|borderStyle=solid}

protected boolean openReader(int sleepMultiplier) {
{
  ...
  catch (IOException ioe) {

      LOG.warn(peerClusterZnode + "" Got: "", ioe);
      // TODO Need a better way to determinate if a file is really gone but
      // TODO without scanning all logs dir
      if (sleepMultiplier == this.maxRetriesMultiplier) {
        LOG.warn(""Waited too long for this file, considering dumping"");
        return !processEndOfFile(); // Open a file failed over maxRetriesMultiplier(default 10)
      }
    }
    return true;


  ...
}

  protected boolean processEndOfFile() {
    if (this.queue.size() != 0) {    // Skipped this Hlog . Data loss
      this.currentPath = null;
      this.position = 0;
      return true;
    } else if (this.queueRecovered) {   // Terminate Failover Replication source thread ,data loss
      this.manager.closeRecoveredQueue(this);
      LOG.info(""Finished recovering the queue"");
      this.running = false;
      return true;
    }
    return false;
  }

{code} 


Some Time HDFS will meet some problem but actually Hlog file is OK , So after HDFS back  ,Some data will lose and can not find them back in slave cluster.", ,1,,,,,,
HBASE-6721,"When we stop the RS carrying ROOT/META, if it is in the splitting for some region, the whole stopping process may take 50 mins.
The reason is :
1.ROOT/META region is closed when stopping the regionserver
2.The Split Transaction failed updating META and it will retry
3.The retry num is 100, and the total time is about 50 mins as default;
This configuration is set by HConnectionManager#setServerSideHConnectionRetries

I think 50 mins is too long to acceptable, my suggested solution is closing MetaTable regions after the compact/split thread is closed", ,,1,,,,,
HBASE-6725,"When multiple threads race using CAP with and a lock on the same row, several instances may be allowed to update the cell with the new value (although the expected value is different).
If all threads race with a wrong expected value and a lock, none will be able to update.", ,,,,,,,
HBASE-6727,"The callQueue size (where requests get queued up if all handlers are busy) is a LinkedBlockingQueue of size 100 * number_of_handlers. So, with say 300 handler threads, the call queue can have upto 30k entries queued up. If the requests are large enough, this can result in OOM or severe GC pauses.

Ideally, we should allow this param to be separately configurable independent of the numberof handlers; perhaps an even better approach would be to specify a memory size based limit, instead of a number of entries based limit.

[I have not looked at the trunk version for this issue. So it may or may not be relevant there.]", ,,,,,,,
HBASE-6728,"The per connection responseQueue is an unbounded queue. The request handler threads today try to send the response in line, but if things start to backup, the response is sent via a per connection responder thread. This intermediate queue, because it has no bounds, can be another source of OOMs.

[Have not looked at this issue in trunk. So it may or may not be applicable there.]", ,1,,,,,,
HBASE-6730,"When we stop the RS carrying ROOT/META, if it is in the splitting for some region, the whole stopping process may take 50 mins.
The reason is :
1.ROOT/META region is closed when stopping the regionserver
2.The Split Transaction failed updating META and it will retry
3.The retry num is 100, and the total time is about 50 mins as default;
This configuration is set by HConnectionManager#setServerSideHConnectionRetries

I think 50 mins is too long to acceptable, my suggested solution is closing MetaTable regions after the compact/split thread is closed", ,,1,,,,,
HBASE-6733,"With default settings for ""hbase.splitlog.manager.timeout"" => 25s and ""hbase.splitlog.max.resubmit"" => 3.

On tests mentionned on HBASE-5843, I have variations around this scenario, 0.94 + HDFS 1.0.3:

The regionserver in charge of the split does not answer in less than 25s, so it gets interrupted but actually continues. Sometimes, we go out of the number of retry, sometimes not, sometimes we're out of retry, but the as the interrupts were ignored we finish nicely. In the mean time, the same single task is executed in parallel by multiple nodes, increasing the probability to get into race conditions.

Details:
t0: unplug a box with DN+RS
t + x: other boxes are already connected, to their connection starts to dies. Nevertheless, they don't consider this node as suspect.
t + 180s: zookeeper -> master detects the node as dead. recovery start. It can be less than 180s sometimes it around 150s.
t + 180s: distributed split starts. There is only 1 task, it's immediately acquired by a one RS.
t + 205s: the RS has multiple errors when splitting, because a datanode is missing as well. The master decides to give the task to someone else. But often the task continues in the first RS. Interrupts are often ignored, as it's well stated in the code (""// TODO interrupt often gets swallowed, do what else?"")
{code}
   2012-09-04 18:27:30,404 INFO org.apache.hadoop.hbase.regionserver.SplitLogWorker: Sending interrupt to stop the worker thread
{code}
t + 211s: two regionsservers are processing the same task. They fight for the leases:
{code}
2012-09-04 18:27:32,004 WARN org.apache.hadoop.hdfs.DFSClient: DataStreamer Exception: org.apache.hadoop.ipc.RemoteException:          org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: Lease mismatch on
   /hbase/TABLE/4d1c1a4695b1df8c58d13382b834332e/recovered.edits/0000000000000000037.temp owned by DFSClient_hb_rs_BOX2,60020,1346775882980 but is accessed by DFSClient_hb_rs_BOX1,60020,1346775719125
{code}
     They can fight like this for many files, until the tasks finally get interrupted or finished.
     The taks on the second box can be cancelled as well. In this case, the task is created again for a new box.
     The master seems to stop after 3 attemps. It can as well renounce to split the files. Sometimes the tasks were not cancelled on the RS side, so the split is finished despites what the master thinks and logs. In this case, the assignement starts. In the other, it's ""we've got a problem"").
{code}
2012-09-04 18:43:52,724 INFO org.apache.hadoop.hbase.master.SplitLogManager: Skipping resubmissions of task /hbase/splitlog/hdfs%3A%2F%2FBOX1%3A9000%2Fhbase%2F.logs%2FBOX0%2C60020%2C1346776587640-splitting%2FBOX0%252C60020%252C1346776587640.1346776587832 because threshold 3 reached     
{code}
t + 300s: split is finished. Assignement starts
t + 330s: assignement is finished, regions are available again.


There are a lot of subcases possible depending on the number of logs files, of region server and so on.

The issues are:
1) it's difficult, especially in HBase but not only, to interrupt a task. The pattern is often
{code}
 void f() throws IOException{
  try {
     // whatever throw InterruptedException
  }catch(InterruptedException){
    throw new InterruptedIOException();
  }
}

 boolean g(){
   int nbRetry= 0;  
   for(;;)
      try{
         f();
         return true;
      }catch(IOException e){
         nbRetry++;
         if ( nbRetry > maxRetry) return false;
      }
   } 
 }
{code}

This tyically shallows the interrupt. There are other variation, but this one seems to be the standard.
Even if we fix this in HBase, we need the other layers to be Interrupteble as well. That's not proven.

2) 25s is very aggressive, considering that we have a default timeout of 180s for zookeeper. In other words, we give 180s to a regionserver before acting, but when it comes to split, it's 25s only. There may be reasons for this, but it seems dangerous, as during a failure the cluster is less available than during normal operations. We could do stuff around this, for example:
=> Obvious option: increase the timeout at each try. Something like *2.
=> Also possible: increase the initial timeout
=> check for an update instead of blindly cancelling + resubmitting.

3) Globally, it seems that this retry mechanism duplicates the failure detection already in place with ZK. Would it not make sense to just hook into this existing detection mechanism, and resubmit a task if and only if we detect that the regionserver in charge died? During a failure scenario we should be much more gentle than during normal operation, not the opposite.
", ,,,,,,,
HBASE-6738,"With default settings for ""hbase.splitlog.manager.timeout"" => 25s and ""hbase.splitlog.max.resubmit"" => 3.

On tests mentionned on HBASE-5843, I have variations around this scenario, 0.94 + HDFS 1.0.3:

The regionserver in charge of the split does not answer in less than 25s, so it gets interrupted but actually continues. Sometimes, we go out of the number of retry, sometimes not, sometimes we're out of retry, but the as the interrupts were ignored we finish nicely. In the mean time, the same single task is executed in parallel by multiple nodes, increasing the probability to get into race conditions.

Details:
t0: unplug a box with DN+RS
t + x: other boxes are already connected, to their connection starts to dies. Nevertheless, they don't consider this node as suspect.
t + 180s: zookeeper -> master detects the node as dead. recovery start. It can be less than 180s sometimes it around 150s.
t + 180s: distributed split starts. There is only 1 task, it's immediately acquired by a one RS.
t + 205s: the RS has multiple errors when splitting, because a datanode is missing as well. The master decides to give the task to someone else. But often the task continues in the first RS. Interrupts are often ignored, as it's well stated in the code (""// TODO interrupt often gets swallowed, do what else?"")
{code}
   2012-09-04 18:27:30,404 INFO org.apache.hadoop.hbase.regionserver.SplitLogWorker: Sending interrupt to stop the worker thread
{code}
t + 211s: two regionsservers are processing the same task. They fight for the leases:
{code}
2012-09-04 18:27:32,004 WARN org.apache.hadoop.hdfs.DFSClient: DataStreamer Exception: org.apache.hadoop.ipc.RemoteException:          org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: Lease mismatch on
   /hbase/TABLE/4d1c1a4695b1df8c58d13382b834332e/recovered.edits/0000000000000000037.temp owned by DFSClient_hb_rs_BOX2,60020,1346775882980 but is accessed by DFSClient_hb_rs_BOX1,60020,1346775719125
{code}
     They can fight like this for many files, until the tasks finally get interrupted or finished.
     The taks on the second box can be cancelled as well. In this case, the task is created again for a new box.
     The master seems to stop after 3 attemps. It can as well renounce to split the files. Sometimes the tasks were not cancelled on the RS side, so the split is finished despites what the master thinks and logs. In this case, the assignement starts. In the other, it's ""we've got a problem"").
{code}
2012-09-04 18:43:52,724 INFO org.apache.hadoop.hbase.master.SplitLogManager: Skipping resubmissions of task /hbase/splitlog/hdfs%3A%2F%2FBOX1%3A9000%2Fhbase%2F.logs%2FBOX0%2C60020%2C1346776587640-splitting%2FBOX0%252C60020%252C1346776587640.1346776587832 because threshold 3 reached     
{code}
t + 300s: split is finished. Assignement starts
t + 330s: assignement is finished, regions are available again.


There are a lot of subcases possible depending on the number of logs files, of region server and so on.

The issues are:
1) it's difficult, especially in HBase but not only, to interrupt a task. The pattern is often
{code}
 void f() throws IOException{
  try {
     // whatever throw InterruptedException
  }catch(InterruptedException){
    throw new InterruptedIOException();
  }
}

 boolean g(){
   int nbRetry= 0;  
   for(;;)
      try{
         f();
         return true;
      }catch(IOException e){
         nbRetry++;
         if ( nbRetry > maxRetry) return false;
      }
   } 
 }
{code}

This tyically shallows the interrupt. There are other variation, but this one seems to be the standard.
Even if we fix this in HBase, we need the other layers to be Interrupteble as well. That's not proven.

2) 25s is very aggressive, considering that we have a default timeout of 180s for zookeeper. In other words, we give 180s to a regionserver before acting, but when it comes to split, it's 25s only. There may be reasons for this, but it seems dangerous, as during a failure the cluster is less available than during normal operations. We could do stuff around this, for example:
=> Obvious option: increase the timeout at each try. Something like *2.
=> Also possible: increase the initial timeout
=> check for an update instead of blindly cancelling + resubmitting.

3) Globally, it seems that this retry mechanism duplicates the failure detection already in place with ZK. Would it not make sense to just hook into this existing detection mechanism, and resubmit a task if and only if we detect that the regionserver in charge died? During a failure scenario we should be much more gentle than during normal operation, not the opposite.
", ,1,,,,,,
HBASE-6746,"When using the trunk of HDFS branch 2, I had two errors linked to HBASE-6435:
- a missing test to null
- a method removed. 

This fixes it:
- add the test
- make the test case less dependant on HDFS internal.", ,,,,,,,
HBASE-6747,"When we stop the RS carrying ROOT/META, if it is in the splitting for some region, the whole stopping process may take 50 mins.
The reason is :
1.ROOT/META region is closed when stopping the regionserver
2.The Split Transaction failed updating META and it will retry
3.The retry num is 100, and the total time is about 50 mins as default;
This configuration is set by HConnectionManager#setServerSideHConnectionRetries

I think 50 mins is too long to acceptable, my suggested solution is closing MetaTable regions after the compact/split thread is closed", ,,1,,,,,
HBASE-6748,"You can ealily understand the problem from the below logs:
{code}
[2012-09-01 11:41:02,062] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$CreateAsyncCallback 978] create rc =SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=3
[2012-09-01 11:41:02,062] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$CreateAsyncCallback 978] create rc =SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=2
[2012-09-01 11:41:02,063] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$CreateAsyncCallback 978] create rc =SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=1
[2012-09-01 11:41:02,063] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$CreateAsyncCallback 978] create rc =SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=0
[2012-09-01 11:41:02,063] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager 393] failed to create task node/hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846
[2012-09-01 11:41:02,063] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager 353] Error splitting /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846
[2012-09-01 11:41:02,063] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$DeleteAsyncCallback 1052] delete rc=SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=9223372036854775807
[2012-09-01 11:41:02,064] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$DeleteAsyncCallback 1052] delete rc=SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=9223372036854775806
[2012-09-01 11:41:02,064] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$DeleteAsyncCallback 1052] delete rc=SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=9223372036854775805
[2012-09-01 11:41:02,064] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$DeleteAsyncCallback 1052] delete rc=SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=9223372036854775804
[2012-09-01 11:41:02,065] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$DeleteAsyncCallback 1052] delete rc=SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=9223372036854775803
...................
[2012-09-01 11:41:03,307] [ERROR] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.zookeeper.ClientCnxn 623] Caught unexpected throwable
java.lang.StackOverflowError
{code}", ,1,,,,,,
HBASE-6752,"When we stop the RS carrying ROOT/META, if it is in the splitting for some region, the whole stopping process may take 50 mins.
The reason is :
1.ROOT/META region is closed when stopping the regionserver
2.The Split Transaction failed updating META and it will retry
3.The retry num is 100, and the total time is about 50 mins as default;
This configuration is set by HConnectionManager#setServerSideHConnectionRetries

I think 50 mins is too long to acceptable, my suggested solution is closing MetaTable regions after the compact/split thread is closed", ,,1,,,,,
HBASE-6755,"I was looking at HBase's implementation of locks and saw that is unnecessarily uses an AtomicInteger to obtain a unique lockid.
The observation is that we only need a unique one and don't care if we happen to skip one.
In a very unscientific test I saw the %system CPU reduced when the AtomicInteger is avoided.", ,,1,,,,,
HBASE-6757,"The behaviour of scan is very inefficient when using with FilterList.

The FilterList rewrites the return code from NEXT_ROW to SKIP from a filter if Operator.MUST_PASS_ALL is used. 

This happens when using ColumnPrefixFilter. Even though the ColumnPrefixFilter indicates to jump to NEXT_ROW because no further match can be found, the scan continues to scan all versions of a column in that row and all columns of that row because the ReturnCode from ColumnPrefixFilter has been rewritten by the FilterList from NEXT_ROW to SKIP. 

This is particularly inefficient when there are many versions in a column because the check is performed on all versions of the column instead of just by checking the qualifier of the column name.", ,,1,,,,,
HBASE-6758,"I have seen cases where the replication-executor would lose data to replicate since the file hasn't been closed yet. Upon closing, the new data becomes visible. Before that happens the ZK node shouldn't be deleted in ReplicationSourceManager.logPositionAndCleanOldLogs. Changes need to be made in ReplicationSource.processEndOfFile as well (currentPath related).", ,,,,,,,
HBASE-6760,"The issue seems to exists due to oversight during the rewrite. In line 1289, the variable 'plans' is created as a 'new ArrayList<RegionPlan>()' and then in line 1298, balancerRan is calculated as (plans != null) which for obvious reason, will always return true.

{code:title=HMaster.java (trunk:1383496)}
....
1289        List<RegionPlan> plans = new ArrayList<RegionPlan>();
1290        //Give the balancer the current cluster state.
1291        this.balancer.setClusterStatus(getClusterStatus());
1292        for (Map<ServerName, List<HRegionInfo>> assignments : assignmentsByTable.values()) {
1293          List<RegionPlan> partialPlans = this.balancer.balanceCluster(assignments);
1294          if (partialPlans != null) plans.addAll(partialPlans);
1295        }
1296        int rpCount = 0;  // number of RegionPlans balanced so far
1297        long totalRegPlanExecTime = 0;
1298        balancerRan = plans != null;
1299        if (plans != null && !plans.isEmpty()) {
....
{code}

A simple fix is to initialize 'balancerRan' to 'false', remove ""balancerRan = plans != null"" and add ""balancerRan = true"" after ""if (plans != null && !plans.isEmpty()) {"".

However, a question remains that should we call ""this.cpHost.postBalance();"" if the balancer did not run at this point?

I'll attach the patch shortly if I get a confirmation on this.", ,,,,,,,
HBASE-6768,"I have a CF with one qualifier, data size is > 5 MB, when i try to read the raw binary data as octet-stream using curl, rest server got crashed and curl throws exception as

{code}
 curl -v -H ""Accept: application/octet-stream"" http://abcdefgh-hbase003.test1.test.com:9090/table1/row_key1/cf:qualifer1 > /tmp/out

* About to connect() to abcdefgh-hbase003.test1.test.com port 9090
*   Trying xx.xx.xx.xxx... connected
* Connected to abcdefgh-hbase003.test1.test.com (xx.xxx.xx.xxx) port 9090
> GET /table1/row_key1/cf:qualifer1 HTTP/1.1
> User-Agent: curl/7.15.5 (x86_64-redhat-linux-gnu) libcurl/7.15.5 OpenSSL/0.9.8b zlib/1.2.3 libidn/0.6.5
> Host: abcdefgh-hbase003.test1.test.com:9090
> Accept: application/octet-stream
> 
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0< HTTP/1.1 200 OK
< Content-Length: 5129836
< X-Timestamp: 1347338813129
< Content-Type: application/octet-stream
  0 5009k    0 16272    0     0   7460      0  0:11:27  0:00:02  0:11:25 13872transfer closed with 1148524 bytes remaining to read
 77 5009k   77 3888k    0     0  1765k      0  0:00:02  0:00:02 --:--:-- 3253k* Closing connection #0

curl: (18) transfer closed with 1148524 bytes remaining to read

{code}

Couldn't find the exception in rest server log or no core dump either. This issue is constantly reproducible. Even i tried with HBase Rest client (HRemoteTable) and i could recreate this issue if the data size is > 10 MB (even with MIME_PROTOBUF accept header)
", ,,,,,,,
HBASE-6769,"You can ealily understand the problem from the below logs:
{code}
[2012-09-01 11:41:02,062] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$CreateAsyncCallback 978] create rc =SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=3
[2012-09-01 11:41:02,062] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$CreateAsyncCallback 978] create rc =SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=2
[2012-09-01 11:41:02,063] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$CreateAsyncCallback 978] create rc =SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=1
[2012-09-01 11:41:02,063] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$CreateAsyncCallback 978] create rc =SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=0
[2012-09-01 11:41:02,063] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager 393] failed to create task node/hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846
[2012-09-01 11:41:02,063] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager 353] Error splitting /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846
[2012-09-01 11:41:02,063] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$DeleteAsyncCallback 1052] delete rc=SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=9223372036854775807
[2012-09-01 11:41:02,064] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$DeleteAsyncCallback 1052] delete rc=SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=9223372036854775806
[2012-09-01 11:41:02,064] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$DeleteAsyncCallback 1052] delete rc=SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=9223372036854775805
[2012-09-01 11:41:02,064] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$DeleteAsyncCallback 1052] delete rc=SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=9223372036854775804
[2012-09-01 11:41:02,065] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$DeleteAsyncCallback 1052] delete rc=SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=9223372036854775803
...................
[2012-09-01 11:41:03,307] [ERROR] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.zookeeper.ClientCnxn 623] Caught unexpected throwable
java.lang.StackOverflowError
{code}", ,1,,,,,,
HBASE-6772,"The behaviour of scan is very inefficient when using with FilterList.

The FilterList rewrites the return code from NEXT_ROW to SKIP from a filter if Operator.MUST_PASS_ALL is used. 

This happens when using ColumnPrefixFilter. Even though the ColumnPrefixFilter indicates to jump to NEXT_ROW because no further match can be found, the scan continues to scan all versions of a column in that row and all columns of that row because the ReturnCode from ColumnPrefixFilter has been rewritten by the FilterList from NEXT_ROW to SKIP. 

This is particularly inefficient when there are many versions in a column because the check is performed on all versions of the column instead of just by checking the qualifier of the column name.", ,,1,,,,,
HBASE-6774,"You can ealily understand the problem from the below logs:
{code}
[2012-09-01 11:41:02,062] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$CreateAsyncCallback 978] create rc =SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=3
[2012-09-01 11:41:02,062] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$CreateAsyncCallback 978] create rc =SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=2
[2012-09-01 11:41:02,063] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$CreateAsyncCallback 978] create rc =SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=1
[2012-09-01 11:41:02,063] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$CreateAsyncCallback 978] create rc =SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=0
[2012-09-01 11:41:02,063] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager 393] failed to create task node/hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846
[2012-09-01 11:41:02,063] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager 353] Error splitting /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846
[2012-09-01 11:41:02,063] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$DeleteAsyncCallback 1052] delete rc=SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=9223372036854775807
[2012-09-01 11:41:02,064] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$DeleteAsyncCallback 1052] delete rc=SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=9223372036854775806
[2012-09-01 11:41:02,064] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$DeleteAsyncCallback 1052] delete rc=SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=9223372036854775805
[2012-09-01 11:41:02,064] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$DeleteAsyncCallback 1052] delete rc=SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=9223372036854775804
[2012-09-01 11:41:02,065] [WARN ] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.hadoop.hbase.master.SplitLogManager$DeleteAsyncCallback 1052] delete rc=SESSIONEXPIRED for /hbase/splitlog/hdfs%3A%2F%2Fxh01%3A9000%2Fhbase%2F.logs%2Fxh01%2C20020%2C1339552105088-splitting%2Fxh01%252C20020%252C1339552105088.1339557014846 remaining retries=9223372036854775803
...................
[2012-09-01 11:41:03,307] [ERROR] [MASTER_SERVER_OPERATIONS-xh03,20000,1339549619270-1] [org.apache.zookeeper.ClientCnxn 623] Caught unexpected throwable
java.lang.StackOverflowError
{code}", ,1,,,,,,
HBASE-6780,The number of requests per second is getting divided when it shouldn't be., ,1,,,,,,
HBASE-6784,"The problem is not seen in jenkins build.  
When we run TestCoprocessorScanPolicy.testBaseCases locally or in our internal jenkins we tend to get random failures.  The reason is the 2 puts that we do here is sometimes getting the same timestamp.  This is leading to improper scan results as the version check tends to skip one of the row seeing the timestamp to be same. Marking this as minor.  As we are trying to solve testcase related failures just raising this incase we need to resolve this also.

For eg,
Both the puts are getting the time
{code}
time 1347635287360
time 1347635287360
{code}
", ,,,,,,,
HBASE-6796,See HBASE-5547, ,1,,,,,,
HBASE-6800,"The behaviour of scan is very inefficient when using with FilterList.

The FilterList rewrites the return code from NEXT_ROW to SKIP from a filter if Operator.MUST_PASS_ALL is used. 

This happens when using ColumnPrefixFilter. Even though the ColumnPrefixFilter indicates to jump to NEXT_ROW because no further match can be found, the scan continues to scan all versions of a column in that row and all columns of that row because the ReturnCode from ColumnPrefixFilter has been rewritten by the FilterList from NEXT_ROW to SKIP. 

This is particularly inefficient when there are many versions in a column because the check is performed on all versions of the column instead of just by checking the qualifier of the column name.", ,,1,,,,,
HBASE-6811,"TestDrainingServer#testDrainingServerWithAbort failed in trunk build #3348:
{code}
Error Message

Test conditions are not met: regions were created/deleted during the test.  expected:<27> but was:<24>

Stacktrace

junit.framework.AssertionFailedError: Test conditions are not met: regions were created/deleted during the test.  expected:<27> but was:<24>
	at junit.framework.Assert.fail(Assert.java:50)
	at junit.framework.Assert.failNotEquals(Assert.java:287)
	at junit.framework.Assert.assertEquals(Assert.java:67)
	at junit.framework.Assert.assertEquals(Assert.java:134)
	at org.apache.hadoop.hbase.TestDrainingServer.testDrainingServerWithAbort(TestDrainingServer.java:241)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:62)

Standard Output

Shutting down the Mini HDFS Cluster
Shutting down DataNode 4
Shutting down DataNode 3
Shutting down DataNode 2
Shutting down DataNode 1
Shutting down DataNode 0

Standard Error

2012-09-18 20:18:30,026 INFO  [pool-1-thread-1] hbase.ResourceChecker(144): before TestDrainingServer#testDrainingServerWithAbort: 441 threads, 700 file descriptors 7 connections, 
2012-09-18 20:18:30,044 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZooKeeperWatcher(265): master:35050-0x139db080e690000 Received ZooKeeper Event, type=NodeCreated, state=SyncConnected, path=/hbase/balancer
2012-09-18 20:18:30,044 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZKUtil(1141): master:35050-0x139db080e690000 Retrieved 6 byte(s) of data from znode /hbase/balancer and set watcher; PBUF\x08\x00
2012-09-18 20:18:30,045 DEBUG [IPC Server handler 2 on 35050] zookeeper.ZKUtil(1141): master:35050-0x139db080e690000 Retrieved 6 byte(s) of data from znode /hbase/balancer and set watcher; PBUF\x08\x00
2012-09-18 20:18:30,045 INFO  [IPC Server handler 2 on 35050] master.HMaster(1363): BalanceSwitch=false
2012-09-18 20:18:30,047 INFO  [Thread-604] hbase.TestDrainingServer(211): Regions of drained server are: [t,yyy,1347999506717.bdd1a3ab3105de63248038bd0eb40564.]
2012-09-18 20:18:30,047 INFO  [Thread-604] hbase.TestDrainingServer(112): Making hemera.apache.org,33334,1347999502311 the draining server; it has 1 online regions
2012-09-18 20:18:30,048 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZooKeeperWatcher(265): master:35050-0x139db080e690000 Received ZooKeeper Event, type=NodeChildrenChanged, state=SyncConnected, path=/hbase/draining
2012-09-18 20:18:30,049 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZKUtil(235): master:35050-0x139db080e690000 Set watcher on existing znode /hbase/draining/hemera.apache.org,33334,1347999502311
2012-09-18 20:18:30,049 INFO  [pool-1-thread-1-EventThread] zookeeper.DrainingServerTracker(83): Draining RS node created, adding to list [hemera.apache.org,33334,1347999502311]
2012-09-18 20:18:30,049 INFO  [Thread-604] hbase.TestDrainingServer(220): The available servers are: [hemera.apache.org,38814,1347999502374, hemera.apache.org,58959,1347999502361, hemera.apache.org,43875,1347999502347, hemera.apache.org,51601,1347999502333]
2012-09-18 20:18:30,049 FATAL [Thread-604] regionserver.HRegionServer(1811): ABORTING region server hemera.apache.org,51601,1347999502333: Aborting
2012-09-18 20:18:30,050 FATAL [Thread-604] regionserver.HRegionServer(1818): RegionServer abort: loaded coprocessors are: []
2012-09-18 20:18:30,054 INFO  [Thread-604] regionserver.HRegionServer(1821): Dump of metrics: requestsPerSecond=0, numberOfOnlineRegions=3, numberOfStores=3, numberOfStorefiles=0, storefileIndexSizeMB=0, rootIndexSizeKB=0, totalStaticIndexSizeKB=0, totalStaticBloomSizeKB=0, memstoreSizeMB=0, mbInMemoryWithoutWAL=0, numberOfPutsWithoutWAL=0, readRequestsCount=0, writeRequestsCount=0, compactionQueueSize=0, flushQueueSize=0, usedHeapMB=167, maxHeapMB=1688, blockCacheSizeMB=1.98, blockCacheFreeMB=420.25, blockCacheCount=1, blockCacheHitCount=41, blockCacheMissCount=1, blockCacheEvictedCount=0, blockCacheHitRatio=97%, blockCacheHitCachingRatio=97%, hdfsBlocksLocalityIndex=0, slowHLogAppendCount=0, fsReadLatencyHistogramMean=0, fsReadLatencyHistogramCount=0, fsReadLatencyHistogramMedian=0, fsReadLatencyHistogram75th=0, fsReadLatencyHistogram95th=0, fsReadLatencyHistogram99th=0, fsReadLatencyHistogram999th=0, fsPreadLatencyHistogramMean=0, fsPreadLatencyHistogramCount=0, fsPreadLatencyHistogramMedian=0, fsPreadLatencyHistogram75th=0, fsPreadLatencyHistogram95th=0, fsPreadLatencyHistogram99th=0, fsPreadLatencyHistogram999th=0, fsWriteLatencyHistogramMean=0, fsWriteLatencyHistogramCount=0, fsWriteLatencyHistogramMedian=0, fsWriteLatencyHistogram75th=0, fsWriteLatencyHistogram95th=0, fsWriteLatencyHistogram99th=0, fsWriteLatencyHistogram999th=0
2012-09-18 20:18:30,056 ERROR [IPC Server handler 0 on 35050] master.HMaster(1193): Region server &#0;&#0;hemera.apache.org,51601,1347999502333 reported a fatal error:
ABORTING region server hemera.apache.org,51601,1347999502333: Aborting
2012-09-18 20:18:30,058 INFO  [Thread-604] regionserver.HRegionServer(1737): STOPPED: Aborting
2012-09-18 20:18:30,058 FATAL [Thread-604] regionserver.HRegionServer(1811): ABORTING region server hemera.apache.org,43875,1347999502347: Aborting
2012-09-18 20:18:30,058 FATAL [Thread-604] regionserver.HRegionServer(1818): RegionServer abort: loaded coprocessors are: []
2012-09-18 20:18:30,062 INFO  [Thread-604] regionserver.HRegionServer(1821): Dump of metrics: requestsPerSecond=0, numberOfOnlineRegions=7, numberOfStores=7, numberOfStorefiles=0, storefileIndexSizeMB=0, rootIndexSizeKB=0, totalStaticIndexSizeKB=0, totalStaticBloomSizeKB=0, memstoreSizeMB=0, mbInMemoryWithoutWAL=0, numberOfPutsWithoutWAL=0, readRequestsCount=0, writeRequestsCount=0, compactionQueueSize=0, flushQueueSize=0, usedHeapMB=167, maxHeapMB=1688, blockCacheSizeMB=1.98, blockCacheFreeMB=420.25, blockCacheCount=1, blockCacheHitCount=41, blockCacheMissCount=1, blockCacheEvictedCount=0, blockCacheHitRatio=97%, blockCacheHitCachingRatio=97%, hdfsBlocksLocalityIndex=0, slowHLogAppendCount=0, fsReadLatencyHistogramMean=0, fsReadLatencyHistogramCount=0, fsReadLatencyHistogramMedian=0, fsReadLatencyHistogram75th=0, fsReadLatencyHistogram95th=0, fsReadLatencyHistogram99th=0, fsReadLatencyHistogram999th=0, fsPreadLatencyHistogramMean=0, fsPreadLatencyHistogramCount=0, fsPreadLatencyHistogramMedian=0, fsPreadLatencyHistogram75th=0, fsPreadLatencyHistogram95th=0, fsPreadLatencyHistogram99th=0, fsPreadLatencyHistogram999th=0, fsWriteLatencyHistogramMean=0, fsWriteLatencyHistogramCount=0, fsWriteLatencyHistogramMedian=0, fsWriteLatencyHistogram75th=0, fsWriteLatencyHistogram95th=0, fsWriteLatencyHistogram99th=0, fsWriteLatencyHistogram999th=0
2012-09-18 20:18:30,062 INFO  [RegionServer:1;hemera.apache.org,51601,1347999502333] regionserver.SplitLogWorker(522): Sending interrupt to stop the worker thread
2012-09-18 20:18:30,063 INFO  [SplitLogWorker-hemera.apache.org,51601,1347999502333] regionserver.SplitLogWorker(206): SplitLogWorker interrupted while waiting for task, exiting: java.lang.InterruptedException
2012-09-18 20:18:30,063 INFO  [SplitLogWorker-hemera.apache.org,51601,1347999502333] regionserver.SplitLogWorker(170): SplitLogWorker hemera.apache.org,51601,1347999502333 exiting
2012-09-18 20:18:30,063 ERROR [IPC Server handler 1 on 35050] master.HMaster(1193): Region server &#0;&#0;hemera.apache.org,43875,1347999502347 reported a fatal error:
ABORTING region server hemera.apache.org,43875,1347999502347: Aborting
2012-09-18 20:18:30,063 INFO  [Thread-604] regionserver.HRegionServer(1737): STOPPED: Aborting
2012-09-18 20:18:30,063 INFO  [RegionServer:1;hemera.apache.org,51601,1347999502333.compactionChecker] hbase.Chore(81): RegionServer:1;hemera.apache.org,51601,1347999502333.compactionChecker exiting
2012-09-18 20:18:30,063 FATAL [Thread-604] regionserver.HRegionServer(1811): ABORTING region server hemera.apache.org,58959,1347999502361: Aborting
2012-09-18 20:18:30,063 DEBUG [pool-1-thread-1.LruBlockCache.EvictionThread] hfile.LruBlockCache(418): Block cache LRU eviction started; Attempting to free -408721.95 KB of total=1.98 MB
2012-09-18 20:18:30,063 INFO  [RegionServer:1;hemera.apache.org,51601,1347999502333.cacheFlusher] regionserver.MemStoreFlusher(264): RegionServer:1;hemera.apache.org,51601,1347999502333.cacheFlusher exiting
2012-09-18 20:18:30,064 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-0] handler.CloseRegionHandler(124): Processing close of t,lll,1347999506678.1c1644d5428c228bace562ef1bed5118.
2012-09-18 20:18:30,063 INFO  [RegionServer:1;hemera.apache.org,51601,1347999502333.logRoller] regionserver.LogRoller(118): LogRoller exiting.
2012-09-18 20:18:30,064 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-0] regionserver.HRegion(954): Closing t,lll,1347999506678.1c1644d5428c228bace562ef1bed5118.: disabling compactions & flushes
2012-09-18 20:18:30,064 INFO  [RegionServer:1;hemera.apache.org,51601,1347999502333] regionserver.HRegionServer(943): aborting server hemera.apache.org,51601,1347999502333
2012-09-18 20:18:30,064 INFO  [RegionServer:2;hemera.apache.org,43875,1347999502347] regionserver.SplitLogWorker(522): Sending interrupt to stop the worker thread
2012-09-18 20:18:30,064 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-1] handler.CloseRegionHandler(124): Processing close of t,iii,1347999506669.ae0257433f2941163b1d397f3d4f6efc.
2012-09-18 20:18:30,065 INFO  [RegionServer:2;hemera.apache.org,43875,1347999502347.cacheFlusher] regionserver.MemStoreFlusher(264): RegionServer:2;hemera.apache.org,43875,1347999502347.cacheFlusher exiting
2012-09-18 20:18:30,065 INFO  [RegionServer:2;hemera.apache.org,43875,1347999502347.logRoller] regionserver.LogRoller(118): LogRoller exiting.
2012-09-18 20:18:30,065 INFO  [RegionServer:2;hemera.apache.org,43875,1347999502347.compactionChecker] hbase.Chore(81): RegionServer:2;hemera.apache.org,43875,1347999502347.compactionChecker exiting
2012-09-18 20:18:30,063 FATAL [Thread-604] regionserver.HRegionServer(1818): RegionServer abort: loaded coprocessors are: []
2012-09-18 20:18:30,065 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] handler.CloseRegionHandler(124): Processing close of t,eee,1347999506656.3b9d9fdc58f2319931d38a906c8a1289.
2012-09-18 20:18:30,065 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-1] regionserver.HRegion(954): Closing t,iii,1347999506669.ae0257433f2941163b1d397f3d4f6efc.: disabling compactions & flushes
2012-09-18 20:18:30,067 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] regionserver.HRegion(954): Closing t,eee,1347999506656.3b9d9fdc58f2319931d38a906c8a1289.: disabling compactions & flushes
2012-09-18 20:18:30,065 INFO  [SplitLogWorker-hemera.apache.org,43875,1347999502347] regionserver.SplitLogWorker(206): SplitLogWorker interrupted while waiting for task, exiting: java.lang.InterruptedException
2012-09-18 20:18:30,067 INFO  [RegionServer:2;hemera.apache.org,43875,1347999502347] regionserver.HRegionServer(943): aborting server hemera.apache.org,43875,1347999502347
2012-09-18 20:18:30,067 DEBUG [RegionServer:2;hemera.apache.org,43875,1347999502347] catalog.CatalogTracker(257): Stopping catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@11a59ce
2012-09-18 20:18:30,064 DEBUG [RegionServer:1;hemera.apache.org,51601,1347999502333] catalog.CatalogTracker(257): Stopping catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@f7bf2d
2012-09-18 20:18:30,064 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-0] regionserver.HRegion(975): Updates disabled for region t,lll,1347999506678.1c1644d5428c228bace562ef1bed5118.
2012-09-18 20:18:30,064 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-2] handler.CloseRegionHandler(124): Processing close of t,ttt,1347999506703.73c7f7079688e986da1850e53fb74f9d.
2012-09-18 20:18:30,067 INFO  [RegionServer:2;hemera.apache.org,43875,1347999502347] client.HConnectionManager$HConnectionImplementation(1523): Closing zookeeper sessionid=0x139db080e690009
2012-09-18 20:18:30,067 INFO  [Thread-604] regionserver.HRegionServer(1821): Dump of metrics: requestsPerSecond=0, numberOfOnlineRegions=4, numberOfStores=4, numberOfStorefiles=0, storefileIndexSizeMB=0, rootIndexSizeKB=0, totalStaticIndexSizeKB=0, totalStaticBloomSizeKB=0, memstoreSizeMB=0, mbInMemoryWithoutWAL=0, numberOfPutsWithoutWAL=0, readRequestsCount=0, writeRequestsCount=0, compactionQueueSize=0, flushQueueSize=0, usedHeapMB=168, maxHeapMB=1688, blockCacheSizeMB=1.98, blockCacheFreeMB=420.25, blockCacheCount=1, blockCacheHitCount=41, blockCacheMissCount=1, blockCacheEvictedCount=0, blockCacheHitRatio=97%, blockCacheHitCachingRatio=97%, hdfsBlocksLocalityIndex=0, slowHLogAppendCount=0, fsReadLatencyHistogramMean=0, fsReadLatencyHistogramCount=0, fsReadLatencyHistogramMedian=0, fsReadLatencyHistogram75th=0, fsReadLatencyHistogram95th=0, fsReadLatencyHistogram99th=0, fsReadLatencyHistogram999th=0, fsPreadLatencyHistogramMean=0, fsPreadLatencyHistogramCount=0, fsPreadLatencyHistogramMedian=0, fsPreadLatencyHistogram75th=0, fsPreadLatencyHistogram95th=0, fsPreadLatencyHistogram99th=0, fsPreadLatencyHistogram999th=0, fsWriteLatencyHistogramMean=0, fsWriteLatencyHistogramCount=0, fsWriteLatencyHistogramMedian=0, fsWriteLatencyHistogram75th=0, fsWriteLatencyHistogram95th=0, fsWriteLatencyHistogram99th=0, fsWriteLatencyHistogram999th=0
2012-09-18 20:18:30,067 INFO  [SplitLogWorker-hemera.apache.org,43875,1347999502347] regionserver.SplitLogWorker(170): SplitLogWorker hemera.apache.org,43875,1347999502347 exiting
2012-09-18 20:18:30,067 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] regionserver.HRegion(975): Updates disabled for region t,eee,1347999506656.3b9d9fdc58f2319931d38a906c8a1289.
2012-09-18 20:18:30,067 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-2] handler.CloseRegionHandler(124): Processing close of t,kkk,1347999506675.d91c2dc99d3bac1d51686c9a6b4e74aa.
2012-09-18 20:18:30,069 ERROR [IPC Server handler 3 on 35050] master.HMaster(1193): Region server &#0;&#0;hemera.apache.org,58959,1347999502361 reported a fatal error:
ABORTING region server hemera.apache.org,58959,1347999502361: Aborting
2012-09-18 20:18:30,069 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-2] regionserver.HRegion(954): Closing t,kkk,1347999506675.d91c2dc99d3bac1d51686c9a6b4e74aa.: disabling compactions & flushes
2012-09-18 20:18:30,067 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-1] regionserver.HRegion(975): Updates disabled for region t,iii,1347999506669.ae0257433f2941163b1d397f3d4f6efc.
2012-09-18 20:18:30,066 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-1] handler.CloseRegionHandler(124): Processing close of t,fff,1347999506661.7d7428c069f7e46b3ded4161a6d8d961.
2012-09-18 20:18:30,070 INFO  [RegionServer:1;hemera.apache.org,51601,1347999502333] client.HConnectionManager$HConnectionImplementation(1523): Closing zookeeper sessionid=0x139db080e690007
2012-09-18 20:18:30,070 INFO  [Thread-604] regionserver.HRegionServer(1737): STOPPED: Aborting
2012-09-18 20:18:30,070 INFO  [StoreCloserThread-t,iii,1347999506669.ae0257433f2941163b1d397f3d4f6efc.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,070 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-2] regionserver.HRegion(975): Updates disabled for region t,kkk,1347999506675.d91c2dc99d3bac1d51686c9a6b4e74aa.
2012-09-18 20:18:30,070 INFO  [StoreCloserThread-t,eee,1347999506656.3b9d9fdc58f2319931d38a906c8a1289.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,071 INFO  [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-1] regionserver.HRegion(1023): Closed t,iii,1347999506669.ae0257433f2941163b1d397f3d4f6efc.
2012-09-18 20:18:30,068 INFO  [StoreCloserThread-t,lll,1347999506678.1c1644d5428c228bace562ef1bed5118.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,071 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-1] handler.CloseRegionHandler(168): Closed region t,iii,1347999506669.ae0257433f2941163b1d397f3d4f6efc.
2012-09-18 20:18:30,071 INFO  [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] regionserver.HRegion(1023): Closed t,eee,1347999506656.3b9d9fdc58f2319931d38a906c8a1289.
2012-09-18 20:18:30,068 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-2] regionserver.HRegion(954): Closing t,ttt,1347999506703.73c7f7079688e986da1850e53fb74f9d.: disabling compactions & flushes
2012-09-18 20:18:30,071 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] handler.CloseRegionHandler(168): Closed region t,eee,1347999506656.3b9d9fdc58f2319931d38a906c8a1289.
2012-09-18 20:18:30,071 INFO  [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-0] regionserver.HRegion(1023): Closed t,lll,1347999506678.1c1644d5428c228bace562ef1bed5118.
2012-09-18 20:18:30,071 INFO  [RegionServer:1;hemera.apache.org,51601,1347999502333] regionserver.HRegionServer(1091): Waiting on 2 regions to close
2012-09-18 20:18:30,071 DEBUG [RegionServer:1;hemera.apache.org,51601,1347999502333] regionserver.HRegionServer(1095): {73c7f7079688e986da1850e53fb74f9d=t,ttt,1347999506703.73c7f7079688e986da1850e53fb74f9d.}
2012-09-18 20:18:30,071 INFO  [RegionServer:3;hemera.apache.org,58959,1347999502361] regionserver.SplitLogWorker(522): Sending interrupt to stop the worker thread
2012-09-18 20:18:30,070 INFO  [Thread-604] hbase.TestDrainingServer(239): Regions of drained server are: [t,yyy,1347999506717.bdd1a3ab3105de63248038bd0eb40564.]
2012-09-18 20:18:30,072 INFO  [RegionServer:3;hemera.apache.org,58959,1347999502361.cacheFlusher] regionserver.MemStoreFlusher(264): RegionServer:3;hemera.apache.org,58959,1347999502361.cacheFlusher exiting
2012-09-18 20:18:30,070 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-1] regionserver.HRegion(954): Closing t,fff,1347999506661.7d7428c069f7e46b3ded4161a6d8d961.: disabling compactions & flushes
2012-09-18 20:18:30,072 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-0] handler.CloseRegionHandler(124): Processing close of t,ggg,1347999506664.b48b882c9992c9773e9bfda020d2137c.
2012-09-18 20:18:30,070 INFO  [RegionServer:2;hemera.apache.org,43875,1347999502347] regionserver.HRegionServer(1091): Waiting on 8 regions to close
2012-09-18 20:18:30,072 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-0] regionserver.HRegion(954): Closing t,ggg,1347999506664.b48b882c9992c9773e9bfda020d2137c.: disabling compactions & flushes
2012-09-18 20:18:30,072 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-2] handler.CloseRegionHandler(124): Processing close of t,hhh,1347999506667.5a1faf1d76da0300d16f9c8dbb752824.
2012-09-18 20:18:30,072 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-0] regionserver.HRegion(975): Updates disabled for region t,ggg,1347999506664.b48b882c9992c9773e9bfda020d2137c.
2012-09-18 20:18:30,073 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-2] regionserver.HRegion(954): Closing t,hhh,1347999506667.5a1faf1d76da0300d16f9c8dbb752824.: disabling compactions & flushes
2012-09-18 20:18:30,072 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-1] handler.CloseRegionHandler(124): Processing close of t,www,1347999506711.985b3c0de2cbfe789c69850bb128887d.
2012-09-18 20:18:30,073 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZooKeeperWatcher(265): master:35050-0x139db080e690000 Received ZooKeeper Event, type=NodeDeleted, state=SyncConnected, path=/hbase/draining/hemera.apache.org,33334,1347999502311
2012-09-18 20:18:30,072 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-1] regionserver.HRegion(975): Updates disabled for region t,fff,1347999506661.7d7428c069f7e46b3ded4161a6d8d961.
2012-09-18 20:18:30,073 INFO  [StoreCloserThread-t,ggg,1347999506664.b48b882c9992c9773e9bfda020d2137c.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,072 INFO  [RegionServer:3;hemera.apache.org,58959,1347999502361.compactionChecker] hbase.Chore(81): RegionServer:3;hemera.apache.org,58959,1347999502361.compactionChecker exiting
2012-09-18 20:18:30,073 INFO  [StoreCloserThread-t,fff,1347999506661.7d7428c069f7e46b3ded4161a6d8d961.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,073 INFO  [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-0] regionserver.HRegion(1023): Closed t,ggg,1347999506664.b48b882c9992c9773e9bfda020d2137c.
2012-09-18 20:18:30,075 INFO  [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-1] regionserver.HRegion(1023): Closed t,fff,1347999506661.7d7428c069f7e46b3ded4161a6d8d961.
2012-09-18 20:18:30,075 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-0] handler.CloseRegionHandler(168): Closed region t,ggg,1347999506664.b48b882c9992c9773e9bfda020d2137c.
2012-09-18 20:18:30,072 INFO  [RegionServer:3;hemera.apache.org,58959,1347999502361.logRoller] regionserver.LogRoller(118): LogRoller exiting.
2012-09-18 20:18:30,072 INFO  [SplitLogWorker-hemera.apache.org,58959,1347999502361] regionserver.SplitLogWorker(206): SplitLogWorker interrupted while waiting for task, exiting: java.lang.InterruptedException
2012-09-18 20:18:30,071 INFO  [StoreCloserThread-t,kkk,1347999506675.d91c2dc99d3bac1d51686c9a6b4e74aa.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,076 INFO  [SplitLogWorker-hemera.apache.org,58959,1347999502361] regionserver.SplitLogWorker(170): SplitLogWorker hemera.apache.org,58959,1347999502361 exiting
2012-09-18 20:18:30,076 INFO  [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-2] regionserver.HRegion(1023): Closed t,kkk,1347999506675.d91c2dc99d3bac1d51686c9a6b4e74aa.
2012-09-18 20:18:30,071 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-0] handler.CloseRegionHandler(168): Closed region t,lll,1347999506678.1c1644d5428c228bace562ef1bed5118.
2012-09-18 20:18:30,071 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] handler.CloseRegionHandler(124): Processing close of t,sss,1347999506698.c1e5810326004add2ab532ccc9e8c24e.
2012-09-18 20:18:30,071 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-2] regionserver.HRegion(975): Updates disabled for region t,ttt,1347999506703.73c7f7079688e986da1850e53fb74f9d.
2012-09-18 20:18:30,079 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-2] handler.CloseRegionHandler(168): Closed region t,kkk,1347999506675.d91c2dc99d3bac1d51686c9a6b4e74aa.
2012-09-18 20:18:30,079 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-2] handler.CloseRegionHandler(124): Processing close of t,,1347999506624.6b45157a30794a82fc5cbdb3589032e2.
2012-09-18 20:18:30,078 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] regionserver.HRegion(954): Closing t,sss,1347999506698.c1e5810326004add2ab532ccc9e8c24e.: disabling compactions & flushes
2012-09-18 20:18:30,076 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-0] handler.CloseRegionHandler(124): Processing close of t,nnn,1347999506684.a2e6ab866441d82e1124ad6dcaf7fa03.
2012-09-18 20:18:30,075 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-1] handler.CloseRegionHandler(168): Closed region t,fff,1347999506661.7d7428c069f7e46b3ded4161a6d8d961.
2012-09-18 20:18:30,079 INFO  [StoreCloserThread-t,ttt,1347999506703.73c7f7079688e986da1850e53fb74f9d.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,073 INFO  [pool-1-thread-1-EventThread] zookeeper.DrainingServerTracker(101): Draining RS node deleted, removing from list [hemera.apache.org,33334,1347999502311]
2012-09-18 20:18:30,084 INFO  [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-2] regionserver.HRegion(1023): Closed t,ttt,1347999506703.73c7f7079688e986da1850e53fb74f9d.
2012-09-18 20:18:30,084 INFO  [pool-1-thread-1] hbase.ResourceChecker(144): after TestDrainingServer#testDrainingServerWithAbort: 348 threads (was 441), 584 file descriptors (was 700). 5 connections (was 7), 
2012-09-18 20:18:30,073 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-1] regionserver.HRegion(954): Closing t,www,1347999506711.985b3c0de2cbfe789c69850bb128887d.: disabling compactions & flushes
2012-09-18 20:18:30,073 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-2] regionserver.HRegion(975): Updates disabled for region t,hhh,1347999506667.5a1faf1d76da0300d16f9c8dbb752824.
2012-09-18 20:18:30,072 INFO  [RegionServer:3;hemera.apache.org,58959,1347999502361] regionserver.HRegionServer(943): aborting server hemera.apache.org,58959,1347999502361
2012-09-18 20:18:30,086 INFO  [StoreCloserThread-t,hhh,1347999506667.5a1faf1d76da0300d16f9c8dbb752824.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,072 DEBUG [RegionServer:2;hemera.apache.org,43875,1347999502347] regionserver.HRegionServer(1095): {c1e5810326004add2ab532ccc9e8c24e=t,sss,1347999506698.c1e5810326004add2ab532ccc9e8c24e., 6b45157a30794a82fc5cbdb3589032e2=t,,1347999506624.6b45157a30794a82fc5cbdb3589032e2., 95255bd4b3285804a2c818d1bf7f459f=t,xxx,1347999506714.95255bd4b3285804a2c818d1bf7f459f., cb765c4343d7b8c707c88b4d4b79a165=t,qqq,1347999506692.cb765c4343d7b8c707c88b4d4b79a165., 78622cfb2af5eca037e2375d7566cac6=t,mmm,1347999506681.78622cfb2af5eca037e2375d7566cac6.}
2012-09-18 20:18:30,086 INFO  [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-2] regionserver.HRegion(1023): Closed t,hhh,1347999506667.5a1faf1d76da0300d16f9c8dbb752824.
2012-09-18 20:18:30,086 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-2] handler.CloseRegionHandler(168): Closed region t,hhh,1347999506667.5a1faf1d76da0300d16f9c8dbb752824.
2012-09-18 20:18:30,086 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-2] handler.CloseRegionHandler(124): Processing close of t,ooo,1347999506686.c4178dd0ed59cff7c16b018f8936b194.
2012-09-18 20:18:30,086 DEBUG [RegionServer:3;hemera.apache.org,58959,1347999502361] catalog.CatalogTracker(257): Stopping catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@1880b02
2012-09-18 20:18:30,086 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-1] regionserver.HRegion(975): Updates disabled for region t,www,1347999506711.985b3c0de2cbfe789c69850bb128887d.
2012-09-18 20:18:30,084 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-2] handler.CloseRegionHandler(168): Closed region t,ttt,1347999506703.73c7f7079688e986da1850e53fb74f9d.
2012-09-18 20:18:30,084 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZooKeeperWatcher(265): master:35050-0x139db080e690000 Received ZooKeeper Event, type=NodeChildrenChanged, state=SyncConnected, path=/hbase/draining
2012-09-18 20:18:30,084 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-0] regionserver.HRegion(954): Closing t,nnn,1347999506684.a2e6ab866441d82e1124ad6dcaf7fa03.: disabling compactions & flushes
2012-09-18 20:18:30,081 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-1] handler.CloseRegionHandler(124): Processing close of t,xxx,1347999506714.95255bd4b3285804a2c818d1bf7f459f.
2012-09-18 20:18:30,090 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-1] regionserver.HRegion(954): Closing t,xxx,1347999506714.95255bd4b3285804a2c818d1bf7f459f.: disabling compactions & flushes
2012-09-18 20:18:30,090 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-1] regionserver.HRegion(975): Updates disabled for region t,xxx,1347999506714.95255bd4b3285804a2c818d1bf7f459f.
2012-09-18 20:18:30,079 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] regionserver.HRegion(975): Updates disabled for region t,sss,1347999506698.c1e5810326004add2ab532ccc9e8c24e.
2012-09-18 20:18:30,079 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-2] regionserver.HRegion(954): Closing t,,1347999506624.6b45157a30794a82fc5cbdb3589032e2.: disabling compactions & flushes
2012-09-18 20:18:30,090 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-0] regionserver.HRegion(975): Updates disabled for region t,nnn,1347999506684.a2e6ab866441d82e1124ad6dcaf7fa03.
2012-09-18 20:18:30,090 INFO  [StoreCloserThread-t,sss,1347999506698.c1e5810326004add2ab532ccc9e8c24e.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,090 INFO  [StoreCloserThread-t,www,1347999506711.985b3c0de2cbfe789c69850bb128887d.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,089 INFO  [RegionServer:3;hemera.apache.org,58959,1347999502361] client.HConnectionManager$HConnectionImplementation(1523): Closing zookeeper sessionid=0x139db080e690008
2012-09-18 20:18:30,091 INFO  [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-1] regionserver.HRegion(1023): Closed t,www,1347999506711.985b3c0de2cbfe789c69850bb128887d.
2012-09-18 20:18:30,091 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-1] handler.CloseRegionHandler(168): Closed region t,www,1347999506711.985b3c0de2cbfe789c69850bb128887d.
2012-09-18 20:18:30,091 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-1] handler.CloseRegionHandler(124): Processing close of t,rrr,1347999506695.29ca9f1226c85d9ad4523735386186da.
2012-09-18 20:18:30,089 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-2] regionserver.HRegion(954): Closing t,ooo,1347999506686.c4178dd0ed59cff7c16b018f8936b194.: disabling compactions & flushes
2012-09-18 20:18:30,091 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-2] regionserver.HRegion(975): Updates disabled for region t,ooo,1347999506686.c4178dd0ed59cff7c16b018f8936b194.
2012-09-18 20:18:30,091 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-1] regionserver.HRegion(954): Closing t,rrr,1347999506695.29ca9f1226c85d9ad4523735386186da.: disabling compactions & flushes
2012-09-18 20:18:30,091 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-1] regionserver.HRegion(975): Updates disabled for region t,rrr,1347999506695.29ca9f1226c85d9ad4523735386186da.
2012-09-18 20:18:30,091 INFO  [StoreCloserThread-t,nnn,1347999506684.a2e6ab866441d82e1124ad6dcaf7fa03.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,092 INFO  [StoreCloserThread-t,rrr,1347999506695.29ca9f1226c85d9ad4523735386186da.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,092 INFO  [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-0] regionserver.HRegion(1023): Closed t,nnn,1347999506684.a2e6ab866441d82e1124ad6dcaf7fa03.
2012-09-18 20:18:30,092 INFO  [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-1] regionserver.HRegion(1023): Closed t,rrr,1347999506695.29ca9f1226c85d9ad4523735386186da.
2012-09-18 20:18:30,092 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-0] handler.CloseRegionHandler(168): Closed region t,nnn,1347999506684.a2e6ab866441d82e1124ad6dcaf7fa03.
2012-09-18 20:18:30,091 INFO  [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] regionserver.HRegion(1023): Closed t,sss,1347999506698.c1e5810326004add2ab532ccc9e8c24e.
2012-09-18 20:18:30,092 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] handler.CloseRegionHandler(168): Closed region t,sss,1347999506698.c1e5810326004add2ab532ccc9e8c24e.
2012-09-18 20:18:30,092 INFO  [pool-1-thread-1] hbase.HBaseTestingUtility(747): Shutting down minicluster
2012-09-18 20:18:30,092 DEBUG [pool-1-thread-1] util.JVMClusterUtil(223): Shutting down HBase Cluster
2012-09-18 20:18:30,090 INFO  [StoreCloserThread-t,xxx,1347999506714.95255bd4b3285804a2c818d1bf7f459f.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,090 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-2] regionserver.HRegion(975): Updates disabled for region t,,1347999506624.6b45157a30794a82fc5cbdb3589032e2.
2012-09-18 20:18:30,093 INFO  [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-1] regionserver.HRegion(1023): Closed t,xxx,1347999506714.95255bd4b3285804a2c818d1bf7f459f.
2012-09-18 20:18:30,092 INFO  [pool-1-thread-1] master.HMaster(2049): Cluster shutdown requested
2012-09-18 20:18:30,092 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] handler.CloseRegionHandler(124): Processing close of t,qqq,1347999506692.cb765c4343d7b8c707c88b4d4b79a165.
2012-09-18 20:18:30,093 INFO  [StoreCloserThread-t,,1347999506624.6b45157a30794a82fc5cbdb3589032e2.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,092 DEBUG [RS_CLOSE_REGION-hemera.apache.org",,,,,,,,
,,,,,,,,,
,,,,,,,,,
HBASE-6813,"TestDrainingServer#testDrainingServerWithAbort failed in trunk build #3348:
{code}
Error Message

Test conditions are not met: regions were created/deleted during the test.  expected:<27> but was:<24>

Stacktrace

junit.framework.AssertionFailedError: Test conditions are not met: regions were created/deleted during the test.  expected:<27> but was:<24>
	at junit.framework.Assert.fail(Assert.java:50)
	at junit.framework.Assert.failNotEquals(Assert.java:287)
	at junit.framework.Assert.assertEquals(Assert.java:67)
	at junit.framework.Assert.assertEquals(Assert.java:134)
	at org.apache.hadoop.hbase.TestDrainingServer.testDrainingServerWithAbort(TestDrainingServer.java:241)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:62)

Standard Output

Shutting down the Mini HDFS Cluster
Shutting down DataNode 4
Shutting down DataNode 3
Shutting down DataNode 2
Shutting down DataNode 1
Shutting down DataNode 0

Standard Error

2012-09-18 20:18:30,026 INFO  [pool-1-thread-1] hbase.ResourceChecker(144): before TestDrainingServer#testDrainingServerWithAbort: 441 threads, 700 file descriptors 7 connections, 
2012-09-18 20:18:30,044 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZooKeeperWatcher(265): master:35050-0x139db080e690000 Received ZooKeeper Event, type=NodeCreated, state=SyncConnected, path=/hbase/balancer
2012-09-18 20:18:30,044 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZKUtil(1141): master:35050-0x139db080e690000 Retrieved 6 byte(s) of data from znode /hbase/balancer and set watcher; PBUF\x08\x00
2012-09-18 20:18:30,045 DEBUG [IPC Server handler 2 on 35050] zookeeper.ZKUtil(1141): master:35050-0x139db080e690000 Retrieved 6 byte(s) of data from znode /hbase/balancer and set watcher; PBUF\x08\x00
2012-09-18 20:18:30,045 INFO  [IPC Server handler 2 on 35050] master.HMaster(1363): BalanceSwitch=false
2012-09-18 20:18:30,047 INFO  [Thread-604] hbase.TestDrainingServer(211): Regions of drained server are: [t,yyy,1347999506717.bdd1a3ab3105de63248038bd0eb40564.]
2012-09-18 20:18:30,047 INFO  [Thread-604] hbase.TestDrainingServer(112): Making hemera.apache.org,33334,1347999502311 the draining server; it has 1 online regions
2012-09-18 20:18:30,048 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZooKeeperWatcher(265): master:35050-0x139db080e690000 Received ZooKeeper Event, type=NodeChildrenChanged, state=SyncConnected, path=/hbase/draining
2012-09-18 20:18:30,049 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZKUtil(235): master:35050-0x139db080e690000 Set watcher on existing znode /hbase/draining/hemera.apache.org,33334,1347999502311
2012-09-18 20:18:30,049 INFO  [pool-1-thread-1-EventThread] zookeeper.DrainingServerTracker(83): Draining RS node created, adding to list [hemera.apache.org,33334,1347999502311]
2012-09-18 20:18:30,049 INFO  [Thread-604] hbase.TestDrainingServer(220): The available servers are: [hemera.apache.org,38814,1347999502374, hemera.apache.org,58959,1347999502361, hemera.apache.org,43875,1347999502347, hemera.apache.org,51601,1347999502333]
2012-09-18 20:18:30,049 FATAL [Thread-604] regionserver.HRegionServer(1811): ABORTING region server hemera.apache.org,51601,1347999502333: Aborting
2012-09-18 20:18:30,050 FATAL [Thread-604] regionserver.HRegionServer(1818): RegionServer abort: loaded coprocessors are: []
2012-09-18 20:18:30,054 INFO  [Thread-604] regionserver.HRegionServer(1821): Dump of metrics: requestsPerSecond=0, numberOfOnlineRegions=3, numberOfStores=3, numberOfStorefiles=0, storefileIndexSizeMB=0, rootIndexSizeKB=0, totalStaticIndexSizeKB=0, totalStaticBloomSizeKB=0, memstoreSizeMB=0, mbInMemoryWithoutWAL=0, numberOfPutsWithoutWAL=0, readRequestsCount=0, writeRequestsCount=0, compactionQueueSize=0, flushQueueSize=0, usedHeapMB=167, maxHeapMB=1688, blockCacheSizeMB=1.98, blockCacheFreeMB=420.25, blockCacheCount=1, blockCacheHitCount=41, blockCacheMissCount=1, blockCacheEvictedCount=0, blockCacheHitRatio=97%, blockCacheHitCachingRatio=97%, hdfsBlocksLocalityIndex=0, slowHLogAppendCount=0, fsReadLatencyHistogramMean=0, fsReadLatencyHistogramCount=0, fsReadLatencyHistogramMedian=0, fsReadLatencyHistogram75th=0, fsReadLatencyHistogram95th=0, fsReadLatencyHistogram99th=0, fsReadLatencyHistogram999th=0, fsPreadLatencyHistogramMean=0, fsPreadLatencyHistogramCount=0, fsPreadLatencyHistogramMedian=0, fsPreadLatencyHistogram75th=0, fsPreadLatencyHistogram95th=0, fsPreadLatencyHistogram99th=0, fsPreadLatencyHistogram999th=0, fsWriteLatencyHistogramMean=0, fsWriteLatencyHistogramCount=0, fsWriteLatencyHistogramMedian=0, fsWriteLatencyHistogram75th=0, fsWriteLatencyHistogram95th=0, fsWriteLatencyHistogram99th=0, fsWriteLatencyHistogram999th=0
2012-09-18 20:18:30,056 ERROR [IPC Server handler 0 on 35050] master.HMaster(1193): Region server &#0;&#0;hemera.apache.org,51601,1347999502333 reported a fatal error:
ABORTING region server hemera.apache.org,51601,1347999502333: Aborting
2012-09-18 20:18:30,058 INFO  [Thread-604] regionserver.HRegionServer(1737): STOPPED: Aborting
2012-09-18 20:18:30,058 FATAL [Thread-604] regionserver.HRegionServer(1811): ABORTING region server hemera.apache.org,43875,1347999502347: Aborting
2012-09-18 20:18:30,058 FATAL [Thread-604] regionserver.HRegionServer(1818): RegionServer abort: loaded coprocessors are: []
2012-09-18 20:18:30,062 INFO  [Thread-604] regionserver.HRegionServer(1821): Dump of metrics: requestsPerSecond=0, numberOfOnlineRegions=7, numberOfStores=7, numberOfStorefiles=0, storefileIndexSizeMB=0, rootIndexSizeKB=0, totalStaticIndexSizeKB=0, totalStaticBloomSizeKB=0, memstoreSizeMB=0, mbInMemoryWithoutWAL=0, numberOfPutsWithoutWAL=0, readRequestsCount=0, writeRequestsCount=0, compactionQueueSize=0, flushQueueSize=0, usedHeapMB=167, maxHeapMB=1688, blockCacheSizeMB=1.98, blockCacheFreeMB=420.25, blockCacheCount=1, blockCacheHitCount=41, blockCacheMissCount=1, blockCacheEvictedCount=0, blockCacheHitRatio=97%, blockCacheHitCachingRatio=97%, hdfsBlocksLocalityIndex=0, slowHLogAppendCount=0, fsReadLatencyHistogramMean=0, fsReadLatencyHistogramCount=0, fsReadLatencyHistogramMedian=0, fsReadLatencyHistogram75th=0, fsReadLatencyHistogram95th=0, fsReadLatencyHistogram99th=0, fsReadLatencyHistogram999th=0, fsPreadLatencyHistogramMean=0, fsPreadLatencyHistogramCount=0, fsPreadLatencyHistogramMedian=0, fsPreadLatencyHistogram75th=0, fsPreadLatencyHistogram95th=0, fsPreadLatencyHistogram99th=0, fsPreadLatencyHistogram999th=0, fsWriteLatencyHistogramMean=0, fsWriteLatencyHistogramCount=0, fsWriteLatencyHistogramMedian=0, fsWriteLatencyHistogram75th=0, fsWriteLatencyHistogram95th=0, fsWriteLatencyHistogram99th=0, fsWriteLatencyHistogram999th=0
2012-09-18 20:18:30,062 INFO  [RegionServer:1;hemera.apache.org,51601,1347999502333] regionserver.SplitLogWorker(522): Sending interrupt to stop the worker thread
2012-09-18 20:18:30,063 INFO  [SplitLogWorker-hemera.apache.org,51601,1347999502333] regionserver.SplitLogWorker(206): SplitLogWorker interrupted while waiting for task, exiting: java.lang.InterruptedException
2012-09-18 20:18:30,063 INFO  [SplitLogWorker-hemera.apache.org,51601,1347999502333] regionserver.SplitLogWorker(170): SplitLogWorker hemera.apache.org,51601,1347999502333 exiting
2012-09-18 20:18:30,063 ERROR [IPC Server handler 1 on 35050] master.HMaster(1193): Region server &#0;&#0;hemera.apache.org,43875,1347999502347 reported a fatal error:
ABORTING region server hemera.apache.org,43875,1347999502347: Aborting
2012-09-18 20:18:30,063 INFO  [Thread-604] regionserver.HRegionServer(1737): STOPPED: Aborting
2012-09-18 20:18:30,063 INFO  [RegionServer:1;hemera.apache.org,51601,1347999502333.compactionChecker] hbase.Chore(81): RegionServer:1;hemera.apache.org,51601,1347999502333.compactionChecker exiting
2012-09-18 20:18:30,063 FATAL [Thread-604] regionserver.HRegionServer(1811): ABORTING region server hemera.apache.org,58959,1347999502361: Aborting
2012-09-18 20:18:30,063 DEBUG [pool-1-thread-1.LruBlockCache.EvictionThread] hfile.LruBlockCache(418): Block cache LRU eviction started; Attempting to free -408721.95 KB of total=1.98 MB
2012-09-18 20:18:30,063 INFO  [RegionServer:1;hemera.apache.org,51601,1347999502333.cacheFlusher] regionserver.MemStoreFlusher(264): RegionServer:1;hemera.apache.org,51601,1347999502333.cacheFlusher exiting
2012-09-18 20:18:30,064 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-0] handler.CloseRegionHandler(124): Processing close of t,lll,1347999506678.1c1644d5428c228bace562ef1bed5118.
2012-09-18 20:18:30,063 INFO  [RegionServer:1;hemera.apache.org,51601,1347999502333.logRoller] regionserver.LogRoller(118): LogRoller exiting.
2012-09-18 20:18:30,064 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-0] regionserver.HRegion(954): Closing t,lll,1347999506678.1c1644d5428c228bace562ef1bed5118.: disabling compactions & flushes
2012-09-18 20:18:30,064 INFO  [RegionServer:1;hemera.apache.org,51601,1347999502333] regionserver.HRegionServer(943): aborting server hemera.apache.org,51601,1347999502333
2012-09-18 20:18:30,064 INFO  [RegionServer:2;hemera.apache.org,43875,1347999502347] regionserver.SplitLogWorker(522): Sending interrupt to stop the worker thread
2012-09-18 20:18:30,064 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-1] handler.CloseRegionHandler(124): Processing close of t,iii,1347999506669.ae0257433f2941163b1d397f3d4f6efc.
2012-09-18 20:18:30,065 INFO  [RegionServer:2;hemera.apache.org,43875,1347999502347.cacheFlusher] regionserver.MemStoreFlusher(264): RegionServer:2;hemera.apache.org,43875,1347999502347.cacheFlusher exiting
2012-09-18 20:18:30,065 INFO  [RegionServer:2;hemera.apache.org,43875,1347999502347.logRoller] regionserver.LogRoller(118): LogRoller exiting.
2012-09-18 20:18:30,065 INFO  [RegionServer:2;hemera.apache.org,43875,1347999502347.compactionChecker] hbase.Chore(81): RegionServer:2;hemera.apache.org,43875,1347999502347.compactionChecker exiting
2012-09-18 20:18:30,063 FATAL [Thread-604] regionserver.HRegionServer(1818): RegionServer abort: loaded coprocessors are: []
2012-09-18 20:18:30,065 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] handler.CloseRegionHandler(124): Processing close of t,eee,1347999506656.3b9d9fdc58f2319931d38a906c8a1289.
2012-09-18 20:18:30,065 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-1] regionserver.HRegion(954): Closing t,iii,1347999506669.ae0257433f2941163b1d397f3d4f6efc.: disabling compactions & flushes
2012-09-18 20:18:30,067 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] regionserver.HRegion(954): Closing t,eee,1347999506656.3b9d9fdc58f2319931d38a906c8a1289.: disabling compactions & flushes
2012-09-18 20:18:30,065 INFO  [SplitLogWorker-hemera.apache.org,43875,1347999502347] regionserver.SplitLogWorker(206): SplitLogWorker interrupted while waiting for task, exiting: java.lang.InterruptedException
2012-09-18 20:18:30,067 INFO  [RegionServer:2;hemera.apache.org,43875,1347999502347] regionserver.HRegionServer(943): aborting server hemera.apache.org,43875,1347999502347
2012-09-18 20:18:30,067 DEBUG [RegionServer:2;hemera.apache.org,43875,1347999502347] catalog.CatalogTracker(257): Stopping catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@11a59ce
2012-09-18 20:18:30,064 DEBUG [RegionServer:1;hemera.apache.org,51601,1347999502333] catalog.CatalogTracker(257): Stopping catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@f7bf2d
2012-09-18 20:18:30,064 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-0] regionserver.HRegion(975): Updates disabled for region t,lll,1347999506678.1c1644d5428c228bace562ef1bed5118.
2012-09-18 20:18:30,064 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-2] handler.CloseRegionHandler(124): Processing close of t,ttt,1347999506703.73c7f7079688e986da1850e53fb74f9d.
2012-09-18 20:18:30,067 INFO  [RegionServer:2;hemera.apache.org,43875,1347999502347] client.HConnectionManager$HConnectionImplementation(1523): Closing zookeeper sessionid=0x139db080e690009
2012-09-18 20:18:30,067 INFO  [Thread-604] regionserver.HRegionServer(1821): Dump of metrics: requestsPerSecond=0, numberOfOnlineRegions=4, numberOfStores=4, numberOfStorefiles=0, storefileIndexSizeMB=0, rootIndexSizeKB=0, totalStaticIndexSizeKB=0, totalStaticBloomSizeKB=0, memstoreSizeMB=0, mbInMemoryWithoutWAL=0, numberOfPutsWithoutWAL=0, readRequestsCount=0, writeRequestsCount=0, compactionQueueSize=0, flushQueueSize=0, usedHeapMB=168, maxHeapMB=1688, blockCacheSizeMB=1.98, blockCacheFreeMB=420.25, blockCacheCount=1, blockCacheHitCount=41, blockCacheMissCount=1, blockCacheEvictedCount=0, blockCacheHitRatio=97%, blockCacheHitCachingRatio=97%, hdfsBlocksLocalityIndex=0, slowHLogAppendCount=0, fsReadLatencyHistogramMean=0, fsReadLatencyHistogramCount=0, fsReadLatencyHistogramMedian=0, fsReadLatencyHistogram75th=0, fsReadLatencyHistogram95th=0, fsReadLatencyHistogram99th=0, fsReadLatencyHistogram999th=0, fsPreadLatencyHistogramMean=0, fsPreadLatencyHistogramCount=0, fsPreadLatencyHistogramMedian=0, fsPreadLatencyHistogram75th=0, fsPreadLatencyHistogram95th=0, fsPreadLatencyHistogram99th=0, fsPreadLatencyHistogram999th=0, fsWriteLatencyHistogramMean=0, fsWriteLatencyHistogramCount=0, fsWriteLatencyHistogramMedian=0, fsWriteLatencyHistogram75th=0, fsWriteLatencyHistogram95th=0, fsWriteLatencyHistogram99th=0, fsWriteLatencyHistogram999th=0
2012-09-18 20:18:30,067 INFO  [SplitLogWorker-hemera.apache.org,43875,1347999502347] regionserver.SplitLogWorker(170): SplitLogWorker hemera.apache.org,43875,1347999502347 exiting
2012-09-18 20:18:30,067 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] regionserver.HRegion(975): Updates disabled for region t,eee,1347999506656.3b9d9fdc58f2319931d38a906c8a1289.
2012-09-18 20:18:30,067 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-2] handler.CloseRegionHandler(124): Processing close of t,kkk,1347999506675.d91c2dc99d3bac1d51686c9a6b4e74aa.
2012-09-18 20:18:30,069 ERROR [IPC Server handler 3 on 35050] master.HMaster(1193): Region server &#0;&#0;hemera.apache.org,58959,1347999502361 reported a fatal error:
ABORTING region server hemera.apache.org,58959,1347999502361: Aborting
2012-09-18 20:18:30,069 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-2] regionserver.HRegion(954): Closing t,kkk,1347999506675.d91c2dc99d3bac1d51686c9a6b4e74aa.: disabling compactions & flushes
2012-09-18 20:18:30,067 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-1] regionserver.HRegion(975): Updates disabled for region t,iii,1347999506669.ae0257433f2941163b1d397f3d4f6efc.
2012-09-18 20:18:30,066 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-1] handler.CloseRegionHandler(124): Processing close of t,fff,1347999506661.7d7428c069f7e46b3ded4161a6d8d961.
2012-09-18 20:18:30,070 INFO  [RegionServer:1;hemera.apache.org,51601,1347999502333] client.HConnectionManager$HConnectionImplementation(1523): Closing zookeeper sessionid=0x139db080e690007
2012-09-18 20:18:30,070 INFO  [Thread-604] regionserver.HRegionServer(1737): STOPPED: Aborting
2012-09-18 20:18:30,070 INFO  [StoreCloserThread-t,iii,1347999506669.ae0257433f2941163b1d397f3d4f6efc.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,070 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-2] regionserver.HRegion(975): Updates disabled for region t,kkk,1347999506675.d91c2dc99d3bac1d51686c9a6b4e74aa.
2012-09-18 20:18:30,070 INFO  [StoreCloserThread-t,eee,1347999506656.3b9d9fdc58f2319931d38a906c8a1289.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,071 INFO  [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-1] regionserver.HRegion(1023): Closed t,iii,1347999506669.ae0257433f2941163b1d397f3d4f6efc.
2012-09-18 20:18:30,068 INFO  [StoreCloserThread-t,lll,1347999506678.1c1644d5428c228bace562ef1bed5118.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,071 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-1] handler.CloseRegionHandler(168): Closed region t,iii,1347999506669.ae0257433f2941163b1d397f3d4f6efc.
2012-09-18 20:18:30,071 INFO  [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] regionserver.HRegion(1023): Closed t,eee,1347999506656.3b9d9fdc58f2319931d38a906c8a1289.
2012-09-18 20:18:30,068 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-2] regionserver.HRegion(954): Closing t,ttt,1347999506703.73c7f7079688e986da1850e53fb74f9d.: disabling compactions & flushes
2012-09-18 20:18:30,071 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] handler.CloseRegionHandler(168): Closed region t,eee,1347999506656.3b9d9fdc58f2319931d38a906c8a1289.
2012-09-18 20:18:30,071 INFO  [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-0] regionserver.HRegion(1023): Closed t,lll,1347999506678.1c1644d5428c228bace562ef1bed5118.
2012-09-18 20:18:30,071 INFO  [RegionServer:1;hemera.apache.org,51601,1347999502333] regionserver.HRegionServer(1091): Waiting on 2 regions to close
2012-09-18 20:18:30,071 DEBUG [RegionServer:1;hemera.apache.org,51601,1347999502333] regionserver.HRegionServer(1095): {73c7f7079688e986da1850e53fb74f9d=t,ttt,1347999506703.73c7f7079688e986da1850e53fb74f9d.}
2012-09-18 20:18:30,071 INFO  [RegionServer:3;hemera.apache.org,58959,1347999502361] regionserver.SplitLogWorker(522): Sending interrupt to stop the worker thread
2012-09-18 20:18:30,070 INFO  [Thread-604] hbase.TestDrainingServer(239): Regions of drained server are: [t,yyy,1347999506717.bdd1a3ab3105de63248038bd0eb40564.]
2012-09-18 20:18:30,072 INFO  [RegionServer:3;hemera.apache.org,58959,1347999502361.cacheFlusher] regionserver.MemStoreFlusher(264): RegionServer:3;hemera.apache.org,58959,1347999502361.cacheFlusher exiting
2012-09-18 20:18:30,070 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-1] regionserver.HRegion(954): Closing t,fff,1347999506661.7d7428c069f7e46b3ded4161a6d8d961.: disabling compactions & flushes
2012-09-18 20:18:30,072 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-0] handler.CloseRegionHandler(124): Processing close of t,ggg,1347999506664.b48b882c9992c9773e9bfda020d2137c.
2012-09-18 20:18:30,070 INFO  [RegionServer:2;hemera.apache.org,43875,1347999502347] regionserver.HRegionServer(1091): Waiting on 8 regions to close
2012-09-18 20:18:30,072 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-0] regionserver.HRegion(954): Closing t,ggg,1347999506664.b48b882c9992c9773e9bfda020d2137c.: disabling compactions & flushes
2012-09-18 20:18:30,072 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-2] handler.CloseRegionHandler(124): Processing close of t,hhh,1347999506667.5a1faf1d76da0300d16f9c8dbb752824.
2012-09-18 20:18:30,072 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-0] regionserver.HRegion(975): Updates disabled for region t,ggg,1347999506664.b48b882c9992c9773e9bfda020d2137c.
2012-09-18 20:18:30,073 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-2] regionserver.HRegion(954): Closing t,hhh,1347999506667.5a1faf1d76da0300d16f9c8dbb752824.: disabling compactions & flushes
2012-09-18 20:18:30,072 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-1] handler.CloseRegionHandler(124): Processing close of t,www,1347999506711.985b3c0de2cbfe789c69850bb128887d.
2012-09-18 20:18:30,073 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZooKeeperWatcher(265): master:35050-0x139db080e690000 Received ZooKeeper Event, type=NodeDeleted, state=SyncConnected, path=/hbase/draining/hemera.apache.org,33334,1347999502311
2012-09-18 20:18:30,072 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-1] regionserver.HRegion(975): Updates disabled for region t,fff,1347999506661.7d7428c069f7e46b3ded4161a6d8d961.
2012-09-18 20:18:30,073 INFO  [StoreCloserThread-t,ggg,1347999506664.b48b882c9992c9773e9bfda020d2137c.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,072 INFO  [RegionServer:3;hemera.apache.org,58959,1347999502361.compactionChecker] hbase.Chore(81): RegionServer:3;hemera.apache.org,58959,1347999502361.compactionChecker exiting
2012-09-18 20:18:30,073 INFO  [StoreCloserThread-t,fff,1347999506661.7d7428c069f7e46b3ded4161a6d8d961.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,073 INFO  [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-0] regionserver.HRegion(1023): Closed t,ggg,1347999506664.b48b882c9992c9773e9bfda020d2137c.
2012-09-18 20:18:30,075 INFO  [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-1] regionserver.HRegion(1023): Closed t,fff,1347999506661.7d7428c069f7e46b3ded4161a6d8d961.
2012-09-18 20:18:30,075 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-0] handler.CloseRegionHandler(168): Closed region t,ggg,1347999506664.b48b882c9992c9773e9bfda020d2137c.
2012-09-18 20:18:30,072 INFO  [RegionServer:3;hemera.apache.org,58959,1347999502361.logRoller] regionserver.LogRoller(118): LogRoller exiting.
2012-09-18 20:18:30,072 INFO  [SplitLogWorker-hemera.apache.org,58959,1347999502361] regionserver.SplitLogWorker(206): SplitLogWorker interrupted while waiting for task, exiting: java.lang.InterruptedException
2012-09-18 20:18:30,071 INFO  [StoreCloserThread-t,kkk,1347999506675.d91c2dc99d3bac1d51686c9a6b4e74aa.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,076 INFO  [SplitLogWorker-hemera.apache.org,58959,1347999502361] regionserver.SplitLogWorker(170): SplitLogWorker hemera.apache.org,58959,1347999502361 exiting
2012-09-18 20:18:30,076 INFO  [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-2] regionserver.HRegion(1023): Closed t,kkk,1347999506675.d91c2dc99d3bac1d51686c9a6b4e74aa.
2012-09-18 20:18:30,071 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-0] handler.CloseRegionHandler(168): Closed region t,lll,1347999506678.1c1644d5428c228bace562ef1bed5118.
2012-09-18 20:18:30,071 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] handler.CloseRegionHandler(124): Processing close of t,sss,1347999506698.c1e5810326004add2ab532ccc9e8c24e.
2012-09-18 20:18:30,071 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-2] regionserver.HRegion(975): Updates disabled for region t,ttt,1347999506703.73c7f7079688e986da1850e53fb74f9d.
2012-09-18 20:18:30,079 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-2] handler.CloseRegionHandler(168): Closed region t,kkk,1347999506675.d91c2dc99d3bac1d51686c9a6b4e74aa.
2012-09-18 20:18:30,079 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-2] handler.CloseRegionHandler(124): Processing close of t,,1347999506624.6b45157a30794a82fc5cbdb3589032e2.
2012-09-18 20:18:30,078 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] regionserver.HRegion(954): Closing t,sss,1347999506698.c1e5810326004add2ab532ccc9e8c24e.: disabling compactions & flushes
2012-09-18 20:18:30,076 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-0] handler.CloseRegionHandler(124): Processing close of t,nnn,1347999506684.a2e6ab866441d82e1124ad6dcaf7fa03.
2012-09-18 20:18:30,075 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-1] handler.CloseRegionHandler(168): Closed region t,fff,1347999506661.7d7428c069f7e46b3ded4161a6d8d961.
2012-09-18 20:18:30,079 INFO  [StoreCloserThread-t,ttt,1347999506703.73c7f7079688e986da1850e53fb74f9d.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,073 INFO  [pool-1-thread-1-EventThread] zookeeper.DrainingServerTracker(101): Draining RS node deleted, removing from list [hemera.apache.org,33334,1347999502311]
2012-09-18 20:18:30,084 INFO  [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-2] regionserver.HRegion(1023): Closed t,ttt,1347999506703.73c7f7079688e986da1850e53fb74f9d.
2012-09-18 20:18:30,084 INFO  [pool-1-thread-1] hbase.ResourceChecker(144): after TestDrainingServer#testDrainingServerWithAbort: 348 threads (was 441), 584 file descriptors (was 700). 5 connections (was 7), 
2012-09-18 20:18:30,073 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-1] regionserver.HRegion(954): Closing t,www,1347999506711.985b3c0de2cbfe789c69850bb128887d.: disabling compactions & flushes
2012-09-18 20:18:30,073 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-2] regionserver.HRegion(975): Updates disabled for region t,hhh,1347999506667.5a1faf1d76da0300d16f9c8dbb752824.
2012-09-18 20:18:30,072 INFO  [RegionServer:3;hemera.apache.org,58959,1347999502361] regionserver.HRegionServer(943): aborting server hemera.apache.org,58959,1347999502361
2012-09-18 20:18:30,086 INFO  [StoreCloserThread-t,hhh,1347999506667.5a1faf1d76da0300d16f9c8dbb752824.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,072 DEBUG [RegionServer:2;hemera.apache.org,43875,1347999502347] regionserver.HRegionServer(1095): {c1e5810326004add2ab532ccc9e8c24e=t,sss,1347999506698.c1e5810326004add2ab532ccc9e8c24e., 6b45157a30794a82fc5cbdb3589032e2=t,,1347999506624.6b45157a30794a82fc5cbdb3589032e2., 95255bd4b3285804a2c818d1bf7f459f=t,xxx,1347999506714.95255bd4b3285804a2c818d1bf7f459f., cb765c4343d7b8c707c88b4d4b79a165=t,qqq,1347999506692.cb765c4343d7b8c707c88b4d4b79a165., 78622cfb2af5eca037e2375d7566cac6=t,mmm,1347999506681.78622cfb2af5eca037e2375d7566cac6.}
2012-09-18 20:18:30,086 INFO  [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-2] regionserver.HRegion(1023): Closed t,hhh,1347999506667.5a1faf1d76da0300d16f9c8dbb752824.
2012-09-18 20:18:30,086 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-2] handler.CloseRegionHandler(168): Closed region t,hhh,1347999506667.5a1faf1d76da0300d16f9c8dbb752824.
2012-09-18 20:18:30,086 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-2] handler.CloseRegionHandler(124): Processing close of t,ooo,1347999506686.c4178dd0ed59cff7c16b018f8936b194.
2012-09-18 20:18:30,086 DEBUG [RegionServer:3;hemera.apache.org,58959,1347999502361] catalog.CatalogTracker(257): Stopping catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@1880b02
2012-09-18 20:18:30,086 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-1] regionserver.HRegion(975): Updates disabled for region t,www,1347999506711.985b3c0de2cbfe789c69850bb128887d.
2012-09-18 20:18:30,084 DEBUG [RS_CLOSE_REGION-hemera.apache.org,51601,1347999502333-2] handler.CloseRegionHandler(168): Closed region t,ttt,1347999506703.73c7f7079688e986da1850e53fb74f9d.
2012-09-18 20:18:30,084 DEBUG [pool-1-thread-1-EventThread] zookeeper.ZooKeeperWatcher(265): master:35050-0x139db080e690000 Received ZooKeeper Event, type=NodeChildrenChanged, state=SyncConnected, path=/hbase/draining
2012-09-18 20:18:30,084 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-0] regionserver.HRegion(954): Closing t,nnn,1347999506684.a2e6ab866441d82e1124ad6dcaf7fa03.: disabling compactions & flushes
2012-09-18 20:18:30,081 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-1] handler.CloseRegionHandler(124): Processing close of t,xxx,1347999506714.95255bd4b3285804a2c818d1bf7f459f.
2012-09-18 20:18:30,090 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-1] regionserver.HRegion(954): Closing t,xxx,1347999506714.95255bd4b3285804a2c818d1bf7f459f.: disabling compactions & flushes
2012-09-18 20:18:30,090 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-1] regionserver.HRegion(975): Updates disabled for region t,xxx,1347999506714.95255bd4b3285804a2c818d1bf7f459f.
2012-09-18 20:18:30,079 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] regionserver.HRegion(975): Updates disabled for region t,sss,1347999506698.c1e5810326004add2ab532ccc9e8c24e.
2012-09-18 20:18:30,079 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-2] regionserver.HRegion(954): Closing t,,1347999506624.6b45157a30794a82fc5cbdb3589032e2.: disabling compactions & flushes
2012-09-18 20:18:30,090 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-0] regionserver.HRegion(975): Updates disabled for region t,nnn,1347999506684.a2e6ab866441d82e1124ad6dcaf7fa03.
2012-09-18 20:18:30,090 INFO  [StoreCloserThread-t,sss,1347999506698.c1e5810326004add2ab532ccc9e8c24e.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,090 INFO  [StoreCloserThread-t,www,1347999506711.985b3c0de2cbfe789c69850bb128887d.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,089 INFO  [RegionServer:3;hemera.apache.org,58959,1347999502361] client.HConnectionManager$HConnectionImplementation(1523): Closing zookeeper sessionid=0x139db080e690008
2012-09-18 20:18:30,091 INFO  [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-1] regionserver.HRegion(1023): Closed t,www,1347999506711.985b3c0de2cbfe789c69850bb128887d.
2012-09-18 20:18:30,091 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-1] handler.CloseRegionHandler(168): Closed region t,www,1347999506711.985b3c0de2cbfe789c69850bb128887d.
2012-09-18 20:18:30,091 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-1] handler.CloseRegionHandler(124): Processing close of t,rrr,1347999506695.29ca9f1226c85d9ad4523735386186da.
2012-09-18 20:18:30,089 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-2] regionserver.HRegion(954): Closing t,ooo,1347999506686.c4178dd0ed59cff7c16b018f8936b194.: disabling compactions & flushes
2012-09-18 20:18:30,091 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-2] regionserver.HRegion(975): Updates disabled for region t,ooo,1347999506686.c4178dd0ed59cff7c16b018f8936b194.
2012-09-18 20:18:30,091 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-1] regionserver.HRegion(954): Closing t,rrr,1347999506695.29ca9f1226c85d9ad4523735386186da.: disabling compactions & flushes
2012-09-18 20:18:30,091 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-1] regionserver.HRegion(975): Updates disabled for region t,rrr,1347999506695.29ca9f1226c85d9ad4523735386186da.
2012-09-18 20:18:30,091 INFO  [StoreCloserThread-t,nnn,1347999506684.a2e6ab866441d82e1124ad6dcaf7fa03.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,092 INFO  [StoreCloserThread-t,rrr,1347999506695.29ca9f1226c85d9ad4523735386186da.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,092 INFO  [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-0] regionserver.HRegion(1023): Closed t,nnn,1347999506684.a2e6ab866441d82e1124ad6dcaf7fa03.
2012-09-18 20:18:30,092 INFO  [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-1] regionserver.HRegion(1023): Closed t,rrr,1347999506695.29ca9f1226c85d9ad4523735386186da.
2012-09-18 20:18:30,092 DEBUG [RS_CLOSE_REGION-hemera.apache.org,58959,1347999502361-0] handler.CloseRegionHandler(168): Closed region t,nnn,1347999506684.a2e6ab866441d82e1124ad6dcaf7fa03.
2012-09-18 20:18:30,091 INFO  [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] regionserver.HRegion(1023): Closed t,sss,1347999506698.c1e5810326004add2ab532ccc9e8c24e.
2012-09-18 20:18:30,092 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] handler.CloseRegionHandler(168): Closed region t,sss,1347999506698.c1e5810326004add2ab532ccc9e8c24e.
2012-09-18 20:18:30,092 INFO  [pool-1-thread-1] hbase.HBaseTestingUtility(747): Shutting down minicluster
2012-09-18 20:18:30,092 DEBUG [pool-1-thread-1] util.JVMClusterUtil(223): Shutting down HBase Cluster
2012-09-18 20:18:30,090 INFO  [StoreCloserThread-t,xxx,1347999506714.95255bd4b3285804a2c818d1bf7f459f.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,090 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-2] regionserver.HRegion(975): Updates disabled for region t,,1347999506624.6b45157a30794a82fc5cbdb3589032e2.
2012-09-18 20:18:30,093 INFO  [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-1] regionserver.HRegion(1023): Closed t,xxx,1347999506714.95255bd4b3285804a2c818d1bf7f459f.
2012-09-18 20:18:30,092 INFO  [pool-1-thread-1] master.HMaster(2049): Cluster shutdown requested
2012-09-18 20:18:30,092 DEBUG [RS_CLOSE_REGION-hemera.apache.org,43875,1347999502347-0] handler.CloseRegionHandler(124): Processing close of t,qqq,1347999506692.cb765c4343d7b8c707c88b4d4b79a165.
2012-09-18 20:18:30,093 INFO  [StoreCloserThread-t,,1347999506624.6b45157a30794a82fc5cbdb3589032e2.-1] regionserver.HStore(635): Closed f
2012-09-18 20:18:30,092 DEBUG [RS_CLOSE_REGION-hemera.apache.org",,,,,,,,
,,,,,,,,,
,,,,,,,,,
HBASE-6820,"MiniZookeeperCluster.shutdown() shuts down the ZookeeperServer and NIOServerCnxnFactory. However, MiniZookeeperCluster uses a deprecated ZookeeperServer constructor, which in turn constructs its own FileTxnSnapLog, and ZKDatabase. Since ZookeeperServer.shutdown() does not close() the ZKDatabase, we have to explicitly close it in MiniZookeeperCluster.shutdown().

Tests effected by this are
{code}
TestSplitLogManager
TestSplitLogWorker
TestOfflineMetaRebuildBase
TestOfflineMetaRebuildHole
TestOfflineMetaRebuildOverlap
{code}

", ,,,,,,,
HBASE-6822,"TestHBaseTestingUtility.testMiniZooKeeper() tests whether the mini zk cluster is working by launching 5 threads corresponding to zk servers. 

NIOServerCnxnFactory.configure() configures the socket as:

{code}
        this.ss = ServerSocketChannel.open();
        ss.socket().setReuseAddress(true);
{code}

setReuseAddress() is set, because it allows the server to come back up and bind to the same port before the socket is timed-out by the kernel.

Under windows, the behavior on ServerSocket.setReuseAddress() is different than on linux, in which it allows any process to bind to an already-bound port. This causes ZK nodes starting on the same node, to be able to bind to the same port. 

The following part of the patch at https://issues.apache.org/jira/browse/HADOOP-8223 deals with this case for Hadoop:

{code}
if(Shell.WINDOWS) {
+      // result of setting the SO_REUSEADDR flag is different on Windows
+      // http://msdn.microsoft.com/en-us/library/ms740621(v=vs.85).aspx
+      // without this 2 NN's can start on the same machine and listen on 
+      // the same port with indeterminate routing of incoming requests to them
+      ret.setReuseAddress(false);
+    }
{code}

We should do the same in Zookeeper (I'll open a ZOOK issue). But in the meantime, we can fix hbase tests to not rely on BindException to resolve for bind errors. Especially, in  MiniZKCluster.startup() when starting more than 1 servers, we already know that we have to increment the port number. ", ,1,,,,,,
HBASE-6823,"There are two unit test cases in HBase RegionServer test failed in the clean up stage that failed to delete the files/folders created in the test. 
testWholesomeSplit(org.apache.hadoop.hbase.regionserver.TestSplitTransaction): Failed delete of ./target/test-
data/1c386abc-f159-492e-b21f-e89fab24d85b/org.apache.hadoop.hbase.regionserver.TestSplitTransaction/table/a588d813fd26280c2b42e93565ed960c
testRollback(org.apache.hadoop.hbase.regionserver.TestSplitTransaction): Failed delete of ./target/test-data/6
1a1a14b-0cc9-4dd6-93fd-4dc021e2bfcc/org.apache.hadoop.hbase.regionserver.TestSplitTransaction/table/8090abc89528461fa284288c257662cd
The root cause is triggered by ta call to the DaughterOpener.start() in \src\hbase\src\main\java\org\apache\hadoop\hbase\regionserver\SplitTransactopn.Java (openDaughters() function). It left handles to the splited folder/file and causing deleting of the file/folder failed in the Windows OS.

Windows does not allow to delete a file, while there are open file handlers.", ,1,,,,,,
HBASE-6824,"We need to make the temp directory where coprocessor jars are saved configurable. For this we will add hbase.local.dir configuration parameter. 

Windows tests are failing due to the pathing problems for coprocessor jars:
Two HBase TestClassLoading unit tests failed due to a failiure in loading the test file from HDFS:
{code}
testClassLoadingFromHDFS(org.apache.hadoop.hbase.coprocessor.TestClassLoading): Class TestCP1 was missing on a region
testClassLoadingFromLibDirInJar(org.apache.hadoop.hbase.coprocessor.TestClassLoading): Class TestCP1 was missing on a region
{code}

The problem is that CoprocessorHost.load() copies the jar file locally, and schedules the local file to be deleted on exit, but calling FileSystem.deleteOnExit(). However, the filesystem is not the file system of the local file, it is the distributed file system, so on windows, the Path fails.", ,1,,,,,,
HBASE-6825,"TestScannerTimeout.test2481() fails with:
{code}
java.lang.AssertionError: We should be timing out
	at org.junit.Assert.fail(Assert.java:93)
	at org.apache.hadoop.hbase.client.TestScannerTimeout.test2481(TestScannerTimeout.java:117)
{code}", ,,,,,,,
HBASE-6827,"TestScannerTimeout.test2481() fails with:
{code}
java.lang.AssertionError: We should be timing out
	at org.junit.Assert.fail(Assert.java:93)
	at org.apache.hadoop.hbase.client.TestScannerTimeout.test2481(TestScannerTimeout.java:117)
{code}", ,1,,,,,,
HBASE-6829,"TestCacheOnWriteInSchema and TestCompactSelection fails with 
{code}
java.io.IOException: Target HLog directory already exists: ./target/test-data/2d814e66-75d3-4c1b-92c7-a49d9972e8fd/TestCacheOnWriteInSchema/logs
	at org.apache.hadoop.hbase.regionserver.wal.HLog.<init>(HLog.java:385)
	at org.apache.hadoop.hbase.regionserver.wal.HLog.<init>(HLog.java:316)
	at org.apache.hadoop.hbase.regionserver.TestCacheOnWriteInSchema.setUp(TestCacheOnWriteInSchema.java:162)
{code}", ,,,,,,,
HBASE-6830,"Some of the tests resolve the local temp directory for temporary test data, but use this directory path in dfs. Since on windows, local temp dir is resolved to something like: c:\\<path_to_local_dir>, DistributedFileSystem.getPathName() throws an IllegalArgumentException complaining that it is not a valid path name. 

Instead of relying on a local temp dir name, we should create a temp dir on dfs, and use this as a basis dir for test data. 

At least the following test cases are affected by this: 
{code}
TestHFileOutputFormat
TestHRegionServerBulkLoad
{code}
", ,1,,,,,,
HBASE-6832,"TestRegionObserverBypass.testMulti() fails with 
{code}
java.lang.AssertionError: expected:<1> but was:<0>
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.failNotEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:128)
	at org.junit.Assert.assertEquals(Assert.java:472)
	at org.junit.Assert.assertEquals(Assert.java:456)
	at org.apache.hadoop.hbase.coprocessor.TestRegionObserverBypass.checkRowAndDelete(TestRegionObserverBypass.java:173)
	at org.apache.hadoop.hbase.coprocessor.TestRegionObserverBypass.testMulti(TestRegionObserverBypass.java:166)
{code}
", ,1,,,,,,
HBASE-6838,"In current implementation of HRegionserver#addScanner, it may generate same scanner name, thus make scanner confusion.", ,,,,,,,
HBASE-6839,"HRegion#internalObtainRowLock will return null if timed out,
but many place which call this method don't handle this case

The bad result is operation will be executed even if it havn't obtained the row lock. Such as putdeleteincrement?
HBASE-6840,12608280,SplitLogManager should reassign tasks even on a clean RS shutdown.,Resolved,SplitLogManager does not reassign tasks if the regionserver does a clean", ,,,,,,,
HBASE-6841,"The behaviour of scan is very inefficient when using with FilterList.

The FilterList rewrites the return code from NEXT_ROW to SKIP from a filter if Operator.MUST_PASS_ALL is used. 

This happens when using ColumnPrefixFilter. Even though the ColumnPrefixFilter indicates to jump to NEXT_ROW because no further match can be found, the scan continues to scan all versions of a column in that row and all columns of that row because the ReturnCode from ColumnPrefixFilter has been rewritten by the FilterList from NEXT_ROW to SKIP. 

This is particularly inefficient when there are many versions in a column because the check is performed on all versions of the column instead of just by checking the qualifier of the column name.", ,,1,,,,,
HBASE-6843,"After applying HBASE-6308,we found error followed

2012-09-06 00:44:38,341 DEBUG org.apache.hadoop.hbase.coprocessor.CoprocessorClassLoader: Finding class: com.hadoop.compression.lzo.LzoCodec
2012-09-06 00:44:38,351 ERROR com.hadoop.compression.lzo.GPLNativeCodeLoader: Could not load native gpl library
java.lang.UnsatisfiedLinkError: Native Library /home/zhuzhuang/hbase/0.94.0-ali-1.0/lib/native/Linux-amd64-64/libgplcompression.so already loaded in another classloade
r
at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1772)
at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1732)
at java.lang.Runtime.loadLibrary0(Runtime.java:823)
at java.lang.System.loadLibrary(System.java:1028)
at com.hadoop.compression.lzo.GPLNativeCodeLoader.<clinit>(GPLNativeCodeLoader.java:32)
at com.hadoop.compression.lzo.LzoCodec.<clinit>(LzoCodec.java:67)
at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:113)
at org.apache.hadoop.hbase.io.hfile.Compression$Algorithm$1.getCodec(Compression.java:107)
at org.apache.hadoop.hbase.io.hfile.Compression$Algorithm.getCompressor(Compression.java:243)
at org.apache.hadoop.hbase.util.CompressionTest.testCompression(CompressionTest.java:85)
at org.apache.hadoop.hbase.regionserver.HRegion.checkCompressionCodecs(HRegion.java:3793)
at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:3782)
at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:3732)
at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.openRegion(OpenRegionHandler.java:332)
at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:108)
at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:169)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:662)
2012-09-06 00:44:38,355 DEBUG org.apache.hadoop.hbase.coprocessor.CoprocessorClassLoader: Skipping exempt class java.io.PrintWriter - delegating directly to parent
2012-09-06 00:44:38,355 ERROR com.hadoop.compression.lzo.LzoCodec: Cannot load native-lzo without native-hadoop", ,1,,,,,,
HBASE-6847,"After running with HBASE-6646 and replication enabled I encountered this:

{noformat}
2012-09-17 20:04:08,111 DEBUG org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Opening log for replication va1r3s24%2C10304%2C1347911704238.1347911706318 at 78617132
2012-09-17 20:04:08,120 DEBUG org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Break on IOE: hdfs://va1r5s41:10101/va1-backup/.logs/va1r3s24,10304,1347911704238/va1r3s24%2C10304%2C1347911704238.1347911706318, entryStart=78641557, pos=78771200, end=78771200, edit=84
2012-09-17 20:04:08,120 DEBUG org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: currentNbOperations:164529 and seenEntries:84 and size: 154068
2012-09-17 20:04:08,120 DEBUG org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Replicating 84
2012-09-17 20:04:08,146 INFO org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager: Going to report log #va1r3s24%2C10304%2C1347911704238.1347911706318 for position 78771200 in hdfs://va1r5s41:10101/va1-backup/.logs/va1r3s24,10304,1347911704238/va1r3s24%2C10304%2C1347911704238.1347911706318
2012-09-17 20:04:08,158 INFO org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager: Removing 0 logs in the list: []
2012-09-17 20:04:08,158 DEBUG org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Replicated in total: 93234
2012-09-17 20:04:08,158 DEBUG org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Opening log for replication va1r3s24%2C10304%2C1347911704238.1347911706318 at 78771200
2012-09-17 20:04:08,163 ERROR org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Unexpected exception in ReplicationSource, currentPath=hdfs://va1r5s41:10101/va1-backup/.logs/va1r3s24,10304,1347911704238/va1r3s24%2C10304%2C1347911704238.1347911706318
java.lang.IndexOutOfBoundsException
        at java.io.DataInputStream.readFully(DataInputStream.java:175)
        at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:63)
        at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:101)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2001)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1901)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1947)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:235)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.readAllEntriesToReplicateOrNextFile(ReplicationSource.java:394)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:307)
{noformat}

There's something weird at the end of the file and it's killing replication. We used to just retry.", ,,,,,,,
HBASE-6851,"When new global permissions are assigned, there is a race condition, during which further authorization checks relying on global permissions may fail.

In TableAuthManager.updateGlobalCache(), we have:
{code:java}
    USER_CACHE.clear();
    GROUP_CACHE.clear();
    try {
      initGlobal(conf);
    } catch (IOException e) {
      // Never happens
      LOG.error(""Error occured while updating the user cache"", e);
    }
    for (Map.Entry<String,TablePermission> entry : userPerms.entries()) {
      if (AccessControlLists.isGroupPrincipal(entry.getKey())) {
        GROUP_CACHE.put(AccessControlLists.getGroupName(entry.getKey()),
                        new Permission(entry.getValue().getActions()));
      } else {
        USER_CACHE.put(entry.getKey(), new Permission(entry.getValue().getActions()));
      }
    }
{code}

If authorization checks come in following the .clear() but before repopulating, they will fail.

We should have some synchronization here to serialize multiple updates and use a COW type rebuild and reassign of the new maps.

This particular issue crept in with the fix in HBASE-6157, so I'm flagging for 0.94 and 0.96.",1,,,,,,,
HBASE-6852,"The behaviour of scan is very inefficient when using with FilterList.

The FilterList rewrites the return code from NEXT_ROW to SKIP from a filter if Operator.MUST_PASS_ALL is used. 

This happens when using ColumnPrefixFilter. Even though the ColumnPrefixFilter indicates to jump to NEXT_ROW because no further match can be found, the scan continues to scan all versions of a column in that row and all columns of that row because the ReturnCode from ColumnPrefixFilter has been rewritten by the FilterList from NEXT_ROW to SKIP. 

This is particularly inefficient when there are many versions in a column because the check is performed on all versions of the column instead of just by checking the qualifier of the column name.", ,,1,,,,,
HBASE-6853,"This is w.r.t a mail sent in the dev mail list.

Empty region split should be handled gracefully.  Either we should not allow the split to happen if we know that the region is empty or we should allow the split to happen by setting the no of threads to the thread pool executor as 1.
{code}
int nbFiles = hstoreFilesToSplit.size();
ThreadFactoryBuilder builder = new ThreadFactoryBuilder();
    builder.setNameFormat(""StoreFileSplitter-%1$d"");
    ThreadFactory factory = builder.build();
    ThreadPoolExecutor threadPool =
      (ThreadPoolExecutor) Executors.newFixedThreadPool(nbFiles, factory);
    List<Future<Void>> futures = new ArrayList<Future<Void>>(nbFiles);

{code}
Here the nbFiles needs to be a non zero positive value.

 ", ,,,,,,,
HBASE-6854,"If a failure happens in split before OFFLINING_PARENT, we tend to rollback the split including deleting the znodes created.
On deletion of the RS_ZK_SPLITTING node we are getting a callback but not remvoving from RIT. We need to remove it from RIT, anyway SSH logic is well guarded in case the delete event comes due to RS down scenario.", ,,,,,,,
HBASE-6855,"If a failure happens in split before OFFLINING_PARENT, we tend to rollback the split including deleting the znodes created.
On deletion of the RS_ZK_SPLITTING node we are getting a callback but not remvoving from RIT. We need to remove it from RIT, anyway SSH logic is well guarded in case the delete event comes due to RS down scenario.", ,,,,,,,
HBASE-6868,"The HFile contains checksums for decrease the iops, so when Hbase read HFile , that dont't need to read the checksum from meta file of HDFS.  But HLog file of Hbase don't contain the checksum, so when HBase read the HLog, that must read checksum from meta file of HDFS.  We could  add setSkipChecksum per file to hdfs or we could write checksums into WAL if this skip checksum facility is enabled ", ,,1,,,,,
HBASE-6870,"The HFile contains checksums for decrease the iops, so when Hbase read HFile , that dont't need to read the checksum from meta file of HDFS.  But HLog file of Hbase don't contain the checksum, so when HBase read the HLog, that must read checksum from meta file of HDFS.  We could  add setSkipChecksum per file to hdfs or we could write checksums into WAL if this skip checksum facility is enabled ", ,,1,,,,,
HBASE-6871,"After writing some data, compaction and scan operation both failure, the exception message is below:
2012-09-18 06:32:26,227 ERROR org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest: Compaction failed regionName=hfile_test,,1347778722498.d220df43fb9d8af4633bd7f547613f9e., storeName=page_info, fileCount=7, fileSize=1.3m (188.0k, 188.0k, 188.0k, 188.0k, 188.0k, 185.8k, 223.3k), priority=9, time=45826250816757428java.io.IOException: Could not reseek StoreFileScanner[HFileScanner for reader reader=hdfs://hadoopdev1.cm6:9000/hbase/hfile_test/d220df43fb9d8af4633bd7f547613f9e/page_info/b0f6118f58de47ad9d87cac438ee0895, compression=lzo, cacheConf=CacheConfig:enabled [cacheDataOnRead=true] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheCompressed=false], firstKey=http://com.truereligionbrandjeans.www/Womens_Dresses/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Sweaters/pl/c/Womens_Sweaters/pl/c/Womens_Sweaters/pl/c/Womens_Sweaters/pl/c/Womens_Sweaters/pl/c/Womens_Sweaters/pl/c/Womens_Shirts/pl/c/Womens_Sweaters/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Sweaters/pl/c/Womens_Sweaters/pl/c/Womens_Shirts/pl/c/Womens_Shirts/pl/c/Womens_Sweaters/pl/c/Womens_Sweaters/pl/c/Womens_Sweaters/pl/c/Womens_Sweaters/pl/c/Womens_Sweaters/pl/c/4010.html/page_info:anchor_sig/1347764439449/DeleteColumn, lastKey=http://com.trura.www//page_info:page_type/1347763395089/Put, avgKeyLen=776, avgValueLen=4, entries=12853, length=228611, cur=http://com.truereligionbrandjeans.www/Womens_Exclusive_Details/pl/c/4970.html/page_info:is_deleted/1347764003865/Put/vlen=1/ts=0] to key http://com.truereligionbrandjeans.www/Womens_Exclusive_Details/pl/c/4970.html/page_info:is_deleted/OLDEST_TIMESTAMP/Minimum/vlen=0/ts=0
        at org.apache.hadoop.hbase.regionserver.StoreFileScanner.reseek(StoreFileScanner.java:178)        
        at org.apache.hadoop.hbase.regionserver.NonLazyKeyValueScanner.doRealSeek(NonLazyKeyValueScanner.java:54)        
        at org.apache.hadoop.hbase.regionserver.KeyValueHeap.generalizedSeek(KeyValueHeap.java:299)
        at org.apache.hadoop.hbase.regionserver.KeyValueHeap.reseek(KeyValueHeap.java:244)        
        at org.apache.hadoop.hbase.regionserver.StoreScanner.reseek(StoreScanner.java:521)        
        at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:402)
        at org.apache.hadoop.hbase.regionserver.Store.compactStore(Store.java:1570)        
        at org.apache.hadoop.hbase.regionserver.Store.compact(Store.java:997)        
        at org.apache.hadoop.hbase.regionserver.HRegion.compact(HRegion.java:1216)
        at org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.run(CompactionRequest.java:250)        
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)        
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Expected block type LEAF_INDEX, but got INTERMEDIATE_INDEX: blockType=INTERMEDIATE_INDEX, onDiskSizeWithoutHeader=8514, uncompressedSizeWithoutHeader=131837, prevBlockOffset=-1, dataBeginsWith=\x00\x00\x00\x9B\x00\x00\x00\x00\x00\x00\x03#\x00\x00\x050\x00\x00\x08\xB7\x00\x00\x0Cr\x00\x00\x0F\xFA\x00\x00\x120, fileOffset=218942        at org.apache.hadoop.hbase.io.hfile.HFileReaderV2.validateBlockType(HFileReaderV2.java:378)
        at org.apache.hadoop.hbase.io.hfile.HFileReaderV2.readBlock(HFileReaderV2.java:331)        at org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader.seekToDataBlock(HFileBlockIndex.java:213)
        at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.seekTo(HFileReaderV2.java:455)
        at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.reseekTo(HFileReaderV2.java:493)        
        at org.apache.hadoop.hbase.regionserver.StoreFileScanner.reseekAtOrAfter(StoreFileScanner.java:242)        
        at org.apache.hadoop.hbase.regionserver.StoreFileScanner.reseek(StoreFileScanner.java:167)

After some debug worksI found that when hfile closing, if the rootChunk is empty, the only one curInlineChunk will upgrade to root chunk. But if the last block flushing make curInlineChunk exceed max index block size, the root chunk(upgrade from curInlineChunk) will be splited into intermediate index blocks, and the index level is set to 2. So when BlockIndexReader read the root index, it expects the next level index block is leaf index(index level=2), but the on disk index block is intermediate block, the error happened. 

After I add some code to check curInlineChunk's size when rootChunk is empty in shouldWriteBlock(boolean closing), this bug can be fixed.

", ,,,,,,,
HBASE-6873,"After applying HBASE-6308,we found error followed

2012-09-06 00:44:38,341 DEBUG org.apache.hadoop.hbase.coprocessor.CoprocessorClassLoader: Finding class: com.hadoop.compression.lzo.LzoCodec
2012-09-06 00:44:38,351 ERROR com.hadoop.compression.lzo.GPLNativeCodeLoader: Could not load native gpl library
java.lang.UnsatisfiedLinkError: Native Library /home/zhuzhuang/hbase/0.94.0-ali-1.0/lib/native/Linux-amd64-64/libgplcompression.so already loaded in another classloade
r
at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1772)
at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1732)
at java.lang.Runtime.loadLibrary0(Runtime.java:823)
at java.lang.System.loadLibrary(System.java:1028)
at com.hadoop.compression.lzo.GPLNativeCodeLoader.<clinit>(GPLNativeCodeLoader.java:32)
at com.hadoop.compression.lzo.LzoCodec.<clinit>(LzoCodec.java:67)
at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:113)
at org.apache.hadoop.hbase.io.hfile.Compression$Algorithm$1.getCodec(Compression.java:107)
at org.apache.hadoop.hbase.io.hfile.Compression$Algorithm.getCompressor(Compression.java:243)
at org.apache.hadoop.hbase.util.CompressionTest.testCompression(CompressionTest.java:85)
at org.apache.hadoop.hbase.regionserver.HRegion.checkCompressionCodecs(HRegion.java:3793)
at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:3782)
at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:3732)
at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.openRegion(OpenRegionHandler.java:332)
at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:108)
at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:169)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:662)
2012-09-06 00:44:38,355 DEBUG org.apache.hadoop.hbase.coprocessor.CoprocessorClassLoader: Skipping exempt class java.io.PrintWriter - delegating directly to parent
2012-09-06 00:44:38,355 ERROR com.hadoop.compression.lzo.LzoCodec: Cannot load native-lzo without native-hadoop", ,1,,,,,,
HBASE-6874,"The HFile contains checksums for decrease the iops, so when Hbase read HFile , that dont't need to read the checksum from meta file of HDFS.  But HLog file of Hbase don't contain the checksum, so when HBase read the HLog, that must read checksum from meta file of HDFS.  We could  add setSkipChecksum per file to hdfs or we could write checksums into WAL if this skip checksum facility is enabled ", ,,1,,,,,
HBASE-6877,"When we execute the coprocessor, we will called HTable#getStartKeysInRange first and get the Keys to exec coprocessor,
if then some regions are split before execCoprocessor RPC, the Keys are something wrong now, and the result we get is not integrated, 

for example:
parent region is split into daughter region A and daughter region B,
we executed coprocessor on the parent region, but the result data is only daughter region A or daughter region B


", ,,,,,,,
HBASE-6881,"{noformat}
+        RegionPlan newPlan = plan;
+        if (!regionAlreadyInTransitionException) {
+          // Force a new plan and reassign. Will return null if no servers.
+          newPlan = getRegionPlan(state, plan.getDestination(), true);
+        }
+        if (newPlan == null) {
           this.timeoutMonitor.setAllRegionServersOffline(true);
           LOG.warn(""Unable to find a viable location to assign region "" +
             state.getRegion().getRegionNameAsString());
{noformat}

Here, when newPlan is null, plan.getDestination() could be up actually.

", ,,,,,,,
HBASE-6887,"The security-related HBase shell commands (grant, revoke, user_permission) are still using the old CoprocessorProtocol-based AccessControllerProtocol endpoint for dynamic RPC. These need to be converted to use the protocol buffer based AccessControlService interface added in HBASE-5448.",1,,,,,,,
HBASE-6896,"In regular assignment, in case of socket network timeout, it tries to call openRegion again and again without change the region plan, ZK offline node,
till the region is out of transition, in case the region server is still up.

We may need to sync them up and make sure bulk assignment does the same in this case.", ,,,,,,,
HBASE-6900,"HBASE-5520 introduced reseek() on the RegionScanner.  
Now when a scanner is created we have the StoreScanner heap.  After this if a flush or compaction happens parallely all the StoreScannerObservers are cleared so that whenever a new next() call happens we tend to recreate the scanner based on the latest store files.
The reseek() in StoreScanner expects the heap not to be null because always reseek would be called from next()
{code}
public synchronized boolean reseek(KeyValue kv) throws IOException {
    //Heap cannot be null, because this is only called from next() which
    //guarantees that heap will never be null before this call.
    if (explicitColumnQuery && lazySeekEnabledGlobally) {
      return heap.requestSeek(kv, true, useRowColBloom);
    } else {
      return heap.reseek(kv);
    }
  }
{code}
Now when we call RegionScanner.reseek() directly using CPs we tend to get a NPE.  In our case it happened when a major compaction was going on.  I will also attach a testcase to show the problem.

", ,,,,,,,
HBASE-6901,"When setting <hbase.mapreduce.hfileoutputformat.compaction.exclude> to true, and run compaction to exclude bulk loaded files could cause ArrayIndexOutOfBoundsException since all files are excluded.", ,,,,,,,
HBASE-6904,"On a replication-enabled cluster, querying the list_peers produces the error lines shown below. It doesn't appear that anything is broken in terms of functionality.

Stack trace:

hbase(main):001:0> list_peers
12/09/29 14:41:03 ERROR zookeeper.RecoverableZooKeeper: Node /hbase/replication/peers already exists and this is not a retry
12/09/29 14:41:03 ERROR zookeeper.RecoverableZooKeeper: Node /hbase/replication/rs already exists and this is not a retry
PEER ID CLUSTER KEY
0 row(s) in 0.4650 seconds
", ,,,,,,,
HBASE-6907,"KeyValue.KVComparator includes the memstoreTS when comparing, however the KeyValue.equals() method ignores the memstoreTS.

The Comparator interface has always specified that comparator return 0 when equals would return true and vice versa.  Obeying that rule has been sort of optional in the past, but Java 7 introduces a new default collection sorting algorithm called Tim Sort which relies on that behavior.  http://bugs.sun.com/view_bug.do?bug_id=6804124

Possible problem spots:
* there's a Collections.sort(KeyValues) in RedundantKVGenerator.generateTestKeyValues(..)
* TestColumnSeeking compares two collections of KeyValues using the containsAll method.  It is intentionally ignoring memstoreTS, so will need an alternative method for comparing the two collections.
", ,,,,,,,
HBASE-6912,"Steps to reproduce:

Create a table, load data into it. Flush the table.

Do a scan with
1. Some filter which should not match the first entry in the scan
2. Where one specifies a family and column.
You will notice that the first entry is returned even though it doesn't match the filter.

It looks like the when the first KeyValue of a scan in the column from the point of view of the code

HRegion.java
{code}
} else if (kv != null && !kv.isInternal() && filterRowKey(currentRow)) {
{code}
Is generated by
{code}
public static KeyValue createLastOnRow(final byte [] row,
final int roffset, final int rlength, final byte [] family,
final int foffset, final int flength, final byte [] qualifier,
final int qoffset, final int qlength) { return new KeyValue(row, roffset, rlength, family, foffset, flength, qualifier, qoffset, qlength, HConstants.OLDEST_TIMESTAMP, Type.Minimum, null, 0, 0); }
{code}
So it is always internal from that point of the code.

Only later from within
StoreScanner.java
{code}
public synchronized boolean next(List<KeyValue> outResult, int limit, String metric) throws IOException {
....
LOOP: while((kv = this.heap.peek()) != null) {
{code}
( The second time through)

Do we get the actual kv, with a proper type and timestamp. This seems to mess with filtering.
", ,1,,,,,,
HBASE-6920,"HBASE-5058 appears to have introduced an issue where a timeout in HConnection.getMaster() can cause the client to never be able to connect to the master.  So, for example, an HBaseAdmin object can never successfully be initialized.

The issue is here:
{code}
if (tryMaster.isMasterRunning()) {
  this.master = tryMaster;
  this.masterLock.notifyAll();
  break;
}
{code}

If isMasterRunning times out, it throws an UndeclaredThrowableException, which is already not ideal, because it can be returned to the application.

 But if the first call to getMaster succeeds, it will set masterChecked = true, which makes us never try to reconnect; that is, we will set this.master = null and just throw MasterNotRunningExceptions, without even trying to connect.

I tried out a 94 client (actually a 92 client with some 94 patches) on a cluster with some network issues, and it would constantly get stuck as described above.", ,,1,,,,,
HBASE-6925,"In build #3406, I saw:
{code}
java.lang.AssertionError: Column family prefix used twice: cf.cf.bt.Data.fsReadnumops
	at org.apache.hadoop.hbase.regionserver.metrics.SchemaMetrics.validateMetricChanges(SchemaMetrics.java:822)
	at org.apache.hadoop.hbase.regionserver.TestStoreFile.tearDown(TestStoreFile.java:89)
{code}", ,1,1,,,,,
HBASE-6928,"In build #3406, I saw:
{code}
java.lang.AssertionError: Column family prefix used twice: cf.cf.bt.Data.fsReadnumops
	at org.apache.hadoop.hbase.regionserver.metrics.SchemaMetrics.validateMetricChanges(SchemaMetrics.java:822)
	at org.apache.hadoop.hbase.regionserver.TestStoreFile.tearDown(TestStoreFile.java:89)
{code}", ,,,,,,,
HBASE-6930,"When processing the multiPut, multiMutations or multiDelete operations, each IPC handler thread tries to acquire a lock for each row key in these batches. If there are duplicated row keys in these batches, previously the IPC handler thread will repeatedly acquire the same row key again and again.

So the optimization is to sort each batch operation based on the row key in the client side, and skip acquiring the same row lock repeatedly in the server side.", ,,,,,,,
HBASE-6941,"The LoadIncrementalHFiles tool has pretty complex config loading structured in it, which seems unnecessary and also causes problem since it is ignoring any settings passed to it via Tool's -Dprop=value parameters.

This makes integration with tools such as Oozie harder, as it doesn't accept different addresses of ZK, etc. unless there's a hbase-site.xml on the classpath to load from (which is painful to achieve on Oozie).", ,1,,,,,,
HBASE-6943,"When getting a regionserver connection in 0.89-fb in HBaseClient, we catch all types of Throwable. I have observed a real case when the client looked stuck. On debugging it turned out that a NoSuchMethodError was thrown and caught, leaving the connection in an inconsistent state (initialized socket but null streams). All following attempts resulted in NPEs that were also caught, and no errors were logged. From the user's perspective the client was just stuck. The root cause was the absence of a required jar (hence the NoSuchMethodError) but it was not reported properly.", ,,,,,,,
HBASE-6948,, ,1,,,,,,
HBASE-6950,"HBASE-6552 caused the TestAcidGuarantees system test to flush more aggressively, because flushes are where ACID problems have occurred in the past.

After some more cluster testing, it seems like this too aggressive; my clusters eventually can't keep up with the number of flushes/compactions and start getting SocketTimeoutExceptions.  We could try to optimize the flushes/compactions, but since this workload would never occur in practice, I don't think it is worth the effort.  Instead, let's just only flush once a minute.  This is arbitrary, but seems to work.

Here is my comment in the (upcoming) patch:
{code}
// Flushing has been a source of ACID violations previously (see HBASE-2856), so ideally,
// we would flush as often as possible.  On a running cluster, this isn't practical:
// (1) we will cause a lot of load due to all the flushing and compacting
// (2) we cannot change the flushing/compacting related Configuration options to try to
// alleviate this
// (3) it is an unrealistic workload, since no one would actually flush that often.
// Therefore, let's flush every minute to have more flushes than usual, but not overload
// the running cluster.
{code}", ,,,,,,,
HBASE-6954,"It looks like the max version limit for a table or scanner is not applied to disregard older versions, prior to counting columns within a ColumnPaginationFilter or ColumnCountGetFilter. As a result, a Scan or Get can ultimately retrieve fewer than the requested number of columns when there is a sufficient number of existing columns to satisfy the request, if multiple versions of a column have been added to a row.

A minimal test case demonstrating this behavior is attached.

The javadoc for Get mentions 'Only Filter.filterKeyValue(KeyValue) is called AFTER all tests for ttl, column match, deletes and max versions have been run.'; for these two filters this behavior does not appear to be true, as flattening of multiple versions appears to occur after the filter has been applied.
", ,,,,,,,
HBASE-6956,"Sometimes we see a lot of Exception about closed connections:
{code}
 org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@553fd068 closed
org.apache.hadoop.hbase.client.ClosedConnectionException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@553fd068 closed
{code}

After investigation we assumed that it occurs because closed connection returns back into HTablePool. 

For our opinion best solution is  check whether the table is closed in method HTablePool.putTable and if true don't add it into the queue and release such HTableInterface.

But unfortunatly right now there are no access to HTable#closed field through HTableInterface", ,,,,,,,
HBASE-6958,"From https://builds.apache.org/job/HBase-TRUNK/3432/testReport/junit/org.apache.hadoop.hbase.master/TestAssignmentManager/testBalanceOnMasterFailoverScenarioWithOpenedNode/ :
{code}
Stacktrace

java.lang.Exception: test timed out after 5000 milliseconds
	at java.lang.System.arraycopy(Native Method)
	at java.lang.ThreadGroup.remove(ThreadGroup.java:969)
	at java.lang.ThreadGroup.threadTerminated(ThreadGroup.java:942)
	at java.lang.Thread.exit(Thread.java:732)
...
2012-10-06 00:46:12,521 DEBUG [MASTER_CLOSE_REGION-mockedAMExecutor-0] zookeeper.ZKUtil(1141): mockedServer-0x13a33892de7000e Retrieved 81 byte(s) of data from znode /hbase/unassigned/dc01abf9cd7fd0ea256af4df02811640 and set watcher; region=t,,1349484359011.dc01abf9cd7fd0ea256af4df02811640., state=M_ZK_REGION_OFFLINE, servername=master,1,1, createTime=1349484372509, payload.length=0
2012-10-06 00:46:12,522 ERROR [MASTER_CLOSE_REGION-mockedAMExecutor-0] executor.EventHandler(205): Caught throwable while processing event RS_ZK_REGION_CLOSED
java.lang.NullPointerException
	at org.apache.hadoop.hbase.master.TestAssignmentManager$MockedLoadBalancer.randomAssignment(TestAssignmentManager.java:773)
	at org.apache.hadoop.hbase.master.AssignmentManager.getRegionPlan(AssignmentManager.java:1709)
	at org.apache.hadoop.hbase.master.AssignmentManager.getRegionPlan(AssignmentManager.java:1666)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1435)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1155)
	at org.apache.hadoop.hbase.master.TestAssignmentManager$AssignmentManagerWithExtrasForTesting.assign(TestAssignmentManager.java:1035)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1130)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1125)
	at org.apache.hadoop.hbase.master.handler.ClosedRegionHandler.process(ClosedRegionHandler.java:106)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:202)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
2012-10-06 00:46:12,522 DEBUG [pool-1-thread-1-EventThread] master.AssignmentManager(670): Handling transition=M_ZK_REGION_OFFLINE, server=master,1,1, region=dc01abf9cd7fd0ea256af4df02811640, current state from region state map ={t,,1349484359011.dc01abf9cd7fd0ea256af4df02811640. state=OFFLINE, ts=1349484372508, server=null}
{code}
Looks like NPE happened on this line:
{code}
      this.gate.set(true);
{code}", ,,,,,,,
HBASE-6963,"{noformat}
12/10/02 11:49:07 WARN util.HBaseFsck: Got AccessControlException when preCheckPermission 
org.apache.hadoop.security.AccessControlException: Permission denied: action=WRITE path=hdfs://...:8020/hbase/-ROOT- user=hbase/...
	at org.apache.hadoop.hbase.util.FSUtils.checkAccess(FSUtils.java:882)
	at org.apache.hadoop.hbase.util.HBaseFsck.preCheckPermission(HBaseFsck.java:1230)
	at org.apache.hadoop.hbase.util.HBaseFsck.exec(HBaseFsck.java:3343)
	at org.apache.hadoop.hbase.util.HBaseFsck.main(HBaseFsck.java:3205)
{noformat}",1,1,,,,,,
HBASE-6968,"I am trying HDFS QJM feature in our test env. after a misconfig, i found the HMaster/HRegionServer process still up if the main thread is dead. Here is the stack trace:

xception in thread ""main"" java.net.UnknownHostException: unknown host: cluster1
        at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:214)
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1196)
        at org.apache.hadoop.ipc.Client.call(Client.java:1050)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)
        at $Proxy8.getProtocolVersion(Unknown Source)
        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:396)
        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:379)
        at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:119)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:238)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:203)
        at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:89)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1386)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1404)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:123)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.startRegionServer(HRegionServer.java:3647)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.startRegionServer(HRegionServer.java:3631)
        at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:61)
        at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:75)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:76)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:3691)

Then i need to kill the process manually to cleanup each time, so annoyed.
After applied the attached patch, the process will exist as expected, then i am happy again :)", ,1,,,,,,
HBASE-6977,"I am trying HDFS QJM feature in our test env. after a misconfig, i found the HMaster/HRegionServer process still up if the main thread is dead. Here is the stack trace:

xception in thread ""main"" java.net.UnknownHostException: unknown host: cluster1
        at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:214)
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1196)
        at org.apache.hadoop.ipc.Client.call(Client.java:1050)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)
        at $Proxy8.getProtocolVersion(Unknown Source)
        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:396)
        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:379)
        at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:119)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:238)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:203)
        at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:89)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1386)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1404)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:123)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.startRegionServer(HRegionServer.java:3647)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.startRegionServer(HRegionServer.java:3631)
        at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:61)
        at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:75)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:76)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:3691)

Then i need to kill the process manually to cleanup each time, so annoyed.
After applied the attached patch, the process will exist as expected, then i am happy again :)", ,,,,,,,
HBASE-6979,"I am trying HDFS QJM feature in our test env. after a misconfig, i found the HMaster/HRegionServer process still up if the main thread is dead. Here is the stack trace:

xception in thread ""main"" java.net.UnknownHostException: unknown host: cluster1
        at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:214)
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1196)
        at org.apache.hadoop.ipc.Client.call(Client.java:1050)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)
        at $Proxy8.getProtocolVersion(Unknown Source)
        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:396)
        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:379)
        at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:119)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:238)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:203)
        at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:89)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1386)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1404)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:123)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.startRegionServer(HRegionServer.java:3647)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.startRegionServer(HRegionServer.java:3631)
        at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:61)
        at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:75)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:76)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:3691)

Then i need to kill the process manually to cleanup each time, so annoyed.
After applied the attached patch, the process will exist as expected, then i am happy again :)", ,1,,,,,,
HBASE-6980,"I am trying HDFS QJM feature in our test env. after a misconfig, i found the HMaster/HRegionServer process still up if the main thread is dead. Here is the stack trace:

xception in thread ""main"" java.net.UnknownHostException: unknown host: cluster1
        at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:214)
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1196)
        at org.apache.hadoop.ipc.Client.call(Client.java:1050)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)
        at $Proxy8.getProtocolVersion(Unknown Source)
        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:396)
        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:379)
        at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:119)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:238)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:203)
        at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:89)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1386)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1404)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:123)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.startRegionServer(HRegionServer.java:3647)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.startRegionServer(HRegionServer.java:3631)
        at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:61)
        at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:75)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:76)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:3691)

Then i need to kill the process manually to cleanup each time, so annoyed.
After applied the attached patch, the process will exist as expected, then i am happy again :)", ,,1,,,,,
HBASE-6983,"I am trying HDFS QJM feature in our test env. after a misconfig, i found the HMaster/HRegionServer process still up if the main thread is dead. Here is the stack trace:

xception in thread ""main"" java.net.UnknownHostException: unknown host: cluster1
        at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:214)
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1196)
        at org.apache.hadoop.ipc.Client.call(Client.java:1050)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)
        at $Proxy8.getProtocolVersion(Unknown Source)
        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:396)
        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:379)
        at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:119)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:238)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:203)
        at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:89)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1386)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1404)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:123)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.startRegionServer(HRegionServer.java:3647)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.startRegionServer(HRegionServer.java:3631)
        at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:61)
        at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:75)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:76)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:3691)

Then i need to kill the process manually to cleanup each time, so annoyed.
After applied the attached patch, the process will exist as expected, then i am happy again :)", ,,,,,,,
HBASE-6996,"I am trying HDFS QJM feature in our test env. after a misconfig, i found the HMaster/HRegionServer process still up if the main thread is dead. Here is the stack trace:

xception in thread ""main"" java.net.UnknownHostException: unknown host: cluster1
        at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:214)
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1196)
        at org.apache.hadoop.ipc.Client.call(Client.java:1050)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)
        at $Proxy8.getProtocolVersion(Unknown Source)
        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:396)
        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:379)
        at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:119)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:238)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:203)
        at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:89)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1386)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1404)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:123)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.startRegionServer(HRegionServer.java:3647)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.startRegionServer(HRegionServer.java:3631)
        at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:61)
        at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:75)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:76)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:3691)

Then i need to kill the process manually to cleanup each time, so annoyed.
After applied the attached patch, the process will exist as expected, then i am happy again :)", ,,,,,,,
HBASE-6998,"I am trying HDFS QJM feature in our test env. after a misconfig, i found the HMaster/HRegionServer process still up if the main thread is dead. Here is the stack trace:

xception in thread ""main"" java.net.UnknownHostException: unknown host: cluster1
        at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:214)
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1196)
        at org.apache.hadoop.ipc.Client.call(Client.java:1050)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)
        at $Proxy8.getProtocolVersion(Unknown Source)
        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:396)
        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:379)
        at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:119)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:238)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:203)
        at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:89)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1386)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1404)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:123)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.startRegionServer(HRegionServer.java:3647)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.startRegionServer(HRegionServer.java:3631)
        at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:61)
        at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:75)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:76)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:3691)

Then i need to kill the process manually to cleanup each time, so annoyed.
After applied the attached patch, the process will exist as expected, then i am happy again :)", ,1,,,,,,
HBASE-7006,"per http://search-hadoop.com/m/qaRu9iM2f02/Set+scanner+caching+to+a+better+default%253F&subj=Set+scanner+caching+to+a+better+default+
let's set to 100 by default", ,,1,,,,,
HBASE-7007,"per http://search-hadoop.com/m/qaRu9iM2f02/Set+scanner+caching+to+a+better+default%253F&subj=Set+scanner+caching+to+a+better+default+
let's set to 100 by default", ,,1,,,,,
HBASE-7008,"per http://search-hadoop.com/m/qaRu9iM2f02/Set+scanner+caching+to+a+better+default%253F&subj=Set+scanner+caching+to+a+better+default+
let's set to 100 by default", ,1,1,,,,,
HBASE-7011,"per http://search-hadoop.com/m/qaRu9iM2f02/Set+scanner+caching+to+a+better+default%253F&subj=Set+scanner+caching+to+a+better+default+
let's set to 100 by default", ,,1,,,,,
HBASE-7018,"HBASE-6214 backported HBASE-5998 (Bulk assignment: regionserver optimization by using a temporary cache for table descriptors when receiving an open regions request), but it's buggy on 0.94 (0.96 appears correct):

{code}
    HTableDescriptor htd = null;
    if (htds == null) {
      htd = this.tableDescriptors.get(region.getTableName());
    } else {
      htd = htds.get(region.getTableNameAsString());
      if (htd == null) {
        htd = this.tableDescriptors.get(region.getTableName());
        htds.put(region.getRegionNameAsString(), htd);
      }
    }
{code}

i.e. we get the tableName from the map but write the regionName.

Even fixing this, it looks like there are areas for improvement:
1) FSTableDescriptors already has a cache (though it goes to the NameNode each time through to check we have the latest copy.  May as well combine these two caches, might be a performance win as well since we don't need to write to multiple caches.
2) FSTableDescriptors makes two RPCs to the NameNode when it encounters a new table.  So the total number of RPCs necessary for a bulk assign (without caching is):
#regions + #tables
(with caching):
min(#regions,#tables) + #tables = #tables + #tables = 2 * #tables

We can make this only one RPC, yielding:
#tables

Probably not a big deal for most users, but in a multi-tenant situation where the number of regions being bulk assigned approaches the number of tables being bulk assigned, this could be a nice performance win.

Benchmarks coming.", ,,1,,,,,
HBASE-7034,"I have this in RS log:

{code}
2012-10-22 02:21:50,698 ERROR org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Failed transitioning node b9,\xEE\xAE\x9BiQO\x89]+a\xE0\x7F\xB7'X?,1349052737638.9af7cfc9b15910a0b3d714bf40a3248f. from OPENING to OPENED -- closing region
org.apache.zookeeper.KeeperException$BadVersionException: KeeperErrorCode = BadVersion for /hbase/unassigned/9af7cfc9b15910a0b3d714bf40a3248f
{code}

Master says this (it is bulk assigning):

{code}
....
2012-10-22 02:21:40,673 DEBUG org.apache.hadoop.hbase.zookeeper.ZKUtil: master:10302-0xb3a862e57a503ba Set watcher on existing znode /hbase/unassigned/9af7cfc9b15910a0b3d714bf40a3248f
...

then this
....

2012-10-22 02:23:47,089 DEBUG org.apache.hadoop.hbase.zookeeper.ZKUtil: master:10302-0xb3a862e57a503ba Set watcher on existing znode /hbase/unassigned/9af7cfc9b15910a0b3d714bf40a3248f
....
2012-10-22 02:24:34,176 DEBUG org.apache.hadoop.hbase.zookeeper.ZKUtil: master:10302-0xb3a862e57a503ba Retrieved 112 byte(s) of data from znode /hbase/unassigned/9af7cfc9b15910a0b3d714bf40a3248f and set watcher; region=b9,\xEE\xAE\x9BiQO\x89]+a\xE0\x7F\xB7'X?,1349052737638.9af7cfc9b15910a0b3d714bf40a3248f., origin=sv4r17s44,10304,1350872216778, state=RS_ZK_REGION_OPENED

etc.
{code}

Disagreement as to what is going on here.
", ,,,,,,,
HBASE-7060,"When the table's region count is less than the count of region servers, the region balance algorithm will not move the region. For example, the cluster has 100 RS, the table has 50 regions sitting on one RS, they will not be moved to any of the other 99 RS.

This is because the algorithm did not calculate the under-loaded RS correctly. This is how the algorithm works with the above example:

avg-regions-per-RS=0.5
min-RS-per-RS=0
max-RS-per-RS=1

when they calculate the under loaded RS, the code is as below. Since regionCount=0, which is always >=min, so it will always skip, therefore, no underloaded RS are found.

Map<ServerName, Integer> underloadedServers = new HashMap<ServerName, Integer>();
for (Map.Entry<ServerAndLoad, List<HRegionInfo>> server:
serversByLoad.entrySet()) {
int regionCount = server.getKey().getLoad();
if (regionCount >= min) { break; }
underloadedServers.put(server.getKey().getServerName(), min - regionCount);
}

Later the function returns since underloaded RS size is 0

if (serverUnerloaded ==0) return regionsToReturn;
", ,,1,,,,,
HBASE-7061,"I found this issue when investigating HBASE-7060. Basically, I think  the intention of this code below is to find all the underloaded server. By using break, it will exit earlier, depending on where the non-overloaded server show up in the list.  ""break"" should be changed to ""continue"". 

Map<ServerName, Integer> underloadedServers = new HashMap<ServerName, Integer>();
for (Map.Entry<ServerAndLoad, List<HRegionInfo>> server:
serversByLoad.entrySet()) {
int regionCount = server.getKey().getLoad();
if (regionCount >= min) { break; }
underloadedServers.put(server.getKey().getServerName(), min - regionCount);
}", ,,1,,,,,
HBASE-7066,This is causing HMaster.shutdown() and HMaster.stopMaster() to succeed even when an AccessDeniedException is thrown.,1,,,,,,,
HBASE-7070,"When the table's region count is less than the count of region servers, the region balance algorithm will not move the region. For example, the cluster has 100 RS, the table has 50 regions sitting on one RS, they will not be moved to any of the other 99 RS.

This is because the algorithm did not calculate the under-loaded RS correctly. This is how the algorithm works with the above example:

avg-regions-per-RS=0.5
min-RS-per-RS=0
max-RS-per-RS=1

when they calculate the under loaded RS, the code is as below. Since regionCount=0, which is always >=min, so it will always skip, therefore, no underloaded RS are found.

Map<ServerName, Integer> underloadedServers = new HashMap<ServerName, Integer>();
for (Map.Entry<ServerAndLoad, List<HRegionInfo>> server:
serversByLoad.entrySet()) {
int regionCount = server.getKey().getLoad();
if (regionCount >= min) { break; }
underloadedServers.put(server.getKey().getServerName(), min - regionCount);
}

Later the function returns since underloaded RS size is 0

if (serverUnerloaded ==0) return regionsToReturn;
", ,,1,,,,,
HBASE-7072,"HBase-5286 changes RegionLoad writable in 94.0, making it incompatible with 92. A fix was made in HBase-5795 where a 94 client can read response from a 92 server, but not vice versa. Currently, if a 92 client tries to do read RegionLoad (HBase shell ""status"" command, or, 92 master and 94 regionserver), it just hangs. ", ,1,,,,,,
HBASE-7092,"One instance of ""java.util.concurrent.ConcurrentHashMap"" loaded by ""<system class loader>"" occupies 3,972,154,848 (92.88%) bytes. The instance is referenced by org.apache.hadoop.hbase.regionserver.HRegionServer @ 0x7038d3798 , loaded by ""sun.misc.Launcher$AppClassLoader @ 0x703994668"". The memory is accumulated in one instance of ""java.util.concurrent.ConcurrentHashMap$Segment[]"" loaded by ""<system class loader>"".

Keywords
sun.misc.Launcher$AppClassLoader @ 0x703994668
java.util.concurrent.ConcurrentHashMap
java.util.concurrent.ConcurrentHashMap$Segment[]", ,,,,,,,
HBASE-7103,"This came up after the following mail in dev list
'infinite loop of RS_ZK_REGION_SPLIT on .94.2'.
The following is the reason for the problem
The following steps happen
-> Initially the parent region P1 starts splitting.
-> The split is going on normally.
-> Another split starts at the same time for the same region P1. (Not sure why this started).
-> Rollback happens seeing an already existing node.
-> This node gets deleted in rollback and nodeDeleted Event starts.
-> In nodeDeleted event the RIT for the region P1 gets deleted.
-> Because of this there is no region in RIT.
-> Now the first split gets over.  Here the problem is we try to transit the node to SPLITTING to SPLIT. But the node even does not exist.
But we don take any action on this.  We think it is successful.
-> Because of this SplitRegionHandler never gets invoked.", ,,,,,,,
HBASE-7111,"there are 3 zookeeper servers in my cluster.
s1
s2
s3
after killing  s3, i found the hbase zkcli will not start again.
it will try to connect to s3 continuely. 

/11/07 11:01:01 INFO zookeeper.ClientCnxn: Opening socket connection to server s3
12/11/07 11:01:01 WARN zookeeper.ClientCnxn: Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused


from the code 
{code}
  public String parse(final Configuration c) {
    // Note that we do not simply grab the property
    // HConstants.ZOOKEEPER_QUORUM from the HBaseConfiguration because the
    // user may be using a zoo.cfg file.
    Properties zkProps = ZKConfig.makeZKProps(c);
    String host = null;
    String clientPort = null;
    for (Entry<Object, Object> entry: zkProps.entrySet()) {
      String key = entry.getKey().toString().trim();
      String value = entry.getValue().toString().trim();
      if (key.startsWith(""server."") && host == null) {
        String[] parts = value.split("":"");
        host = parts[0];
      } else if (key.endsWith(""clientPort"")) {
        clientPort = value;
      }
      if (host != null && clientPort != null) break;
    }
    return host != null && clientPort != null? host + "":"" + clientPort: null;
  }
{code}

the code will choose the fixed zookeeper server (here is the unavailable s3), which leads to the script fails", ,,,,,,,
HBASE-7145,"HBase-5286 changes RegionLoad writable in 94.0, making it incompatible with 92. A fix was made in HBase-5795 where a 94 client can read response from a 92 server, but not vice versa. Currently, if a 92 client tries to do read RegionLoad (HBase shell ""status"" command, or, 92 master and 94 regionserver), it just hangs. ", ,1,,,,,,
HBASE-7160,"When the table's region count is less than the count of region servers, the region balance algorithm will not move the region. For example, the cluster has 100 RS, the table has 50 regions sitting on one RS, they will not be moved to any of the other 99 RS.

This is because the algorithm did not calculate the under-loaded RS correctly. This is how the algorithm works with the above example:

avg-regions-per-RS=0.5
min-RS-per-RS=0
max-RS-per-RS=1

when they calculate the under loaded RS, the code is as below. Since regionCount=0, which is always >=min, so it will always skip, therefore, no underloaded RS are found.

Map<ServerName, Integer> underloadedServers = new HashMap<ServerName, Integer>();
for (Map.Entry<ServerAndLoad, List<HRegionInfo>> server:
serversByLoad.entrySet()) {
int regionCount = server.getKey().getLoad();
if (regionCount >= min) { break; }
underloadedServers.put(server.getKey().getServerName(), min - regionCount);
}

Later the function returns since underloaded RS size is 0

if (serverUnerloaded ==0) return regionsToReturn;
", ,,1,,,,,
HBASE-7162,"When the table's region count is less than the count of region servers, the region balance algorithm will not move the region. For example, the cluster has 100 RS, the table has 50 regions sitting on one RS, they will not be moved to any of the other 99 RS.

This is because the algorithm did not calculate the under-loaded RS correctly. This is how the algorithm works with the above example:

avg-regions-per-RS=0.5
min-RS-per-RS=0
max-RS-per-RS=1

when they calculate the under loaded RS, the code is as below. Since regionCount=0, which is always >=min, so it will always skip, therefore, no underloaded RS are found.

Map<ServerName, Integer> underloadedServers = new HashMap<ServerName, Integer>();
for (Map.Entry<ServerAndLoad, List<HRegionInfo>> server:
serversByLoad.entrySet()) {
int regionCount = server.getKey().getLoad();
if (regionCount >= min) { break; }
underloadedServers.put(server.getKey().getServerName(), min - regionCount);
}

Later the function returns since underloaded RS size is 0

if (serverUnerloaded ==0) return regionsToReturn;
", ,,1,,,,,
HBASE-7163,"When the table's region count is less than the count of region servers, the region balance algorithm will not move the region. For example, the cluster has 100 RS, the table has 50 regions sitting on one RS, they will not be moved to any of the other 99 RS.

This is because the algorithm did not calculate the under-loaded RS correctly. This is how the algorithm works with the above example:

avg-regions-per-RS=0.5
min-RS-per-RS=0
max-RS-per-RS=1

when they calculate the under loaded RS, the code is as below. Since regionCount=0, which is always >=min, so it will always skip, therefore, no underloaded RS are found.

Map<ServerName, Integer> underloadedServers = new HashMap<ServerName, Integer>();
for (Map.Entry<ServerAndLoad, List<HRegionInfo>> server:
serversByLoad.entrySet()) {
int regionCount = server.getKey().getLoad();
if (regionCount >= min) { break; }
underloadedServers.put(server.getKey().getServerName(), min - regionCount);
}

Later the function returns since underloaded RS size is 0

if (serverUnerloaded ==0) return regionsToReturn;
", ,,1,,,,,
HBASE-7180,"We just came across a special scenario.

For our Phoenix project (SQL runtime for HBase), we push a lot of work into HBase via coprocessors. One method is to wrap RegionScanner in coprocessor hooks and then do processing in the hook to avoid returning a lot of data to the client unnecessarily.

In this specific case this is pretty bad. Since the wrapped RegionScanner's next() does not ""know"" that it is called this way is still does all of this on each invocation:
# Starts a RegionOperation
# Increments the request count
# set the current read point on a thread local (because generally each call could come from a different thread)
# Finally does the next on its StoreScanner(s)
# Ends the RegionOperation

When this is done in a tight loop millions of times (as is the case for us) it starts to become significant.

Not sure what to do about this, really. Opening this issue for discussion.

One way is to extend the RegionScanner with an ""internal"" next() method of sorts, so that all this overhead can be avoided. The coprocessor could call the regular next() methods once and then just call the cheaper internal version.

Are there better/cleaner ways?
", ,,1,,,,,
HBASE-7190,"We just came across a special scenario.

For our Phoenix project (SQL runtime for HBase), we push a lot of work into HBase via coprocessors. One method is to wrap RegionScanner in coprocessor hooks and then do processing in the hook to avoid returning a lot of data to the client unnecessarily.

In this specific case this is pretty bad. Since the wrapped RegionScanner's next() does not ""know"" that it is called this way is still does all of this on each invocation:
# Starts a RegionOperation
# Increments the request count
# set the current read point on a thread local (because generally each call could come from a different thread)
# Finally does the next on its StoreScanner(s)
# Ends the RegionOperation

When this is done in a tight loop millions of times (as is the case for us) it starts to become significant.

Not sure what to do about this, really. Opening this issue for discussion.

One way is to extend the RegionScanner with an ""internal"" next() method of sorts, so that all this overhead can be avoided. The coprocessor could call the regular next() methods once and then just call the cheaper internal version.

Are there better/cleaner ways?
", ,1,1,,,,,
HBASE-7191,"using HBaseAdmin.deleteColumn() the files are not archived but deleted directory.

This causes problems with snapshots, and other systems that relies on files to be archived.", ,,,,,,,
HBASE-7199,"using HBaseAdmin.deleteColumn() the files are not archived but deleted directory.

This causes problems with snapshots, and other systems that relies on files to be archived.", ,,,,,,,
HBASE-7202,"using HBaseAdmin.deleteColumn() the files are not archived but deleted directory.

This causes problems with snapshots, and other systems that relies on files to be archived.", ,,,,,,,
HBASE-7205,"HBASE-6308 introduced a new custom CoprocessorClassLoader to load the coprocessor classes and a new instance of this CL is created for each single HRegion opened. This leads to OOME-PermGen when the number of regions go above hundres / region server. 
Having the table coprocessor jailed in a separate classloader is good however we should create only one for all regions of a table in each HRS.
", ,,,,,,,
HBASE-7213,"We just came across a special scenario.

For our Phoenix project (SQL runtime for HBase), we push a lot of work into HBase via coprocessors. One method is to wrap RegionScanner in coprocessor hooks and then do processing in the hook to avoid returning a lot of data to the client unnecessarily.

In this specific case this is pretty bad. Since the wrapped RegionScanner's next() does not ""know"" that it is called this way is still does all of this on each invocation:
# Starts a RegionOperation
# Increments the request count
# set the current read point on a thread local (because generally each call could come from a different thread)
# Finally does the next on its StoreScanner(s)
# Ends the RegionOperation

When this is done in a tight loop millions of times (as is the case for us) it starts to become significant.

Not sure what to do about this, really. Opening this issue for discussion.

One way is to extend the RegionScanner with an ""internal"" next() method of sorts, so that all this overhead can be avoided. The coprocessor could call the regular next() methods once and then just call the cheaper internal version.

Are there better/cleaner ways?
", ,,1,,,,,
HBASE-7220,"I'm trying to create a table with 3000 regions on two regions servers, from the shell.

It's ok on trunk a standalone config.
It's ok on 0.94
It's not ok on trunk: it fails after around 1 hour.

If I remove all the code related to metrics in HRegion, the 3000 regions are created in 3 minutes (twice faster than the 0.94).

On trunk, the region server spends its time in ""waitForWork"", while the master is in the tcp connection related code. It's a 1Gb network.

I haven't looked at the metric code itself.

Patch used to remove the metrics from HRegion:
{noformat}
index c70e9ab..6677e65 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
@@ -364,7 +364,7 @@ public class HRegion implements HeapSize { // , Writable{
   private HTableDescriptor htableDescriptor = null;
   private RegionSplitPolicy splitPolicy;
 
-  private final MetricsRegion metricsRegion;
+  private final MetricsRegion metricsRegion = null;
 
   /**
    * Should only be used for testing purposes
@@ -388,7 +388,7 @@ public class HRegion implements HeapSize { // , Writable{
     this.coprocessorHost = null;
     this.scannerReadPoints = new ConcurrentHashMap<RegionScanner, Long>();
 
-    this.metricsRegion = new MetricsRegion(new MetricsRegionWrapperImpl(this));
+    //this.metricsRegion = new MetricsRegion(new MetricsRegionWrapperImpl(this));
   }
 
   /**
@@ -451,7 +451,7 @@ public class HRegion implements HeapSize { // , Writable{
     this.regiondir = getRegionDir(this.tableDir, encodedNameStr);
     this.scannerReadPoints = new ConcurrentHashMap<RegionScanner, Long>();
 
-    this.metricsRegion = new MetricsRegion(new MetricsRegionWrapperImpl(this));
+    //this.metricsRegion = new MetricsRegion(new MetricsRegionWrapperImpl(this));
 
     /*
      * timestamp.slop provides a server-side constraint on the timestamp. This
@@ -1024,7 +1024,7 @@ public class HRegion implements HeapSize { // , Writable{
         status.setStatus(""Running coprocessor post-close hooks"");
         this.coprocessorHost.postClose(abort);
       }
-      this.metricsRegion.close();
+      //this.metricsRegion.close();
       status.markComplete(""Closed"");
       LOG.info(""Closed "" + this);
       return result;
@@ -2331,11 +2331,11 @@ public class HRegion implements HeapSize { // , Writable{
       if (noOfPuts > 0) {
         // There were some Puts in the batch.
         double noOfMutations = noOfPuts + noOfDeletes;
-        this.metricsRegion.updatePut();
+        //this.metricsRegion.updatePut();
       }
       if (noOfDeletes > 0) {
         // There were some Deletes in the batch.
-        this.metricsRegion.updateDelete();
+        //this.metricsRegion.updateDelete();
       }
       if (!success) {
         for (int i = firstIndex; i < lastIndexExclusive; i++) {
@@ -4270,7 +4270,7 @@ public class HRegion implements HeapSize { // , Writable{
 
     // do after lock
 
-    this.metricsRegion.updateGet();
+    //this.metricsRegion.updateGet();
 
     return results;
   }
@@ -4657,7 +4657,7 @@ public class HRegion implements HeapSize { // , Writable{
       closeRegionOperation();
     }
 
-    this.metricsRegion.updateAppend();
+    //this.metricsRegion.updateAppend();
 
 
     if (flush) {
@@ -4795,7 +4795,7 @@ public class HRegion implements HeapSize { // , Writable{
         mvcc.completeMemstoreInsert(w);
       }
       closeRegionOperation();
-      this.metricsRegion.updateIncrement();
+      //this.metricsRegion.updateIncrement();
     }
 
     if (flush) {
{noformat}", ,,1,,,,,
HBASE-7247,"I'm trying to create a table with 3000 regions on two regions servers, from the shell.

It's ok on trunk a standalone config.
It's ok on 0.94
It's not ok on trunk: it fails after around 1 hour.

If I remove all the code related to metrics in HRegion, the 3000 regions are created in 3 minutes (twice faster than the 0.94).

On trunk, the region server spends its time in ""waitForWork"", while the master is in the tcp connection related code. It's a 1Gb network.

I haven't looked at the metric code itself.

Patch used to remove the metrics from HRegion:
{noformat}
index c70e9ab..6677e65 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
@@ -364,7 +364,7 @@ public class HRegion implements HeapSize { // , Writable{
   private HTableDescriptor htableDescriptor = null;
   private RegionSplitPolicy splitPolicy;
 
-  private final MetricsRegion metricsRegion;
+  private final MetricsRegion metricsRegion = null;
 
   /**
    * Should only be used for testing purposes
@@ -388,7 +388,7 @@ public class HRegion implements HeapSize { // , Writable{
     this.coprocessorHost = null;
     this.scannerReadPoints = new ConcurrentHashMap<RegionScanner, Long>();
 
-    this.metricsRegion = new MetricsRegion(new MetricsRegionWrapperImpl(this));
+    //this.metricsRegion = new MetricsRegion(new MetricsRegionWrapperImpl(this));
   }
 
   /**
@@ -451,7 +451,7 @@ public class HRegion implements HeapSize { // , Writable{
     this.regiondir = getRegionDir(this.tableDir, encodedNameStr);
     this.scannerReadPoints = new ConcurrentHashMap<RegionScanner, Long>();
 
-    this.metricsRegion = new MetricsRegion(new MetricsRegionWrapperImpl(this));
+    //this.metricsRegion = new MetricsRegion(new MetricsRegionWrapperImpl(this));
 
     /*
      * timestamp.slop provides a server-side constraint on the timestamp. This
@@ -1024,7 +1024,7 @@ public class HRegion implements HeapSize { // , Writable{
         status.setStatus(""Running coprocessor post-close hooks"");
         this.coprocessorHost.postClose(abort);
       }
-      this.metricsRegion.close();
+      //this.metricsRegion.close();
       status.markComplete(""Closed"");
       LOG.info(""Closed "" + this);
       return result;
@@ -2331,11 +2331,11 @@ public class HRegion implements HeapSize { // , Writable{
       if (noOfPuts > 0) {
         // There were some Puts in the batch.
         double noOfMutations = noOfPuts + noOfDeletes;
-        this.metricsRegion.updatePut();
+        //this.metricsRegion.updatePut();
       }
       if (noOfDeletes > 0) {
         // There were some Deletes in the batch.
-        this.metricsRegion.updateDelete();
+        //this.metricsRegion.updateDelete();
       }
       if (!success) {
         for (int i = firstIndex; i < lastIndexExclusive; i++) {
@@ -4270,7 +4270,7 @@ public class HRegion implements HeapSize { // , Writable{
 
     // do after lock
 
-    this.metricsRegion.updateGet();
+    //this.metricsRegion.updateGet();
 
     return results;
   }
@@ -4657,7 +4657,7 @@ public class HRegion implements HeapSize { // , Writable{
       closeRegionOperation();
     }
 
-    this.metricsRegion.updateAppend();
+    //this.metricsRegion.updateAppend();
 
 
     if (flush) {
@@ -4795,7 +4795,7 @@ public class HRegion implements HeapSize { // , Writable{
         mvcc.completeMemstoreInsert(w);
       }
       closeRegionOperation();
-      this.metricsRegion.updateIncrement();
+      //this.metricsRegion.updateIncrement();
     }
 
     if (flush) {
{noformat}", ,,1,,,,,
HBASE-7259,"HBaseClient was running after a period of time, all of get operation became too slow.

From the client logs I could see the following:

1. Unable to get data of znode /hbase/root-region-server
{code}
java.lang.InterruptedException
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:485)
        at org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1253)
        at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1129)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.getData(RecoverableZooKeeper.java:264)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.getDataInternal(ZKUtil.java:522)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.getDataAndWatch(ZKUtil.java:498)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.getData(ZooKeeperNodeTracker.java:156)
        at org.apache.hadoop.hbase.zookeeper.RootRegionTracker.getRootRegionLocation(RootRegionTracker.java:62)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:821)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:801)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:933)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:832)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:801)
        at org.apache.hadoop.hbase.client.HTable.finishSetup(HTable.java:234)
        at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:174)
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:150)
        at org.apache.hadoop.hbase.client.MetaScanner.access$000(MetaScanner.java:48)
        at org.apache.hadoop.hbase.client.MetaScanner$1.connect(MetaScanner.java:126)
        at org.apache.hadoop.hbase.client.MetaScanner$1.connect(MetaScanner.java:123)
        at org.apache.hadoop.hbase.client.HConnectionManager.execute(HConnectionManager.java:359)
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:123)
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:99)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.prefetchRegionCache(HConnectionManager.java:894)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:948)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:836)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:801)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionLocation(HConnectionManager.java:725)
        at org.apache.hadoop.hbase.client.ServerCallable.connect(ServerCallable.java:82)
        at org.apache.hadoop.hbase.client.ServerCallable.withRetries(ServerCallable.java:162)
        at org.apache.hadoop.hbase.client.HTable.get(HTable.java:685)
        at org.apache.hadoop.hbase.client.HTablePool$PooledHTable.get(HTablePool.java:366)
{code}

2. Catalina.out found one Java-level deadlock:
{code}
=============================

""catalina-exec-800"":
  waiting to lock monitor 0x000000005f1f6530 (object 0x0000000731902200, a java.lang.Object),
  which is held by ""catalina-exec-710""
""catalina-exec-710"":
  waiting to lock monitor 0x00002aaab9a05bd0 (object 0x00000007321f8708, a org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation),
  which is held by ""catalina-exec-29-EventThread""
""catalina-exec-29-EventThread"":
  waiting to lock monitor 0x000000005f9f0af0 (object 0x0000000732a9c7e0, a org.apache.hadoop.hbase.zookeeper.RootRegionTracker),
  which is held by ""catalina-exec-710""
Java stack information for the threads listed above:

===================================================

""catalina-exec-800"":
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:943)
        - waiting to lock <0x0000000731902200> (a java.lang.Object)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:836)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.relocateRegion(HConnectionManager.java:807)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionLocation(HConnectionManager.java:725)
        at org.apache.hadoop.hbase.client.ServerCallable.connect(ServerCallable.java:82)
        at org.apache.hadoop.hbase.client.ServerCallable.withRetries(ServerCallable.java:162)
        at org.apache.hadoop.hbase.client.HTable.get(HTable.java:685)
        at org.apache.hadoop.hbase.client.HTablePool$PooledHTable.get(HTablePool.java:366)
""catalina-exec-710"":
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.resetZooKeeperTrackers(HConnectionManager.java:599)
        - waiting to lock <0x00000007321f8708> (a org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.abort(HConnectionManager.java:1660)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.getData(ZooKeeperNodeTracker.java:158)
        - locked <0x0000000732a9c7e0> (a org.apache.hadoop.hbase.zookeeper.RootRegionTracker)
        at org.apache.hadoop.hbase.zookeeper.RootRegionTracker.getRootRegionLocation(RootRegionTracker.java:62)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:821)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:801)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:933)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:832)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:801)
        at org.apache.hadoop.hbase.client.HTable.finishSetup(HTable.java:234)
        at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:174)
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:150)
        at org.apache.hadoop.hbase.client.MetaScanner.access$000(MetaScanner.java:48)
        at org.apache.hadoop.hbase.client.MetaScanner$1.connect(MetaScanner.java:126)
        at org.apache.hadoop.hbase.client.MetaScanner$1.connect(MetaScanner.java:123)
        at org.apache.hadoop.hbase.client.HConnectionManager.execute(HConnectionManager.java:359)
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:123)
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:99)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.prefetchRegionCache(HConnectionManager.java:894)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:948)
        - locked <0x0000000731902200> (a java.lang.Object)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:836)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.relocateRegion(HConnectionManager.java:807)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionLocation(HConnectionManager.java:725)
        at org.apache.hadoop.hbase.client.ServerCallable.connect(ServerCallable.java:82)
        at org.apache.hadoop.hbase.client.ServerCallable.withRetries(ServerCallable.java:162)
        at org.apache.hadoop.hbase.client.HTable.get(HTable.java:685)
        at org.apache.hadoop.hbase.client.HTablePool$PooledHTable.get(HTablePool.java:366)
""catalina-exec-29-EventThread"":
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.stop(ZooKeeperNodeTracker.java:98)
        - waiting to lock <0x0000000732a9c7e0> (a org.apache.hadoop.hbase.zookeeper.RootRegionTracker)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.resetZooKeeperTrackers(HConnectionManager.java:604)
        - locked <0x00000007321f8708> (a org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.abort(HConnectionManager.java:1660)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:374)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:271)
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:521)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:497)
Found 1 deadlock.
{code}
From the source code , the reason for this problem is doing ZooKeeperNodeTracker.getData that lead to KeeperException. And try to resetZookeeperTracker. At the same time, ClientCnxn.EventThread  also do resetZookeeperTracker ,too. Because of getData have already held the lock of  ZooKeeperNodeTracke , that lead to the order of the lock two threads to obtain does not accord. So deadlock happened.

In order to avoid the problem, we can add if reseting condition in abortable.abort()
See the patch.", ,,1,,,,,
HBASE-7263,"HBaseClient was running after a period of time, all of get operation became too slow.

From the client logs I could see the following:

1. Unable to get data of znode /hbase/root-region-server
{code}
java.lang.InterruptedException
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:485)
        at org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1253)
        at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1129)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.getData(RecoverableZooKeeper.java:264)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.getDataInternal(ZKUtil.java:522)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.getDataAndWatch(ZKUtil.java:498)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.getData(ZooKeeperNodeTracker.java:156)
        at org.apache.hadoop.hbase.zookeeper.RootRegionTracker.getRootRegionLocation(RootRegionTracker.java:62)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:821)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:801)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:933)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:832)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:801)
        at org.apache.hadoop.hbase.client.HTable.finishSetup(HTable.java:234)
        at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:174)
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:150)
        at org.apache.hadoop.hbase.client.MetaScanner.access$000(MetaScanner.java:48)
        at org.apache.hadoop.hbase.client.MetaScanner$1.connect(MetaScanner.java:126)
        at org.apache.hadoop.hbase.client.MetaScanner$1.connect(MetaScanner.java:123)
        at org.apache.hadoop.hbase.client.HConnectionManager.execute(HConnectionManager.java:359)
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:123)
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:99)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.prefetchRegionCache(HConnectionManager.java:894)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:948)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:836)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:801)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionLocation(HConnectionManager.java:725)
        at org.apache.hadoop.hbase.client.ServerCallable.connect(ServerCallable.java:82)
        at org.apache.hadoop.hbase.client.ServerCallable.withRetries(ServerCallable.java:162)
        at org.apache.hadoop.hbase.client.HTable.get(HTable.java:685)
        at org.apache.hadoop.hbase.client.HTablePool$PooledHTable.get(HTablePool.java:366)
{code}

2. Catalina.out found one Java-level deadlock:
{code}
=============================

""catalina-exec-800"":
  waiting to lock monitor 0x000000005f1f6530 (object 0x0000000731902200, a java.lang.Object),
  which is held by ""catalina-exec-710""
""catalina-exec-710"":
  waiting to lock monitor 0x00002aaab9a05bd0 (object 0x00000007321f8708, a org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation),
  which is held by ""catalina-exec-29-EventThread""
""catalina-exec-29-EventThread"":
  waiting to lock monitor 0x000000005f9f0af0 (object 0x0000000732a9c7e0, a org.apache.hadoop.hbase.zookeeper.RootRegionTracker),
  which is held by ""catalina-exec-710""
Java stack information for the threads listed above:

===================================================

""catalina-exec-800"":
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:943)
        - waiting to lock <0x0000000731902200> (a java.lang.Object)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:836)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.relocateRegion(HConnectionManager.java:807)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionLocation(HConnectionManager.java:725)
        at org.apache.hadoop.hbase.client.ServerCallable.connect(ServerCallable.java:82)
        at org.apache.hadoop.hbase.client.ServerCallable.withRetries(ServerCallable.java:162)
        at org.apache.hadoop.hbase.client.HTable.get(HTable.java:685)
        at org.apache.hadoop.hbase.client.HTablePool$PooledHTable.get(HTablePool.java:366)
""catalina-exec-710"":
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.resetZooKeeperTrackers(HConnectionManager.java:599)
        - waiting to lock <0x00000007321f8708> (a org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.abort(HConnectionManager.java:1660)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.getData(ZooKeeperNodeTracker.java:158)
        - locked <0x0000000732a9c7e0> (a org.apache.hadoop.hbase.zookeeper.RootRegionTracker)
        at org.apache.hadoop.hbase.zookeeper.RootRegionTracker.getRootRegionLocation(RootRegionTracker.java:62)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:821)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:801)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:933)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:832)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:801)
        at org.apache.hadoop.hbase.client.HTable.finishSetup(HTable.java:234)
        at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:174)
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:150)
        at org.apache.hadoop.hbase.client.MetaScanner.access$000(MetaScanner.java:48)
        at org.apache.hadoop.hbase.client.MetaScanner$1.connect(MetaScanner.java:126)
        at org.apache.hadoop.hbase.client.MetaScanner$1.connect(MetaScanner.java:123)
        at org.apache.hadoop.hbase.client.HConnectionManager.execute(HConnectionManager.java:359)
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:123)
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:99)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.prefetchRegionCache(HConnectionManager.java:894)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:948)
        - locked <0x0000000731902200> (a java.lang.Object)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:836)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.relocateRegion(HConnectionManager.java:807)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionLocation(HConnectionManager.java:725)
        at org.apache.hadoop.hbase.client.ServerCallable.connect(ServerCallable.java:82)
        at org.apache.hadoop.hbase.client.ServerCallable.withRetries(ServerCallable.java:162)
        at org.apache.hadoop.hbase.client.HTable.get(HTable.java:685)
        at org.apache.hadoop.hbase.client.HTablePool$PooledHTable.get(HTablePool.java:366)
""catalina-exec-29-EventThread"":
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.stop(ZooKeeperNodeTracker.java:98)
        - waiting to lock <0x0000000732a9c7e0> (a org.apache.hadoop.hbase.zookeeper.RootRegionTracker)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.resetZooKeeperTrackers(HConnectionManager.java:604)
        - locked <0x00000007321f8708> (a org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.abort(HConnectionManager.java:1660)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:374)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:271)
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:521)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:497)
Found 1 deadlock.
{code}
From the source code , the reason for this problem is doing ZooKeeperNodeTracker.getData that lead to KeeperException. And try to resetZookeeperTracker. At the same time, ClientCnxn.EventThread  also do resetZookeeperTracker ,too. Because of getData have already held the lock of  ZooKeeperNodeTracke , that lead to the order of the lock two threads to obtain does not accord. So deadlock happened.

In order to avoid the problem, we can add if reseting condition in abortable.abort()
See the patch.", ,,1,,,,,
HBASE-7271,"The current strategy is to have an array of monothreaded executor, and hash the zk path to ensure that there are no two events on the same region executed in parallel  

I think a single executor, as presented in the attached patch, is better because:
- we're guaranteed to use all threads at any time
- if managing one of the event takes longer that expected, the slowness is limited to this region, and not to all regions that have the same hashed/moduloed code
- For the nodeChildrenChanged, there is no need to choose randomly one of the worker (or, once again, the risk to get stuck if one of the event takes time to be managed).

", ,,1,,,,,
HBASE-7279,"Did some profiling again.
I we can gain some performance [1] when passing buffer, rowoffset, and rowlength instead of making a copy of the row key.
That way we can also remove the row key caching (and this patch also removes the timestamps caching). Considering the sheer number in which we create KVs, every byte save is good.

[1] (15-20% when data is in the block cache we setup a Filter such that only a single row is returned to the client).", ,,1,,,,,
HBASE-7285,"In current trunk, HMaster will fail to start with secure Hadoop if the user starting the process has not obtained a kerberos TGT.  The user starting the process should not be required to have a TGT, as the HMaster process self logs in using the configured keytab and principal.

This is due to a log line in the HMaster constructor executing prior to the {{User.login()}} step:
{code}
    LOG.info(""hbase.rootdir="" + FSUtils.getRootDir(this.conf) +
        "", hbase.cluster.distributed="" + this.conf.getBoolean(""hbase.cluster.distributed"", false));
{code}

Here the FSUtils.getRootDir() winds up hitting the NameNode.  The fix is trivial, moving the log line to follow {{User.login()}}.",1,,,,,,,
HBASE-7289,"Did some profiling again.
I we can gain some performance [1] when passing buffer, rowoffset, and rowlength instead of making a copy of the row key.
That way we can also remove the row key caching (and this patch also removes the timestamps caching). Considering the sheer number in which we create KVs, every byte save is good.

[1] (15-20% when data is in the block cache we setup a Filter such that only a single row is returned to the client).", ,,1,,,,,
HBASE-7293,"I happened to look at a log today where I saw a lot lines like this:

{noformat}
2012-12-06 23:29:08,318 INFO org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Slave cluster looks down: This server is in the failed servers list: sv4r20s49/10.4.20.49:10304
2012-12-06 23:29:15,987 WARN org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Can't replicate because of a local or network error: 
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:519)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:484)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupConnection(HBaseClient.java:416)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupIOstreams(HBaseClient.java:462)
	at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:1150)
	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:1000)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:150)
	at $Proxy14.replicateLogEntries(Unknown Source)
	at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.shipEdits(ReplicationSource.java:627)
	at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:365)
2012-12-06 23:29:15,988 INFO org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Slave cluster looks down: Connection refused
{noformat}

What struck me as weird is this had been going on for some days, I would expect the RS to find new servers if it wasn't able to replicate. But the reality is that only a few of the chosen sink RS were down so eventually the source hits one that's good and is never able to refresh its list of servers.

We should remove the dead servers, it's spammy and probably adds some slave lag.", ,,,,,,,
HBASE-7294,"I happened to look at a log today where I saw a lot lines like this:

{noformat}
2012-12-06 23:29:08,318 INFO org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Slave cluster looks down: This server is in the failed servers list: sv4r20s49/10.4.20.49:10304
2012-12-06 23:29:15,987 WARN org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Can't replicate because of a local or network error: 
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:519)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:484)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupConnection(HBaseClient.java:416)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupIOstreams(HBaseClient.java:462)
	at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:1150)
	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:1000)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:150)
	at $Proxy14.replicateLogEntries(Unknown Source)
	at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.shipEdits(ReplicationSource.java:627)
	at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:365)
2012-12-06 23:29:15,988 INFO org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Slave cluster looks down: Connection refused
{noformat}

What struck me as weird is this had been going on for some days, I would expect the RS to find new servers if it wasn't able to replicate. But the reality is that only a few of the chosen sink RS were down so eventually the source hits one that's good and is never able to refresh its list of servers.

We should remove the dead servers, it's spammy and probably adds some slave lag.", ,,,,,,,
HBASE-7295,"Did some profiling again.
I we can gain some performance [1] when passing buffer, rowoffset, and rowlength instead of making a copy of the row key.
That way we can also remove the row key caching (and this patch also removes the timestamps caching). Considering the sheer number in which we create KVs, every byte save is good.

[1] (15-20% when data is in the block cache we setup a Filter such that only a single row is returned to the client).", ,,1,,,,,
HBASE-7305,"This has started as forward porting of HBASE-5494 and HBASE-5991 from the 89-fb branch to trunk, but diverged enough to have it's own issue. 

The idea is to implement a zk based read/write lock per table. Master initiated operations should get the write lock, and region operations (region split, moving, balance?, etc) acquire a shared read lock. 

", ,,1,,,,,
HBASE-7307,"If a region is split parent we are not adding it to META scan results during full scan. 
{code}
        if (!isInsideTable(this.current, tableNameBytes)) return false;
        if (this.current.isSplitParent()) return true;
        // Else call super and add this Result to the collection.
        super.visit(r);
{code}

If all regions of a table has been split then result size will be zero and returning false.
{code} 
    fullScan(catalogTracker, visitor, getTableStartRowForMeta(tableNameBytes));
    // If visitor has results >= 1 then table exists.
    return visitor.getResults().size() >= 1;
{code}
Even table is present we are returning false which is not correct(its highly possible in case of tables with one region).", ,,,,,,,
HBASE-7309,"While investigating HBASE-7205 by repeatedly enabling and disabling one table having 100 regions I noticed that closed HRegion objects are kept forever in memory. 
The memory analyzer tool indicates a reference to HRegion object in metrics refresh-task ({{MetricsRegionWrapperImpl.HRegionMetricsWrapperRunnable}}) that prevents the HRegion object to be collected.
", ,,1,,,,,
HBASE-7315,See comments in HBASE-7263.,1,,,,,,,
HBASE-7331,"The following APIs in HRegionServer are either missing hooks to coprocessor or the hooks are not implemented in the AccessController class for security. As a result any unauthorized user can:
1.Open a region
2. Close a region
3. Stop region server
4. Lock a row
5. Unlock a row.",1,,,,,,,
HBASE-7334,"While investigating HBASE-7205 by repeatedly enabling and disabling one table having 100 regions I noticed that closed HRegion objects are kept forever in memory. 
The memory analyzer tool indicates a reference to HRegion object in metrics refresh-task ({{MetricsRegionWrapperImpl.HRegionMetricsWrapperRunnable}}) that prevents the HRegion object to be collected.
", ,,1,,,,,
HBASE-7336,"While investigating HBASE-7205 by repeatedly enabling and disabling one table having 100 regions I noticed that closed HRegion objects are kept forever in memory. 
The memory analyzer tool indicates a reference to HRegion object in metrics refresh-task ({{MetricsRegionWrapperImpl.HRegionMetricsWrapperRunnable}}) that prevents the HRegion object to be collected.
", ,,1,,,,,
HBASE-7339,"While investigating HBASE-7205 by repeatedly enabling and disabling one table having 100 regions I noticed that closed HRegion objects are kept forever in memory. 
The memory analyzer tool indicates a reference to HRegion object in metrics refresh-task ({{MetricsRegionWrapperImpl.HRegionMetricsWrapperRunnable}}) that prevents the HRegion object to be collected.
", ,,,,,,,
HBASE-7352,"While investigating HBASE-7205 by repeatedly enabling and disabling one table having 100 regions I noticed that closed HRegion objects are kept forever in memory. 
The memory analyzer tool indicates a reference to HRegion object in metrics refresh-task ({{MetricsRegionWrapperImpl.HRegionMetricsWrapperRunnable}}) that prevents the HRegion object to be collected.
", ,,,,,,,
HBASE-7357,"This came up in the context of testing HBASE-6788.  Currently HBaseClient and HBaseServer call UserGroupInformation.isSecurityEnabled() when determining whether or not to use SASL to negotiate connections.  This means they are using the hadoop.security.authentication configuration value.  Since this is in the context of HBase RPC connections, it seems more correct to use the hbase.security.authentication configuration value by calling User.isHBaseSecurityEnabled().",1,,,,,,,
HBASE-7365,"While investigating HBASE-7205 by repeatedly enabling and disabling one table having 100 regions I noticed that closed HRegion objects are kept forever in memory. 
The memory analyzer tool indicates a reference to HRegion object in metrics refresh-task ({{MetricsRegionWrapperImpl.HRegionMetricsWrapperRunnable}}) that prevents the HRegion object to be collected.
", ,,,,,,,
HBASE-7367,"Currently snapshot don't care about ACL...
and in the first draft snapshots should be disabled if the ACL coprocessor is enabled.
After the first step, we can discuss how to handle the snapshot/restore/clone.
Is saving and restoring the acl related rights, the right way? maybe after 3 months we don't want to give the access the guys listed in the old acl...",1,,,,,,,
HBASE-7373,"We should fix the proto file, add unit test for this case, and verify it works from hbase shell with table to be nil.
",1,,,,,,,
HBASE-7380,"Right now it's a pain if you remove a peer and still have rogue queues because they get moved on and on and on. NodeFailoverWorker needs to run the check:

bq. if (!zkHelper.getPeerClusters().containsKey(src.getPeerClusterId())) {

before this:

bq. SortedMap<String, SortedSet<String>> newQueues = zkHelper.copyQueuesFromRS(rsZnode);

And test.", ,,,,,,,
HBASE-7386,"Right now it's a pain if you remove a peer and still have rogue queues because they get moved on and on and on. NodeFailoverWorker needs to run the check:

bq. if (!zkHelper.getPeerClusters().containsKey(src.getPeerClusterId())) {

before this:

bq. SortedMap<String, SortedSet<String>> newQueues = zkHelper.copyQueuesFromRS(rsZnode);

And test.", ,,,,,,,
HBASE-7391,"From Ram in http://mail-archives.apache.org/mod_mbox/hbase-dev/201205.mbox/%3C00bc01cd31e6$7caf1320$760d3960$%25vasudevan@huawei.com%3E:

{quote}
One small observation after giving +1 on the RC.
The WAL compression feature causes OOME and causes Full GC.

The problem is, if we have 1500 regions and I need to create recovered.edits
for each of the region (I dont have much data in the regions (~300MB)).
Now when I try to build the dictionary there is a Node object getting
created.
Each node object occupies 32 bytes.
We have 5 such dictionaries.

Initially we create indexToNodes array and its size is 32767.

So now we have 32*5*32767 = ~5MB.

Now I have 1500 regions.

So 5MB*1500 = ~7GB.(Excluding actual data).  This seems to a very high
initial memory foot print and this never allows me to split the logs and I
am not able to make the cluster up at all.

Our configured heap size was 8GB, tested in 3 node cluster with 5000
regions, very less data( 1GB in hdfs cluster including replication), some
small data is spread evenly across all regions.

The formula is 32(Node object size)*5(No of dictionary)*32767(no of node
objects)*noofregions.
{quote}", ,,1,,,,,
HBASE-7403,"From Ram in http://mail-archives.apache.org/mod_mbox/hbase-dev/201205.mbox/%3C00bc01cd31e6$7caf1320$760d3960$%25vasudevan@huawei.com%3E:

{quote}
One small observation after giving +1 on the RC.
The WAL compression feature causes OOME and causes Full GC.

The problem is, if we have 1500 regions and I need to create recovered.edits
for each of the region (I dont have much data in the regions (~300MB)).
Now when I try to build the dictionary there is a Node object getting
created.
Each node object occupies 32 bytes.
We have 5 such dictionaries.

Initially we create indexToNodes array and its size is 32767.

So now we have 32*5*32767 = ~5MB.

Now I have 1500 regions.

So 5MB*1500 = ~7GB.(Excluding actual data).  This seems to a very high
initial memory foot print and this never allows me to split the logs and I
am not able to make the cluster up at all.

Our configured heap size was 8GB, tested in 3 node cluster with 5000
regions, very less data( 1GB in hdfs cluster including replication), some
small data is spread evenly across all regions.

The formula is 32(Node object size)*5(No of dictionary)*32767(no of node
objects)*noofregions.
{quote}", ,,1,,,,,
HBASE-7404,"From Ram in http://mail-archives.apache.org/mod_mbox/hbase-dev/201205.mbox/%3C00bc01cd31e6$7caf1320$760d3960$%25vasudevan@huawei.com%3E:

{quote}
One small observation after giving +1 on the RC.
The WAL compression feature causes OOME and causes Full GC.

The problem is, if we have 1500 regions and I need to create recovered.edits
for each of the region (I dont have much data in the regions (~300MB)).
Now when I try to build the dictionary there is a Node object getting
created.
Each node object occupies 32 bytes.
We have 5 such dictionaries.

Initially we create indexToNodes array and its size is 32767.

So now we have 32*5*32767 = ~5MB.

Now I have 1500 regions.

So 5MB*1500 = ~7GB.(Excluding actual data).  This seems to a very high
initial memory foot print and this never allows me to split the logs and I
am not able to make the cluster up at all.

Our configured heap size was 8GB, tested in 3 node cluster with 5000
regions, very less data( 1GB in hdfs cluster including replication), some
small data is spread evenly across all regions.

The formula is 32(Node object size)*5(No of dictionary)*32767(no of node
objects)*noofregions.
{quote}", ,,1,,,,,
HBASE-7405,"From Ram in http://mail-archives.apache.org/mod_mbox/hbase-dev/201205.mbox/%3C00bc01cd31e6$7caf1320$760d3960$%25vasudevan@huawei.com%3E:

{quote}
One small observation after giving +1 on the RC.
The WAL compression feature causes OOME and causes Full GC.

The problem is, if we have 1500 regions and I need to create recovered.edits
for each of the region (I dont have much data in the regions (~300MB)).
Now when I try to build the dictionary there is a Node object getting
created.
Each node object occupies 32 bytes.
We have 5 such dictionaries.

Initially we create indexToNodes array and its size is 32767.

So now we have 32*5*32767 = ~5MB.

Now I have 1500 regions.

So 5MB*1500 = ~7GB.(Excluding actual data).  This seems to a very high
initial memory foot print and this never allows me to split the logs and I
am not able to make the cluster up at all.

Our configured heap size was 8GB, tested in 3 node cluster with 5000
regions, very less data( 1GB in hdfs cluster including replication), some
small data is spread evenly across all regions.

The formula is 32(Node object size)*5(No of dictionary)*32767(no of node
objects)*noofregions.
{quote}", ,,1,,,,,
HBASE-7423,"HFileArchiver gets the configuration from the FileSystem in 
{code}
 public static void archiveRegion(FileSystem fs, HRegionInfo info)
      throws IOException {
    Path rootDir = FSUtils.getRootDir(fs.getConf());
{code}

In Pig's test cases, they construct a MiniDFSCluster and pass it to HBaseTestingUtil, which causes the delete table to fail because it will refer to the FileSystem's configuration rather than HBase's one. 




", ,1,,,,,,
HBASE-7437,"HFileArchiver gets the configuration from the FileSystem in 
{code}
 public static void archiveRegion(FileSystem fs, HRegionInfo info)
      throws IOException {
    Path rootDir = FSUtils.getRootDir(fs.getConf());
{code}

In Pig's test cases, they construct a MiniDFSCluster and pass it to HBaseTestingUtil, which causes the delete table to fail because it will refer to the FileSystem's configuration rather than HBase's one. 




", ,,1,,,,,
HBASE-7440,"While adding a peer, ReplicationZK does the znodes creation in three transactions. Create :
a) peers znode
b) peerId specific znode, and
c) peerState znode

There is a PeerWatcher which invokes getPeer() (after steps b) and c)). If it happens that while adding a peer, the control flows to getPeer() and step c) has not been processed, it may results in a state where the peer will not be added. This happens while running TestMasterReplication#testCyclicReplication().
{code}
2012-12-26 07:36:35,187 INFO  [RegionServer:0;p0120.XXXXX,38423,1356536179470-EventThread] zookeeper.RecoverableZooKeeper(447): Node /2/replication/peers/1/peer-state already exists and this is not a retry
2012-12-26 07:36:35,188 ERROR [RegionServer:0;p0120.XXXXX,38423,1356536179470-EventThread] regionserver.ReplicationSourceManager$PeersWatcher(527): Error while adding a new peer
org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /2/replication/peers/1/peer-state
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:119)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.createNonSequential(RecoverableZooKeeper.java:428)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.create(RecoverableZooKeeper.java:410)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.createAndWatch(ZKUtil.java:1044)
	at org.apache.hadoop.hbase.replication.ReplicationPeer.startStateTracker(ReplicationPeer.java:82)
	at org.apache.hadoop.hbase.replication.ReplicationZookeeper.getPeer(ReplicationZookeeper.java:344)
	at org.apache.hadoop.hbase.replication.ReplicationZookeeper.connectToPeer(ReplicationZookeeper.java:307)
	at org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager$PeersWatcher.nodeChildrenChanged(ReplicationSourceManager.java:519)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:315)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:519)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:495)
2012-12-26 07:36:35,188 DEBUG [RegionServer:0;p0120.XXXXX,55742,1356536171947-EventThread] zookeeper.ZKUtil(1545): regionserver:55742-0x13bd7db39580004 Retrieved 36 byte(s) of data from znode /1/hbaseid; data=9ce66123-d3e8-4ae9-a249-afe03...

{code}
", ,,,,,,,
HBASE-7442,"When security is enabled, HBase CopyTable fails with Kerberos exception:

{code}
FATAL org.apache.hadoop.ipc.SecureClient: SASL authentication failed. The most likely cause is missing or invalid credentials. Consider 'kinit'.
javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
{code}

This is only when copying to remote HBase cluster (using either MRv1 or YARN), local copy works fine.",1,1,,,,,,
HBASE-7445,"while the regionserver that META table deployed crashed, the .META. table can't migrate to other available regionservers. Then the region spliting, can't find META table, cause the whole cluster is unavailable.
Code path: org.apache.hadoop.hbase.master.handler.ServerShutdownHandler
{code}
      // Carrying meta
      if (isCarryingMeta()) {
        LOG.info(""Server "" + serverName + "" was carrying META. Trying to assign."");
        this.services.getAssignmentManager().
          regionOffline(HRegionInfo.FIRST_META_REGIONINFO);
        this.services.getAssignmentManager().assignMeta();
      }
{code}", ,,,,,,,
HBASE-7450,"Just like:https://issues.apache.org/jira/browse/HADOOP-7428
Exceptions except IOException thrown in setupIOstreams would leave the connection half-setup. But the connection would not close utill it become timeout. The orphane connection cause NPE when is used in HCM.", ,,,,,,,
HBASE-7467,"CleanerChore checkAndDeleteDirectory is not deleting empty directories. As a result, some directories are kept in the FS but should have been removed.

To reproduce, simply create an empty directory under /hbase/.archive/table_name/. If you place a file into this directory, it's not more empty and therefore it's correctly removed.

", ,,,,,,,
HBASE-7495,"HFileArchiver gets the configuration from the FileSystem in 
{code}
 public static void archiveRegion(FileSystem fs, HRegionInfo info)
      throws IOException {
    Path rootDir = FSUtils.getRootDir(fs.getConf());
{code}

In Pig's test cases, they construct a MiniDFSCluster and pass it to HBaseTestingUtil, which causes the delete table to fail because it will refer to the FileSystem's configuration rather than HBase's one. 




", ,,1,,,,,
HBASE-7504,"1.FullGC happen on ROOT regionserver.
2.ZK session timeout, master expire the regionserver and submit to ServerShutdownHandler
3.Regionserver complete the FullGC
4.In the process of ServerShutdownHandler, verifyRootRegionLocation returns true
5.ServerShutdownHandler skip assigning ROOT region
6.Regionserver abort itself because it reveive YouAreDeadException after a regionserver report
7.ROOT is offline now, and won't be assigned any more unless we restart master



Master Log:
{code}
2012-10-31 19:51:39,043 DEBUG org.apache.hadoop.hbase.master.ServerManager: Added=dw88.kgb.sqa.cm4,60020,1351671478752 to dead servers, submitted shutdown handler to be executed, root=true, meta=false
2012-10-31 19:51:39,045 INFO org.apache.hadoop.hbase.master.handler.ServerShutdownHandler: Splitting logs for dw88.kgb.sqa.cm4,60020,1351671478752
2012-10-31 19:51:50,113 INFO org.apache.hadoop.hbase.master.handler.ServerShutdownHandler: Server dw88.kgb.sqa.cm4,60020,1351671478752 was carrying ROOT. Trying to assign.
2012-10-31 19:52:15,939 DEBUG org.apache.hadoop.hbase.master.ServerManager: Server REPORT rejected; currently processing dw88.kgb.sqa.cm4,60020,1351671478752 as dead server
2012-10-31 19:52:15,945 INFO org.apache.hadoop.hbase.master.handler.ServerShutdownHandler: Skipping log splitting for dw88.kgb.sqa.cm4,60020,1351671478752
{code}

No log of assigning ROOT

Regionserver log:
{code}
2012-10-31 19:52:15,923 WARN org.apache.hadoop.hbase.util.Sleeper: We slept 229128ms instead of 100000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
{code}


", ,,,,,,,
HBASE-7505,"We will retry 100 times (about 3200 minitues) for HRegionServer#postOpenDeployTasks now, see HConnectionManager#setServerSideHConnectionRetries.

However, 
when we stopping the cluster, we will wait for split threads in  HRegionServer#join,
if META/ROOT server has already been stopped, the split thread won't exit because it is in the retrying for HRegionServer#postOpenDeployTasks
", ,,1,,,,,
HBASE-7506,"We will check whether server carrying ROOT/META when expiring the server.
See ServerManager#expireServer.

If the dead server carrying META, we assign meta directly in the process of ServerShutdownHandler.
If the dead server carrying ROOT, we will offline ROOT and then verifyAndAssignRootWithRetries()

How judgement of carrtying ROOT/META become wrong?
If region is in RIT, and isCarryingRegion() return true after addressing from zk.
However, once RIT time out(could be caused by this.allRegionServersOffline && !noRSAvailable, see AssignmentManager#TimeoutMonitor)   and we assign it to otherwhere, this judgement become wrong.
See AssignmentManager#isCarryingRegion for details

With the wrong judgement of carrtying ROOT/META, we would assign ROOT/META twice.", ,,,,,,,
HBASE-7507,"We will abort regionserver if memstore flush throws exception.

I thinks we could do retry to make regionserver more stable because file system may be not ok in a transient time. e.g. Switching namenode in the NamenodeHA environment



{code}
HRegion#internalFlushcache(){

...
try {
...
}catch(Throwable t){
DroppedSnapshotException dse = new DroppedSnapshotException(""region: "" +
          Bytes.toStringBinary(getRegionName()));
dse.initCause(t);
throw dse;
}
...

}

MemStoreFlusher#flushRegion(){
...
region.flushcache();
...
 try {
}catch(DroppedSnapshotException ex){
server.abort(""Replay of HLog required. Forcing server shutdown"", ex);
}

...
}
{code}", ,,,,,,,
HBASE-7515,"Related to HBASE-7513. If a RS is able to open a few store files in {{Store.loadStoreFiles}} but one of them fails like in 7513, the opened files won't be closed and file descriptors will remain in a CLOSED_WAIT state.

The situation we encountered is that over the weekend one region was bounced between >100 region servers and eventually they all started dying on ""Too many open files"".", ,,,,,,,
HBASE-7521,"Discussion in HBASE-6060 implies that the fix there does not work on 0.94. Still, we may want to fix the issue in 0.94 (via some different fix) because the regions stuck in opening for ridiculous amounts of time is not a good thing to have.", ,,,,,,,
HBASE-7523,"After HBASE-5683 we have no longer the .regioninfo written on disk during the table creation.
so, if we fail before adding entries to .META. we end up with regions on disk that has no information, and hbck is not able to recover this situation.

The .regioninfo is written in checkRegioninfoOnFilesystem() that was called by initialize(), during the table creation and region opening. With HBASE-5683 we skip the call to initialize(), in during the region creation, to avoid to initialize the memstore & co.", ,,,,,,,
HBASE-7537,"After HBASE-5683 we have no longer the .regioninfo written on disk during the table creation.
so, if we fail before adding entries to .META. we end up with regions on disk that has no information, and hbck is not able to recover this situation.

The .regioninfo is written in checkRegioninfoOnFilesystem() that was called by initialize(), during the table creation and region opening. With HBASE-5683 we skip the call to initialize(), in during the region creation, to avoid to initialize the memstore & co.", ,,,,,,,
HBASE-7544,"Introduce transparent encryption of HBase on disk data.
Depends on a separate contribution of an encryption codec framework to Hadoop core and an AES-NI (native code) codec. This is work done in the context of MAPREDUCE-4491 but I'd gather there will be additional JIRAs for common and HDFS parts of it.
Requirements:
Transparent encryption at the CF or table level
Protect against all data leakage from files at rest
Two-tier key architecture for consistency with best practices for this feature in the RDBMS world
Built-in key management
Flexible and non-intrusive key rotation
Mechanisms not exposed to or modifiable by users
Hardware security module integration (via Java KeyStore)
HBCK support for transparently encrypted files (+ plugin architecture for HBCK)
Additional goals:
Shell support for administrative functions
Avoid performance impact for the null crypto codec case
Play nicely with other changes underway: in HFile, block coding, etc.
We're aiming for rough parity with Oracle's transparent tablespace encryption feature, described in http://www.oracle.com/technetwork/database/owp-security-advanced-security-11gr-133411.pdf as
Transparent Data Encryption uses a 2-tier key architecture for flexible and non-intrusive key rotation and least operational and performance impact: Each application table with at least one encrypted column has its own table key, which is applied to all encrypted columns in that table. Equally, each encrypted tablespace has its own tablespace key. Table keys are stored in the data dictionary of the database, while tablespace keys are stored in the header of the tablespace and additionally, the header of each underlying OS file that makes up the tablespace. Each of these keys is encrypted with the TDE master encryption key, which is stored outside of the database in an external security module: either the Oracle Wallet (a PKCS#12 formatted file that is encrypted using a passphrase supplied either by the designated security administrator or DBA during setup), or a Hardware Security Module (HSM) device for higher assurance []
Further design details forthcoming in a design document and patch as soon as we have all of the clearances in place.",1,,,,,,,
HBASE-7546,"As discussed in the parent issue HBASE-7305, we should be coordinating between splits and table operations to ensure that they don't happen at the same time. In this issue we will acquire shared read locks for region splits. ", ,,,,,,,
HBASE-7551,"This came from HBASE-7468.
I got the issue. I am able to reproduce this
See the logs
{code}
2013-01-14 14:37:21,760 INFO  [main] regionserver.SplitTransaction(216): Starting split of region testShouldClearRITWhenNodeFoundInSplittingState,,1358154439514.a9e57d09c58b3ef3b949d602232fb2c2.

2013-01-14 14:37:21,760 DEBUG [main] regionserver.SplitTransaction(871): regionserver:61665-0x13c384e4e4f0002 Creating ephemeral node for a9e57d09c58b3ef3b949d602232fb2c2 in SPLITTING state

2013-01-14 14:37:21,844 DEBUG [main] zookeeper.ZKAssign(757): regionserver:61665-0x13c384e4e4f0002 Attempting to transition node a9e57d09c58b3ef3b949d602232fb2c2 from RS_ZK_REGION_SPLITTING to RS_ZK_REGION_SPLITTING

2013-01-14 14:37:21,849 DEBUG [Thread-873-EventThread] zookeeper.ZooKeeperWatcher(277): master:62334-0x13c384e4e4f001b Received ZooKeeper Event, type=NodeChildrenChanged, state=SyncConnected, path=/hbase/unassigned

2013-01-14 14:37:21,853 DEBUG [main] zookeeper.ZKUtil(1565): regionserver:61665-0x13c384e4e4f0002 Retrieved 140 byte(s) of data from znode /hbase/unassigned/a9e57d09c58b3ef3b949d602232fb2c2; data=region=testShouldClearRITWhenNodeFoundInSplittingState,,1358154439514.a9e57d09c58b3ef3b949d602232fb2c2., origin=Ram.Home,61665,1358154325430, state=RS_ZK_REGION_SPLITTING

2013-01-14 14:37:21,918 DEBUG [main] zookeeper.ZKAssign(820): regionserver:61665-0x13c384e4e4f0002 Successfully transitioned node a9e57d09c58b3ef3b949d602232fb2c2 from RS_ZK_REGION_SPLITTING to RS_ZK_REGION_SPLITTING

2013-01-14 14:37:21,919 DEBUG [Thread-873-EventThread] zookeeper.ZKUtil(417): master:62334-0x13c384e4e4f001b Set watcher on existing znode /hbase/unassigned/a9e57d09c58b3ef3b949d602232fb2c2
{code}
Here we can observe that the SPLITTING node was first created. Then we transit it to SPLITTING to SPLITTING so that AM can have the nodeDataChange event. But for the nodeDataChange event to happen first nodeChildrenChange event should happen so that the master can set a watcher on the node.
Now when this hang happens, we can see that after the transition happens only then the watcher is set by nodeChildrenChange event and so the SPLITTING to SPLITTING event itself is missed or skipped.

Ideally the nodeChildrenChange event iterates thro the list of new znodes on the /hbase/assignment nodes. And then creates a watcher on that. One reason could be there are more than one znode and so the watch setting operation takes time. The order of execution is different when we try running from eclipse and when we run mvn tests. 
My conclusion is that the testcase actually reveals the problem but the same can happen in any case where the SPLITTING event can get missed out. May be some of the SPLIT related bugs that were raised is due to this? Need to analyse.
Any suggestions welcome. We should ensure that the transition from SPLITTING to SPLITTING should happen only after the master has set the watch on the znode and we should be sure of that.", ,,,,,,,
HBASE-7558,"HBASE-6068, /hbase/unassigned is not listed as open node
and catalogTracker tries to read from both: RootRegionTracker (/hbase/root-region-server) and MetaNodeTracker (/hbase/unassigned)
Unable to get data of znode /hbase/unassigned/1028785192
org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /hbase/unassigned/1028785192",1,1,,,,,,
HBASE-7590,"We will retry 100 times (about 3200 minitues) for HRegionServer#postOpenDeployTasks now, see HConnectionManager#setServerSideHConnectionRetries.

However, 
when we stopping the cluster, we will wait for split threads in  HRegionServer#join,
if META/ROOT server has already been stopped, the split thread won't exit because it is in the retrying for HRegionServer#postOpenDeployTasks
", ,,1,,,,,
HBASE-7634,"The current handling of changes to the region servers in a replication peer cluster is currently quite inefficient. The list of region servers that are being replicated to is only updated if there are a large number of issues encountered while replicating.

This can cause it to take quite a while to recognize that a number of the regionserver in a peer cluster are no longer available. A potentially bigger problem is that if a replication peer cluster is started with a small number of regionservers, and then more region servers are added after replication has started, the additional region servers will never be used for replication (unless there are failures on the in-use regionservers).



Part of the current issue is that the retry code in ReplicationSource#shipEdits checks a randomly-chosen replication peer regionserver (in ReplicationSource#isSlaveDown) to see if it is up after a replication write has failed on a different randonly-chosen replication peer. If the peer is seen as not down, another randomly-chosen peer is used for writing.



A second part of the issue is that changes to the list of region servers in a peer cluster are not detected at all, and are only picked up if a certain number of failures have occurred when trying to ship edits.
", ,,1,,,,,
HBASE-7643," * The master have an hfile cleaner thread (that is responsible for cleaning the /hbase/.archive dir)
 ** /hbase/.archive/table/region/family/hfile
 ** if the family/region/family directory is empty the cleaner removes it
 * The master can archive files (from another thread, e.g. DeleteTableHandler)
 * The region can archive files (from another server/process, e.g. compaction)

The simplified file archiving code looks like this:
{code}
HFileArchiver.resolveAndArchive(...) {
  // ensure that the archive dir exists
  fs.mkdir(archiveDir);

  // move the file to the archiver
  success = fs.rename(originalPath/fileName, archiveDir/fileName)

  // if the rename is failed, delete the file without archiving
  if (!success) fs.delete(originalPath/fileName);
}
{code}

Since there's no synchronization between HFileArchiver.resolveAndArchive() and the cleaner run (different process, thread, ...) you can end up in the situation where you are moving something in a directory that doesn't exists.
{code}
fs.mkdir(archiveDir);

// HFileCleaner chore starts at this point
// and the archiveDirectory that we just ensured to be present gets removed.

// The rename at this point will fail since the parent directory is missing.
success = fs.rename(originalPath/fileName, archiveDir/fileName)
{code}

The bad thing of deleting the file without archiving is that if you've a snapshot that relies on the file to be present, or you've a clone table that relies on that file is that you're losing data.

Possible solutions
 * Create a ZooKeeper lock, to notify the master (""Hey I'm archiving something, wait a bit"")
 * Add a RS -> Master call to let the master removes files and avoid this kind of situations
 * Avoid to remove empty directories from the archive if the table exists or is not disabled
 * Add a try catch around the fs.rename

The last one, the easiest one, looks like:
{code}
for (int i = 0; i < retries; ++i) {
  // ensure archive directory to be present
  fs.mkdir(archiveDir);

  // ----> possible race <-----

  // try to archive file
  success = fs.rename(originalPath/fileName, archiveDir/fileName);
  if (success) break;
}
{code}", ,,,,,,,
HBASE-7649,"The current handling of changes to the region servers in a replication peer cluster is currently quite inefficient. The list of region servers that are being replicated to is only updated if there are a large number of issues encountered while replicating.

This can cause it to take quite a while to recognize that a number of the regionserver in a peer cluster are no longer available. A potentially bigger problem is that if a replication peer cluster is started with a small number of regionservers, and then more region servers are added after replication has started, the additional region servers will never be used for replication (unless there are failures on the in-use regionservers).



Part of the current issue is that the retry code in ReplicationSource#shipEdits checks a randomly-chosen replication peer regionserver (in ReplicationSource#isSlaveDown) to see if it is up after a replication write has failed on a different randonly-chosen replication peer. If the peer is seen as not down, another randomly-chosen peer is used for writing.



A second part of the issue is that changes to the list of region servers in a peer cluster are not detected at all, and are only picked up if a certain number of failures have occurred when trying to ship edits.
", ,,1,,,,,
HBASE-7651,"We can improve the performance of per-cell authorization if the read of the cell ACL, if any, is combined with the sequential read of the cell data already in progress. When tags are inlined with KVs in block encoding (see HBASE-7448, and more generally HBASE-7233), we can use them to carry cell ACLs instead of using out-of-line storage (HBASE-7661) for that purpose.",1,,,,,,,
HBASE-7662,"We can improve the performance of per-cell authorization if the read of the cell ACL, if any, is combined with the sequential read of the cell data already in progress. When tags are inlined with KVs in block encoding (see HBASE-7448, and more generally HBASE-7233), we can use them to carry cell ACLs instead of using out-of-line storage (HBASE-7661) for that purpose.",1,,1,,,,,
HBASE-7663,"Implement Accumulo-style visibility labels. Consider the following design principles:
Coprocessor based implementation
Minimal to no changes to core code
Use KeyValue tags (HBASE-7448) to carry labels
Use OperationWithAttributes# {get,set} 
Attribute for handling visibility labels in the API
Implement a new filter for evaluating visibility labels as KVs are streamed through.
This approach would be consistent in deployment and API details with other per-KV security work, supporting environments where they might be both be employed, even stacked on some tables.
See the parent issue for more discussion.",1,,,,,,,
HBASE-7671,"See the following logs first:
{code}
2013-01-23 18:58:38,801 INFO org.apache.hadoop.hbase.regionserver.Store: Flushed , sequenceid=9746535080, memsize=101.8m, into tmp file hdfs://dw77.kgb.sqa.cm4:9900/hbase-test3/writetest1/8dc14e35b4d7c0e481e0bb30849cff7d/.tmp/bebeeecc56364b6c8126cf1dc6782a25

2013-01-23 18:58:41,982 WARN org.apache.hadoop.hbase.regionserver.MemStore: Snapshot called again without clearing previous. Doing nothing. Another ongoing flush or did we fail last attempt?


2013-01-23 18:58:43,274 INFO org.apache.hadoop.hbase.regionserver.Store: Flushed , sequenceid=9746599334, memsize=101.8m, into tmp file hdfs://dw77.kgb.sqa.cm4:9900/hbase-test3/writetest1/8dc14e35b4d7c0e481e0bb30849cff7d/.tmp/4eede32dc469480bb3d469aaff332313
{code}

The first time memstore flush is failed when commitFile()(Logged the first edit above), then trigger server abort, but another flush is coming immediately(could caused by move/split,Logged the third edit above) and successful.

For the same memstore's snapshot, we get different sequenceid, it causes data loss when replaying log edits

See details from the unit test case in the patch", ,,,,,,,
HBASE-7679,"The current handling of changes to the region servers in a replication peer cluster is currently quite inefficient. The list of region servers that are being replicated to is only updated if there are a large number of issues encountered while replicating.

This can cause it to take quite a while to recognize that a number of the regionserver in a peer cluster are no longer available. A potentially bigger problem is that if a replication peer cluster is started with a small number of regionservers, and then more region servers are added after replication has started, the additional region servers will never be used for replication (unless there are failures on the in-use regionservers).



Part of the current issue is that the retry code in ReplicationSource#shipEdits checks a randomly-chosen replication peer regionserver (in ReplicationSource#isSlaveDown) to see if it is up after a replication write has failed on a different randonly-chosen replication peer. If the peer is seen as not down, another randomly-chosen peer is used for writing.



A second part of the issue is that changes to the list of region servers in a peer cluster are not detected at all, and are only picked up if a certain number of failures have occurred when trying to ship edits.
", ,,1,,,,,
HBASE-7689,"2013-01-22 17:59:03,237 INFO  [Shutdown of org.apache.hadoop.hbase.fs.HFileSystem@5984cf08] hbase.MiniHBaseCluster$SingleFileSystemShutdownThread(186): Hook closing fs=org.apache.hadoop.hbase.fs.HFileSystem@5984cf08
...
2013-01-22 17:59:03,411 DEBUG [RS_OPEN_REGION-10.11.2.92,50661,1358906192942-0] regionserver.HRegion(1001): Closing IntegrationTestRebalanceAndKillServersTargeted,66666660,1358906196709.0200b366bc37c5afd1185f7d487c7dfb.: disabling compactions &amp; flushes
2013-01-22 17:59:03,411 DEBUG [RS_OPEN_REGION-10.11.2.92,50661,1358906192942-0] regionserver.HRegion(1023): Updates disabled for region IntegrationTestRebalanceAndKillServersTargeted,66666660,1358906196709.0200b366bc37c5afd1185f7d487c7dfb.
2013-01-22 17:59:03,415 ERROR [RS_OPEN_REGION-10.11.2.92,50661,1358906192942-0] executor.EventHandler(205): Caught throwable while processing event M_RS_OPEN_REGION
java.io.IOException: java.io.IOException: java.io.IOException: Filesystem closed
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1058)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:974)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:945)
	at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.cleanupFailedOpen(OpenRegionHandler.java:459)
	at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:143)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:202)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)

tryTransitionFromOpeningToFailedOpen or transitionToOpened below is never called and region can get stuck.
As an added benefit, the meta is already written by that time.", ,,,,,,,
HBASE-7698,"2013-01-22 17:59:03,237 INFO  [Shutdown of org.apache.hadoop.hbase.fs.HFileSystem@5984cf08] hbase.MiniHBaseCluster$SingleFileSystemShutdownThread(186): Hook closing fs=org.apache.hadoop.hbase.fs.HFileSystem@5984cf08
...
2013-01-22 17:59:03,411 DEBUG [RS_OPEN_REGION-10.11.2.92,50661,1358906192942-0] regionserver.HRegion(1001): Closing IntegrationTestRebalanceAndKillServersTargeted,66666660,1358906196709.0200b366bc37c5afd1185f7d487c7dfb.: disabling compactions &amp; flushes
2013-01-22 17:59:03,411 DEBUG [RS_OPEN_REGION-10.11.2.92,50661,1358906192942-0] regionserver.HRegion(1023): Updates disabled for region IntegrationTestRebalanceAndKillServersTargeted,66666660,1358906196709.0200b366bc37c5afd1185f7d487c7dfb.
2013-01-22 17:59:03,415 ERROR [RS_OPEN_REGION-10.11.2.92,50661,1358906192942-0] executor.EventHandler(205): Caught throwable while processing event M_RS_OPEN_REGION
java.io.IOException: java.io.IOException: java.io.IOException: Filesystem closed
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1058)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:974)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:945)
	at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.cleanupFailedOpen(OpenRegionHandler.java:459)
	at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:143)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:202)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)

tryTransitionFromOpeningToFailedOpen or transitionToOpened below is never called and region can get stuck.
As an added benefit, the meta is already written by that time.", ,,,,,,,
HBASE-7701,"Closed regions are not removed from assignments. I am not sure if it's a general state problem, or just a small bug; for now, one manifestation is that moved region is ignored by SSH of the target server if target server dies before updating ZK.

{code}
2013-01-22 17:59:00,524 DEBUG [IPC Server handler 3 on 50658] master.AssignmentManager(1475): Sent CLOSE to 10.11.2.92,51231,1358906285048 for region IntegrationTestRebalanceAndKillServersTargeted,66666660,1358906196709.0200b366bc37c5afd1185f7d487c7dfb.
2013-01-22 17:59:00,997 DEBUG [RS_CLOSE_REGION-10.11.2.92,51231,1358906285048-1] handler.CloseRegionHandler(167): set region closed state in zk successfully for region IntegrationTestRebalanceAndKillServersTargeted,66666660,1358906196709.0200b366bc37c5afd1185f7d487c7dfb. sn name: 10.11.2.92,51231,1358906285048
2013-01-22 17:59:01,088 INFO  [MASTER_CLOSE_REGION-10.11.2.92,50658,1358906192673-0] master.RegionStates(242): Region {NAME =&gt; &apos;IntegrationTestRebalanceAndKillServersTargeted,66666660,1358906196709.0200b366bc37c5afd1185f7d487c7dfb.&apos;, STARTKEY =&gt; &apos;66666660&apos;, ENDKEY =&gt; &apos;7333332c&apos;, ENCODED =&gt; 0200b366bc37c5afd1185f7d487c7dfb,} transitioned from {IntegrationTestRebalanceAndKillServersTargeted,66666660,1358906196709.0200b366bc37c5afd1185f7d487c7dfb. state=CLOSED, ts=1358906341087, server=null} to {IntegrationTestRebalanceAndKillServersTargeted,66666660,1358906196709.0200b366bc37c5afd1185f7d487c7dfb. state=OFFLINE, ts=1358906341088, server=null}
2013-01-22 17:59:01,128 INFO  [MASTER_CLOSE_REGION-10.11.2.92,50658,1358906192673-0] master.AssignmentManager(1596): Assigning region IntegrationTestRebalanceAndKillServersTargeted,66666660,1358906196709.0200b366bc37c5afd1185f7d487c7dfb. to 10.11.2.92,50661,1358906192942

... (50661 didn't update ZK to OPEN, only OPENING)

2013-01-22 17:59:06,605 INFO  [MASTER_SERVER_OPERATIONS-10.11.2.92,50658,1358906192673-2] handler.ServerShutdownHandler(202): Reassigning 7 region(s) that 10.11.2.92,50661,1358906192942 was carrying (skipping 0 regions(s) that are already in transition)
2013-01-22 17:59:06,605 DEBUG [MASTER_SERVER_OPERATIONS-10.11.2.92,50658,1358906192673-2] handler.ServerShutdownHandler(219): Skip assigning region IntegrationTestRebalanceAndKillServersTargeted,66666660,1358906196709.0200b366bc37c5afd1185f7d487c7dfb. because it has been opened in 10.11.2.92,51231,1358906285048
{code}

Note the server in the last line - the one that has long closed the region.", ,,,,,,,
HBASE-7703," We just discovered the following scenario:
# Cluster A and B are setup in master/master replication
# By accident we had Cluster C replicate to Cluster A.

Now all edit originating from C will be bouncing between A and B. Forever!
The reason is that when the edit come in from C the cluster ID is already set and won't be reset.

We have a couple of options here:
# Optionally only support master/master (not cycles of more than two clusters). In that case we can always reset the cluster ID in the ReplicationSource. That means that now cycles > 2 will have the data cycle forever. This is the only option that requires no changes in the HLog format.
# Instead of a single cluster id per edit maintain a (unordered) set of cluster id that have seen this edit. Then in ReplicationSource we drop any edit that the sink has seen already. The is the cleanest approach, but it might need a lot of data stored per edit if there are many clusters involved.
# Maintain a configurable counter of the maximum cycle side we want to support. Could default to 10 (even maybe even just). Store a hop-count in the WAL and the ReplicationSource increases that hop-count on each hop. If we're over the max, just drop the edit.

", ,,,,,,,
HBASE-7709," We just discovered the following scenario:
# Cluster A and B are setup in master/master replication
# By accident we had Cluster C replicate to Cluster A.

Now all edit originating from C will be bouncing between A and B. Forever!
The reason is that when the edit come in from C the cluster ID is already set and won't be reset.

We have a couple of options here:
# Optionally only support master/master (not cycles of more than two clusters). In that case we can always reset the cluster ID in the ReplicationSource. That means that now cycles > 2 will have the data cycle forever. This is the only option that requires no changes in the HLog format.
# Instead of a single cluster id per edit maintain a (unordered) set of cluster id that have seen this edit. Then in ReplicationSource we drop any edit that the sink has seen already. The is the cleanest approach, but it might need a lot of data stored per edit if there are many clusters involved.
# Maintain a configurable counter of the maximum cycle side we want to support. Could default to 10 (even maybe even just). Store a hop-count in the WAL and the ReplicationSource increases that hop-count on each hop. If we're over the max, just drop the edit.

", ,,,,,,,
HBASE-7711,"An earlier version of snapshots would thread interrupt operations.  In longer term testing we ran into an exception stack trace that indicated that a rowlock was taken an never released.

{code}
2013-01-26 01:54:56,417 ERROR org.apache.hadoop.hbase.procedure.ProcedureMember: Propagating foreign exception to subprocedure pe-1
org.apache.hadoop.hbase.errorhandling.ForeignException$ProxyThrowable via timer-java.util.Timer@1cea3151:org.apache.hadoop.hbase.errorhandling.ForeignException$ProxyThrowable: org.apache.hadoop.hbase.errorhandling.TimeoutException: Timeout elapsed! Source:Timeout caused Foreign E
xception Start:1359194035004, End:1359194095004, diff:60000, max:60000 ms
        at org.apache.hadoop.hbase.errorhandling.ForeignException.deserialize(ForeignException.java:184)
        at org.apache.hadoop.hbase.procedure.ZKProcedureMemberRpcs.abort(ZKProcedureMemberRpcs.java:321)
        at org.apache.hadoop.hbase.procedure.ZKProcedureMemberRpcs.watchForAbortedProcedures(ZKProcedureMemberRpcs.java:150)
        at org.apache.hadoop.hbase.procedure.ZKProcedureMemberRpcs.access$200(ZKProcedureMemberRpcs.java:56)
        at org.apache.hadoop.hbase.procedure.ZKProcedureMemberRpcs$1.nodeChildrenChanged(ZKProcedureMemberRpcs.java:112)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:315)
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:519)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:495)
Caused by: org.apache.hadoop.hbase.errorhandling.ForeignException$ProxyThrowable: org.apache.hadoop.hbase.errorhandling.TimeoutException: Timeout elapsed! Source:Timeout caused Foreign Exception Start:1359194035004, End:1359194095004, diff:60000, max:60000 ms
        at org.apache.hadoop.hbase.errorhandling.TimeoutExceptionInjector$1.run(TimeoutExceptionInjector.java:71)
        at java.util.TimerThread.mainLoop(Timer.java:512)
        at java.util.TimerThread.run(Timer.java:462)
2013-01-26 01:54:56,648 WARN org.apache.hadoop.hbase.regionserver.HRegion: Failed getting lock in batch put, row=0001558252
java.io.IOException: Timed out on getting lock for row=0001558252
        at org.apache.hadoop.hbase.regionserver.HRegion.internalObtainRowLock(HRegion.java:3239)
        at org.apache.hadoop.hbase.regionserver.HRegion.getLock(HRegion.java:3315)
        at org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchMutation(HRegion.java:2150)
        at org.apache.hadoop.hbase.regionserver.HRegion.batchMutate(HRegion.java:2021)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.java:3511)
        at sun.reflect.GeneratedMethodAccessor46.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1400)
....
.. every snapshot attempt that used this region for the next two days encountered this problem.
{code}

Snapshots will now bypass this problem with the fix in HBASE-7703.  However, we should make sure hbase regionserver operations are safe when interrupted.", ,,,,,,,
HBASE-7715,"We encountered an issue where HMaster failed to start with an active NN not in safe mode and a standby NN in safemode. The relevant lines in {{FSUtils.java}} show the issue:

{noformat}
    while (dfs.setSafeMode(org.apache.hadoop.hdfs.protocol.FSConstants.SafeModeAction.SAFEMODE_GET)) {
{noformat}

This call skips the normal client failover from the standby to active NN, so it will loop polling the standby NN if it unfortunately talks to the standby first.", ,,,,,,,
HBASE-7721,When moving to HDFS HA or removing HA we end up changing the NN namespace.  This can cause the HMaster not to start up fully due to trying to split phantom HLogs pointing to the wrong FS - java.lang.IllegalArgumentException: Wrong FS: error messages.  The HLogs in question might not even be on HDFS anymore.  You have to go in a manually clear out the ZK splitlogs directory to get HBase to properly boot up., ,,,,,,,
HBASE-7723,When moving to HDFS HA or removing HA we end up changing the NN namespace.  This can cause the HMaster not to start up fully due to trying to split phantom HLogs pointing to the wrong FS - java.lang.IllegalArgumentException: Wrong FS: error messages.  The HLogs in question might not even be on HDFS anymore.  You have to go in a manually clear out the ZK splitlogs directory to get HBase to properly boot up., ,,,,,,,
HBASE-7726,"Similar to HBASE-6564, ModifyTableHandler doesn't remove the families from hdfs (families no longer present in the new descriptor)", ,,,,,,,
HBASE-7727,"Similar to HBASE-6564, ModifyTableHandler doesn't remove the families from hdfs (families no longer present in the new descriptor)", ,,,,,,,
HBASE-7728,"Similar to HBASE-6564, ModifyTableHandler doesn't remove the families from hdfs (families no longer present in the new descriptor)", ,,,,,,,
HBASE-7731,"Similar to HBASE-6564, ModifyTableHandler doesn't remove the families from hdfs (families no longer present in the new descriptor)", ,,,,,,,
HBASE-7735,"Similar to HBASE-6564, ModifyTableHandler doesn't remove the families from hdfs (families no longer present in the new descriptor)", ,,,,,,,
HBASE-7741,"The current handling of changes to the region servers in a replication peer cluster is currently quite inefficient. The list of region servers that are being replicated to is only updated if there are a large number of issues encountered while replicating.

This can cause it to take quite a while to recognize that a number of the regionserver in a peer cluster are no longer available. A potentially bigger problem is that if a replication peer cluster is started with a small number of regionservers, and then more region servers are added after replication has started, the additional region servers will never be used for replication (unless there are failures on the in-use regionservers).



Part of the current issue is that the retry code in ReplicationSource#shipEdits checks a randomly-chosen replication peer regionserver (in ReplicationSource#isSlaveDown) to see if it is up after a replication write has failed on a different randonly-chosen replication peer. If the peer is seen as not down, another randomly-chosen peer is used for writing.



A second part of the issue is that changes to the list of region servers in a peer cluster are not detected at all, and are only picked up if a certain number of failures have occurred when trying to ship edits.
", ,,1,,,,,
HBASE-7728,"the hlog roller thread and hlog syncer thread may occur dead lock with the 'flushLock' and 'updateLock', and then cause all 'IPC Server handler' thread blocked on hlog append. the jstack info is as follow :
""regionserver60020.logRoller"":
        at org.apache.hadoop.hbase.regionserver.wal.HLog.syncer(HLog.java:1305)
        - waiting to lock <0x000000067bf88d58> (a java.lang.Object)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.syncer(HLog.java:1283)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.sync(HLog.java:1456)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.cleanupCurrentWriter(HLog.java:876)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.rollWriter(HLog.java:657)
        - locked <0x000000067d54ace0> (a java.lang.Object)
        at org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:94)
        at java.lang.Thread.run(Thread.java:662)
""regionserver60020.logSyncer"":
        at org.apache.hadoop.hbase.regionserver.wal.HLog.syncer(HLog.java:1314)
        - waiting to lock <0x000000067d54ace0> (a java.lang.Object)
        - locked <0x000000067bf88d58> (a java.lang.Object)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.syncer(HLog.java:1283)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.sync(HLog.java:1456)
        at org.apache.hadoop.hbase.regionserver.wal.HLog$LogSyncer.run(HLog.java:1235)
        at java.lang.Thread.run(Thread.java:662)", ,,,,,,,
HBASE-7731,"I bumped into this one - All the mutation calls like Put, Delete check whether the region in question is readonly. The append and increment calls don't.",1,,,,,,,
HBASE-7763,"Currently compaction selection is not sorting based on size.  This causes selection to choose larger files to re-write than are needed when bulk loads are involved.


", ,,1,,,,,
HBASE-7771,"This seems to be a regression caused by HBASE-4791 wherein we check if secure zookeeper is enabled and if so we make use of saslLatch to verify security handshake is completed. But in the case of MR, we won't be negotiating a secure connection thus we end up waiting forever for the saslLatch.

Since the bug the saslLatch workaround is trying to fix (ZOOKEEPER-1437) is already fixed in zookeeper-3.4.5. Removal of the workaround fixes the problem.",1,,,,,,,
HBASE-7772,"clusterId is not set in the job's conf correctly if only TableMapReduceUtil.initCredentials() is called and thus fails to include the token when using an hbase client in any of the job's tasks. This is a regression.

", ,,,,,,,
HBASE-7777,"HBCK checks for split parent regions being in meta and hdfs, and reports this as a transient error. 

However, split parents, by design, linger around for some time, until its children stops referring to it. 
Instead we should check whether the children are there, and do not report anything if it is so. ", ,,,,,,,
HBASE-7789,"I need to do some changes in DeadServer because of HBASE-7590. To minimize the patch size and simplifies the feedback, I prefer to isolate the issue.

Changes are:
 - Add the time when the server was declared as dead. It's what I need in HBASE-7590, but it makes sense even without it, for example to be shown in the UI.
 - suppress the extends on Set & clean up all the not used methods
 - use directly the object instead of a copy.


For connection utils, we currently have a jitter of 1%. I need a bigger one for sure in one case, but I wonder if we should not increase it in all cases? instead of plus 1%, we should have plus or minus 10% imho.

Tests are in progress locally, I will add the patch when they're ok.
", ,,,,,,,
HBASE-7799,"(logs grepped by region name, and abridged.

META server was dead so OpenRegionHandler for the region took a while, and was interrupted:
{code}
2013-02-08 14:35:01,555 DEBUG [RS_OPEN_REGION-10.11.2.92,64485,1360362800564-2] handler.OpenRegionHandler(255): Interrupting thread Thread[PostOpenDeployTasks:871d1c3bdf98a2c93b527cb6cc61327d,5,main]
{code}

Then master tried to force region offline and reassign:
{code}
2013-02-08 14:35:06,500 INFO  [MASTER_SERVER_OPERATIONS-10.11.2.92,64483,1360362800340-1] master.RegionStates(347): Found opening region {IntegrationTestRebalanceAndKillServersTargeted,7333332c,1360362805563.871d1c3bdf98a2c93b527cb6cc61327d. state=OPENING, ts=1360362901596, server=10.11.2.92,64485,1360362800564} to be reassigned by SSH for 10.11.2.92,64485,1360362800564
2013-02-08 14:35:06,500 INFO  [MASTER_SERVER_OPERATIONS-10.11.2.92,64483,1360362800340-1] master.RegionStates(242): Region {NAME => 'IntegrationTestRebalanceAndKillServersTargeted,7333332c,1360362805563.871d1c3bdf98a2c93b527cb6cc61327d.', STARTKEY => '7333332c', ENDKEY => '7ffffff8', ENCODED => 871d1c3bdf98a2c93b527cb6cc61327d,} transitioned from {IntegrationTestRebalanceAndKillServersTargeted,7333332c,1360362805563.871d1c3bdf98a2c93b527cb6cc61327d. state=OPENING, ts=1360362901596, server=10.11.2.92,64485,1360362800564} to {IntegrationTestRebalanceAndKillServersTargeted,7333332c,1360362805563.871d1c3bdf98a2c93b527cb6cc61327d. state=CLOSED, ts=1360362906500, server=null}
2013-02-08 14:35:06,505 DEBUG [10.11.2.92,64483,1360362800340-GeneralBulkAssigner-1] master.AssignmentManager(1530): Forcing OFFLINE; was={IntegrationTestRebalanceAndKillServersTargeted,7333332c,1360362805563.871d1c3bdf98a2c93b527cb6cc61327d. state=CLOSED, ts=1360362906500, server=null}
2013-02-08 14:35:06,506 DEBUG [10.11.2.92,64483,1360362800340-GeneralBulkAssigner-1] zookeeper.ZKAssign(176): master:64483-0x13cbbf1025d0000 Async create of unassigned node for 871d1c3bdf98a2c93b527cb6cc61327d with OFFLINE state
{code}
But didn't delete the original ZK node?
{code}
2013-02-08 14:35:06,509 WARN  [main-EventThread] master.OfflineCallback(59): Node for /hbase/region-in-transition/871d1c3bdf98a2c93b527cb6cc61327d already exists
2013-02-08 14:35:06,509 DEBUG [main-EventThread] master.OfflineCallback(69): rs={IntegrationTestRebalanceAndKillServersTargeted,7333332c,1360362805563.871d1c3bdf98a2c93b527cb6cc61327d. state=OFFLINE, ts=1360362906506, server=null}, server=10.11.2.92,64488,1360362800651
2013-02-08 14:35:06,512 DEBUG [main-EventThread] master.OfflineCallback$ExistCallback(106): rs={IntegrationTestRebalanceAndKillServersTargeted,7333332c,1360362805563.871d1c3bdf98a2c93b527cb6cc61327d. state=OFFLINE, ts=1360362906506, server=null}, server=10.11.2.92,64488,1360362800651
{code}
So it went into infinite cycle of failing to assign due to this:
{code}
2013-02-08 14:35:06,517 INFO  [PRI IPC Server handler 7 on 64488] regionserver.HRegionServer(3435): Received request to open region: IntegrationTestRebalanceAndKillServersTargeted,7333332c,1360362805563.871d1c3bdf98a2c93b527cb6cc61327d. on 10.11.2.92,64488,1360362800651
2013-02-08 14:35:06,521 WARN  [RS_OPEN_REGION-10.11.2.92,64488,1360362800651-0] zookeeper.ZKAssign(762): regionserver:64488-0x13cbbf1025d0004 Attempt to transition the unassigned node for 871d1c3bdf98a2c93b527cb6cc61327d from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING failed, the node existed but was in the state RS_ZK_REGION_OPENING set by the server [wrong server name redacted, see HBASE-7798]
{code}
Transitioning failed-to-open similarly fails.

It seems like master needs to nuke ZK node unconditionally to offline?", ,,,,,,,
HBASE-7820,"HBase servers are part of the Hadoop domain, controlled by Hadoop Active Directory. 
The users belong to the CORP domain, controlled by the CORP Active Directory. 
In the absence of a one way trust from HADOOP DOMAIN to CORP DOMAIN, how will HBase servers authenticate CORP users ?
This is the HBase equivalent of HADOOP-9296",1,,,,,,,
HBASE-7824,"When there is log split work going on, master start up waits till all log split work completes even though the log split has nothing to do with meta region servers.

It's a bad behavior considering a master node can run when log split is happening while its start up is blocking by log split work. 

Since master is kind of single point of failure, we should start it ASAP.
", ,,1,,,,,
HBASE-7826,"When there is log split work going on, master start up waits till all log split work completes even though the log split has nothing to do with meta region servers.

It's a bad behavior considering a master node can run when log split is happening while its start up is blocking by log split work. 

Since master is kind of single point of failure, we should start it ASAP.
", ,,,,,,,
HBASE-7835,"When there is log split work going on, master start up waits till all log split work completes even though the log split has nothing to do with meta region servers.

It's a bad behavior considering a master node can run when log split is happening while its start up is blocking by log split work. 

Since master is kind of single point of failure, we should start it ASAP.
", ,,1,,,,,
HBASE-7838,"Currently running full stack (Hadoop + Hbase + zookeeper) on a VM I want to reboot, every services stopped but HBase regionserver. For 2 days now, it's aligning dots on the console ...
I propose a timeout (default 600 overidable by $HBASE_STOP_TIMEOUT) to force kill -9 on the process.", ,1,1,,,,,
HBASE-7842,"When there is log split work going on, master start up waits till all log split work completes even though the log split has nothing to do with meta region servers.

It's a bad behavior considering a master node can run when log split is happening while its start up is blocking by log split work. 

Since master is kind of single point of failure, we should start it ASAP.
", ,,1,,,,,
HBASE-7845,"When there is log split work going on, master start up waits till all log split work completes even though the log split has nothing to do with meta region servers.

It's a bad behavior considering a master node can run when log split is happening while its start up is blocking by log split work. 

Since master is kind of single point of failure, we should start it ASAP.
", ,,1,,,,,
HBASE-7848,"Given the current mechanism, it is possible for users to flood a single region with 1k+ store files via the bulkload API and basically cause the region to become a flying dutchman - never getting assigned successfully again.
Ideally, an administrative limit could solve this. If the bulkload RPC call can check if the region already has X store files, then it can reject the request to add another and throw a failure at the client with an appropriate message.
This may be an intrusive change, but seems necessary in perfecting the gap between devs and ops in managing a HBase clusters. This would especially prevent abuse in form of unaware devs not pre-splitting tables before bulkloading things in. Currently, this leads to ops pain, as the devs think HBase has gone non-functional and begin complaining.",,,,,,,,
HBASE-7849,"Given the current mechanism, it is possible for users to flood a single region with 1k+ store files via the bulkload API and basically cause the region to become a flying dutchman - never getting assigned successfully again.
Ideally, an administrative limit could solve this. If the bulkload RPC call can check if the region already has X store files, then it can reject the request to add another and throw a failure at the client with an appropriate message.
This may be an intrusive change, but seems necessary in perfecting the gap between devs and ops in managing a HBase clusters. This would especially prevent abuse in form of unaware devs not pre-splitting tables before bulkloading things in. Currently, this leads to ops pain, as the devs think HBase has gone non-functional and begin complaining.",1,,,,,,,
HBASE-7860,"We are currently unable to use ACLs without having Kerberos setup.  That is a pain for testing and environments that have other authentication methods that are not Kerberos-centric.

safety valve:
<property>
     <name>hbase.security.authorization</name>
     <value>true</value>
</property>
<property>
     <name>hbase.coprocessor.master.classes</name>
     <value>org.apache.hadoop.hbase.security.access.AccessController</value>
</property>
<property>
     <name>hbase.coprocessor.region.classes</name>
     <value>org.apache.hadoop.hbase.security.token.TokenProvider,org.apache.hadoop.hbase.security.access.AccessController</value>
</property>

[root@cdh4-oozie-1 ~]# hbase shell
hbase(main):001:0> create 't1', 'cf1'

ERROR: org.apache.hadoop.hbase.security.AccessDeniedException: org.apache.hadoop.hbase.security.AccessDeniedException: Insufficient permissions for user 'null' (global, action=CREATE)
	at org.apache.hadoop.hbase.security.access.AccessController.requirePermission(AccessController.java:402)
	at org.apache.hadoop.hbase.security.access.AccessController.preCreateTable(AccessController.java:525)
	at org.apache.hadoop.hbase.master.MasterCoprocessorHost.preCreateTable(MasterCoprocessorHost.java:89)
	at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.java:1056)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1345)

[root@cdh4-oozie-1 ~]# su hbase
bash-4.1$ hbase shell

hbase(main):001:0> create 't1', 'cf1'

ERROR: org.apache.hadoop.hbase.security.AccessDeniedException: org.apache.hadoop.hbase.security.AccessDeniedException: Insufficient permissions for user 'null' (global, action=CREATE)
	at org.apache.hadoop.hbase.security.access.AccessController.requirePermission(AccessController.java:402)
	at org.apache.hadoop.hbase.security.access.AccessController.preCreateTable(AccessController.java:525)
	at org.apache.hadoop.hbase.master.MasterCoprocessorHost.preCreateTable(MasterCoprocessorHost.java:89)
	at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.java:1056)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1345)

It looks like we are relying on Kerberos to tell us who the user is, but since we are not using authentication, we are just passing NULL.  We should be able to just rely on the local fs account.",1,1,,,,,,
HBASE-7861,"I faced 3 regions (out of 8) never stopping today. This is pretty bad because the script is supposed to wait until all the RS stopped to re-start everything, therefor, servers are never going back online.

HBASE-7838 will help with that and will kill the RSs. But that will not really solve the root cause.

Attached are the jstack for the 3 servers.", ,,,,,,,
HBASE-7865,"I faced 3 regions (out of 8) never stopping today. This is pretty bad because the script is supposed to wait until all the RS stopped to re-start everything, therefor, servers are never going back online.

HBASE-7838 will help with that and will kill the RSs. But that will not really solve the root cause.

Attached are the jstack for the 3 servers.", ,,,,,,,
HBASE-7868,"By HFilePerformanceEvaluation seems that 0.94 is slower then 0.92

Looking at the profiler for the Scan path, seems that most of the time, compared to 92, is spent in the metrics dictionary lookup. [~eclark] pointed out the new per family/block metrics.

By commenting the metrics call in HFileReaderV2, the performance seems to get better, but maybe metrics is not the only problem.", ,,1,,,,,
HBASE-7871,"The attached test fails ~1% of the the time on 0.96. It seems it does not fail on 0.94.5. It's simple: a table creation and some puts.

I attach the stack. Logs says nothing it seems.
The suspicious part is:
{noformat}
""RS_CLOSE_REGION-localhost,57575,1361197489166-2"" prio=10 tid=0x00007fb0c8775800 nid=0x61ac runnable [0x00007fb09f272000]
   java.lang.Thread.State: RUNNABLE
        at java.util.TreeMap.fixAfterDeletion(TreeMap.java:2193)
        at java.util.TreeMap.deleteEntry(TreeMap.java:2151)
        at java.util.TreeMap.remove(TreeMap.java:585)
        at java.util.TreeSet.remove(TreeSet.java:259)
        at org.apache.hadoop.hbase.regionserver.MetricsRegionAggregateSourceImpl.deregister(MetricsRegionAggregateSourceImpl.java:55)
        at org.apache.hadoop.hbase.regionserver.MetricsRegionSourceImpl.close(MetricsRegionSourceImpl.java:86)
        at org.apache.hadoop.hbase.regionserver.MetricsRegion.close(MetricsRegion.java:40)
        at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1063)
        at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:969)
        - locked <0x00000006944e2558> (a java.lang.Object)
        at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:146)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:203)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

{noformat}
", ,,,,,,,
HBASE-7874,"By HFilePerformanceEvaluation seems that 0.94 is slower then 0.92

Looking at the profiler for the Scan path, seems that most of the time, compared to 92, is spent in the metrics dictionary lookup. [~eclark] pointed out the new per family/block metrics.

By commenting the metrics call in HFileReaderV2, the performance seems to get better, but maybe metrics is not the only problem.", ,,1,,,,,
HBASE-7875,"By HFilePerformanceEvaluation seems that 0.94 is slower then 0.92

Looking at the profiler for the Scan path, seems that most of the time, compared to 92, is spent in the metrics dictionary lookup. [~eclark] pointed out the new per family/block metrics.

By commenting the metrics call in HFileReaderV2, the performance seems to get better, but maybe metrics is not the only problem.", ,,1,,,,,
HBASE-7878,"I think this is a problem, so I'm opening a ticket so an HBase person takes a look.

Apache Accumulo has moved its write-ahead log to HDFS. I modeled the lease recovery for Accumulo after HBase's lease recovery.  During testing, we experienced data loss.  I found it is necessary to wait until recoverLease returns true to know that the file has been truly closed.  In FSHDFSUtils, the return result of recoverLease is not checked. In the unit tests created to check lease recovery in HBASE-2645, the return result of recoverLease is always checked.

I think FSHDFSUtils should be modified to check the return result, and wait until it returns true.", ,1,,,,,,
HBASE-7883,"In case of Appends/Increments with VERSION of CF set to 1, the memstore size is not updated when the previous entries are removed from the memstore. ", ,,,,,,,
HBASE-7884,ByteBloomFilter's performance can be optimized by avoiding multiplication operation when generating hash , ,,1,,,,,
HBASE-7885,"For HFile V2, the bloom filter will take a initial size, 128KB. 
When there are not that much records inserted into the bloom filter, the bloom fitler will start to shrink itself to do compaction. 
For example, for 128K, it will compact to 64K ->32K->16K->8K->4K->2K->1K->512->256->128->64->32, as long as it think that it can be bounded by the estimate error rate. 

If we puts only a few records in the HFile, the bloom filter will be compacted to too small, then it will break the assumption that shrinking will still be bounded by the estimated error rate. The False positive rate will becomes un-acceptable high. 
For example, if we set the expected error rate is 0.00001, for 10 records, after compaction, The size of the bloom filter will be 64 bytes. The real effective false positive rate will be 50%.

The use case is like this, if we are using HBase to store big record like images, and binaries, each record will take megabytes. Then for a 128M file, it will only contains dozens of records.

The suggested fix is to set a lower limit for the bloom filter compaction process. I suggest to use 1000 bytes.", ,,1,,,,,
HBASE-7886,"if we use the hbase shell command ""hlog_roll"" on a regionserver which is configured replication. the Hlog zk node under /hbase/replication/rs/1 can not be deleted.

this issue is caused by HBASE-6758. ", ,,,,,,,
HBASE-7893,"For disabling auto major compaction, I configured hbase.hregion.majorcompaction = 0 in the config file and restarted the cluster. In spite of this, major compaction continues to run everyday. Here is the config I set:

<property>
  <name>hbase.hregion.majorcompaction</name>
  <value>0</value>
</property>

What other way can I disable auto major compaction? I expected this config to work. What am I doing wrong here? 

Please advice. Thanks a lot.", ,1,,,,,,
HBASE-7912,"For HFile V2, the bloom filter will take a initial size, 128KB. 
When there are not that much records inserted into the bloom filter, the bloom fitler will start to shrink itself to do compaction. 
For example, for 128K, it will compact to 64K ->32K->16K->8K->4K->2K->1K->512->256->128->64->32, as long as it think that it can be bounded by the estimate error rate. 

If we puts only a few records in the HFile, the bloom filter will be compacted to too small, then it will break the assumption that shrinking will still be bounded by the estimated error rate. The False positive rate will becomes un-acceptable high. 
For example, if we set the expected error rate is 0.00001, for 10 records, after compaction, The size of the bloom filter will be 64 bytes. The real effective false positive rate will be 50%.

The use case is like this, if we are using HBase to store big record like images, and binaries, each record will take megabytes. Then for a 128M file, it will only contains dozens of records.

The suggested fix is to set a lower limit for the bloom filter compaction process. I suggest to use 1000 bytes.", ,,1,,,,,
HBASE-7913,"Fails with exception

{code}
avax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:194)
        at org.apache.hadoop.hbase.security.HBaseSaslRpcClient.saslConnect(HBaseSaslRpcClient.java:139)
        at org.apache.hadoop.hbase.ipc.SecureClient$SecureConnection.setupSaslConnection(SecureClient.java:194)
        at org.apache.hadoop.hbase.ipc.SecureClient$SecureConnection.access$500(SecureClient.java:92)
        at org.apache.hadoop.hbase.ipc.SecureClient$SecureConnection$2.run(SecureClient.java:302)
        at org.apache.hadoop.hbase.ipc.SecureClient$SecureConnection$2.run(SecureClient.java:299)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1178)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.util.Methods.call(Methods.java:37)
        at org.apache.hadoop.hbase.security.User.call(User.java:590)
        at org.apache.hadoop.hbase.security.User.access$700(User.java:51)
        at org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:444)
        at org.apache.hadoop.hbase.ipc.SecureClient$SecureConnection.setupIOstreams(SecureClient.java:298)
        at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:1124)
        at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:974)
        at org.apache.hadoop.hbase.ipc.SecureRpcEngine$Invoker.invoke(SecureRpcEngine.java:104)
        at $Proxy5.getProtocolVersion(Unknown Source)
        at org.apache.hadoop.hbase.ipc.SecureRpcEngine.getProxy(SecureRpcEngine.java:146)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getMaster(HConnectionManager.java:711)
        at org.apache.hadoop.hbase.client.HBaseAdmin.<init>(HBaseAdmin.java:116)
        at org.apache.hadoop.hbase.rest.RESTServlet.<init>(RESTServlet.java:74)
        at org.apache.hadoop.hbase.rest.RESTServlet.getInstance(RESTServlet.java:57)
        at org.apache.hadoop.hbase.rest.Main.main(Main.java:81)
Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)
        at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:130)
        at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:106)
        at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:172)
        at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:209)
        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:195)
        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:162)
        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:175)
{code}",1,,,,,,,
HBASE-7915,"in ThriftServer.java the following call is made

{code}
serverRunner = new ThriftServerRunner(conf);
{code}

which invokes

{code}
public ThriftServerRunner(Configuration conf) throws IOException {
    this(conf, new ThriftServerRunner.HBaseHandler(conf));
  }
{code}

All of this is happening before the user has logged in and fails. ",1,,,,,,,
HBASE-7917,"For disabling auto major compaction, I configured hbase.hregion.majorcompaction = 0 in the config file and restarted the cluster. In spite of this, major compaction continues to run everyday. Here is the config I set:

<property>
  <name>hbase.hregion.majorcompaction</name>
  <value>0</value>
</property>

What other way can I disable auto major compaction? I expected this config to work. What am I doing wrong here? 

Please advice. Thanks a lot.", ,1,,,,,,
HBASE-7919,"ServerManager#getServerConnection() try to retrieve the cached connection from Map serverConnections. ServerName objects are the key with which items are saved in this map. But we use String type to get. This always returns null and in turn the Master creates a new connection with RS again and again.

{code}
private final Map<ServerName, HRegionInterface> serverConnections =
    new HashMap<ServerName, HRegionInterface>();
...........
private HRegionInterface getServerConnection(final ServerName sn)
  throws IOException {
    HRegionInterface hri = this.serverConnections.get(sn.toString());
    if (hri == null) {
      LOG.debug(""New connection to "" + sn.toString());
      hri = this.connection.getHRegionConnection(sn.getHostname(), sn.getPort());
      this.serverConnections.put(sn, hri);
    }
    return hri;
  }
{code}", ,,,,,,,
HBASE-7923,"{code}
    try {
      HTable metaTable = new HTable(config, Bytes.toBytes("".META.""));
      Scan scan = new Scan();
      scan.setStartRow(Bytes.toBytes(""e""));
      scan.setStopRow(Bytes.toBytes(""z""));
      ResultScanner scanner = metaTable.getScanner(scan);
      Result[] results = scanner.next(100);
      while (results.length > 0) {
        for (Result result : results) {
          System.out.println(Bytes.toString(result.getRow()));
        }
        results = scanner.next(100);
      }
      scanner.close();
      metaTable.close();
    } catch (Exception e) {
      e.printStackTrace();
    }
{code}

This code will not return any result even if there is 10 tables with names starting with ""d"" to ""w"", including one table called ""entry"". If you comment the setStopRow you will get results, but will still get rows starting with ""d"" even if setStartRow is set to ""e"".

Same code using with a user table is working fine.

Facing the same issue with the shell.

scan '.META.' , {STARTROW => 'e', LIMIT => 10} is returning rows starting by ""d"".

scan '.META.' , {STARTROW => 'e', STOPROW => 'v', LIMIT => 10} is not returning anything.", ,,,,,,,
HBASE-7928,"{code}
    try {
      HTable metaTable = new HTable(config, Bytes.toBytes("".META.""));
      Scan scan = new Scan();
      scan.setStartRow(Bytes.toBytes(""e""));
      scan.setStopRow(Bytes.toBytes(""z""));
      ResultScanner scanner = metaTable.getScanner(scan);
      Result[] results = scanner.next(100);
      while (results.length > 0) {
        for (Result result : results) {
          System.out.println(Bytes.toString(result.getRow()));
        }
        results = scanner.next(100);
      }
      scanner.close();
      metaTable.close();
    } catch (Exception e) {
      e.printStackTrace();
    }
{code}

This code will not return any result even if there is 10 tables with names starting with ""d"" to ""w"", including one table called ""entry"". If you comment the setStopRow you will get results, but will still get rows starting with ""d"" even if setStartRow is set to ""e"".

Same code using with a user table is working fine.

Facing the same issue with the shell.

scan '.META.' , {STARTROW => 'e', LIMIT => 10} is returning rows starting by ""d"".

scan '.META.' , {STARTROW => 'e', STOPROW => 'v', LIMIT => 10} is not returning anything.", ,,,,,,,
HBASE-7932,"For HFile V2, the bloom filter will take a initial size, 128KB. 
When there are not that much records inserted into the bloom filter, the bloom fitler will start to shrink itself to do compaction. 
For example, for 128K, it will compact to 64K ->32K->16K->8K->4K->2K->1K->512->256->128->64->32, as long as it think that it can be bounded by the estimate error rate. 

If we puts only a few records in the HFile, the bloom filter will be compacted to too small, then it will break the assumption that shrinking will still be bounded by the estimated error rate. The False positive rate will becomes un-acceptable high. 
For example, if we set the expected error rate is 0.00001, for 10 records, after compaction, The size of the bloom filter will be 64 bytes. The real effective false positive rate will be 50%.

The use case is like this, if we are using HBase to store big record like images, and binaries, each record will take megabytes. Then for a 128M file, it will only contains dozens of records.

The suggested fix is to set a lower limit for the bloom filter compaction process. I suggest to use 1000 bytes.", ,,1,,,,,
HBASE-7933,"We are getting NPE in TableLockManager sometimes in tests. 
", ,,,,,,,
HBASE-7937,"A failure in log rolling causes regionserver abort. In case of HA NN, it will be good if there is a retry mechanism to roll the logs.
A corresponding jira for MemStore retries is HBASE-7507.", ,,,,,,,
HBASE-7944,"For HFile V2, the bloom filter will take a initial size, 128KB. 
When there are not that much records inserted into the bloom filter, the bloom fitler will start to shrink itself to do compaction. 
For example, for 128K, it will compact to 64K ->32K->16K->8K->4K->2K->1K->512->256->128->64->32, as long as it think that it can be bounded by the estimate error rate. 

If we puts only a few records in the HFile, the bloom filter will be compacted to too small, then it will break the assumption that shrinking will still be bounded by the estimated error rate. The False positive rate will becomes un-acceptable high. 
For example, if we set the expected error rate is 0.00001, for 10 records, after compaction, The size of the bloom filter will be 64 bytes. The real effective false positive rate will be 50%.

The use case is like this, if we are using HBase to store big record like images, and binaries, each record will take megabytes. Then for a 128M file, it will only contains dozens of records.

The suggested fix is to set a lower limit for the bloom filter compaction process. I suggest to use 1000 bytes.", ,,1,,,,,
HBASE-7948,"For HFile V2, the bloom filter will take a initial size, 128KB. 
When there are not that much records inserted into the bloom filter, the bloom fitler will start to shrink itself to do compaction. 
For example, for 128K, it will compact to 64K ->32K->16K->8K->4K->2K->1K->512->256->128->64->32, as long as it think that it can be bounded by the estimate error rate. 

If we puts only a few records in the HFile, the bloom filter will be compacted to too small, then it will break the assumption that shrinking will still be bounded by the estimated error rate. The False positive rate will becomes un-acceptable high. 
For example, if we set the expected error rate is 0.00001, for 10 records, after compaction, The size of the bloom filter will be 64 bytes. The real effective false positive rate will be 50%.

The use case is like this, if we are using HBase to store big record like images, and binaries, each record will take megabytes. Then for a 128M file, it will only contains dozens of records.

The suggested fix is to set a lower limit for the bloom filter compaction process. I suggest to use 1000 bytes.", ,,1,,,,,
HBASE-7950,"Making repeated requests (e.g., every 10 seconds) to the regionserver JMX metrics servlet crashes the regionserver. It will serve several requests correctly but then throw an exception upon the next request and then crash.

I can reproduce the error using a bash loop that will wget the /jmx servlet every 2 seconds. My regionserver ultimately crashes within a few minutes.

The regionserver log shows these exceptions (grep ""Caused by""):

{noformat}
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 177
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.NullPointerException
Caused by: java.lang.NullPointerException
Caused by: java.lang.NullPointerException
Caused by: java.lang.NullPointerException
Caused by: java.lang.NullPointerException
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 8192
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 159785336
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 159869880
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.NullPointerException
{noformat}
", ,1,,,,,,
HBASE-7952,"Making repeated requests (e.g., every 10 seconds) to the regionserver JMX metrics servlet crashes the regionserver. It will serve several requests correctly but then throw an exception upon the next request and then crash.

I can reproduce the error using a bash loop that will wget the /jmx servlet every 2 seconds. My regionserver ultimately crashes within a few minutes.

The regionserver log shows these exceptions (grep ""Caused by""):

{noformat}
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 177
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.NullPointerException
Caused by: java.lang.NullPointerException
Caused by: java.lang.NullPointerException
Caused by: java.lang.NullPointerException
Caused by: java.lang.NullPointerException
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 8192
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 159785336
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 159869880
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.NullPointerException
{noformat}
", ,,1,,,,,
HBASE-7963,"When security is enabled, HBase VerifyReplication fails for two reasons:
1.MapReduce do not have the auth to read the replication paths ""/hbase/replication/*"" on ZK;
2.VerifyReplication does not get the token for slave cluster, it's different from HBASE-7442, this VerifyReplication does not have the output.

{noformat}
WARN [main] org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation: RemoteException connecting to RS
javax.security.sasl.SaslException: DIGEST-MD5: digest response format violation. Mismatched response.
		 at org.apache.hadoop.hbase.security.HBaseSaslRpcClient.readStatus(HBaseSaslRpcClient.java:112)
		 at org.apache.hadoop.hbase.security.HBaseSaslRpcClient.saslConnect(HBaseSaslRpcClient.java:174)
		 at org.apache.hadoop.hbase.ipc.SecureClient$SecureConnection.setupSaslConnection(SecureClient.java:177)
		 at org.apache.hadoop.hbase.ipc.SecureClient$SecureConnection.access$500(SecureClient.java:85)
		 at org.apache.hadoop.hbase.ipc.SecureClient$SecureConnection$2.run(SecureClient.java:284)
		 at org.apache.hadoop.hbase.ipc.SecureClient$SecureConnection$2.run(SecureClient.java:281)
		 at java.security.AccessController.doPrivileged(Native Method)
		 at javax.security.auth.Subject.doAs(Subject.java:396)
		 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1232)
		 at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
		 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
		 at java.lang.reflect.Method.invoke(Method.java:597)
		 at org.apache.hadoop.hbase.util.Methods.call(Methods.java:37)
		 at org.apache.hadoop.hbase.security.User.call(User.java:586)
		 at org.apache.hadoop.hbase.security.User.access$700(User.java:50)
		 at org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:440)
		 at org.apache.hadoop.hbase.ipc.SecureClient$SecureConnection.setupIOstreams(SecureClient.java:280)
		 at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:1150)
		 at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:1000)
		 at org.apache.hadoop.hbase.ipc.SecureRpcEngine$Invoker.invoke(SecureRpcEngine.java:164)
		 at $Proxy13.getProtocolVersion(Unknown Source)
		 at org.apache.hadoop.hbase.ipc.SecureRpcEngine.getProxy(SecureRpcEngine.java:208)
		 at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:335)
		 at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:312)
		 at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:364)
		 at org.apache.hadoop.hbase.ipc.HBaseRPC.waitForProxy(HBaseRPC.java:236)
		 at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:1313)
		 at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:1269)
		 at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:1256)
		 at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:965)
		 at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:860)
		 at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:962)
		 at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:864)
		 at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:821)
		 at org.apache.hadoop.hbase.client.HTable.finishSetup(HTable.java:234)
		 at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:174)
		 at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:133)
		 at org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication$Verifier$1.connect(VerifyReplication.java:117)
		 at org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication$Verifier$1.connect(VerifyReplication.java:110)
		 at org.apache.hadoop.hbase.client.HConnectionManager.execute(HConnectionManager.java:360)
		 at org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication$Verifier.map(VerifyReplication.java:110)
		 at org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication$Verifier.map(VerifyReplication.java:74)
		 at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
		 at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:726)
		 at org.apache.hadoop.mapred.MapTask.run(MapTask.java:333)
		 at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:154)
		 at java.security.AccessController.doPrivileged(Native Method)
		 at javax.security.auth.Subject.doAs(Subject.java:396)
		 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1232)
		 at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:149)
{noformat}",1,,,,,,,
HBASE-7977,"Making repeated requests (e.g., every 10 seconds) to the regionserver JMX metrics servlet crashes the regionserver. It will serve several requests correctly but then throw an exception upon the next request and then crash.

I can reproduce the error using a bash loop that will wget the /jmx servlet every 2 seconds. My regionserver ultimately crashes within a few minutes.

The regionserver log shows these exceptions (grep ""Caused by""):

{noformat}
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 177
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.NullPointerException
Caused by: java.lang.NullPointerException
Caused by: java.lang.NullPointerException
Caused by: java.lang.NullPointerException
Caused by: java.lang.NullPointerException
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 8192
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 159785336
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 159869880
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.NullPointerException
{noformat}
", ,,,,,,,
HBASE-7987,"Making repeated requests (e.g., every 10 seconds) to the regionserver JMX metrics servlet crashes the regionserver. It will serve several requests correctly but then throw an exception upon the next request and then crash.

I can reproduce the error using a bash loop that will wget the /jmx servlet every 2 seconds. My regionserver ultimately crashes within a few minutes.

The regionserver log shows these exceptions (grep ""Caused by""):

{noformat}
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 177
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.NullPointerException
Caused by: java.lang.NullPointerException
Caused by: java.lang.NullPointerException
Caused by: java.lang.NullPointerException
Caused by: java.lang.NullPointerException
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 8192
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 159785336
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 159869880
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.NullPointerException
{noformat}
", ,,1,,,,,
HBASE-7989,"Scenario is:
- fetch the cache in the client
- a server dies
- try to use a region that is on the dead server

This will lead to a 20 second connect timeout. We don't have this in unit test because we have this only is the remote box does not answer. In the unit tests we have immediately a connection refused from the OS.", ,1,,,,,,
HBASE-7992,presently no hooks to provide access control to offline region in master., ,,,,,,,
HBASE-8001,"Making repeated requests (e.g., every 10 seconds) to the regionserver JMX metrics servlet crashes the regionserver. It will serve several requests correctly but then throw an exception upon the next request and then crash.

I can reproduce the error using a bash loop that will wget the /jmx servlet every 2 seconds. My regionserver ultimately crashes within a few minutes.

The regionserver log shows these exceptions (grep ""Caused by""):

{noformat}
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 177
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.NullPointerException
Caused by: java.lang.NullPointerException
Caused by: java.lang.NullPointerException
Caused by: java.lang.NullPointerException
Caused by: java.lang.NullPointerException
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 8192
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 159785336
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 159869880
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.InternalError: Unsupported VMGlobal Type 0
Caused by: java.lang.NullPointerException
{noformat}
", ,,1,,,,,
HBASE-8002,See HBASE-7327, ,1,,,,,,
HBASE-8004,"When i try to create a same table from shell i don't get TableExistsException instead i get
{code}
ERROR: cannot load Java class org.apache.hadoop.hbase.TableNotFoundException

Here is some help for this command:
Creates a table. Pass a table name, and a set of column family
specifications (at least one), and, optionally, table configuration.
Column specification can be a simple string (name), or a dictionary
(dictionaries are described below in main help output), necessarily
including NAME attribute.
Examples:

  hbase> create 't1', {NAME => 'f1', VERSIONS => 5}
  hbase> create 't1', {NAME => 'f1'}, {NAME => 'f2'}, {NAME => 'f3'}
  hbase> # The above in shorthand would be the following:

{code}", ,,,,,,,
HBASE-8008,"Richard Ding reported long delay in shutting down RegionServerSnapshotManager

Looks like HBASE-7779 wasn't included in the backport", ,,,,,,,
HBASE-8019,"Richard Ding reported long delay in shutting down RegionServerSnapshotManager

Looks like HBASE-7779 wasn't included in the backport", ,,1,,,,,
HBASE-8025,"HBASE-7091 added logic to separate GC logging options for some client commands versus server commands.  It uses a list of known client commands (""shell"" ""hbck"" ""hlog"" ""hfile"" ""zkcli"") and uses the server GC logging options for all other invocations of bin/hbase.  When zkcli is invoked, it in turn invokes ""hbase org.apache.hadoop.hbase.zookeeper.ZooKeeperMainServerArg"" to gather the server command line arguments, but because org.apache.hadoop.hbase.zookeeper.ZooKeeperMainServerArg is not on the white list it enables server GC logging, which causes extra output that causes the zkcli invocation to break.  HBASE-7153 addressed this but the fix only solved the array syntax - not the white list, so the zkcli command still fails.

There are many other tools you can invoke that are more likely to ""client"" than ""server"" options. For example, ""bin/hbase org.jruby.Main region_mover.rb"" or ""bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable"" or ""bin/hbase version"" or ""bin/hbase org.apache.hadoop.hbase.mapreduce.Export"". The whitelist of server commands is shorter and easier to maintain than a whitelist of client commands.", ,1,,,,,,
HBASE-8028,"In case there is an exception while doing the log-sync, the memstore is not rollbacked, while the mvcc is _always_ forwarded to the writeentry created at the beginning of the operation. This may lead to scanners seeing results which are not synched to the fs.


", ,,1,,,,,
HBASE-8036,"ProtobufUtil splits operations by regions and performs multiple client.multi calls. In case if there are certain errors inside RS, HRegionServer adds the corresponding exceptions to MultiResponse, PU continues the multi request for other regions, and returns partial failure. 
In case of other errors (for example, region not served exception), the entire multi operation stops executing, and previous successes and partial results are disregarded.
ProtobufUtil should probably catch ServiceException separately for each client.multi call, make it a partial-failure exception for all actions for this region, and also continue the batch, to make the behavior consistent.
Alternatively, if we want to avoid continuing the batch in case of some server-wide errors/connection problems/etc., server should do that for region-specific errors (add exception to results for each action).", ,,,,,,,
HBASE-8037,"RegionMovedException is currently thrown on global level, and due to how ProtobufUtil does things, it fails the entire multi-request, see HBASE-8036. RME also doesn't specify the region.
Thus, if it's thrown for one region and there are multiple regions in the request, HCM applies it to all of them, which causes clients to become confused temporarily. We should either fix HBASE-8036 or add region encoded name in the description. ", ,,,,,,,
HBASE-8040,"This is a problem that introduced when we tried to solve HBASE-7521.
https://issues.apache.org/jira/browse/HBASE-7521?focusedCommentId=13576083&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13576083

See the above comment and exactly the same has happened.  Will come up with a solution for the same.
", ,,,,,,,
HBASE-8042,"B.4.3. Special case: Root and META are corrupt.
The most drastic corruption scenario is the case where the ROOT or META is corrupted and HBase will not start. In this case you can use the OfflineMetaRepair tool create new ROOT and META regions and tables. This tool assumes that HBase is offline. It then marches through the existing HBase home directory, loads as much information from region metadata files (.regioninfo files) as possible from the file system. If the region metadata has proper table integrity, it sidelines the original root and meta table directories, and builds new ones with pointers to the region directories and their data.
$ ./bin/hbase org.apache.hadoop.hbase.util.OfflineMetaRepair


The path should be org.apache.hadoop.hbase.util.hbck.OfflineMetaRepair", ,1,,,,,,
HBASE-8055,"Observed Behavior:
When a user attempts to create a table but there is an issue with the codec, the attempt continues repeatedly. The shell command times out but the RS and Master are both occupied, leading to HBase being down. Further, HBase creates the folders for the table in HDFS.
The only way to restore the service is by disabling and dropping the table.
Here are the log lines when a table, t8, is created with this definition:
create 't8', {NAME=>'f1',COMPRESSION=>'lzo'}
Error from shell:
hbase(main):003:0> create 't8', {NAME=>'f1',BLOOMFILTER=>'row', COMPRESSION=>'lzo'}
ERROR: org.apache.hadoop.hbase.client.RegionOfflineException: Only 0 of 1 regions are online; retries exhausted.
Log lines on Master (repeats a few times/second):
2013-03-07 22:55:31,389 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Using pre-existing plan for region t8,,1362725678436.311edabcc1fe52001cb00e7c3e7f75d4.; plan=hri=t8,,1362725678436.311edabcc1fe52001cb00e7c3e7f75d4., src=, dest=upgrade-vm-1.ent.cloudera.com,60020,1362709586485
2013-03-07 22:55:31,389 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region t8,,1362725678436.311edabcc1fe52001cb00e7c3e7f75d4. to upgrade-vm-1.ent.cloudera.com,60020,1362709586485
2013-03-07 22:55:31,398 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=upgrade-vm-1.ent.cloudera.com,60020,1362709586485, region=311edabcc1fe52001cb00e7c3e7f75d4
2013-03-07 22:55:31,406 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_FAILED_OPEN, server=upgrade-vm-1.ent.cloudera.com,60020,1362709586485, region=311edabcc1fe52001cb00e7c3e7f75d4
2013-03-07 22:55:31,406 DEBUG org.apache.hadoop.hbase.master.handler.ClosedRegionHandler: Handling CLOSED event for 311edabcc1fe52001cb00e7c3e7f75d4
2013-03-07 22:55:31,406 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=t8,,1362725678436.311edabcc1fe52001cb00e7c3e7f75d4. state=CLOSED, ts=1362725731398, server=upgrade-vm-1.ent.cloudera.com,60020,1362709586485
2013-03-07 22:55:31,406 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x13d47d214830000 Creating (or updating) unassigned node for 311edabcc1fe52001cb00e7c3e7f75d4 with OFFLINE state
2013-03-07 22:55:31,414 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=M_ZK_REGION_OFFLINE, server=upgrade-vm-1.ent.cloudera.com:60000, region=311edabcc1fe52001cb00e7c3e7f75d4
Log lines on RS (repeats a few times/second):
2013-03-07 22:58:23,323 ERROR org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Failed open of region=t8,,1362725678436.311edabcc1fe52001cb00e7c3e7f75d4.
java.io.IOException: Compression algorithm 'lzo' previously failed test.
at org.apache.hadoop.hbase.util.CompressionTest.testCompression(CompressionTest.java:78)
at org.apache.hadoop.hbase.regionserver.HRegion.checkCompressionCodecs(HRegion.java:2797)
at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:2786)
at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:2774)
at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.openRegion(OpenRegionHandler.java:319)
at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:105)
at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:163)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:662)


Expected behavior:
We expect to fail fast (after a few retries). This should take <1 sec.", ,,,,,,,
HBASE-8049,"Observed Behavior:
When a user attempts to create a table but there is an issue with the codec, the attempt continues repeatedly. The shell command times out but the RS and Master are both occupied, leading to HBase being down. Further, HBase creates the folders for the table in HDFS.
The only way to restore the service is by disabling and dropping the table.
Here are the log lines when a table, t8, is created with this definition:
create 't8', {NAME=>'f1',COMPRESSION=>'lzo'}
Error from shell:
hbase(main):003:0> create 't8', {NAME=>'f1',BLOOMFILTER=>'row', COMPRESSION=>'lzo'}
ERROR: org.apache.hadoop.hbase.client.RegionOfflineException: Only 0 of 1 regions are online; retries exhausted.
Log lines on Master (repeats a few times/second):
2013-03-07 22:55:31,389 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Using pre-existing plan for region t8,,1362725678436.311edabcc1fe52001cb00e7c3e7f75d4.; plan=hri=t8,,1362725678436.311edabcc1fe52001cb00e7c3e7f75d4., src=, dest=upgrade-vm-1.ent.cloudera.com,60020,1362709586485
2013-03-07 22:55:31,389 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region t8,,1362725678436.311edabcc1fe52001cb00e7c3e7f75d4. to upgrade-vm-1.ent.cloudera.com,60020,1362709586485
2013-03-07 22:55:31,398 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=upgrade-vm-1.ent.cloudera.com,60020,1362709586485, region=311edabcc1fe52001cb00e7c3e7f75d4
2013-03-07 22:55:31,406 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_FAILED_OPEN, server=upgrade-vm-1.ent.cloudera.com,60020,1362709586485, region=311edabcc1fe52001cb00e7c3e7f75d4
2013-03-07 22:55:31,406 DEBUG org.apache.hadoop.hbase.master.handler.ClosedRegionHandler: Handling CLOSED event for 311edabcc1fe52001cb00e7c3e7f75d4
2013-03-07 22:55:31,406 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=t8,,1362725678436.311edabcc1fe52001cb00e7c3e7f75d4. state=CLOSED, ts=1362725731398, server=upgrade-vm-1.ent.cloudera.com,60020,1362709586485
2013-03-07 22:55:31,406 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x13d47d214830000 Creating (or updating) unassigned node for 311edabcc1fe52001cb00e7c3e7f75d4 with OFFLINE state
2013-03-07 22:55:31,414 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=M_ZK_REGION_OFFLINE, server=upgrade-vm-1.ent.cloudera.com:60000, region=311edabcc1fe52001cb00e7c3e7f75d4
Log lines on RS (repeats a few times/second):
2013-03-07 22:58:23,323 ERROR org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Failed open of region=t8,,1362725678436.311edabcc1fe52001cb00e7c3e7f75d4.
java.io.IOException: Compression algorithm 'lzo' previously failed test.
at org.apache.hadoop.hbase.util.CompressionTest.testCompression(CompressionTest.java:78)
at org.apache.hadoop.hbase.regionserver.HRegion.checkCompressionCodecs(HRegion.java:2797)
at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:2786)
at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:2774)
at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.openRegion(OpenRegionHandler.java:319)
at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:105)
at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:163)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:662)


Expected behavior:
We expect to fail fast (after a few retries). This should take <1 sec.", ,,1,,,,,
HBASE-8055,"We just ran into a scenario where we got the following NPE:
{code}
13/03/08 11:52:13 INFO regionserver.Store: Successfully loaded store file file:/tmp/hfile-import-00Dxx0000001lmJ-09Cxx00000000Jm/COLFAM/file09Cxx00000000Jm into store COLFAM (new location: file:/tmp/localhbase/data/SFDC.ENTITY_HISTORY_ARCHIVE/aeacee43aaf1748c6e60b9cc12bcac3d/COLFAM/120d683414e44478984b50ddd79b6826)
13/03/08 11:52:13 ERROR regionserver.HRegionServer: Failed openScanner
java.lang.NullPointerException
    at org.apache.hadoop.hbase.regionserver.StoreFile$Reader.getMaxTimestamp(StoreFile.java:1702)
    at org.apache.hadoop.hbase.regionserver.StoreFileScanner.requestSeek(StoreFileScanner.java:301)
    at org.apache.hadoop.hbase.regionserver.StoreScanner.<init>(StoreScanner.java:127)
    at org.apache.hadoop.hbase.regionserver.Store.getScanner(Store.java:2070)
    at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.<init>(HRegion.java:3383)
    at org.apache.hadoop.hbase.regionserver.HRegion.instantiateRegionScanner(HRegion.java:1628)
    at org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:1620)
    at org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:1596)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.openScanner(HRegionServer.java:2342)
    at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364)
    at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1400)
13/03/08 11:52:14 ERROR regionserver.HRegionServer: Failed openScanner
{code}

It's not clear, yet, how we got into this situation (we are generating HFiles via HFileOutputFormat and bulk load those). It seems that can only happen when the HFile itself is corrupted.

Looking at the code, though, I see this is the only place where we access StoreFile.reader.timeRangeTracker without a null check. So it appears we are expecting scenarios in which it can be null.

A simple fix would be:
{code}
    public long getMaxTimestamp() {
      return timeRangeTracker == null ? Long.MAX_VALUE : timeRangeTracker.maximumTimestamp;
    }
{code}
", ,,,,,,,
HBASE-8056,"Observed Behavior:
When a user attempts to create a table but there is an issue with the codec, the attempt continues repeatedly. The shell command times out but the RS and Master are both occupied, leading to HBase being down. Further, HBase creates the folders for the table in HDFS.
The only way to restore the service is by disabling and dropping the table.
Here are the log lines when a table, t8, is created with this definition:
create 't8', {NAME=>'f1',COMPRESSION=>'lzo'}
Error from shell:
hbase(main):003:0> create 't8', {NAME=>'f1',BLOOMFILTER=>'row', COMPRESSION=>'lzo'}
ERROR: org.apache.hadoop.hbase.client.RegionOfflineException: Only 0 of 1 regions are online; retries exhausted.
Log lines on Master (repeats a few times/second):
2013-03-07 22:55:31,389 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Using pre-existing plan for region t8,,1362725678436.311edabcc1fe52001cb00e7c3e7f75d4.; plan=hri=t8,,1362725678436.311edabcc1fe52001cb00e7c3e7f75d4., src=, dest=upgrade-vm-1.ent.cloudera.com,60020,1362709586485
2013-03-07 22:55:31,389 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region t8,,1362725678436.311edabcc1fe52001cb00e7c3e7f75d4. to upgrade-vm-1.ent.cloudera.com,60020,1362709586485
2013-03-07 22:55:31,398 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=upgrade-vm-1.ent.cloudera.com,60020,1362709586485, region=311edabcc1fe52001cb00e7c3e7f75d4
2013-03-07 22:55:31,406 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_FAILED_OPEN, server=upgrade-vm-1.ent.cloudera.com,60020,1362709586485, region=311edabcc1fe52001cb00e7c3e7f75d4
2013-03-07 22:55:31,406 DEBUG org.apache.hadoop.hbase.master.handler.ClosedRegionHandler: Handling CLOSED event for 311edabcc1fe52001cb00e7c3e7f75d4
2013-03-07 22:55:31,406 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=t8,,1362725678436.311edabcc1fe52001cb00e7c3e7f75d4. state=CLOSED, ts=1362725731398, server=upgrade-vm-1.ent.cloudera.com,60020,1362709586485
2013-03-07 22:55:31,406 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x13d47d214830000 Creating (or updating) unassigned node for 311edabcc1fe52001cb00e7c3e7f75d4 with OFFLINE state
2013-03-07 22:55:31,414 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=M_ZK_REGION_OFFLINE, server=upgrade-vm-1.ent.cloudera.com:60000, region=311edabcc1fe52001cb00e7c3e7f75d4
Log lines on RS (repeats a few times/second):
2013-03-07 22:58:23,323 ERROR org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Failed open of region=t8,,1362725678436.311edabcc1fe52001cb00e7c3e7f75d4.
java.io.IOException: Compression algorithm 'lzo' previously failed test.
at org.apache.hadoop.hbase.util.CompressionTest.testCompression(CompressionTest.java:78)
at org.apache.hadoop.hbase.regionserver.HRegion.checkCompressionCodecs(HRegion.java:2797)
at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:2786)
at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:2774)
at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.openRegion(OpenRegionHandler.java:319)
at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:105)
at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:163)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:662)


Expected behavior:
We expect to fail fast (after a few retries). This should take <1 sec.", ,,1,,,,,
HBASE-8073,"Observed Behavior:
When a user attempts to create a table but there is an issue with the codec, the attempt continues repeatedly. The shell command times out but the RS and Master are both occupied, leading to HBase being down. Further, HBase creates the folders for the table in HDFS.
The only way to restore the service is by disabling and dropping the table.
Here are the log lines when a table, t8, is created with this definition:
create 't8', {NAME=>'f1',COMPRESSION=>'lzo'}
Error from shell:
hbase(main):003:0> create 't8', {NAME=>'f1',BLOOMFILTER=>'row', COMPRESSION=>'lzo'}
ERROR: org.apache.hadoop.hbase.client.RegionOfflineException: Only 0 of 1 regions are online; retries exhausted.
Log lines on Master (repeats a few times/second):
2013-03-07 22:55:31,389 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Using pre-existing plan for region t8,,1362725678436.311edabcc1fe52001cb00e7c3e7f75d4.; plan=hri=t8,,1362725678436.311edabcc1fe52001cb00e7c3e7f75d4., src=, dest=upgrade-vm-1.ent.cloudera.com,60020,1362709586485
2013-03-07 22:55:31,389 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region t8,,1362725678436.311edabcc1fe52001cb00e7c3e7f75d4. to upgrade-vm-1.ent.cloudera.com,60020,1362709586485
2013-03-07 22:55:31,398 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=upgrade-vm-1.ent.cloudera.com,60020,1362709586485, region=311edabcc1fe52001cb00e7c3e7f75d4
2013-03-07 22:55:31,406 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_FAILED_OPEN, server=upgrade-vm-1.ent.cloudera.com,60020,1362709586485, region=311edabcc1fe52001cb00e7c3e7f75d4
2013-03-07 22:55:31,406 DEBUG org.apache.hadoop.hbase.master.handler.ClosedRegionHandler: Handling CLOSED event for 311edabcc1fe52001cb00e7c3e7f75d4
2013-03-07 22:55:31,406 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=t8,,1362725678436.311edabcc1fe52001cb00e7c3e7f75d4. state=CLOSED, ts=1362725731398, server=upgrade-vm-1.ent.cloudera.com,60020,1362709586485
2013-03-07 22:55:31,406 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x13d47d214830000 Creating (or updating) unassigned node for 311edabcc1fe52001cb00e7c3e7f75d4 with OFFLINE state
2013-03-07 22:55:31,414 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=M_ZK_REGION_OFFLINE, server=upgrade-vm-1.ent.cloudera.com:60000, region=311edabcc1fe52001cb00e7c3e7f75d4
Log lines on RS (repeats a few times/second):
2013-03-07 22:58:23,323 ERROR org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Failed open of region=t8,,1362725678436.311edabcc1fe52001cb00e7c3e7f75d4.
java.io.IOException: Compression algorithm 'lzo' previously failed test.
at org.apache.hadoop.hbase.util.CompressionTest.testCompression(CompressionTest.java:78)
at org.apache.hadoop.hbase.regionserver.HRegion.checkCompressionCodecs(HRegion.java:2797)
at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:2786)
at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:2774)
at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.openRegion(OpenRegionHandler.java:319)
at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:105)
at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:163)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:662)


Expected behavior:
We expect to fail fast (after a few retries). This should take <1 sec.", ,,1,,,,,
HBASE-8078,"This issue only happens when doing a Multi-put using the REST interface and with the JSON data format.

The bug is that the ""key"" entry must come before the ""Cell"" entry.  The ""key"" entry order shouldn't matter.  If it doesn't come first, the REST interface won't find the key and will use the URL's key over and over.  For example, if the row key in the URL is ""fakekey"", then every cell would be added to the same row despite the key being in the JSON.

Here is the workaround in Python:
	cell = OrderedDict([
		(""key"", rowKeyEncoded),
		(""Cell"", 
		[
			{ ""column"" : messagecolumnencoded, ""$"" : line },
			{ ""column"" : usernamecolumnencoded, ""$"" : usernameEncoded },
			{ ""column"" : linenumbercolumnencoded, ""$"" : lineNumberEncoded },
		])
	])", ,,,,,,,
HBASE-8081,I am interested in backporting HBASE-7213 to 0.94. Helps to address more of the MTTR story. Offline discussion with Lars indicated he is interested as well., ,,1,,,,,
HBASE-8092,"It can abort the master if node already exists, and as far as I see, if exists check fails it will get stuck forever (the latter I haven't seen though).", ,,,,,,,
HBASE-8096,"We're getting an NPE during replication, which causes replication for that RegionServer to stop until we restart it.

{noformat}
2013-03-10 12:49:12,679 ERROR org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Unexpected exception in ReplicationSource, currentPath=hdfs://hmaster1:9000/hbase/.logs/hslave1177,60020,1362549511446/hslave1177%2C60020%2C1362549511446.1362944946489
java.lang.NullPointerException
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.updateBlockInfo(DFSClient.java:1882)
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:1855)
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.<init>(DFSClient.java:1831)
        at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:578)
        at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:154)
        at org.apache.hadoop.fs.FilterFileSystem.open(FilterFileSystem.java:108)
        at org.apache.hadoop.io.SequenceFile$Reader.openFile(SequenceFile.java:1495)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader$WALReader.openFile(SequenceFileLogReader.java:62)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1482)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1475)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1470)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader$WALReader.<init>(SequenceFileLogReader.java:55)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.reset(SequenceFileLogReader.java:308)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationHLogReaderManager.openReader(ReplicationHLogReaderManager.java:69)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.openReader(ReplicationSource.java:505)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:313)
{noformat}

Some extra digging into the DataNode and NameNode logs makes this seem related to HBASE-7530 and HDFS-4380

Here's the relevant snipped portions of the RS, DN, and NN logs:
{noformat}
RS 2013-03-10 12:49:12,618 INFO org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager: Going to report log #hslave1177%2C60020%2C1362549511446.1362944946489 for position 59670826 in hdfs://hmaster1:9000/hbase/.logs/hslave1177,60020,1362549511446/hslave1177%2C60020%2C1362549511446.1362944946489
RS 2013-03-10 12:49:12,621 DEBUG org.apache.hadoop.hbase.regionserver.LogRoller: HLog roll requested
RS 2013-03-10 12:49:12,623 DEBUG org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Replicated in total: 31500300
RS 2013-03-10 12:49:12,623 DEBUG org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Opening log for replication hslave1177%2C60020%2C1362549511446.1362944946489 at 59670826
NN 2013-03-10 12:49:12,627 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /hbase/.logs/hslave1177,60020,1362549511446/hslave1177%2C60020%2C1362549511446.1362944946489. blk_6905758215335505153_656717631
RS 2013-03-10 12:49:12,679 ERROR org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Unexpected exception in ReplicationSource, currentPath=hdfs://hmaster1:9000/hbase/.logs/hslave1177,60020,1362549511446/hslave1177%2C60020%2C1362549511446.1362944946489
DN 2013-03-10 12:49:12,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving block blk_6905758215335505153_656717631 src: /192.168.44.1:43503 dest: /192.168.44.1:50010
NN 2013-03-10 12:49:12,804 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.fsync: file /hbase/.logs/hslave1177,60020,1362549511446/hslave1177%2C60020%2C1362549511446.1362944946489 for DFSClient_hb_rs_hslave1177,60020,1362549511446
{noformat}", ,,,,,,,
HBASE-8097,"{code}
    } catch (IOException ioe) {
      this.services.getExecutorService().submit(this);
      this.deadServers.add(serverName);
      throw new IOException(""failed log splitting for "" +
          serverName + "", will retry"", ioe);
    }
{code}

this.deadServers.add(serverName); will keep incrementing DeadServer.numProcessing

We can't get rid of numProcessing by just checking deadServers.size() because deadServers is also used to report some historically failed RSs.

", ,,,,,,,
HBASE-8099,"We just ran into an interesting scenario. We restarted a cluster that was setup as a replication source.
The stop went cleanly.

Upon restart *all* regionservers aborted within a few seconds with variations of these errors:
http://pastebin.com/3iQVuBqS
", ,,,,,,,
HBASE-8119,"On a 5 node trunk cluster, I ran into a weird problem with StochasticLoadBalancer:

server1 	Thu Mar 14 03:42:50 UTC 2013 	0.0 	33
server2 	Thu Mar 14 03:47:53 UTC 2013 	0.0 	34
server3 	Thu Mar 14 03:46:53 UTC 2013 	465.0 	42
server4 	Thu Mar 14 03:47:53 UTC 2013 	11455.0 	282
server5 	Thu Mar 14 03:47:53 UTC 2013 	0.0 	34
Total:5 		11920 	425

Notice that server4 has 282 regions, while the others have much less. Plus for one table with 260 regions has been super imbalanced:
{code}
Regions by Region Server
Region Server	Region Count
http://server3:60030/ 	10
http://server4:60030/ 	250
{code}

", ,,1,,,,,
HBASE-8126,"Go to log4j.properties. Note this sequence of definitions:

{noformat}
hbase.root.logger=INFO,console
log4j.rootLogger=${hbase.root.logger}
{noformat}

One would think they could change hbase.root.logger to something else. Say ""INFO,console,SYSLOG"" and then define syslogging, but they'd be wrong. The syslog won't take effect. The second line will always go back to ""INFO,console"" (or something, we're not able to determine what it's reverting to exactly).

What's happening is that the HBASE_ROOT_LOGGER environment variable from hbase-env.sh ALWAYS overwrites that variable before it's used.

However, in our environment, HBASE_ROOT_LOGGER wasn't being defined. It was commented out. Still, something sets that environment variable to a default and uses it, always clobbering the log4j.properties hbase.root.logger.

Suggestion: either don't define hbase.root.logger in log4j.properties at all (instead place a comment stating that HBASE_ROOT_LOGGER environment variable in hbase-env.sh is the proper place to change it), or have modifications in log4j take precedence (and comment such in hbase-env.sh).", ,,,,,,,
HBASE-8127,"The issue happens when a RS dies during a master starts up. After the RS reports open to the new master instance and dies immediately thereafter, the RITs of disabling tables(or disabled table) on the died RS will be in RIT state forever.

I attached a patch to simulate the situation and you can run the following command to reproduce the issue:

{code}mvn test -PlocalTests -Dtest=TestMasterFailover#testMasterFailoverWithMockedRITOnDeadRS{code}

Basically, we skip regions of a dead server inside AM.processDeadServersAndRecoverLostRegions as the following code and relies on SSH to process those skipped regions:
{code}
          for (Pair<HRegionInfo, Result> deadRegion : deadServer.getValue()) {
            nodes.remove(deadRegion.getFirst().getEncodedName());
          }
{code} 

While in SSH, we skip regions of disabling(or disabled table) again by function processDeadRegion. Finally comes to the issue that RITs of disabling(or disabled table) stuck there forever.
 ", ,,,,,,,
HBASE-8130,"Getting NullPointerException while recovering disabling/enabling tables in AM.
AM.tableLockManager is not pointing to HM.tableLockManager initalized after AM initialization.So its always null.
", ,,,,,,,
HBASE-8131,"In CreateTable Handler there are number of failure cases.  
IOExceptions are common while creation of regioninfos, htableDescriptors etc.

After this exception if i try to recreate the table using admin, we need to remove the acquired table lock and also clear the ZKTable in memory cache so that the operation can be retried.", ,,,,,,,
HBASE-8140,"TableMapReduceUtils#addDependencyJar is used when configuring a mapreduce job to make sure dependencies of the job are shipped to the cluster. The code depends on finding an actual jar file containing the necessary classes. This is not always the case, for instance, when run at the end of another mapreduce job. In that case, dependency jars have already been shipped to the cluster and expanded in the parent job's run folder. Those dependencies are there, just not available as jars.", ,1,,,,,,
HBASE-8143,"We've run into an issue with HBase 0.94 on Hadoop2, with SSR turned on that the memory usage of the HBase process grows to 7g, on an -Xmx3g, after some time, this causes OOM for the RSs. 

Upon further investigation, I've found out that we end up with 200 regions, each having 3-4 store files open. Under hadoop2 SSR, BlockReaderLocal allocates DirectBuffers, which is unlike HDFS 1 where there is no direct buffer allocation. 

It seems that there is no guards against the memory used by local buffers in hdfs 2, and having a large number of open files causes multiple GB of memory to be consumed from the RS process. 

This issue is to further investigate what is going on. Whether we can limit the memory usage in HDFS, or HBase, and/or document the setup. 

Possible mitigation scenarios are: 
 - Turn off SSR for Hadoop 2
 - Ensure that there is enough unallocated memory for the RS based on expected # of store files
 - Ensure that there is lower number of regions per region server (hence number of open files)

Stack trace:
{code}
org.apache.hadoop.hbase.DroppedSnapshotException: region: IntegrationTestLoadAndVerify,yC^P\xD7\x945\xD4,1363388517630.24655343d8d356ef708732f34cfe8946.
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1560)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1439)
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:1380)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:449)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushOneForGlobalPressure(MemStoreFlusher.java:215)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.access$500(MemStoreFlusher.java:63)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushHandler.run(MemStoreFlusher.java:237)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.OutOfMemoryError: Direct buffer memory
        at java.nio.Bits.reserveMemory(Bits.java:632)
        at java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:97)
        at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:288)
        at org.apache.hadoop.hdfs.util.DirectBufferPool.getBuffer(DirectBufferPool.java:70)
        at org.apache.hadoop.hdfs.BlockReaderLocal.<init>(BlockReaderLocal.java:315)
        at org.apache.hadoop.hdfs.BlockReaderLocal.newBlockReader(BlockReaderLocal.java:208)
        at org.apache.hadoop.hdfs.DFSClient.getLocalBlockReader(DFSClient.java:790)
        at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:888)
        at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
        at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:645)
        at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:689)
        at java.io.DataInputStream.readFully(DataInputStream.java:178)
        at org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.readFromStream(FixedFileTrailer.java:312)
        at org.apache.hadoop.hbase.io.hfile.HFile.pickReaderVersion(HFile.java:543)
        at org.apache.hadoop.hbase.io.hfile.HFile.createReaderWithEncoding(HFile.java:589)
        at org.apache.hadoop.hbase.regionserver.StoreFile$Reader.<init>(StoreFile.java:1261)
        at org.apache.hadoop.hbase.regionserver.StoreFile.open(StoreFile.java:512)
        at org.apache.hadoop.hbase.regionserver.StoreFile.createReader(StoreFile.java:603)
        at org.apache.hadoop.hbase.regionserver.Store.validateStoreFile(Store.java:1568)
        at org.apache.hadoop.hbase.regionserver.Store.commitFile(Store.java:845)
        at org.apache.hadoop.hbase.regionserver.Store.access$500(Store.java:109)
        at org.apache.hadoop.hbase.regionserver.Store$StoreFlusherImpl.commit(Store.java:2209)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1541)
{code}", ,1,1,,,,,
HBASE-8150,"The code in 0.94 AM sets the region plan to point to the same server when retrying the assignment due to RAITE.
{code}
LOG.warn(""Failed assignment of ""
            + state.getRegion().getRegionNameAsString()
            + "" to ""
            + plan.getDestination()
            + "", trying to assign ""
            + (regionAlreadyInTransitionException ? ""to the same region server""
                + "" because of RegionAlreadyInTransitionException;"" : ""elsewhere instead; "")
            + ""retry="" + i, t);
{code}

However, there's no wait time in the loop that retries the assignment, and if region is being marked failed to open, which may take some time, master can easily exhaust retries in less than half a second, going to the same server every time and getting the same exception (unfortunately I no longer have logs); then the region will be stuck.

Do you think this is worth fixing (for example, by not using the same server here after a few retries, or by adding timed backoff in such cases)?", ,,,,,,,
HBASE-8151,"HFiles V2 store the memstoreTS of each KV.
In many cases all the KVs in an HFile will have a memstoreTS of 0 (that is the case when at the time the HFile was written there are no KVs that were created after the oldest still active scanner - which is frequently the case).
In that case we:
# do not need to decode the memstoreTS (a vlong), since we know its value is 0 and its length is 1 byte.
# when we compact HFiles and all of the involved files have only KVs with memstoreTS = 0 we know ahead of time that all KVs meet this condition and we do not need to store the memstoreTS in the new HFile.

This issue will cover the first part. The performance improvement will be modest as it is fairly cheap to decode vlongs of size 1.
", ,,1,,,,,
HBASE-8160,"Before the master is initialized, if a move request comes in, it could cause the master to abort, if the region was already in transition before the master started up.", ,,,,,,,
HBASE-8161,MemStoreFlusher always uses server-wide config... it should ask the store correct question instead, ,1,,,,,,
HBASE-8163,"HFiles V2 store the memstoreTS of each KV.
In many cases all the KVs in an HFile will have a memstoreTS of 0 (that is the case when at the time the HFile was written there are no KVs that were created after the oldest still active scanner - which is frequently the case).
In that case we:
# do not need to decode the memstoreTS (a vlong), since we know its value is 0 and its length is 1 byte.
# when we compact HFiles and all of the involved files have only KVs with memstoreTS = 0 we know ahead of time that all KVs meet this condition and we do not need to store the memstoreTS in the new HFile.

This issue will cover the first part. The performance improvement will be modest as it is fairly cheap to decode vlongs of size 1.
", ,,1,,,,,
HBASE-8166,"See discussion in HBASE-8151.
Upon compaction we can check all the involved HFiles. If all of them have a MAX_MEMSTORE_TS < the smallest scannner readpoint, we create a writer that does not write the memstoreTS.", ,,1,,,,,
HBASE-8167,"See discussion in HBASE-8151.
Upon compaction we can check all the involved HFiles. If all of them have a MAX_MEMSTORE_TS < the smallest scannner readpoint, we create a writer that does not write the memstoreTS.", ,,1,,,,,
HBASE-8170,"createTable(HTableDescriptor desc, byte [] startKey, byte [] endKey, int numRegions) (line #370) dictates that you must specify a minimum of three regions, however is not able to handle being fed a value three. This is a result of line #379 where it attempts to create the key splits, and calls Bytes.Split with a value of 0 for the third parameter. createTable should instead just create a byte[][] with the startKey and endKey in this scenario.", ,1,,,,,,
HBASE-8171,"HBaseClient maintains a cache of Regions to region locations that gets cleared whenever the RS that the client talks to is unresponsive.

This will result in the Client having to talk to META to fetch the region location before doing the get/put.

https://issues.apache.org/jira/browse/HBASE-2468 introduces the feature to prewarm the cache, where the client gets some rows from META to warm the cache.

This is a costly operation. And,  it looks like all client threads that block on the regionLockObject, will go in and do the cache warmup; although the cache might have been already warmed up by an earlier thread. 

When there is a network issue going on, we see lots of Client threads blocking in locateRegionInMeta.


The fix would be to check the cache again; before we do the prefetch after grabbing the lock. Currently, the client threads look in the cache at the begining of the method. But a lot could have changed while waiting for the regionLockObject.


", ,,1,,,,,
HBASE-8173,HMaster#move wraps IOException in UnknownRegionException since the client expects UnknownRegionException. We should use IOException instead. This changes the client interface (HBaseAdmin).  So 0.95 is the right time., ,,,,,,,
HBASE-8174,"HBaseClient maintains a cache of Regions to region locations that gets cleared whenever the RS that the client talks to is unresponsive.

This will result in the Client having to talk to META to fetch the region location before doing the get/put.

https://issues.apache.org/jira/browse/HBASE-2468 introduces the feature to prewarm the cache, where the client gets some rows from META to warm the cache.

This is a costly operation. And,  it looks like all client threads that block on the regionLockObject, will go in and do the cache warmup; although the cache might have been already warmed up by an earlier thread. 

When there is a network issue going on, we see lots of Client threads blocking in locateRegionInMeta.


The fix would be to check the cache again; before we do the prefetch after grabbing the lock. Currently, the client threads look in the cache at the begining of the method. But a lot could have changed while waiting for the regionLockObject.


", ,1,,,,,,
HBASE-8176,"With HBASE-5335, we can support per-table configuration and per-family configurations.

We can use it to customize the compaction on table/family basis, customize the flush, and etc..", ,1,,,,,,
HBASE-8185,"HBaseClient maintains a cache of Regions to region locations that gets cleared whenever the RS that the client talks to is unresponsive.

This will result in the Client having to talk to META to fetch the region location before doing the get/put.

https://issues.apache.org/jira/browse/HBASE-2468 introduces the feature to prewarm the cache, where the client gets some rows from META to warm the cache.

This is a costly operation. And,  it looks like all client threads that block on the regionLockObject, will go in and do the cache warmup; although the cache might have been already warmed up by an earlier thread. 

When there is a network issue going on, we see lots of Client threads blocking in locateRegionInMeta.


The fix would be to check the cache again; before we do the prefetch after grabbing the lock. Currently, the client threads look in the cache at the begining of the method. But a lot could have changed while waiting for the regionLockObject.


", ,,1,,,,,
HBASE-8188,"When scanner batching disable (which is the default) a row compare in StoreScanner avoided, because only entire rows of data are requested.
This provides a slight performance gain, especially for tall tables (and when all data is in the cache)", ,,1,,,,,
HBASE-8192,"the wrong logic is here:
  when a ColumnFamily does not exist, it gets a null store object, then ioes.add(ioe); failures.add(p)
  but the code below, if (failures.size() != 0), it prints a warn log and return false, so it will never go into the code if (ioes.size() != 0) below, and IOException will not be thrown, then the client will keep retry forever.
  there is the same situation when doing store.assertBulkLoadHFileOk, if any WrongRegionException is caught and failures.add(p), then all the other IOException thrown by assertBulkLoadHFileOk will be ignored.

  so i think if (failures.size() != 0) {} should be dealt with after if (ioes.size() !=0) {}

{code}
for (Pair<byte[], String> p : familyPaths) {
    byte[] familyName = p.getFirst();
    String path = p.getSecond();

    Store store = getStore(familyName);
    if (store == null) {
        IOException ioe = new DoNotRetryIOException(
                ""No such column family "" + Bytes.toStringBinary(familyName));
        ioes.add(ioe);
        failures.add(p);
    } else {
        try {
            store.assertBulkLoadHFileOk(new Path(path));
        } catch (WrongRegionException wre) {
            // recoverable (file doesn't fit in region)
            failures.add(p);
        } catch (IOException ioe) {
            // unrecoverable (hdfs problem)
            ioes.add(ioe);
        }
    }
}


// validation failed, bail out before doing anything permanent.
if (failures.size() != 0) {
    StringBuilder list = new StringBuilder();
    for (Pair<byte[], String> p : failures) {
        list.append(""\n"").append(Bytes.toString(p.getFirst())).append("" : "")
            .append(p.getSecond());
    }
    // problem when validating
    LOG.warn(""There was a recoverable bulk load failure likely due to a"" +
            "" split.  These (family, HFile) pairs were not loaded: "" + list);
    return false;
}

// validation failed because of some sort of IO problem.
if (ioes.size() != 0) {
    LOG.error(""There were IO errors when checking if bulk load is ok.  "" +
            ""throwing exception!"");
    throw MultipleIOException.createIOException(ioes);
}
{code}
", ,,,,,,,
HBASE-8193,"When scanner batching disable (which is the default) a row compare in StoreScanner avoided, because only entire rows of data are requested.
This provides a slight performance gain, especially for tall tables (and when all data is in the cache)", ,,1,,,,,
HBASE-8204,See patch :-)., ,,,,,,,
HBASE-8205,"When scanner batching disable (which is the default) a row compare in StoreScanner avoided, because only entire rows of data are requested.
This provides a slight performance gain, especially for tall tables (and when all data is in the cache)", ,1,,,,,,
HBASE-8207,"In the recent test case TestReplication* failures, I'm finally able to find the cause(or one of causes) for its intermittent failures.

When a machine name contains ""-"", it breaks the function ReplicationSource.checkIfQueueRecovered. It causes the following issue:

deadRegionServers list is way off so that replication doesn't wait for log splitting finish for a wal file and move on to the next one(data loss)

You can see that replication use those weird paths constructed from deadRegionServers to check a file existence
{code}
2013-03-26 21:26:51,385 INFO  [ReplicationExecutor-0.replicationSource,2-ip-10-197-0-156.us-west-1.compute.internal,52170,1364333181125] regionserver.ReplicationSource(524): Possible location hdfs://localhost:52882/user/ec2-user/hbase/.logs/1.compute.internal,52170,1364333181125/ip-10-197-0-156.us-west-1.compute.internal%252C52170%252C1364333181125.1364333199540
2013-03-26 21:26:51,386 INFO  [ReplicationExecutor-0.replicationSource,2-ip-10-197-0-156.us-west-1.compute.internal,52170,1364333181125] regionserver.ReplicationSource(524): Possible location hdfs://localhost:52882/user/ec2-user/hbase/.logs/1.compute.internal,52170,1364333181125-splitting/ip-10-197-0-156.us-west-1.compute.internal%252C52170%252C1364333181125.1364333199540
2013-03-26 21:26:51,387 INFO  [ReplicationExecutor-0.replicationSource,2-ip-10-197-0-156.us-west-1.compute.internal,52170,1364333181125] regionserver.ReplicationSource(524): Possible location hdfs://localhost:52882/user/ec2-user/hbase/.logs/west/ip-10-197-0-156.us-west-1.compute.internal%252C52170%252C1364333181125.1364333199540
2013-03-26 21:26:51,389 INFO  [ReplicationExecutor-0.replicationSource,2-ip-10-197-0-156.us-west-1.compute.internal,52170,1364333181125] regionserver.ReplicationSource(524): Possible location hdfs://localhost:52882/user/ec2-user/hbase/.logs/west-splitting/ip-10-197-0-156.us-west-1.compute.internal%252C52170%252C1364333181125.1364333199540
2013-03-26 21:26:51,391 INFO  [ReplicationExecutor-0.replicationSource,2-ip-10-197-0-156.us-west-1.compute.internal,52170,1364333181125] regionserver.ReplicationSource(524): Possible location hdfs://localhost:52882/user/ec2-user/hbase/.logs/156.us/ip-10-197-0-156.us-west-1.compute.internal%252C52170%252C1364333181125.1364333199540
2013-03-26 21:26:51,394 INFO  [ReplicationExecutor-0.replicationSource,2-ip-10-197-0-156.us-west-1.compute.internal,52170,1364333181125] regionserver.ReplicationSource(524): Possible location hdfs://localhost:52882/user/ec2-user/hbase/.logs/156.us-splitting/ip-10-197-0-156.us-west-1.compute.internal%252C52170%252C1364333181125.1364333199540
2013-03-26 21:26:51,396 INFO  [ReplicationExecutor-0.replicationSource,2-ip-10-197-0-156.us-west-1.compute.internal,52170,1364333181125] regionserver.ReplicationSource(524): Possible location hdfs://localhost:52882/user/ec2-user/hbase/.logs/0/ip-10-197-0-156.us-west-1.compute.internal%252C52170%252C1364333181125.1364333199540
2013-03-26 21:26:51,398 INFO  [ReplicationExecutor-0.replicationSource,2-ip-10-197-0-156.us-west-1.compute.internal,52170,1364333181125] regionserver.ReplicationSource(524): Possible location hdfs://localhost:52882/user/ec2-user/hbase/.logs/0-splitting/ip-10-197-0-156.us-west-1.compute.internal%252C52170%252C1364333181125.1364333199540
{code}

This happened in the recent test failure in http://54.241.6.143/job/HBase-0.94/org.apache.hbase$hbase/21/testReport/junit/org.apache.hadoop.hbase.replication/TestReplicationQueueFailover/queueFailover/?auto_refresh=false

Search for 
{code}
File does not exist: hdfs://localhost:52882/user/ec2-user/hbase/.oldlogs/ip-10-197-0-156.us-west-1.compute.internal%2C52170%2C1364333181125.1364333199540
{code}

After 10 times retries, replication source gave up and move on to the next file. Data loss happens. 

Since lots of EC2 machine names contain ""-"" including our Jenkin servers, this is a high impact issue.", ,1,,,,,,
HBASE-8208,"This is a subtle issue. When deferredLogSync is enabled, there are chances we could flush data before syncing all HLog entries. Assuming we just flush the internal cache and the server dies with some unsynced hlog entries. 

Data is not lost at the source cluster while replication is based on WAL files and some changes we flushed at the source won't be replicated the slave clusters. 

Although enabling deferredLogSync with tolerances of data loss, it breaks the replication assumption that whatever persisted in the source should be replicated to its slave clusters. 

In short, the slave cluster could end up with double losses: the data loss in the source and some data stored in source cluster may not be replicated to slaves either.

The fix of the issue isn't hard. Basically we can invoke sync during each flush when replication is enabled for a region server. Since sync returns immediately when nothing to sync so there should be no performance impact.

Please let me know what you think!

Thanks,
-Jeffrey", ,1,,,,,,
HBASE-8213,"It depends on the order of which region be opened first.  
Suppose we have one 1 regionserver and only 1 user region REGION-A on this server, _acl_ region was on another regionserver. _acl_ was opened a few seconds before REGION-A.
The global authorization data read from Zookeeper was overwritten by the data read from configuration.
{code}
  private TableAuthManager(ZooKeeperWatcher watcher, Configuration conf)
      throws IOException {
    this.conf = conf;
    this.zkperms = new ZKPermissionWatcher(watcher, this, conf);
    try {
	  // Read global authorization data from zookeeper. 
      this.zkperms.start();
    } catch (KeeperException ke) {
      LOG.error(""ZooKeeper initialization failed"", ke);
    }
    // It will overwrite globalCache.
    // initialize global permissions based on configuration
    globalCache = initGlobal(conf);
  }
{code}

This issue can be easily reproduced by below steps:
1. Start a cluster with 3 regionservers.
2. Create a new table T1.
3. grant a new user USER-A with global authorization.
4. Kill 1 regionserver RS3 and switch balance off.
5. Start regionserver RS3.
6. Assign region T1 to RS3.
7. Put data with user USER-A.",1,,,,,,,
HBASE-8214,"Attached patch is not done.  Removes two to three layers -- depending on how you count -- between client and rpc client and similar on server side (between rpc and server implementation).  Strips ProtobufRpcServer/Client and HBaseClientRpc/HBaseServerRpc.  Also gets rid of proxy.

", ,,1,,,,,
HBASE-8215,"If HRegion#writeRegioninfoOnFilesystem:

{code}
    if (fs.exists(regioninfoPath) &&
        fs.getFileStatus(regioninfoPath).getLen() > 0) {
      return;
    }
{code}

If the file exists and its length is 0, the file should be removed.  This issue has been fixed in trunk with HBASE-7807.  We need to fix it in 0.94 as well. Otherwise, we will get this error:

{noformat}
2013-03-27 16:57:27,143 ERROR org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Failed open of region=t1,r4_38,1364392356229.dec30602f4805a3e640871185ae900aa.
java.io.IOException: Unable to rename hdfs://hbase-6.ent.cloudera.com:17020/hbase/t1/dec30602f4805a3e640871185ae900aa/.tmp/.regioninfo to hdfs://hbase-6.ent.cloudera.com:17020/hbase/t1/dec30602f4805a3e640871185ae900aa/.regioninfo
	at org.apache.hadoop.hbase.regionserver.HRegion.checkRegioninfoOnFilesystem(HRegion.java:639)
	at org.apache.hadoop.hbase.regionserver.HRegion.initialize(HRegion.java:426)
	at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:3308)
	at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:3256)
	at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.openRegion(OpenRegionHandler.java:331)
	at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:107)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:169)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{noformat}

And the region can't be opened, so that stuck in transition.
", ,1,,,,,,
HBASE-8216,"The master in 0.89-fb waits for 5-6 mins to check if RS'es become accessible; when it sees a co-related failure such as a rack-switch-reboot.

The rationale behind doing this is that it is not worth assigning and reassigning regions -- causing churn, when the rack switch reboots are expected to heal themselves in 5-6 mins. In earlier deployments, where this feature was not present, we used to find ourselves in a bad situation for 30mins-1hr.

However, co-related failures also happen when there is a power failure for the rack. These cases take much longer to heal; so waiting for 5-6 mins is a wasted effort.

The master should be able to differentiate the two scenario, by checking if *any* of the RS in the rack is able to communicate. Unless all the servers in the rack are unaccessible, we should proceed with reassigning the regions.
", ,,1,,,,,
HBASE-8226,"HBaseTestingUtility#waitUntilAllRegionsAssigned does not check if the region it is counting belongs to the table created by the test and will not return if it accidentally counts ""too many"" regions, for example the regions of the ACL table when security is enabled.

Made an attempt at fixing this as part of HBASE-8209 but broke TestMasterTransitions. Try again here.", ,1,,,,,,
HBASE-8230,"RegionServer got Exception on calling setupWALAndReplication, so entered abort flow. Since replicationSink had not been inialized yet, we got below exception:
{noformat}
Exception in thread ""regionserver26003"" java.lang.NullPointerException
 at org.apache.hadoop.hbase.replication.regionserver.Replication.join(Replication.java:129)
 at org.apache.hadoop.hbase.replication.regionserver.Replication.stopReplicationService(Replication.java:120)
 at org.apache.hadoop.hbase.regionserver.HRegionServer.join(HRegionServer.java:1803)
 at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:834)
 at java.lang.Thread.run(Thread.java:662)
{noformat}
", ,1,,,,,,
HBASE-8241,"Even if it's fixed on trunk & branch-2, the current version of hdfs still has a previous version of netty, with a different group id. Let's fix this.", ,1,,,,,,
HBASE-8242,"HBase 0.95.0RC0 is failing out of the box because of some ZooKeeper exceptions to write in /var/folders/

jmspaggi@virtual:~/hbase-0.95.0-hadoop1$ bin/start-hbase.sh 
jmspaggi@virtual:~/hbase-0.95.0-hadoop1$ tail -100f logs/hbase-jmspaggi-master-virtual.log 
mardi 2 avril 2013, 07:24:13 (UTC-0400) Starting master on virtual
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 31634
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 1024
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 31634
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
2013-04-02 07:24:16,093 INFO org.apache.hadoop.hbase.util.VersionInfo: HBase 0.95.0-hadoop1
2013-04-02 07:24:16,132 INFO org.apache.hadoop.hbase.util.VersionInfo: Subversion file:///Users/stack/checkouts/0.95/hbase-common -r Unknown
2013-04-02 07:24:16,132 INFO org.apache.hadoop.hbase.util.VersionInfo: Compiled by stack on Mon Apr  1 15:38:48 PDT 2013
2013-04-02 07:24:17,475 INFO org.apache.zookeeper.server.ZooKeeperServer: Server environment:zookeeper.version=3.4.5-1392090, built on 09/30/2012 17:52 GMT
2013-04-02 07:24:17,475 INFO org.apache.zookeeper.server.ZooKeeperServer: Server environment:host.name=virtual.distparser.com
2013-04-02 07:24:17,586 INFO org.apache.zookeeper.server.ZooKeeperServer: Server environment:java.version=1.7.0_13
2013-04-02 07:24:17,587 INFO org.apache.zookeeper.server.ZooKeeperServer: Server environment:java.vendor=Oracle Corporation
2013-04-02 07:24:17,587 INFO org.apache.zookeeper.server.ZooKeeperServer: Server environment:java.home=/home/jmspaggi/jdk/jre
2013-04-02 07:24:17,587 INFO org.apache.zookeeper.server.ZooKeeperServer: Server environment:java.class.path=/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../conf:/home/jmspaggi/jdk//lib/tools.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/..:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/activation-1.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/asm-3.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-beanutils-1.7.0.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-cli-1.2.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-codec-1.7.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-collections-3.2.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-configuration-1.6.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-digester-1.8.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-el-1.0.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-httpclient-3.0.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-io-2.4.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-lang-2.6.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-logging-1.1.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-math-2.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-net-1.4.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/core-3.1.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/guava-12.0.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hadoop-core-1.1.2.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-client-0.95.0-hadoop1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-common-0.95.0-hadoop1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-common-0.95.0-hadoop1-tests.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-examples-0.95.0-hadoop1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-hadoop1-compat-0.95.0-hadoop1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-hadoop-compat-0.95.0-hadoop1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-it-0.95.0-hadoop1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-it-0.95.0-hadoop1-tests.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-prefix-tree-0.95.0-hadoop1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-protocol-0.95.0-hadoop1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-server-0.95.0-hadoop1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-server-0.95.0-hadoop1-tests.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/high-scale-lib-1.1.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/htrace-1.50.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/httpclient-4.1.3.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/httpcore-4.1.3.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jackson-core-asl-1.8.8.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jackson-xc-1.8.8.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jamon-runtime-2.3.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jasper-compiler-5.5.23.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jasper-runtime-5.5.23.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jaxb-api-2.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jersey-core-1.8.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jersey-json-1.8.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jersey-server-1.8.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jetty-6.1.26.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jetty-util-6.1.26.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jruby-complete-1.6.8.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jsp-2.1-6.1.14.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jsr305-1.3.9.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/libthrift-0.9.0.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/log4j-1.2.17.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/metrics-core-2.1.2.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/netty-3.5.9.Final.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/protobuf-java-2.4.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/slf4j-api-1.4.3.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/slf4j-log4j12-1.4.3.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/stax-api-1.0.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/xmlenc-0.52.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/zookeeper-3.4.5.jar:/etc/hadoop:/usr/lib/jvm/java-6-sun/lib/tools.jar:/usr/libexec/../share/hadoop:/usr/libexec/../share/hadoop/hadoop-core-1.1.1.jar:/usr/libexec/../share/hadoop/lib/asm-3.2.jar:/usr/libexec/../share/hadoop/lib/aspectjrt-1.6.11.jar:/usr/libexec/../share/hadoop/lib/aspectjtools-1.6.11.jar:/usr/libexec/../share/hadoop/lib/commons-beanutils-1.7.0.jar:/usr/libexec/../share/hadoop/lib/commons-beanutils-core-1.8.0.jar:/usr/libexec/../share/hadoop/lib/commons-cli-1.2.jar:/usr/libexec/../share/hadoop/lib/commons-codec-1.4.jar:/usr/libexec/../share/hadoop/lib/commons-collections-3.2.1.jar:/usr/libexec/../share/hadoop/lib/commons-configuration-1.6.jar:/usr/libexec/../share/hadoop/lib/commons-daemon-1.0.1.jar:/usr/libexec/../share/hadoop/lib/commons-digester-1.8.jar:/usr/libexec/../share/hadoop/lib/commons-el-1.0.jar:/usr/libexec/../share/hadoop/lib/commons-httpclient-3.0.1.jar:/usr/libexec/../share/hadoop/lib/commons-io-2.1.jar:/usr/libexec/../share/hadoop/lib/commons-lang-2.4.jar:/usr/libexec/../share/hadoop/lib/commons-logging-1.1.1.jar:/usr/libexec/../share/hadoop/lib/commons-logging-api-1.0.4.jar:/usr/libexec/../share/hadoop/lib/commons-math-2.1.jar:/usr/libexec/../share/hadoop/lib/commons-net-3.1.jar:/usr/libexec/../share/hadoop/lib/core-3.1.1.jar:/usr/libexec/../share/hadoop/lib/hadoop-capacity-scheduler-1.1.1.jar:/usr/libexec/../share/hadoop/lib/hadoop-fairscheduler-1.1.1.jar:/usr/libexec/../share/hadoop/lib/hadoop-thriftfs-1.1.1.jar:/usr/libexec/../share/hadoop/lib/hsqldb-1.8.0.10.jar:/usr/libexec/../share/hadoop/lib/jackson-core-asl-1.8.8.jar:/usr/libexec/../share/hadoop/lib/jackson-mapper-asl-1.8.8.jar:/usr/libexec/../share/hadoop/lib/jasper-compiler-5.5.12.jar:/usr/libexec/../share/hadoop/lib/jasper-runtime-5.5.12.jar:/usr/libexec/../share/hadoop/lib/jdeb-0.8.jar:/usr/libexec/../share/hadoop/lib/jersey-core-1.8.jar:/usr/libexec/../share/hadoop/lib/jersey-json-1.8.jar:/usr/libexec/../share/hadoop/lib/jersey-server-1.8.jar:/usr/libexec/../share/hadoop/lib/jets3t-0.6.1.jar:/usr/libexec/../share/hadoop/lib/jetty-6.1.26.jar:/usr/libexec/../share/hadoop/lib/jetty-util-6.1.26.jar:/usr/libexec/../share/hadoop/lib/jsch-0.1.42.jar:/usr/libexec/../share/hadoop/lib/junit-4.5.jar:/usr/libexec/../share/hadoop/lib/kfs-0.2.2.jar:/usr/libexec/../share/hadoop/lib/log4j-1.2.15.jar:/usr/libexec/../share/hadoop/lib/mockito-all-1.8.5.jar:/usr/libexec/../share/hadoop/lib/oro-2.0.8.jar:/usr/libexec/../share/hadoop/lib/servlet-api-2.5-20081211.jar:/usr/libexec/../share/hadoop/lib/slf4j-api-1.4.3.jar:/usr/libexec/../share/hadoop/lib/slf4j-log4j12-1.4.3.jar:/usr/libexec/../share/hadoop/lib/xmlenc-0.52.jar:/usr/libexec/../share/hadoop/lib/jsp-2.1/jsp-2.1.jar:/usr/libexec/../share/hadoop/lib/jsp-2.1/jsp-api-2.1.jar
2013-04-02 07:24:17,588 INFO org.apache.zookeeper.server.ZooKeeperServer: Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
2013-04-02 07:24:17,588 INFO org.apache.zookeeper.server.ZooKeeperServer: Server environment:java.io.tmpdir=/tmp
2013-04-02 07:24:17,588 INFO org.apache.zookeeper.server.ZooKeeperServer: Server environment:java.compiler=<NA>
2013-04-02 07:24:17,589 INFO org.apache.zookeeper.server.ZooKeeperServer: Server environment:os.name=Linux
2013-04-02 07:24:17,589 INFO org.apache.zookeeper.server.ZooKeeperServer: Server environment:os.arch=amd64
2013-04-02 07:24:17,589 INFO org.apache.zookeeper.server.ZooKeeperServer: Server environment:os.version=3.2.0-4-amd64
2013-04-02 07:24:17,589 INFO org.apache.zookeeper.server.ZooKeeperServer: Server environment:user.name=jmspaggi
2013-04-02 07:24:17,589 INFO org.apache.zookeeper.server.ZooKeeperServer: Server environment:user.home=/home/jmspaggi
2013-04-02 07:24:17,589 INFO org.apache.zookeeper.server.ZooKeeperServer: Server environment:user.dir=/home/jmspaggi/hbase-0.95.0-hadoop1
2013-04-02 07:24:17,598 ERROR org.apache.hadoop.hbase.master.HMasterCommandLine: Failed to start master
java.io.IOException: Unable to create data directory /var/folders/bp/2z1cykc92rs6j24251cg__ph0000gp/T/hbase-stack/zookeeper/zookeeper_0/version-2
	at org.apache.zookeeper.server.persistence.FileTxnSnapLog.<init>(FileTxnSnapLog.java:85)
	at org.apache.zookeeper.server.ZooKeeperServer.<init>(ZooKeeperServer.java:213)
	at org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.startup(MiniZooKeeperCluster.java:161)
	at org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.startup(MiniZooKeeperCluster.java:131)
	at org.apache.hadoop.hbase.master.HMasterCommandLine.startMaster(HMasterCommandLine.java:137)
	at org.apache.hadoop.hbase.master.HMasterCommandLine.run(HMasterCommandLine.java:107)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:78)
	at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:2482)
", ,,,,,,,
HBASE-8246,"Today we found the following error in our tests. Later I found we already fixed the issue in trunk. I think we should backpor the fix because the consequence of the issue is high and the fix isn't complicated.

{code}
2013-04-01 21:23:21,864 INFO org.apache.hadoop.hbase.regionserver.SplitLogWorker: worker ip-10-143-160-121.ec2.internal,60020,1364849529986 done with task /hbase/splitlog/hdfs%3A%2F%2Fip-10-137-16-140.ec2.internal%3A8020%2Fapps%2Fhbase%2Fdata%2F.logs%2Fip-10-137-20-188.ec2.internal%2C60020%2C1364849530779-splitting%2Fip-10-137-20-188.ec2.internal%252C60020%252C1364849530779.1364865556657 in 67129ms
2013-04-01 21:23:21,864 ERROR org.apache.hadoop.hbase.regionserver.SplitLogWorker: unexpected error
java.util.ConcurrentModificationException
        at java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1100)
        at java.util.TreeMap$ValueIterator.next(TreeMap.java:1145)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$OutputSink.closeLogWriters(HLogSplitter.java:1279)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$OutputSink.finishWritingAndClose(HLogSplitter.java:1170)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:475)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:403)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker$1.exec(SplitLogWorker.java:111)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.grabTask(SplitLogWorker.java:264)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.taskLoop(SplitLogWorker.java:195)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.run(SplitLogWorker.java:163)
        at java.lang.Thread.run(Thread.java:662)
2013-04-01 21:23:21,865 INFO org.apache.hadoop.hbase.regionserver.SplitLogWorker: SplitLogWorker ip-10-143-160-121.ec2.internal,60020,1364849529986 exiting
{code}

The impact of this issue is that SplitLogWorker exits so does the region server recovering mechanism of HBase. If any RS failed after all SplitLogWorkers in te cluster exit due to the issue, you'll see a hang log splitting job and the failed RS won't be recovered.

", ,,,,,,,
HBASE-8248,"hbase-hbase-regionserver-ip-10-152-131-248.log:2013-04-02 16:48:24,499 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Adding moved region record: c52aa9838fe14b28b34f1e4e5fa14be9 to ip-10-152-131-248.ec2.internal,60020,1364935613234:60020 as of 51312


The consequences are obvious...", ,,,,,,,
HBASE-8253,"A writting log got corrupted when we forcely power down one node. Only partial of last WALEdit was written into that log. And that log was not the last one in replication queue. 
ReplicationSource was blocked under this scenario. A lot of logs like below were printed:
{noformat}
2013-03-30 06:53:48,628 WARN  [regionserver26003-EventThread.replicationSource,1] 1 Got:  org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:334)
java.io.EOFException: hdfs://hacluster/hbase/.logs/master11,26003,1364530862620/master11%2C26003%2C1364530862620.1364553936510, entryStart=40434738, pos=40450048, end=40450048, edit=0
	at sun.reflect.GeneratedConstructorAccessor42.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.addFileInfoToException(SequenceFileLogReader.java:295)
	at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:240)
	at org.apache.hadoop.hbase.replication.regionserver.ReplicationHLogReaderManager.readNextAndSetPosition(ReplicationHLogReaderManager.java:84)
	at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.readAllEntriesToReplicateOrNextFile(ReplicationSource.java:412)
	at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:330)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readFully(DataInputStream.java:180)
	at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:68)
	at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:106)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2282)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2181)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2227)
	at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:238)
	... 3 more
..........	
2013-03-30 06:54:38,899 WARN  [regionserver26003-EventThread.replicationSource,1] 1 Got:  org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:334)
java.io.EOFException: hdfs://hacluster/hbase/.logs/master11,26003,1364530862620/master11%2C26003%2C1364530862620.1364553936510, entryStart=40434738, pos=40450048, end=40450048, edit=0
	at sun.reflect.GeneratedConstructorAccessor42.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.addFileInfoToException(SequenceFileLogReader.java:295)
	at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:240)
	at org.apache.hadoop.hbase.replication.regionserver.ReplicationHLogReaderManager.readNextAndSetPosition(ReplicationHLogReaderManager.java:84)
	at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.readAllEntriesToReplicateOrNextFile(ReplicationSource.java:412)
	at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:330)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readFully(DataInputStream.java:180)
	at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:68)
	at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:106)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2282)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2181)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2227)
	at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:238)
	... 3 more
...........	
{noformat}", ,,,,,,,
HBASE-8263,"Wrong counters are updated as you can see in MetricsMasterSourceImpl.java#init() 

{code}
    splitTimeHisto = metricsRegistry.newHistogram(SPLIT_SIZE_NAME, SPLIT_SIZE_DESC);
    splitSizeHisto = metricsRegistry.newHistogram(SPLIT_TIME_NAME, SPLIT_TIME_DESC);
{code}
", ,,,,,,,
HBASE-8264,"The master in 0.89-fb waits for 5-6 mins to check if RS'es become accessible; when it sees a co-related failure such as a rack-switch-reboot.

The rationale behind doing this is that it is not worth assigning and reassigning regions -- causing churn, when the rack switch reboots are expected to heal themselves in 5-6 mins. In earlier deployments, where this feature was not present, we used to find ourselves in a bad situation for 30mins-1hr.

However, co-related failures also happen when there is a power failure for the rack. These cases take much longer to heal; so waiting for 5-6 mins is a wasted effort.

The master should be able to differentiate the two scenario, by checking if *any* of the RS in the rack is able to communicate. Unless all the servers in the rack are unaccessible, we should proceed with reassigning the regions.
", ,,1,,,,,
HBASE-8266,"I was trying to create a table. The table creation failed
{code}
java.io.IOException: java.util.concurrent.ExecutionException: java.lang.IllegalStateException: Could not instantiate a region instance.
	at org.apache.hadoop.hbase.util.ModifyRegionUtils.createRegions(ModifyRegionUtils.java:133)
	at org.apache.hadoop.hbase.master.handler.CreateTableHandler.handleCreateHdfsRegions(CreateTableHandler.java:256)
	at org.apache.hadoop.hbase.master.handler.CreateTableHandler.handleCreateTable(CreateTableHandler.java:204)
	at org.apache.hadoop.hbase.master.handler.CreateTableHandler.process(CreateTableHandler.java:153)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:130)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.lang.IllegalStateException: Could not instantiate a region instance.
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.hadoop.hbase.util.ModifyRegionUtils.createRegions(ModifyRegionUtils.java:126)
	... 7 more
Caused by: java.lang.IllegalStateException: Could not instantiate a region instance.
	at org.apache.hadoop.hbase.regionserver.HRegion.newHRegion(HRegion.java:3765)
	at org.apache.hadoop.hbase.regionserver.HRegion.createHRegion(HRegion.java:3870)
	at org.apache.hadoop.hbase.util.ModifyRegionUtils$1.call(ModifyRegionUtils.java:106)
	at org.apache.hadoop.hbase.util.ModifyRegionUtils$1.call(ModifyRegionUtils.java:103)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	... 3 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.hadoop.hbase.regionserver.HRegion.newHRegion(HRegion.java:3762)
	... 11 more
Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/hbase/CompoundConfiguration$1
	at org.apache.hadoop.hbase.CompoundConfiguration.add(CompoundConfiguration.java:82)
	at org.apache.hadoop.hbase.regionserver.HRegion.<init>(HRegion.java:438)
	at org.apache.hadoop.hbase.regionserver.HRegion.<init>(HRegion.java:401)
	... 16 more

{code}
Am not sure of the above failure.  The same setup is able to create new tables.
Now the table is already in ENABLING state.  The master was restarted.
Now as the table was found in ENABLING state but not added to META the EnableTableHandler 
{code}
2013-04-03 18:33:03,850 FATAL org.apache.hadoop.hbase.master.HMaster: Unhandled exception. Starting shutdown.
org.apache.hadoop.hbase.exceptions.TableNotFoundException: TestTable
	at org.apache.hadoop.hbase.master.handler.EnableTableHandler.prepare(EnableTableHandler.java:89)
	at org.apache.hadoop.hbase.master.AssignmentManager.recoverTableInEnablingState(AssignmentManager.java:2586)
	at org.apache.hadoop.hbase.master.AssignmentManager.joinCluster(AssignmentManager.java:390)
	at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:777)
	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:522)
	at java.lang.Thread.run(Thread.java:662)
2013-04-03 18:33:03,880 INFO org.apache.hadoop.hbase.master.HMaster: Aborting

{code}
This does not allow me to start my master further leading to unoperationable state of the whole cluster.", ,,,,,,,
HBASE-8276,"In recent tests, we found situations that when some data nodes are down and file operations are slow depending on underlying hdfs timeout(normally 30 secs and socket connection timeout maybe around 1 min). While split log task heart beat time out is only 25 secs, a split log task will be preempted by SplitLogManager and assign to someone else after the 25 secs. On a small cluster, you'll see the same task is keeping bounced back & force for a while. I pasted a snippet of related logs below. You can search ""preempted from"" to see a task is preempted.

{code}
2013-04-01 21:22:08,599 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Splitting hlog: hdfs://ip-10-137-16-140.ec2.internal:8020/apps/hbase/data/.logs/ip-10-137-20-188.ec2.internal,60020,1364849530779-splitting/ip-10-137-20-188.ec2.internal%2C60020%2C1364849530779.1364865506159, length=127639653
2013-04-01 21:22:08,599 INFO org.apache.hadoop.hbase.util.FSHDFSUtils: Recovering file hdfs://ip-10-137-16-140.ec2.internal:8020/apps/hbase/data/.logs/ip-10-137-20-188.ec2.internal,60020,1364849530779-splitting/ip-10-137-20-188.ec2.internal%2C60020%2C1364849530779.1364865506159
2013-04-01 21:22:09,603 INFO org.apache.hadoop.hbase.util.FSHDFSUtils: Finished lease recover attempt for hdfs://ip-10-137-16-140.ec2.internal:8020/apps/hbase/data/.logs/ip-10-137-20-188.ec2.internal,60020,1364849530779-splitting/ip-10-137-20-188.ec2.internal%2C60020%2C1364849530779.1364865506159
2013-04-01 21:22:09,629 WARN org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Found existing old edits file. It could be the result of a previous failed split attempt. Deleting hdfs://ip-10-137-16-140.ec2.internal:8020/apps/hbase/data/IntegrationTestLoadAndVerify/73387f8d327a45bacf069bd631d70b3b/recovered.edits/0000000000003703447.temp, length=0
2013-04-01 21:22:09,629 WARN org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Found existing old edits file. It could be the result of a previous failed split attempt. Deleting hdfs://ip-10-137-16-140.ec2.internal:8020/apps/hbase/data/IntegrationTestLoadAndVerify/b749cbceaaf037c97f70cc2a6f48f2b8/recovered.edits/0000000000003703446.temp, length=0
2013-04-01 21:22:09,630 WARN org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Found existing old edits file. It could be the result of a previous failed split attempt. Deleting hdfs://ip-10-137-16-140.ec2.internal:8020/apps/hbase/data/IntegrationTestLoadAndVerify/c26b9d4a042d42c1194a8c2f389d33c8/recovered.edits/0000000000003703448.temp, length=0
2013-04-01 21:22:09,666 WARN org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Found existing old edits file. It could be the result of a previous failed split attempt. Deleting hdfs://ip-10-137-16-140.ec2.internal:8020/apps/hbase/data/IntegrationTestLoadAndVerify/adabdb40ccd52140f09f953ff41fd829/recovered.edits/0000000000003703451.temp, length=0
2013-04-01 21:22:09,722 WARN org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Found existing old edits file. It could be the result of a previous failed split attempt. Deleting hdfs://ip-10-137-16-140.ec2.internal:8020/apps/hbase/data/IntegrationTestLoadAndVerify/19f463fe74f4365e7df3e5fdb13aecad/recovered.edits/0000000000003703468.temp, length=0
2013-04-01 21:22:09,734 WARN org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Found existing old edits file. It could be the result of a previous failed split attempt. Deleting hdfs://ip-10-137-16-140.ec2.internal:8020/apps/hbase/data/IntegrationTestLoadAndVerify/b3e759a3fc9c4e83064961cc3cd4a911/recovered.edits/0000000000003703459.temp, length=0
2013-04-01 21:22:09,770 WARN org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Found existing old edits file. It could be the result of a previous failed split attempt. Deleting hdfs://ip-10-137-16-140.ec2.internal:8020/apps/hbase/data/IntegrationTestLoadAndVerify/6f078553be50897a986734ae043a5889/recovered.edits/0000000000003703454.temp, length=0
2013-04-01 21:22:34,985 INFO org.apache.hadoop.hbase.regionserver.SplitLogWorker: task /hbase/splitlog/hdfs%3A%2F%2Fip-10-137-16-140.ec2.internal%3A8020%2Fapps%2Fhbase%2Fdata%2F.logs%2Fip-10-137-20-188.ec2.internal%2C60020%2C1364849530779-splitting%2Fip-10-137-20-188.ec2.internal%252C60020%252C1364849530779.1364865506159 preempted from ip-10-151-29-196.ec2.internal,60020,1364849530671, current task state and owner=unassigned ip-10-137-16-140.ec2.internal,60000,1364849528428
{code}

The exact same issue is fixed by hbase-6738 in trunk so here comes the backport. ", ,1,1,,,,,
HBASE-8279,"Performance evaluation gives a provision to pass the table name.
The table name is considered when we first initialize the table - like the disabling and creation of tables happens with the name that we pass.
But the write and read test again uses only the default table and so the perf evaluation fails.

I think the problem is like this
{code}
 ./hbase org.apache.hadoop.hbase.PerformanceEvaluation --nomapred --table=MyTable2  --presplit=70 randomRead 2
{code}
{code}
13/04/04 21:42:07 DEBUG hbase.HRegionInfo: Current INFO from scan results = {NAME => 'MyTable2,00000000000000000002067171,1365126124904.bc9e936f4f8ca8ee55eb90091d4a13b6.', STARTKEY => '00000000000000000002067171', ENDKEY => '', ENCODED => bc9e936f4f8ca8ee55eb90091d4a13b6,}
13/04/04 21:42:07 INFO hbase.PerformanceEvaluation: Table created with 70 splits
{code}
You can see that the specified table is created with the splits.
But when the read starts
{code}
Caused by: org.apache.hadoop.hbase.exceptions.TableNotFoundException: TestTable
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:1157)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:1034)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:984)
        at org.apache.hadoop.hbase.client.HTable.finishSetup(HTable.java:246)
        at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:187)
        at org.apache.hadoop.hbase.PerformanceEvaluation$Test.testSetup(PerformanceEvaluation.java:851)
        at org.apache.hadoop.hbase.PerformanceEvaluation$Test.test(PerformanceEvaluation.java:869)
        at org.apache.hadoop.hbase.PerformanceEvaluation.runOneClient(PerformanceEvaluation.java:1495)
        at org.apache.hadoop.hbase.PerformanceEvaluation$1.run(PerformanceEvaluation.java:590)

{code}
It says TestTable not found which is the default table.", ,1,1,,,,,
HBASE-8282,"Observe the below logs
{code}
2013-04-04 17:43:45,825 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Flushing MyTable,,1365111467842.f6792086ad3518dee244e7bf2761a1f9.
2013-04-04 17:43:45,826 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memstore flush for MyTable,,1365111467842.f6792086ad3518dee244e7bf2761a1f9., current region memstore size 42.3 M
2013-04-04 17:43:45,826 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished snapshotting MyTable,,1365111467842.f6792086ad3518dee244e7bf2761a1f9., commencing wait for mvcc, flushsize=44388936
2013-04-04 17:43:45,826 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished snapshotting, commencing flushing stores
2013-04-04 17:43:45,831 DEBUG org.apache.hadoop.hbase.util.FSUtils: Creating file=hdfs://localhost:9010/hbase/MyTable/f6792086ad3518dee244e7bf2761a1f9/.tmp/7020db24b38246a5818e7eb4e130ad9e with permission=rwxrwxrwx
2013-04-04 17:43:45,834 DEBUG org.apache.hadoop.hbase.io.hfile.HFileWriterV2: Initialized with CacheConfig:enabled [cacheDataOnRead=true] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheCompressed=false]
2013-04-04 17:43:45,835 INFO org.apache.hadoop.hbase.regionserver.StoreFile: Delete Family Bloom filter type for hdfs://localhost:9010/hbase/MyTable/f6792086ad3518dee244e7bf2761a1f9/.tmp/7020db24b38246a5818e7eb4e130ad9e: CompoundBloomFilterWriter
2013-04-04 17:43:46,074 INFO org.apache.hadoop.hbase.regionserver.StoreFile: NO General Bloom and NO DeleteFamily was added to HFile (hdfs://localhost:9010/hbase/MyTable/f6792086ad3518dee244e7bf2761a1f9/.tmp/7020db24b38246a5818e7eb4e130ad9e) 
2013-04-04 17:43:46,074 INFO org.apache.hadoop.hbase.regionserver.HStore: Flushed , sequenceid=1051817, memsize=42.3 M, into tmp file hdfs://localhost:9010/hbase/MyTable/f6792086ad3518dee244e7bf2761a1f9/.tmp/7020db24b38246a5818e7eb4e130ad9e
2013-04-04 17:43:46,093 DEBUG org.apache.hadoop.hbase.regionserver.HRegionFileSystem: Committing store file hdfs://localhost:9010/hbase/MyTable/f6792086ad3518dee244e7bf2761a1f9/.tmp/7020db24b38246a5818e7eb4e130ad9e as hdfs://localhost:9010/hbase/MyTable/f6792086ad3518dee244e7bf2761a1f9/f1/7020db24b38246a5818e7eb4e130ad9e
2013-04-04 17:43:46,103 INFO org.apache.hadoop.hbase.regionserver.HStore: Added hdfs://localhost:9010/hbase/MyTable/f6792086ad3518dee244e7bf2761a1f9/f1/7020db24b38246a5818e7eb4e130ad9e, entries=54911, sequenceid=1051817, filesize=35.1 M
2013-04-04 17:43:46,103 INFO org.apache.hadoop.hbase.regionserver.HRegion: Finished memstore flush of ~42.3 M/44388936, currentsize=0/0 for region MyTable,,1365111467842.f6792086ad3518dee244e7bf2761a1f9. in 278ms, sequenceid=-1, compaction requested=true
2013-04-04 17:43:56,335 DEBUG org.apache.hadoop.hbase.io.hfile.LruBlockCache: Stats: total=394.09 MB, free=3.61 GB, max=4.00 GB, blocks=5263, accesses=244882, hits=42988, hitRatio=17.55%, , cachingAccesses=49869, cachingHits=24251, cachingHitsRatio=48.63%, , evictions=0, evicted=20349, evictedPerRun=Infinity
2013-04-04 17:44:31,119 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Flushing MyTable,,1365111467842.f6792086ad3518dee244e7bf2761a1f9.
2013-04-04 17:44:31,120 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memstore flush for MyTable,,1365111467842.f6792086ad3518dee244e7bf2761a1f9., current region memstore size 42.7 M
2013-04-04 17:44:31,120 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished snapshotting MyTable,,1365111467842.f6792086ad3518dee244e7bf2761a1f9., commencing wait for mvcc, flushsize=44764248
2013-04-04 17:44:31,120 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished snapshotting, commencing flushing stores
2013-04-04 17:44:31,136 DEBUG org.apache.hadoop.hbase.util.FSUtils: Creating file=hdfs://localhost:9010/hbase/MyTable/f6792086ad3518dee244e7bf2761a1f9/.tmp/c5c2cd6aaf1644daa9c858149da39081 with permission=rwxrwxrwx
2013-04-04 17:44:31,139 DEBUG org.apache.hadoop.hbase.io.hfile.HFileWriterV2: Initialized with CacheConfig:enabled [cacheDataOnRead=true] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheCompressed=false]
2013-04-04 17:44:31,140 INFO org.apache.hadoop.hbase.regionserver.StoreFile: Delete Family Bloom filter type for hdfs://localhost:9010/hbase/MyTable/f6792086ad3518dee244e7bf2761a1f9/.tmp/c5c2cd6aaf1644daa9c858149da39081: CompoundBloomFilterWriter
2013-04-04 17:44:31,341 INFO org.apache.hadoop.hbase.regionserver.StoreFile: NO General Bloom and NO DeleteFamily was added to HFile (hdfs://localhost:9010/hbase/MyTable/f6792086ad3518dee244e7bf2761a1f9/.tmp/c5c2cd6aaf1644daa9c858149da39081) 
2013-04-04 17:44:31,341 INFO org.apache.hadoop.hbase.regionserver.HStore: Flushed , sequenceid=1051858, memsize=42.7 M, into tmp file hdfs://localhost:9010/hbase/MyTable/f6792086ad3518dee244e7bf2761a1f9/.tmp/c5c2cd6aaf1644daa9c858149da39081
2013-04-04 17:44:31,356 DEBUG org.apache.hadoop.hbase.regionserver.HRegionFileSystem: Committing store file hdfs://localhost:9010/hbase/MyTable/f6792086ad3518dee244e7bf2761a1f9/.tmp/c5c2cd6aaf1644daa9c858149da39081 as hdfs://localhost:9010/hbase/MyTable/f6792086ad3518dee244e7bf2761a1f9/f1/c5c2cd6aaf1644daa9c858149da39081
2013-04-04 17:44:31,365 INFO org.apache.hadoop.hbase.regionserver.HStore: Added hdfs://localhost:9010/hbase/MyTable/f6792086ad3518dee244e7bf2761a1f9/f1/c5c2cd6aaf1644daa9c858149da39081, entries=55156, sequenceid=1051858, filesize=35.5 M
2013-04-04 17:44:31,365 INFO org.apache.hadoop.hbase.regionserver.HRegion: Finished memstore flush of ~42.7 M/44764248, currentsize=0/0 for region MyTable,,1365111467842.f6792086ad3518dee244e7bf2761a1f9. in 245ms, sequenceid=-1, compaction requested=true
2013-04-04 17:45:09,450 DEBUG org.apache.hadoop.hbase.regionserver.LogRoller: HLog roll requested
2013-04-04 17:45:09,458 DEBUG org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter: using new createWriter -- HADOOP-6840
2013-04-04 17:45:09,458 DEBUG org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter: Path=hdfs://localhost:9010/hbase/.logs/ram.sh.intel.com,60020,1365040136175/ram.sh.intel.com%2C60020%2C1365040136175.1365111909450, compression=false
2013-04-04 17:45:09,462 INFO org.apache.hadoop.hbase.regionserver.wal.FSHLog: Rolled log for file=/hbase/.logs/ram.sh.intel.com,60020,1365040136175/ram.sh.intel.com%2C60020%2C1365040136175.1365111822437, entries=70, filesize=65061576; new path=/hbase/.logs/ram.sh.intel.com,60020,1365040136175/ram.sh.intel.com%2C60020%2C1365040136175.1365111909450
2013-04-04 17:45:51,419 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Flushing MyTable,,1365111467842.f6792086ad3518dee244e7bf2761a1f9.
2013-04-04 17:45:51,419 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memstore flush for MyTable,,1365111467842.f6792086ad3518dee244e7bf2761a1f9., current region memstore size 42.4 M
2013-04-04 17:45:51,420 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished snapshotting MyTable,,1365111467842.f6792086ad3518dee244e7bf2761a1f9., commencing wait for mvcc, flushsize=44419376
2013-04-04 17:45:51,420 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished snapshotting, commencing flushing stores
2013-04-04 17:45:51,428 DEBUG org.apache.hadoop.hbase.util.FSUtils: Creating file=hdfs://localhost:9010/hbase/MyTable/f6792086ad3518dee244e7bf2761a1f9/.tmp/6c502092c27a4eae847a51109e764be3 with permission=rwxrwxrwx
2013-04-04 17:45:51,432 DEBUG org.apache.hadoop.hbase.io.hfile.HFileWriterV2: Initialized with CacheConfig:enabled [cacheDataOnRead=true] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheCompressed=false]
2013-04-04 17:45:51,433 INFO org.apache.hadoop.hbase.regionserver.StoreFile: Delete Family Bloom filter type for hdfs://localhost:9010/hbase/MyTable/f6792086ad3518dee244e7bf2761a1f9/.tmp/6c502092c27a4eae847a51109e764be3: CompoundBloomFilterWriter
2013-04-04 17:45:51,627 INFO org.apache.hadoop.hbase.regionserver.StoreFile: NO General Bloom and NO DeleteFamily was added to HFile (hdfs://localhost:9010/hbase/MyTable/f6792086ad3518dee244e7bf2761a1f9/.tmp/6c502092c27a4eae847a51109e764be3) 
2013-04-04 17:45:51,627 INFO org.apache.hadoop.hbase.regionserver.HStore: Flushed , sequenceid=1051899, memsize=42.4 M, into tmp file hdfs://localhost:9010/hbase/MyTable/f6792086ad3518dee244e7bf2761a1f9/.tmp/6c502092c27a4eae847a51109e764be3
2013-04-04 17:45:51,645 DEBUG org.apache.hadoop.hbase.regionserver.HRegionFileSystem: Committing store file hdfs://localhost:9010/hbase/MyTable/f6792086ad3518dee244e7bf2761a1f9/.tmp/6c502092c27a4eae847a51109e764be3 as hdfs://localhost:9010/hbase/MyTable/f6792086ad3518dee244e7bf2761a1f9/f1/6c502092c27a4eae847a51109e764be3
2013-04-04 17:45:51,654 INFO org.apache.hadoop.hbase.regionserver.HStore: Added hdfs://localhost:9010/hbase/MyTable/f6792086ad3518dee244e7bf2761a1f9/f1/6c502092c27a4eae847a51109e764be3, entries=54781, sequenceid=1051899, filesize=35.2 M
2013-04-04 17:45:51,654 INFO org.apache.hadoop.hbase.regionserver.HRegion: Finished memstore flush of ~42.4 M/44419376, currentsize=0/0 for region MyTable,,1365111467842.f6792086ad3518dee244e7bf2761a1f9. in 235ms, sequenceid=-1, compaction requested=true
2013-04-04 17:47:36,168 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Flushing MyTable,,1365111467842.f6792086ad3518dee244e7bf2761a1f9.
2013-04-04 17:47:36,168 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memstore flush for MyTable,,1365111467842.f6792086ad3518dee244e7bf2761a1f9., current region memstore size 42.5 M
2013-04-04 17:47:36,169 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished snapshotting MyTable,,1365111467842.f6792086ad3518dee244e7bf2761a1f9., commencing wait for mvcc, flushsize=44568408
2013-04-04 17:47:36,169 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished snapshotting, commencing flushing stores
2013-04-04 17:47:36,177 DEBUG org.apache.hadoop.hbase.util.FSUtils: Creating file=hdfs://localhost:9010/hbase/MyTable/f6792086ad3518dee244e7bf2761a1f9/.tmp/b1f04e64c8be4ea1bd1d95f8a4ee8936 with permission=rwxrwxrwx
2013-04-04 17:47:36,194 DEBUG org.apache.hadoop.hbase.io.hfile.HFileWriterV2: Initialized with CacheConfig:enabled [cacheDataOnRead=true] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheCompressed=false]
2013-04-04 17:47:36,194 INFO org.apache.hadoop.hbase.regionserver.StoreFile: Delete Family Bloom filter type for hdfs://localhost:9010/hbase/MyTable/f6792086ad3518dee244e7bf2761a1f9/.tmp/b1f04e64c8be4ea1bd1d95f8a4ee8936: CompoundBloomFilterWriter
2013-04-04 17:47:36,426 INFO org.apache.hadoop.hbase.regionserver.StoreFile: NO General Bloom and NO DeleteFamily was added to HFile (hdfs://localhost:9010/hbase/MyTable/f6792086ad3518dee244e7bf2761a1f9/.tmp/b1f04e64c8be4ea1bd1d95f8a4ee8936) 
2013-04-04 17:47:36,426 INFO org.apache.hadoop.hbase.regionserver.HStore: Flushed , sequenceid=1051940, memsize=42.5 M, into tmp file hdfs://localhost:9010/hbase/MyTable/f6792086ad3518dee244e7bf2761a1f9/.tmp/b1f04e64c8be4ea1bd1d95f8a4ee8936
2013-04-04 17:47:36,443 DEBUG org.apache.hadoop.hbase.regionserver.HRegionFileSystem: Committing store file hdfs://localhost:9010/hbase/MyTable/f6792086ad3518dee244e7bf2761a1f9/.tmp/b1f04e64c8be4ea1bd1d95f8a4ee8936 as hdfs://localhost:9010/hbase/MyTable/f6792086ad3518dee244e7bf2761a1f9/f1/b1f04e64c8be4ea1bd1d95f8a4ee8936
2013-04-04 17:47:36,452 INFO org.apache.hadoop.hbase.regionserver.HStore: Added hdfs://localhost:9010/hbase/MyTable/f6792086ad3518dee244e7bf2761a1f9/f1/b1f04e64c8be4ea1bd1d95f8a4ee8936, entries=55107, sequenceid=1051940, filesize=35.3 M
2013-04-04 17:47:36,452 INFO org.apache.hadoop.hbase.regionserver.HRegion: Finished memstore flush of ~42.5 M/44568408, currentsize=0/0 for region MyTable,,1365111467842.f6792086ad3518dee244e7bf2761a1f9. in 284ms, sequenceid=-1, compaction requested=true
2013-04-04 17:47:57,196 DEBUG org.apache.hadoop.hbase.regionserver.LogRoller: HLog roll requested
2013-04-04 17:47:57,203 DEBUG org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter: using new createWriter -- HADOOP-6840
2013-04-04 17:47:57,203 DEBUG org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter: Path=hdfs://localhost:9010/hbase/.logs/ram.sh.intel.com,60020,1365040136175/ram.sh.intel.com%2C60020%2C1365040136175.1365112077196, compression=false
2013-04-04 17:47:57,208 INFO org.apache.hadoop.hbase.regionserver.wal.FSHLog: Rolled log for file=/hbase/.logs/ram.sh.intel.com,60020,1365040136175/ram.sh.intel.com%2C60020%2C1365040136175.1365111909450, entries=70, filesize=64871151; new path=/hbase/.logs/ram.sh.intel.com,60020,1365040136175/ram.sh.intel.com%2C60020%2C1365040136175.1365112077196
2013-04-04 17:48:16,796 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Flushing MyTable,,1365111467842.f6792086ad3518dee244e7bf2761a1f9.
.............
{code}
The above logs continued and created around 38 files.

Later when the RS was restarted 
{code}
2013-04-04 18:36:18,007 INFO org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactionPolicy: Default compaction algorithm has selected 38 files from 38 candidates
{code}
The compaction happeneded for the 38 files.
The reason for the above is this, USer triggered flushes happen thro
HRS.flushRegion()
{code}
FlushRegionResponse.Builder builder = FlushRegionResponse.newBuilder();
      if (shouldFlush) {
        builder.setFlushed(region.flushcache());
      }
      builder.setLastFlushTime(region.getLastFlushTime());
      return builder.build();
{code}

Here we call region.flushCache but the return value is never used.  The return value is nothing but the variable which says a compaction is needed after this flush or not (that is returned by internalFlushCache).  In the HRS.flushRegion method the return value is ignored.
But the same when it happens thro MemstoreFlusher the return value is used and we call requestCompaction.
{code}
try {
      boolean shouldCompact = region.flushcache();
      // We just want to check the size
      boolean shouldSplit = region.checkSplit() != null;
      if (shouldSplit) {
        this.server.compactSplitThread.requestSplit(region);
      } else if (shouldCompact) {
        server.compactSplitThread.requestCompaction(region, Thread.currentThread().getName());
      }
{code}

Doing the above step in HRS.flushRegion() should be enough i feel.
I had a doubt here if user triggered flush had to behave differently.  The same can be observed in the 0.94 code also.

", ,,,,,,,
HBASE-8285,"Steps to reproduce this bug:
1) Gracefull restart a region server causing regions to get redistributed.
2) Client call to this region keeps failing since Meta Cache is never purged on the client for the region that moved.

Reason behind the bug:
1) Client continues to hit the old region server.
2) The old region server throws NotServingRegionException which is not handled correctly and the META cache entries are never purged for that server causing the client to keep hitting the old server.

The reason lies in ServerCallable code since we only purge META cache entries when there is a RetriesExhaustedException, SocketTimeoutException or ConnectException. However, there is no case check for NotServingRegionException(s).

Why is this not a problem for Scan(s) and Put(s) ?

a) If a region server is not hosting a region/scanner, then an UnknownScannerException is thrown which causes a relocateRegion() call causing a refresh of the META cache for that particular region.
b) For put(s), the processBatchCallback() interface in HConnectionManager is used which clears out META cache entries for all kinds of exceptions except DoNotRetryException.
", ,,,,,,,
HBASE-8288,"This jira is for two issues I see in the HBaseFileSystem class:
1) Load testing on a 7 node cluster using ycsb insert workload shows that static initialization of conf properties results in a slightly better throughput. Though the initialization uses HBaseConfiguration.create() call which is expensive (and I tried to avoid that in its first version), this class is used for most of the filesystem class, and had to invoke an additional checkAndSetXX call before making the fs call because it is not certain whether the retry properties are set or not. Having initialize them in static block removes that limitation.

2) Correct semantics for CreatePathXXX method. In case the overwrite flag is false and file already exists, underlying fs throws an exception. It should be re-thrown to the caller.", ,,1,,,,,
HBASE-8295,"TestMasterFailover.testMasterFailoverWithMockedRITOnDeadRS failed because asserting before finishing SSH process of dead server. 

https://builds.apache.org/job/PreCommit-HBASE-Build/5182//artifact/trunk/hbase-server/target/surefire-reports/org.apache.hadoop.hbase.master.TestMasterFailover.txt


In SSH Checked how many regions carrying by dead server but not started assignment before asserting.
{code}
2013-04-08 12:17:18,238 INFO  [MASTER_SERVER_OPERATIONS-asf001.sp2.ygridcore.net,41490,1365423432616-1] handler.ServerShutdownHandler(192): Reassigning 14 region(s) that asf001.sp2.ygridcore.net,46030,1365423422112 was carrying (and 0 regions(s) that were opening on this server)
{code}

We need to wait until SSH processing finished to ensure atleast regions assignment started. After that we will any way wait at blockUntilNoRIT until assignment completed. ", ,,,,,,,
HBASE-8299,If the files are very oddly sized then it's possible that ExploringCompactionPolicy can get stuck., ,,,,,,,
HBASE-8300,"This issue is related to HBASE-6823. logs below.

TestSplitTransaction
org.apache.hadoop.hbase.regionserver.TestSplitTransaction
testWholesomeSplit(org.apache.hadoop.hbase.regionserver.TestSplitTransaction)
java.io.IOException: Failed delete of C:/springSpace/org.apache.hbase.hbase-0.95.0-SNAPSHOT/hbase-server/target/test-data/e5089331-c2bf-43d0-816d-25c6bed71f26/org.apache.hadoop.hbase.regionserver.TestSplitTransaction/table/4851a041b5e9befef50c135b5659243b

	at org.apache.hadoop.hbase.regionserver.TestSplitTransaction.teardown(TestSplitTransaction.java:100)

	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)

	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)

	at java.lang.reflect.Method.invoke(Method.java:597)

	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)

	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)

	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)

	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)

	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)

	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)

	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)

	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)

	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)

	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)

	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)

	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)

	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)

	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)

	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)

	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)

	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)

	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)

	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)



testRollback(org.apache.hadoop.hbase.regionserver.TestSplitTransaction)
java.io.IOException: Failed delete of C:/springSpace/org.apache.hbase.hbase-0.95.0-SNAPSHOT/hbase-server/target/test-data/9140a440-3925-4eaf-8d5d-62744609d775/org.apache.hadoop.hbase.regionserver.TestSplitTransaction/table/6f0ef0cbe59b3fb02c081ad1ffc78a9d

	at org.apache.hadoop.hbase.regionserver.TestSplitTransaction.teardown(TestSplitTransaction.java:100)

	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)

	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)

	at java.lang.reflect.Method.invoke(Method.java:597)

	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)

	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)

	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)

	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)

	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)

	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)

	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)

	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)

	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)

	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)

	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)

	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)

	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)

	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)

	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)

	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)

	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)

	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)

	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)



testFailAfterPONR(org.apache.hadoop.hbase.regionserver.TestSplitTransaction)
java.io.IOException: Failed delete of C:/springSpace/org.apache.hbase.hbase-0.95.0-SNAPSHOT/hbase-server/target/test-data/9ad6728e-a425-4c2a-b7f2-00af842ec2a4/org.apache.hadoop.hbase.regionserver.TestSplitTransaction/table/65a714e4df0ea5878925c27f8815cd6f

	at org.apache.hadoop.hbase.regionserver.TestSplitTransaction.teardown(TestSplitTransaction.java:100)

	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)

	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)

	at java.lang.reflect.Method.invoke(Method.java:597)

	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)

	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)

	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)

	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)

	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)

	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)

	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)

	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)

	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)

	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)

	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)

	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)

	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)

	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)

	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)

	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)

	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)

	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)

	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)



", ,,,,,,,
HBASE-8304,"When fs.default.name or fs.defaultFS in hadoop core-site.xml is configured as hdfs://ip, and hbase.rootdir is configured as hdfs://ip:port/hbaserootdir where port is the hdfs namenode's default port. the bulkload operation will not remove the file in bulk output dir. Store::bulkLoadHfile will think hdfs:://ip and hdfs:://ip:port as different filesystem and go with copy approaching instead of rename.

The root cause is that hbase master will rewrite fs.default.name/fs.defaultFS according to hbase.rootdir when regionserver started, thus, dest fs uri from the hregion will not matching src fs uri passed from client.

any suggestion what is the best approaching to fix this issue? 

I kind of think that we could check for default port if src uri come without port info.", ,1,,,,,,
HBASE-8309,"When fs.default.name or fs.defaultFS in hadoop core-site.xml is configured as hdfs://ip, and hbase.rootdir is configured as hdfs://ip:port/hbaserootdir where port is the hdfs namenode's default port. the bulkload operation will not remove the file in bulk output dir. Store::bulkLoadHfile will think hdfs:://ip and hdfs:://ip:port as different filesystem and go with copy approaching instead of rename.

The root cause is that hbase master will rewrite fs.default.name/fs.defaultFS according to hbase.rootdir when regionserver started, thus, dest fs uri from the hregion will not matching src fs uri passed from client.

any suggestion what is the best approaching to fix this issue? 

I kind of think that we could check for default port if src uri come without port info.", ,1,,,,,,
HBASE-8310,"There are a few timeout values and defaults being used by HBase snapshot.
DEFAULT_MAX_WAIT_TIME (60000 milli sec, 1 min) for client response
TIMEOUT_MILLIS_DEFAULT (60000 milli sec, 1 min) for Procedure timeout
SNAPSHOT_TIMEOUT_MILLIS_DEFAULT (60000 milli sec, 1 min) for region server subprocedure  

There is also other timeout involved, for example, 
DEFAULT_TABLE_WRITE_LOCK_TIMEOUT_MS (10 mins) for TakeSnapshotHandler#prepare()

We could have this case:
The user issues a sync snapshot request, waits for 1 min, and gets an exception.
In the meantime the snapshot handler is blocked on the table lock, and the snapshot may continue to finish after 10 mins.
But the user will probably re-issue the snapshot request during the 10 mins.
This is a little confusing and messy when this happens.
To be more reasonable, we should either increase the DEFAULT_MAX_WAIT_TIME or decrease the table lock waiting time.", ,1,,,,,,
HBASE-8314,"In case a HLog file is of size 0, and it is under recovery, HLogSplitter will fail to open it since it can get the file length, therefore, master can't start.

{noformat}
java.io.IOException: Cannot obtain block length for LocatedBlock{...; getBlockSize()=0; corrupt=false; offset=0; locs=[...]}
    at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:238)
    at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:182)
    at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:124)
    at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:117)
    at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1080)
{noformat}", ,,,,,,,
HBASE-8316,"There are a few timeout values and defaults being used by HBase snapshot.
DEFAULT_MAX_WAIT_TIME (60000 milli sec, 1 min) for client response
TIMEOUT_MILLIS_DEFAULT (60000 milli sec, 1 min) for Procedure timeout
SNAPSHOT_TIMEOUT_MILLIS_DEFAULT (60000 milli sec, 1 min) for region server subprocedure  

There is also other timeout involved, for example, 
DEFAULT_TABLE_WRITE_LOCK_TIMEOUT_MS (10 mins) for TakeSnapshotHandler#prepare()

We could have this case:
The user issues a sync snapshot request, waits for 1 min, and gets an exception.
In the meantime the snapshot handler is blocked on the table lock, and the snapshot may continue to finish after 10 mins.
But the user will probably re-issue the snapshot request during the 10 mins.
This is a little confusing and messy when this happens.
To be more reasonable, we should either increase the DEFAULT_MAX_WAIT_TIME or decrease the table lock waiting time.", ,,1,,,,,
HBASE-8317,"TestPrefixTreeEncoding#testSeekWithFixedData from the patch could reproduce the bug.

An example of the bug case:
Suppose the following rows:

1.row3/c1:q1/
2.row3/c1:q2/
3.row3/c1:q3/
4.row4/c1:q1/
5.row4/c1:q2/

After seeking the row 'row30', the expected peek KV is row4/c1:q1/, but actual is row3/c1:q1/.


I just fix this bug case in the patch, 

Maybe we can do more for other potential problems if anyone is familiar with the code of PREFIX_TREE

", ,,,,,,,
HBASE-8321,"Currently, hlog splitter could spend quite sometime to split a log in case any HDFS issue and recoverLease/retry opening is needed.  If distributed log split manager times out the log worker, other log worker to take over will run into the same issue.

Ideally, we should not need a timeout monitor.  Since we have a timeout monitor for DSL now, the worker should heartbeat to avoid wrong/unneeded timeouts.
", ,,1,,,,,
HBASE-8322,"There are a few timeout values and defaults being used by HBase snapshot.
DEFAULT_MAX_WAIT_TIME (60000 milli sec, 1 min) for client response
TIMEOUT_MILLIS_DEFAULT (60000 milli sec, 1 min) for Procedure timeout
SNAPSHOT_TIMEOUT_MILLIS_DEFAULT (60000 milli sec, 1 min) for region server subprocedure  

There is also other timeout involved, for example, 
DEFAULT_TABLE_WRITE_LOCK_TIMEOUT_MS (10 mins) for TakeSnapshotHandler#prepare()

We could have this case:
The user issues a sync snapshot request, waits for 1 min, and gets an exception.
In the meantime the snapshot handler is blocked on the table lock, and the snapshot may continue to finish after 10 mins.
But the user will probably re-issue the snapshot request during the 10 mins.
This is a little confusing and messy when this happens.
To be more reasonable, we should either increase the DEFAULT_MAX_WAIT_TIME or decrease the table lock waiting time.", ,1,,,,,,
HBASE-8324,"There are a few timeout values and defaults being used by HBase snapshot.
DEFAULT_MAX_WAIT_TIME (60000 milli sec, 1 min) for client response
TIMEOUT_MILLIS_DEFAULT (60000 milli sec, 1 min) for Procedure timeout
SNAPSHOT_TIMEOUT_MILLIS_DEFAULT (60000 milli sec, 1 min) for region server subprocedure  

There is also other timeout involved, for example, 
DEFAULT_TABLE_WRITE_LOCK_TIMEOUT_MS (10 mins) for TakeSnapshotHandler#prepare()

We could have this case:
The user issues a sync snapshot request, waits for 1 min, and gets an exception.
In the meantime the snapshot handler is blocked on the table lock, and the snapshot may continue to finish after 10 mins.
But the user will probably re-issue the snapshot request during the 10 mins.
This is a little confusing and messy when this happens.
To be more reasonable, we should either increase the DEFAULT_MAX_WAIT_TIME or decrease the table lock waiting time.", ,1,,,,,,
HBASE-8325,"I'm using  the replication of Hbase in my test environment.

When a replicationSource open a empty HLog, the EOFException throws. 
It is because the Reader can't read the SequenceFile's meta data, but there's no data at all, so it throws the EOFException.
Should we detect the empty file and processed it, like we process the FileNotFoundException?

here's the code:
{code:java}
/**
   * Open a reader on the current path
   *
   * @param sleepMultiplier by how many times the default sleeping time is augmented
   * @return true if we should continue with that file, false if we are over with it
   */
  protected boolean openReader(int sleepMultiplier) {
    try {
      LOG.debug(""Opening log for replication "" + this.currentPath.getName() +
          "" at "" + this.repLogReader.getPosition());
      try {
        this.reader = repLogReader.openReader(this.currentPath);
      } catch (FileNotFoundException fnfe) {
        if (this.queueRecovered) {
          // We didn't find the log in the archive directory, look if it still
          // exists in the dead RS folder (there could be a chain of failures
          // to look at)
          LOG.info(""NB dead servers : "" + deadRegionServers.length);
          for (int i = this.deadRegionServers.length - 1; i >= 0; i--) {

            Path deadRsDirectory =
                new Path(manager.getLogDir().getParent(), this.deadRegionServers[i]);
            Path[] locs = new Path[] {
                new Path(deadRsDirectory, currentPath.getName()),
                new Path(deadRsDirectory.suffix(HLog.SPLITTING_EXT),
                                          currentPath.getName()),
            };
            for (Path possibleLogLocation : locs) {
              LOG.info(""Possible location "" + possibleLogLocation.toUri().toString());
              if (this.manager.getFs().exists(possibleLogLocation)) {
                // We found the right new location
                LOG.info(""Log "" + this.currentPath + "" still exists at "" +
                    possibleLogLocation);
                // Breaking here will make us sleep since reader is null
                return true;
              }
            }
          }
          // TODO What happens if the log was missing from every single location?
          // Although we need to check a couple of times as the log could have
          // been moved by the master between the checks
          // It can also happen if a recovered queue wasn't properly cleaned,
          // such that the znode pointing to a log exists but the log was
          // deleted a long time ago.
          // For the moment, we'll throw the IO and processEndOfFile
          throw new IOException(""File from recovered queue is "" +
              ""nowhere to be found"", fnfe);
        } else {
          // If the log was archived, continue reading from there
          Path archivedLogLocation =
              new Path(manager.getOldLogDir(), currentPath.getName());
          if (this.manager.getFs().exists(archivedLogLocation)) {
            currentPath = archivedLogLocation;
            LOG.info(""Log "" + this.currentPath + "" was moved to "" +
                archivedLogLocation);
            // Open the log at the new location
            this.openReader(sleepMultiplier);

          }
          // TODO What happens the log is missing in both places?
        }
      }
    } catch (IOException ioe) {
      LOG.warn(peerClusterZnode + "" Got: "", ioe);
      this.reader = null;
      // TODO Need a better way to determinate if a file is really gone but
      // TODO without scanning all logs dir
      if (sleepMultiplier == this.maxRetriesMultiplier) {
        LOG.warn(""Waited too long for this file, considering dumping"");
        return !processEndOfFile();
      }
    }
    return true;
  }
{code}

there's a method called {code:java}processEndOfFile(){code}
should we add this case in it?", ,,,,,,,
HBASE-8333,"There are a few timeout values and defaults being used by HBase snapshot.
DEFAULT_MAX_WAIT_TIME (60000 milli sec, 1 min) for client response
TIMEOUT_MILLIS_DEFAULT (60000 milli sec, 1 min) for Procedure timeout
SNAPSHOT_TIMEOUT_MILLIS_DEFAULT (60000 milli sec, 1 min) for region server subprocedure  

There is also other timeout involved, for example, 
DEFAULT_TABLE_WRITE_LOCK_TIMEOUT_MS (10 mins) for TakeSnapshotHandler#prepare()

We could have this case:
The user issues a sync snapshot request, waits for 1 min, and gets an exception.
In the meantime the snapshot handler is blocked on the table lock, and the snapshot may continue to finish after 10 mins.
But the user will probably re-issue the snapshot request during the 10 mins.
This is a little confusing and messy when this happens.
To be more reasonable, we should either increase the DEFAULT_MAX_WAIT_TIME or decrease the table lock waiting time.", ,,,,,,,
HBASE-8337,"There are a few timeout values and defaults being used by HBase snapshot.
DEFAULT_MAX_WAIT_TIME (60000 milli sec, 1 min) for client response
TIMEOUT_MILLIS_DEFAULT (60000 milli sec, 1 min) for Procedure timeout
SNAPSHOT_TIMEOUT_MILLIS_DEFAULT (60000 milli sec, 1 min) for region server subprocedure  

There is also other timeout involved, for example, 
DEFAULT_TABLE_WRITE_LOCK_TIMEOUT_MS (10 mins) for TakeSnapshotHandler#prepare()

We could have this case:
The user issues a sync snapshot request, waits for 1 min, and gets an exception.
In the meantime the snapshot handler is blocked on the table lock, and the snapshot may continue to finish after 10 mins.
But the user will probably re-issue the snapshot request during the 10 mins.
This is a little confusing and messy when this happens.
To be more reasonable, we should either increase the DEFAULT_MAX_WAIT_TIME or decrease the table lock waiting time.", ,1,,,,,,
HBASE-8340,"In next(...):
{code}
    if (compressionContext != null && emptyCompressionContext) {
      emptyCompressionContext = false;
    }
    return ...
{code}
 
In seek()
{code}
    if (compressionContext != null && emptyCompressionContext) {
      while (next() != null) {
        if (getPosition() == pos) {
          emptyCompressionContext = false;
          break;
        }
      }
...
reader.seek(pos);
{code}

So, seek will seek the file directly if either any next, or any seek, has been called before.

I am not sure what this code is for, but my best guess is that it is to populate the dictionary for compression.
If it is so, it would seem that one next() call (or even one seek() call) would not be enough, and seek must always use next(), otherwise it is incorrect.

If we assume that one next() is enough to be able to use reader.seek, as the current code would seem to imply, then there's no need for the first seek to call next() in a loop - it can call next once and then do reader.seek.

Note: even in case if all of this works fine because external usage creates the object and does one seek before any next-s, and no seeks after (the only bug-free pattern currently possible with both methods used if I'm not mistaken), then the code needs to be tightened and bug potential removed.

", ,,1,,,,,
HBASE-8341,"In HBASE-7848, we added table lock to enabled/disabled snapshot handlers, and fixed SnapshotManager to call CloneSnapshotHandler.prepare() in HBASE-7957. It seems that we overlooked the RestoreSnapshotHandler.prepare(). In this issue we should fix that so that we acquire the table lock in restore snapshot. 

There is also a slightly related issue where TakeSnapshotHandler first loads the HTD, then acquires the table lock. The order should be swapped to guard against concurrent alter table statements. ", ,,,,,,,
HBASE-8344,"In next(...):
{code}
    if (compressionContext != null && emptyCompressionContext) {
      emptyCompressionContext = false;
    }
    return ...
{code}
 
In seek()
{code}
    if (compressionContext != null && emptyCompressionContext) {
      while (next() != null) {
        if (getPosition() == pos) {
          emptyCompressionContext = false;
          break;
        }
      }
...
reader.seek(pos);
{code}

So, seek will seek the file directly if either any next, or any seek, has been called before.

I am not sure what this code is for, but my best guess is that it is to populate the dictionary for compression.
If it is so, it would seem that one next() call (or even one seek() call) would not be enough, and seek must always use next(), otherwise it is incorrect.

If we assume that one next() is enough to be able to use reader.seek, as the current code would seem to imply, then there's no need for the first seek to call next() in a loop - it can call next once and then do reader.seek.

Note: even in case if all of this works fine because external usage creates the object and does one seek before any next-s, and no seeks after (the only bug-free pattern currently possible with both methods used if I'm not mistaken), then the code needs to be tightened and bug potential removed.

", ,,1,,,,,
HBASE-8346,"While doing a .META. lookup (HCM#locateRegionInMeta), we also prefetch some other region's info for that table. The usual call to the meta lookup has useCache variable set to true. 
Currently, it calls preFetch irrespective of the value useCache flag:
{code}
            if (Bytes.equals(parentTable, HConstants.META_TABLE_NAME) &&
                (getRegionCachePrefetch(tableName)))  {
              prefetchRegionCache(tableName, row);
            }
{code}
Later on, if useCache flag is set to false, it deletes the entry for that row from the cache with a forceDeleteCachedLocation() call. This always results in two calls to the .META. table in this case. The useCache variable is set to false in case we are retrying to find a region (regionserver failover).

It can be verified from the log statements of a client while having a regionserver failover. In the below example, the client was connected to a1217, when a1217 got killed. The region in question is moved to a1215. Client got this info from META scan, where as client cache this info from META, but then delete it from cache as it want the latest info. 
The result is even the meta provides the latest info, it is still deleted This causes even the latest info to be deleted. Thus, client deletes a1215.abc.com even though it is correct info.
{code}
13/04/15 09:49:12 DEBUG client.HConnectionManager$HConnectionImplementation: Cached location for t,user7225973201630273569,1365536809331.40382355b8c45e1338d620c018f8ff6c. is a1217.abc.com:40020
13/04/15 09:49:12 WARN client.ServerCallable: Received exception, tries=1, numRetries=30 message=Connection refused

13/04/15 09:49:12 DEBUG client.HConnectionManager$HConnectionImplementation: Removed all cached region locations that map to a1217.abc.com,40020,1365621947381
13/04/15 09:49:13 DEBUG client.MetaScanner: Current INFO from scan results = {NAME => 't,user7225973201630273569,1365536809331.40382355b8c45e1338d620c018f8ff6c.', STARTKEY => 'user7225973201630273569', ENDKEY => '', ENCODED => 40382355b8c45e1338d620c018f8ff6c,}

13/04/15 09:49:13 DEBUG client.MetaScanner: Scanning .META. starting at row=t,user7225973201630273569,00000000000000 for max=10 rows using hconnection-0x7786df0f
13/04/15 09:49:13 DEBUG client.MetaScanner: Current INFO from scan results = {NAME => 't,user7225973201630273569,1365536809331.40382355b8c45e1338d620c018f8ff6c.', STARTKEY => 'user7225973201630273569', ENDKEY => '', ENCODED => 40382355b8c45e1338d620c018f8ff6c,}

13/04/15 09:49:13 DEBUG client.HConnectionManager$HConnectionImplementation: Cached location for t,user7225973201630273569,1365536809331.40382355b8c45e1338d620c018f8ff6c. is a1215.abc.com:40020
13/04/15 09:49:13 DEBUG client.HConnectionManager$HConnectionImplementation: Removed a1215.abc.com:40020 as a location of t,user7225973201630273569,1365536809331.40382355b8c45e1338d620c018f8ff6c. for tableName=t from cache

13/04/15 09:49:13 DEBUG client.MetaScanner: Current INFO from scan results = {NAME => 't,user7225973201630273569,1365536809331.40382355b8c45e1338d620c018f8ff6c.', STARTKEY => 'user7225973201630273569', ENDKEY => '', ENCODED => 40382355b8c45e1338d620c018f8ff6c,}
13/04/15 09:49:13 DEBUG client.HConnectionManager$HConnectionImplementation: Cached location for t,user7225973201630273569,1365536809331.40382355b8c45e1338d620c018f8ff6c. is a1215.abc.com:40020
13/04/15 09:49:13 WARN client.ServerCallable: Received exception, tries=2, numRetries=30 message=org.apache.hadoop.hbase.exceptions.UnknownScannerException: Name: -6313340536390503703, already closed?
13/04/15 09:49:13 DEBUG client.ClientScanner: Advancing internal scanner to startKey at 'user760712450403198900'
{code}", ,1,,,,,,
HBASE-8353,"ROOT/META are not getting assigned if master restarted while closing ROOT/META.
Lets suppose catalog table regions in M_ZK_REGION_CLOSING state during master initialization and then just we are adding the them to RIT and waiting for TM. {code}
        if (isOnDeadServer(regionInfo, deadServers) &&
            (data.getOrigin() == null || !serverManager.isServerOnline(data.getOrigin()))) {
          // If was on dead server, its closed now. Force to OFFLINE and this
          // will get it reassigned if appropriate
          forceOffline(regionInfo, data);
        } else {
          // Just insert region into RIT.
          // If this never updates the timeout will trigger new assignment
          regionsInTransition.put(encodedRegionName, new RegionState(
            regionInfo, RegionState.State.CLOSING,
            data.getStamp(), data.getOrigin()));
        }
{code}
isOnDeadServer always return false to ROOT/META because deadServers is null.

Even TM cannot close them properly because its not available in online regions since its not yet assigned.
{code}
    synchronized (this.regions) {
      // Check if this region is currently assigned
      if (!regions.containsKey(region)) {
        LOG.debug(""Attempted to unassign region "" +
          region.getRegionNameAsString() + "" but it is not "" +
          ""currently assigned anywhere"");
        return;
      }
    }
{code}", ,,,,,,,
HBASE-8355,"As pointed out in https://github.com/forcedotcom/phoenix/pull/131, BaseRegionObserver#preCompactScannerOpen returns null by default, which hoses any coprocessors down the line, making override of this method mandatory. The fix is trivial, patch coming momentarily.

Update:
This same behavior is present in the Flush and Store versions of the same method - this should all be moved to the proposed 'return passed scanner' default behavior.", ,,,,,,,
HBASE-8358,"Currently there are five permissions checked by HBase: read, write, exec, create, and admin. An RDBMS allows for more fine-grained control over the operations, and it would be helpful to have such controls in HBase. Specifically, a distinction between ""put"" and ""delete"" would be most useful.",1,,,,,,,
HBASE-8362,"Currently MultiGets are executed on a RegionServer in a single thread in a loop that handles each Get separately (opening a scanner, seeking, etc).
It seems we could optimize this (per region at least) by opening a single scanner and issue a reseek for each Get that was requested.

I have not tested this yet and no patch, but I would like to solicit feedback on this idea.", ,,1,,,,,
HBASE-8365,"The duplicated ZK notifications should happen in trunk as well. Since the way we handle ZK notifications is different in trunk, we don't see the issue there. I'll explain later.

The issue is causing TestMetaReaderEditor.testRetrying flaky with error message {code}reader: count=2, t=null{code} A related link is at https://builds.apache.org/job/HBase-0.94/941/testReport/junit/org.apache.hadoop.hbase.catalog/TestMetaReaderEditor/testRetrying/

The test case failure is due to an IllegalStateException and master is aborted so the rest test cases also failed after testRetrying.

Below are steps why the issue is happening(region fa0e7a5590feb69bd065fbc99c228b36 is in interests):

1) Got first notification event RS_ZK_REGION_FAILED_OPEN at 2013-04-04 17:39:01,197
{code} DEBUG [pool-1-thread-1-EventThread] master.AssignmentManager(744): Handling transition=RS_ZK_REGION_FAILED_OPEN, server=janus.apache.org,42093,1365097126155, region=fa0e7a5590feb69bd065fbc99c228b36{code}

In the step, AM tries to open the region on another RS in a separate thread

2) Got second notification event RS_ZK_REGION_FAILED_OPEN at 2013-04-04 17:39:01,200 
{code}DEBUG [pool-1-thread-1-EventThread] master.AssignmentManager(744): Handling transition=RS_ZK_REGION_FAILED_OPEN, server=janus.apache.org,42093,1365097126155, region=fa0e7a5590feb69bd065fbc99c228b36{code}

3) Later got opening notification event result from the step 1 at 2013-04-04 17:39:01,288 
{code} DEBUG [pool-1-thread-1-EventThread] master.AssignmentManager(744): Handling transition=RS_ZK_REGION_OPENING, server=janus.apache.org,54833,1365097126175, region=fa0e7a5590feb69bd065fbc99c228b36{code}

Step 2 ClosedRegionHandler throws IllegalStateException because ""Cannot transit it to OFFLINE""(state is in opening from notification 3) and abort Master. This could happen in 0.94 because we handle notifications using executorService which opens the door to handle events out of order through receive them in order of updates.

I've confirmed that we don't have duplicated AM listeners and both events triggered by same ZK data of exact same version. The issue can be reproduced once by running testRetrying test case 20 times in a loop.

There are several issues for the failure:

1) duplicated ZK notifications. Since ZK watcher is one time trigger, the duplicated notification should not happen from the same data of the same version in the first place

2) ZooKeeper watcher handling is wrong in both 0.94 and trunk as following:
a) 0.94 handle notifications in async way which may lead to handle notifications out of order of the events happened
b) In trunk, we handle ZK notifications synchronously which slows down other components such as SSH, LogSplitting etc. because we have a single notification queue
c) In trunk & 0.94, we could use stale event data because we have a long listener list. ZK node state could have changed at the time when handling the event. If a listener needs to act upon latest state, it should re-fetch the data to verify if the data triggered the handler hasn't changed.

Suggestions:
For 0.94, we can bandit the CloseRegionHandler to pass in the expected ZK data version to skip event handling on stale data with min impact.

For trunk, I'll open an improvement JIRA on ZK notification handling to provide more parallelism to handle unrelated notifications.

For the duplicated ZK notifications, we need bring some ZK experters to take a look at this.

Please let me know what you think or any better idea.
Thanks!
", ,,,,,,,
HBASE-8367,"I don't reproduce this all the time, but I had it on a fairly clean env.
It occurs every 5 minutes (i.e. the balancer period). Impact is severe: the balancer does not run. When it starts to occurs, it occurs all the time. I haven't tried to restart the master, but I think it should be enough.
Now, looking at the code, the NPE is strange. 

{noformat}
2013-04-18 08:09:52,079 ERROR [box,60000,1366281581983-BalancerChore] org.apache.hadoop.hbase.master.balancer.BalancerChore: Caught exception
java.lang.NullPointerException
	at org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster.<init>(BaseLoadBalancer.java:145)
	at org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer.balanceCluster(StochasticLoadBalancer.java:194)
	at org.apache.hadoop.hbase.master.HMaster.balance(HMaster.java:1295)
	at org.apache.hadoop.hbase.master.balancer.BalancerChore.chore(BalancerChore.java:48)
	at org.apache.hadoop.hbase.Chore.run(Chore.java:81)
	at java.lang.Thread.run(Thread.java:662)
2013-04-18 08:09:52,103 DEBUG [box,60000,1366281581983-CatalogJanitor] org.apache.hadoop.hbase.client.ClientScanner: Creating scanner over .META. starting at key ''
{noformat}

{code}
          if (regionFinder != null) {
            //region location
            List<ServerName> loc = regionFinder.getTopBlockLocations(region);
            regionLocations[regionIndex] = new int[loc.size()];
            for (int i=0; i < loc.size(); i++) {
              regionLocations[regionIndex][i] = serversToIndex.get(loc.get(i));  // <========= NPE here
            }
          }
{code}


pinging [~enis], just in case.", ,,,,,,,
HBASE-8374,"I don't reproduce this all the time, but I had it on a fairly clean env.
It occurs every 5 minutes (i.e. the balancer period). Impact is severe: the balancer does not run. When it starts to occurs, it occurs all the time. I haven't tried to restart the master, but I think it should be enough.
Now, looking at the code, the NPE is strange. 

{noformat}
2013-04-18 08:09:52,079 ERROR [box,60000,1366281581983-BalancerChore] org.apache.hadoop.hbase.master.balancer.BalancerChore: Caught exception
java.lang.NullPointerException
	at org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer$Cluster.<init>(BaseLoadBalancer.java:145)
	at org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer.balanceCluster(StochasticLoadBalancer.java:194)
	at org.apache.hadoop.hbase.master.HMaster.balance(HMaster.java:1295)
	at org.apache.hadoop.hbase.master.balancer.BalancerChore.chore(BalancerChore.java:48)
	at org.apache.hadoop.hbase.Chore.run(Chore.java:81)
	at java.lang.Thread.run(Thread.java:662)
2013-04-18 08:09:52,103 DEBUG [box,60000,1366281581983-CatalogJanitor] org.apache.hadoop.hbase.client.ClientScanner: Creating scanner over .META. starting at key ''
{noformat}

{code}
          if (regionFinder != null) {
            //region location
            List<ServerName> loc = regionFinder.getTopBlockLocations(region);
            regionLocations[regionIndex] = new int[loc.size()];
            for (int i=0; i < loc.size(); i++) {
              regionLocations[regionIndex][i] = serversToIndex.get(loc.get(i));  // <========= NPE here
            }
          }
{code}


pinging [~enis], just in case.", ,,,,,,,
HBASE-8377,"There is a bug in IntegrationTestBigLinkedList that it reads the wrong config key to calculate the wrap size for the linked list. It uses num mappers, instead of num recors per mapper. This has not been caught before, because it causes the test to fail only if 1M is not divisible by num mappers. So launching the job with num mappers 1, 2, 4, 5 would succeed, while 6 will fail, etc. ", ,,,,,,,
HBASE-8380,"Stack is:
{noformat}
2013-04-19 09:22:45,991 WARN  [IPC Client (682317035) connection to ip-10-6-131-32.ec2.internal/10.6.131.32:60020 from root] ipc.HBaseClient (HBaseClient.java:run(664)) - IPC Client (682317035) connection to ip-10-6-131-32.ec2.internal/10.6.131.32:60020 from root: unexpected exception receiving call responses
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.readResponse(HBaseClient.java:1017)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.run(HBaseClient.java:661)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.readResponse(HBaseClient.java:1013)
	... 1 more
 1197 sec: 3411081 operations; 324,27 current ops/sec; [INSERT AverageLatency(us)=29332,6] 
{noformat}

Code:

{code}
    protected void readResponse() {
      if (shouldCloseConnection.get()) return;
      touch();
      try {
        // See HBaseServer.Call.setResponse for where we write out the response.

        // Total size of the response.  Unused.  But have to read it in anyways.
        /*int totalSize =*/ in.readInt();

        // Read the header
        ResponseHeader responseHeader = ResponseHeader.parseDelimitedFrom(in);
        int id = responseHeader.getCallId();
        if (LOG.isDebugEnabled()) {
          LOG.debug(getName() + "": got response header "" +
            TextFormat.shortDebugString(responseHeader));
        }
        Call call = calls.get(id);
        if (responseHeader.hasException()) {
          ExceptionResponse exceptionResponse = responseHeader.getException();
          RemoteException re = createRemoteException(exceptionResponse);
          if (isFatalConnectionException(exceptionResponse)) {
            markClosed(re);
          } else {
            if (call != null) call.setException(re);
          }
        } else {
          Message rpcResponseType;
          try {
            // TODO: Why pb engine pollution in here in this class?  FIX.
            rpcResponseType =
              ProtobufRpcClientEngine.Invoker.getReturnProtoType(
                reflectionCache.getMethod(remoteId.getProtocol(), call.method.getName()));  <=========== NPE, because call is null
          } catch (Exception e) {
            throw new RuntimeException(e); //local exception
          }
{code}
", ,,,,,,,
HBASE-8384,"There's something off with heap size computation for HStore. If one adds a long to HStore, and size of long to the computation, TestHeapSize passes both locally (for me) and in jenkins. If one adds an int and size of int, it passes locally but fails in Jenkins. Perhaps the numbers are already off and some sort of packing is taking/not taking place differently.

On a tangentially related note, if we can obtain size programmatically (the way we do it in test), and only need to do it once, I wonder if we should just do it and remove all the manually modifiable constants stuff.", ,,,,,,,
HBASE-8385,"Expected behavior:
A user should be able to:
1. Take a snapshot of a table
2. Delete that table
3. Use the snapshot to restore that deleted table

Observed behavior:
During a restore, we attempt to create a snapshot of the table should the restore go awry. However, the snapshot fails because the table that we want to snapshot is not present.

{code}
Stack trace:

org.apache.hadoop.hbase.exceptions.SnapshotCreationException: org.apache.hadoop.hbase.exceptions.SnapshotCreationException: Could not build snapshot handler
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:95)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:79)
	at org.apache.hadoop.hbase.ipc.ProtobufRpcClientEngine$Invoker.invoke(ProtobufRpcClientEngine.java:146)
	at $Proxy24.snapshot(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$MasterProtocolHandler.invoke(HConnectionManager.java:1703)
	at org.apache.hadoop.hbase.client.$Proxy25.snapshot(Unknown Source)
	at org.apache.hadoop.hbase.client.HBaseAdmin$18.call(HBaseAdmin.java:2337)
	at org.apache.hadoop.hbase.client.HBaseAdmin$18.call(HBaseAdmin.java:1)
	at org.apache.hadoop.hbase.client.HBaseAdmin.executeCallable(HBaseAdmin.java:2637)
	at org.apache.hadoop.hbase.client.HBaseAdmin.execute(HBaseAdmin.java:2612)
	at org.apache.hadoop.hbase.client.HBaseAdmin.takeSnapshotAsync(HBaseAdmin.java:2334)
	at org.apache.hadoop.hbase.client.HBaseAdmin.snapshot(HBaseAdmin.java:2279)
	at org.apache.hadoop.hbase.client.HBaseAdmin.snapshot(HBaseAdmin.java:2252)
	at org.apache.hadoop.hbase.client.HBaseAdmin.snapshot(HBaseAdmin.java:2204)
	at org.apache.hadoop.hbase.client.HBaseAdmin.restoreSnapshot(HBaseAdmin.java:2417)
	at org.apache.hadoop.hbase.client.TestRestoreSnapshotFromClient.testRestoreSnapshotOfDeleted(TestRestoreSnapshotFromClient.java:266)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)
Caused by: org.apache.hadoop.hbase.ipc.RemoteWithExtrasException: org.apache.hadoop.hbase.exceptions.SnapshotCreationException: Could not build snapshot handler
	at org.apache.hadoop.hbase.master.snapshot.SnapshotManager.snapshotDisabledTable(SnapshotManager.java:573)
	at org.apache.hadoop.hbase.master.snapshot.SnapshotManager.takeSnapshot(SnapshotManager.java:524)
	at org.apache.hadoop.hbase.master.HMaster.snapshot(HMaster.java:2521)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.ProtobufRpcServerEngine$Server.call(ProtobufRpcServerEngine.java:174)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1871)
Caused by: java.io.IOException: HTableDescriptor missing for testtb-1366407250932
	at org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler.loadTableDescriptor(TakeSnapshotHandler.java:125)
	at org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler.prepare(TakeSnapshotHandler.java:136)
	at org.apache.hadoop.hbase.master.snapshot.DisabledTableSnapshotHandler.prepare(DisabledTableSnapshotHandler.java:77)
	at org.apache.hadoop.hbase.master.snapshot.SnapshotManager.snapshotDisabledTable(SnapshotManager.java:557)
	... 8 more

	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:1321)
	at org.apache.hadoop.hbase.ipc.ProtobufRpcClientEngine$Invoker.invoke(ProtobufRpcClientEngine.java:131)
	... 44 more
{code}", ,,,,,,,
HBASE-8388,"Currently only ROWCOL BFs are used during the scanning process. It seems we could make use of ROW BFs in StoreFileScanner.requestSeek.
See HBASE-8362 for background.", ,,1,,,,,
HBASE-8389,"We ran hbase 0.94.3 patched with 8354 and observed too many outstanding lease recoveries because of the short retry interval of 1 second between lease recoveries.

The namenode gets into the following loop:
1) Receives lease recovery request and initiates recovery choosing a primary datanode every second
2) A lease recovery is successful and the namenode tries to commit the block under recovery as finalized - this takes < 10 seconds in our environment since we run with tight HDFS socket timeouts.
3) At step 2), there is a more recent recovery enqueued because of the aggressive retries. This causes the committed block to get preempted and we enter a vicious cycle

So we do,  <initiate_recovery> --> <commit_block> --> <commit_preempted_by_another_recovery>

This loop is paused after 300 seconds which is the ""hbase.lease.recovery.timeout"". Hence the MTTR we are observing is 5 minutes which is terrible. Our ZK session timeout is 30 seconds and HDFS stale node detection timeout is 20 seconds.

Note that before the patch, we do not call recoverLease so aggressively - also it seems that the HDFS namenode is pretty dumb in that it keeps initiating new recoveries for every call. Before the patch, we call recoverLease, assume that the block was recovered, try to get the file, it has zero length since its under recovery, we fail the task and retry until we get a non zero length. So things just work.

Fixes:
1) Expecting recovery to occur within 1 second is too aggressive. We need to have a more generous timeout. The timeout needs to be configurable since typically, the recovery takes as much time as the DFS timeouts. The primary datanode doing the recovery tries to reconcile the blocks and hits the timeouts when it tries to contact the dead node. So the recovery is as fast as the HDFS timeouts.

2) We have another issue I report in HDFS 4721. The Namenode chooses the stale datanode to perform the recovery (since its still alive). Hence the first recovery request is bound to fail. So if we want a tight MTTR, we either need something like HDFS 4721 or we need something like this

  recoverLease(...)
  sleep(1000)
  recoverLease(...)
  sleep(configuredTimeout)
  recoverLease(...)
  sleep(configuredTimeout)

Where configuredTimeout should be large enough to let the recovery happen but the first timeout is short so that we get past the moot recovery in step #1.
 
", ,1,1,,,,,
HBASE-8396,"first time on balancerSwitch we will create balancerZnode.
from second time onwards we will try to create node(1st time) which will fail with NodeExistsException then we will call setData(2nd time).
{code}
    try {
      ZKUtil.createAndWatch(watcher, watcher.balancerZNode, upData);
    } catch(KeeperException.NodeExistsException nee) {
      ZKUtil.setData(watcher, watcher.balancerZNode, upData);
    }
{code}

We can change as below to avoid extra zk call from second time onwards.
{code}
    try {
      ZKUtil.setData(watcher, watcher.balancerZNode, upData);
    } catch(KeeperException.NoNodeException nne) {
      ZKUtil.createAndWatch(watcher, watcher.balancerZNode, upData);
    }
{code}", ,,,,,,,
HBASE-8402,"Currently, scan metrics is not published in case there is one trip to server. I was testing it on a small row range (200 rows) with a large cache value (1000). It doesn't look right as metrics should not depend on number of rpc calls (number of rpc call is just one metrics fwiw).
", ,1,,,,,,
HBASE-8409,"This task adds the security piece to the namespace feature. The work related to migration of the existing acl table to the new namespace is remaining and will be completed in the follow up patch. Permissions can be granted to a namespace by the hbase admin, by appending '@' to the namespace name. A user with write or admin permissions on a given namespace can create tables in that namespace. The other privileges (R, X, C ) do not have any special meaning w.r.t namespaces. Any users of hbase can list tables in a namespace.
The following commands can only be executed by HBase admins.
1. Grant privileges for user on Namespace.
2. Revoke privileges for user on Namespace
Grant Command:
hbase> grant 'tenant-A' 'W' '@N1'
In the above example, the command will grant the user 'tenant-A' write privileges for a namespace named ""N1"".
Revoke Command:
hbase> revoke 'tenant-A''@N1'
In the above example, the command will revoke all privileges from user 'tenant-A' for namespace named ""N1"".
Lets see an example on how privileges work with namespaces.
User ""Mike"" request for a namespace named ""hbase_perf"" with the hbase admin.
whoami: hbase
hbase shell >> namespace_create 'hbase_perf'
hbase shell >> grant 'mike', 'W', '@hbase_perf'
Mike creates two tables ""table20"" and ""table50"" in the above workspace.
whoami: mike
hbase shell >> create 'hbase_perf.table20', 'family1'
hbase shell >> create 'hbase_perf.table50', 'family1'
Note: As Mike was able to create tables 'hbase_perf.table20', 'hbase_perf.table50', he becomes the owner of those tables. 
This means he has ""RWXCA"" perms on those tables.
Another team member of Mike, Alice wants also to share the same workspace ""hbase_perf"". HBase admin grants Alice also permission to create tables in ""hbase_perf"" namespace.
whoami: hbase
hbase shell >> grant 'alice', 'W', '@hbase_perf'
Now Alice can create new tables under ""hbase_perf"" namespace, but cannot read,write,alter,delete existing tables in the namespace.
whoami: alice
hbase shell >> namespace_list_tables 'hbase_perf'
hbase_perf.table20
hbase_perf.table50
hbase shell >> scan 'hbase_perf.table20'
AccessDeniedException 
If Alice wants to read or write to existing tables in the ""hbase_perf"" namespace, hbase admins need to explicitly grant permission.
whoami: hbase
hbase shell >> grant 'alice', 'RW', 'hbase_perf.table20'
hbase shell >> grant 'alice', 'RW', 'hbase_perf.table50'",1,,,,,,,
HBASE-8413,"MasterSnapshotVerifier::verifyRegion():
...
// check to see if hfile is present in the real table
        String fileName = hfile.getPath().getName();
        Path file = new Path(realCfDir, fileName);
        Path archived = new Path(archivedCfDir, fileName);
        if (!fs.exists(file) && !file.equals(archived)) {
          throw new CorruptedSnapshotException(""Can't find hfile: "" + hfile.getPath()
              + "" in the real ("" + realCfDir + "") or archive ("" + archivedCfDir
              + "") directory for the primary table."", snapshot);
        }
....

The second part of the if condition will always be true:
!file.equals(archived)

Corrected:
if (!fs.exists(file) && !fs.exists(archived))", ,1,,,,,,
HBASE-8416,"This morning one our region servers (we have 44) stopped responding to
the '/jmx' request. (It's working for regular activity.)  Additionally,
the region server is now using all the CPU on the host, running all 8
cores at 100%.

A full jstack is at:
http://pastebin.com/dGTmTEN7


Right now, there are 37 threads stuck here:
""38565532@qtp-228776471-196"" prio=10 tid=0x00002aaacc4f2800 nid=0x7f57 runnable [0x0000000054a48000]
   java.lang.Thread.State: RUNNABLE
        at java.util.HashMap.get(HashMap.java:303)
        at org.apache.hadoop.metrics.util.MetricsDynamicMBeanBase.getAttribute(MetricsDynamicMBeanBase.java:137)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:666)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:638)
        at org.apache.hadoop.jmx.JMXJsonServlet.writeAttribute(JMXJsonServlet.java:315)
        at org.apache.hadoop.jmx.JMXJsonServlet.listBeans(JMXJsonServlet.java:293)
        at org.apache.hadoop.jmx.JMXJsonServlet.doGet(JMXJsonServlet.java:193)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:734)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:847)
        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
        at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
        at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:1056)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
        at org.mortbay.jetty.Server.handle(Server.java:326)
        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)

", ,,1,,,,,
HBASE-8422,"Master came up w/ no regionservers.  I then tried to shut it down.  You can see in below that it started to go down....

{code}
2013-04-24 14:28:49,770 INFO  [IPC Server handler 7 on 60000] org.apache.hadoop.hbase.master.HMaster: Cluster shutdown requested
2013-04-24 14:28:49,815 INFO  [master-stack-1.ent.cloudera.com,60000,1366838923135] org.apache.hadoop.hbase.master.ServerManager: Finished waiting for region servers count to settle; checked in 0, slept for 2818 ms, expecting minimum of 1, maximum of 2147483647, master is stopped.
2013-04-24 14:28:49,815 WARN  [master-stack-1.ent.cloudera.com,60000,1366838923135] org.apache.hadoop.hbase.master.MasterFileSystem: Master stopped while splitting logs
2013-04-24 14:28:50,104 INFO  [stack-1.ent.cloudera.com,60000,1366838923135.splitLogManagerTimeoutMonitor] org.apache.hadoop.hbase.master.SplitLogManager$TimeoutMonitor: stack-1.ent.cloudera.com,60000,1366838923135.splitLogManagerTimeoutMonitor exiting
2013-04-24 14:28:50,850 INFO  [master-stack-1.ent.cloudera.com,60000,1366838923135] org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker: Unsetting META region location in ZooKeeper
2013-04-24 14:28:50,884 WARN  [master-stack-1.ent.cloudera.com,60000,1366838923135] org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper: Node /hbase/meta-region-server already deleted, retry=false
2013-04-24 14:28:50,884 INFO  [master-stack-1.ent.cloudera.com,60000,1366838923135] org.apache.hadoop.hbase.master.AssignmentManager: Cluster shutdown is set; skipping assign of .META.,,1.1028785192
2013-04-24 14:28:50,884 INFO  [master-stack-1.ent.cloudera.com,60000,1366838923135] org.apache.hadoop.hbase.master.ServerManager: AssignmentManager hasn't finished failover cleanup
2013-04-24 14:29:46,188 INFO  [master-stack-1.ent.cloudera.com,60000,1366838923135.oldLogCleaner] org.apache.hadoop.hbase.master.cleaner.LogCleaner: master-stack-1.ent.cloudera.com,60000,1366838923135.oldLogCleaner exiting
2013-04-24 14:29:46,193 INFO  [master-stack-1.ent.cloudera.com,60000,1366838923135.archivedHFileCleaner] org.apache.hadoop.hbase.master.cleaner.HFileCleaner: master-stack-1.ent.cloudera.com,60000,1366838923135.archivedHFileCleaner exiting
{code}

... but not it is stuck.

We keep looping here:

{code}
""master-stack-1.ent.cloudera.com,60000,1366838923135"" prio=10 tid=0x00007f154853f000 nid=0x18b in Object.wait() [0x00007f1545fde000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x00000000c727d738> (a org.apache.hadoop.hbase.zookeeper.MetaRegionTracker)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.blockUntilAvailable(ZooKeeperNodeTracker.java:161)
        - locked <0x00000000c727d738> (a org.apache.hadoop.hbase.zookeeper.MetaRegionTracker)
        at org.apache.hadoop.hbase.zookeeper.MetaRegionTracker.waitMetaRegionLocation(MetaRegionTracker.java:105)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMeta(CatalogTracker.java:250)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMeta(CatalogTracker.java:299)
        at org.apache.hadoop.hbase.master.HMaster.enableSSHandWaitForMeta(HMaster.java:905)
        at org.apache.hadoop.hbase.master.HMaster.assignMeta(HMaster.java:879)
        at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:764)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:522)
        at java.lang.Thread.run(Thread.java:722)
{code}

Odd.  It is supposed to be checking the 'stopped' flag; maybe it has wrong stop flag.", ,,,,,,,
HBASE-8426,"I restarted a cluster on 0.95 (1ecd4c7e0b22bba75c76f2fc2ce369541502b6df) and some regions failed to open on their first assignment on an exception like:

{noformat}
Caused by: org.apache.hadoop.metrics2.MetricsException: Metrics source RegionServer,sub=Regions already exists!
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newSourceName(DefaultMetricsSystem.java:126)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.sourceName(DefaultMetricsSystem.java:107)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:217)
	at org.apache.hadoop.hbase.metrics.BaseSourceImpl.<init>(BaseSourceImpl.java:75)
	at org.apache.hadoop.hbase.regionserver.MetricsRegionAggregateSourceImpl.<init>(MetricsRegionAggregateSourceImpl.java:49)
	at org.apache.hadoop.hbase.regionserver.MetricsRegionAggregateSourceImpl.<init>(MetricsRegionAggregateSourceImpl.java:41)
	at org.apache.hadoop.hbase.regionserver.MetricsRegionServerSourceFactoryImpl.getAggregate(MetricsRegionServerSourceFactoryImpl.java:33)
	at org.apache.hadoop.hbase.regionserver.MetricsRegionServerSourceFactoryImpl.createRegion(MetricsRegionServerSourceFactoryImpl.java:50)
	at org.apache.hadoop.hbase.regionserver.MetricsRegion.<init>(MetricsRegion.java:35)
	at org.apache.hadoop.hbase.regionserver.HRegion.<init>(HRegion.java:488)
	at org.apache.hadoop.hbase.regionserver.HRegion.<init>(HRegion.java:400)

{noformat}

I'm attaching a bigger log.", ,,,,,,,
HBASE-8430,"Cell scanner, base decoder, etc., hide IOException inside runtime exception. This can lead to unexpected behavior because a lot of code only expects IOException. There's no logical justification behind this hiding so it should be removed before it's too late (the sooner we do it the less throws declarations need to be added)", ,,,,,,,
HBASE-8432,"it happened that a table with unbalanced regions, as follows in my cluster(the cluster has 20 regionservers, the table has 12 regions):
http://hadoopdev19.cm6:60030/	1
http://hadoopdev8.cm6:60030/	2
http://hadoopdev17.cm6:60030/	1
http://hadoopdev12.cm6:60030/	1
http://hadoopdev5.cm6:60030/	1
http://hadoopdev9.cm6:60030/	1
http://hadoopdev22.cm6:60030/	1
http://hadoopdev11.cm6:60030/	1
http://hadoopdev21.cm6:60030/	1
http://hadoopdev16.cm6:60030/	1
http://hadoopdev10.cm6:60030/	1
with the 'org.apache.hadoop.hbase.master.DefaultLoadBalancer', after 5 times load-balances, the table are still unbalanced:
http://hadoopdev3.cm6:60030/	1
http://hadoopdev20.cm6:60030/	1
http://hadoopdev4.cm6:60030/	2
http://hadoopdev18.cm6:60030/	1
http://hadoopdev12.cm6:60030/	1
http://hadoopdev14.cm6:60030/	1
http://hadoopdev15.cm6:60030/	1
http://hadoopdev6.cm6:60030/	1
http://hadoopdev13.cm6:60030/	1
http://hadoopdev11.cm6:60030/	1
http://hadoopdev10.cm6:60030/	1

http://hadoopdev19.cm6:60030/	1
http://hadoopdev17.cm6:60030/	1
http://hadoopdev8.cm6:60030/	1
http://hadoopdev5.cm6:60030/	1
http://hadoopdev12.cm6:60030/	1
http://hadoopdev22.cm6:60030/	1
http://hadoopdev11.cm6:60030/	1
http://hadoopdev21.cm6:60030/	1
http://hadoopdev7.cm6:60030/	2
http://hadoopdev10.cm6:60030/	1
http://hadoopdev16.cm6:60030/	1

http://hadoopdev3.cm6:60030/	1
http://hadoopdev20.cm6:60030/	1
http://hadoopdev4.cm6:60030/	1
http://hadoopdev18.cm6:60030/	2
http://hadoopdev12.cm6:60030/	1
http://hadoopdev14.cm6:60030/	1
http://hadoopdev15.cm6:60030/	1
http://hadoopdev6.cm6:60030/	1
http://hadoopdev13.cm6:60030/	1
http://hadoopdev11.cm6:60030/	1
http://hadoopdev10.cm6:60030/	1

http://hadoopdev19.cm6:60030/	1
http://hadoopdev8.cm6:60030/	1
http://hadoopdev17.cm6:60030/	1
http://hadoopdev12.cm6:60030/	1
http://hadoopdev5.cm6:60030/	1
http://hadoopdev22.cm6:60030/	1
http://hadoopdev11.cm6:60030/	1
http://hadoopdev7.cm6:60030/	1
http://hadoopdev21.cm6:60030/	2
http://hadoopdev16.cm6:60030/	1
http://hadoopdev10.cm6:60030/	1

http://hadoopdev3.cm6:60030/	1
http://hadoopdev20.cm6:60030/	1
http://hadoopdev18.cm6:60030/	1
http://hadoopdev4.cm6:60030/	1
http://hadoopdev12.cm6:60030/	1
http://hadoopdev15.cm6:60030/	1
http://hadoopdev14.cm6:60030/	2
http://hadoopdev6.cm6:60030/	1
http://hadoopdev13.cm6:60030/	1
http://hadoopdev11.cm6:60030/	1
http://hadoopdev10.cm6:60030/	1

from the above logs, we can also find that some regions needn't move, but they moved. follow into 'org.apache.hadoop.hbase.master.DefaultLoadBalancer.balanceCluster()', I found that 'maxToTake' is error calculated. ", ,,1,,,,,
HBASE-8433,"I found the scan will be in a forever loop when using PREFIX_TREE DataBlockEncoding.

And found the root cause is:
CellComparator#compare returns incorrect result for faked KeyValue.

Patch is simple...

Provide a rude test case to verify", ,,,,,,,
HBASE-8436,"In SeekBefore case, if we seek the key which is bigger than the last keyvalue
PrefixTreeSeeker#seekToKeyInBlock will not seek correctly.

Make the test case TestPrefixTreeEncoding#testSeekBeforeWithFixedData in the patch to show this problem.", ,,,,,,,
HBASE-8441,"The code added to make sure it doesn't get stuck, doesn't take into account filesCompacting.
This is the cause of recent TestHFileArchiving failures...", ,,,,,,,
HBASE-8448,"The code added to make sure it doesn't get stuck, doesn't take into account filesCompacting.
This is the cause of recent TestHFileArchiving failures...", ,,,,,,,
HBASE-8449,"MasterSnapshotVerifier::verifyRegion():
...
// check to see if hfile is present in the real table
        String fileName = hfile.getPath().getName();
        Path file = new Path(realCfDir, fileName);
        Path archived = new Path(archivedCfDir, fileName);
        if (!fs.exists(file) && !file.equals(archived)) {
          throw new CorruptedSnapshotException(""Can't find hfile: "" + hfile.getPath()
              + "" in the real ("" + realCfDir + "") or archive ("" + archivedCfDir
              + "") directory for the primary table."", snapshot);
        }
....

The second part of the if condition will always be true:
!file.equals(archived)

Corrected:
if (!fs.exists(file) && !fs.exists(archived))", ,1,,,,,,
HBASE-8452,"it happened that a table with unbalanced regions, as follows in my cluster(the cluster has 20 regionservers, the table has 12 regions):
http://hadoopdev19.cm6:60030/	1
http://hadoopdev8.cm6:60030/	2
http://hadoopdev17.cm6:60030/	1
http://hadoopdev12.cm6:60030/	1
http://hadoopdev5.cm6:60030/	1
http://hadoopdev9.cm6:60030/	1
http://hadoopdev22.cm6:60030/	1
http://hadoopdev11.cm6:60030/	1
http://hadoopdev21.cm6:60030/	1
http://hadoopdev16.cm6:60030/	1
http://hadoopdev10.cm6:60030/	1
with the 'org.apache.hadoop.hbase.master.DefaultLoadBalancer', after 5 times load-balances, the table are still unbalanced:
http://hadoopdev3.cm6:60030/	1
http://hadoopdev20.cm6:60030/	1
http://hadoopdev4.cm6:60030/	2
http://hadoopdev18.cm6:60030/	1
http://hadoopdev12.cm6:60030/	1
http://hadoopdev14.cm6:60030/	1
http://hadoopdev15.cm6:60030/	1
http://hadoopdev6.cm6:60030/	1
http://hadoopdev13.cm6:60030/	1
http://hadoopdev11.cm6:60030/	1
http://hadoopdev10.cm6:60030/	1

http://hadoopdev19.cm6:60030/	1
http://hadoopdev17.cm6:60030/	1
http://hadoopdev8.cm6:60030/	1
http://hadoopdev5.cm6:60030/	1
http://hadoopdev12.cm6:60030/	1
http://hadoopdev22.cm6:60030/	1
http://hadoopdev11.cm6:60030/	1
http://hadoopdev21.cm6:60030/	1
http://hadoopdev7.cm6:60030/	2
http://hadoopdev10.cm6:60030/	1
http://hadoopdev16.cm6:60030/	1

http://hadoopdev3.cm6:60030/	1
http://hadoopdev20.cm6:60030/	1
http://hadoopdev4.cm6:60030/	1
http://hadoopdev18.cm6:60030/	2
http://hadoopdev12.cm6:60030/	1
http://hadoopdev14.cm6:60030/	1
http://hadoopdev15.cm6:60030/	1
http://hadoopdev6.cm6:60030/	1
http://hadoopdev13.cm6:60030/	1
http://hadoopdev11.cm6:60030/	1
http://hadoopdev10.cm6:60030/	1

http://hadoopdev19.cm6:60030/	1
http://hadoopdev8.cm6:60030/	1
http://hadoopdev17.cm6:60030/	1
http://hadoopdev12.cm6:60030/	1
http://hadoopdev5.cm6:60030/	1
http://hadoopdev22.cm6:60030/	1
http://hadoopdev11.cm6:60030/	1
http://hadoopdev7.cm6:60030/	1
http://hadoopdev21.cm6:60030/	2
http://hadoopdev16.cm6:60030/	1
http://hadoopdev10.cm6:60030/	1

http://hadoopdev3.cm6:60030/	1
http://hadoopdev20.cm6:60030/	1
http://hadoopdev18.cm6:60030/	1
http://hadoopdev4.cm6:60030/	1
http://hadoopdev12.cm6:60030/	1
http://hadoopdev15.cm6:60030/	1
http://hadoopdev14.cm6:60030/	2
http://hadoopdev6.cm6:60030/	1
http://hadoopdev13.cm6:60030/	1
http://hadoopdev11.cm6:60030/	1
http://hadoopdev10.cm6:60030/	1

from the above logs, we can also find that some regions needn't move, but they moved. follow into 'org.apache.hadoop.hbase.master.DefaultLoadBalancer.balanceCluster()', I found that 'maxToTake' is error calculated. ", ,,1,,,,,
HBASE-8453,"MasterSnapshotVerifier::verifyRegion():
...
// check to see if hfile is present in the real table
        String fileName = hfile.getPath().getName();
        Path file = new Path(realCfDir, fileName);
        Path archived = new Path(archivedCfDir, fileName);
        if (!fs.exists(file) && !file.equals(archived)) {
          throw new CorruptedSnapshotException(""Can't find hfile: "" + hfile.getPath()
              + "" in the real ("" + realCfDir + "") or archive ("" + archivedCfDir
              + "") directory for the primary table."", snapshot);
        }
....

The second part of the if condition will always be true:
!file.equals(archived)

Corrected:
if (!fs.exists(file) && !fs.exists(archived))", ,1,,,,,,
HBASE-8458,"it happened that a table with unbalanced regions, as follows in my cluster(the cluster has 20 regionservers, the table has 12 regions):
http://hadoopdev19.cm6:60030/	1
http://hadoopdev8.cm6:60030/	2
http://hadoopdev17.cm6:60030/	1
http://hadoopdev12.cm6:60030/	1
http://hadoopdev5.cm6:60030/	1
http://hadoopdev9.cm6:60030/	1
http://hadoopdev22.cm6:60030/	1
http://hadoopdev11.cm6:60030/	1
http://hadoopdev21.cm6:60030/	1
http://hadoopdev16.cm6:60030/	1
http://hadoopdev10.cm6:60030/	1
with the 'org.apache.hadoop.hbase.master.DefaultLoadBalancer', after 5 times load-balances, the table are still unbalanced:
http://hadoopdev3.cm6:60030/	1
http://hadoopdev20.cm6:60030/	1
http://hadoopdev4.cm6:60030/	2
http://hadoopdev18.cm6:60030/	1
http://hadoopdev12.cm6:60030/	1
http://hadoopdev14.cm6:60030/	1
http://hadoopdev15.cm6:60030/	1
http://hadoopdev6.cm6:60030/	1
http://hadoopdev13.cm6:60030/	1
http://hadoopdev11.cm6:60030/	1
http://hadoopdev10.cm6:60030/	1

http://hadoopdev19.cm6:60030/	1
http://hadoopdev17.cm6:60030/	1
http://hadoopdev8.cm6:60030/	1
http://hadoopdev5.cm6:60030/	1
http://hadoopdev12.cm6:60030/	1
http://hadoopdev22.cm6:60030/	1
http://hadoopdev11.cm6:60030/	1
http://hadoopdev21.cm6:60030/	1
http://hadoopdev7.cm6:60030/	2
http://hadoopdev10.cm6:60030/	1
http://hadoopdev16.cm6:60030/	1

http://hadoopdev3.cm6:60030/	1
http://hadoopdev20.cm6:60030/	1
http://hadoopdev4.cm6:60030/	1
http://hadoopdev18.cm6:60030/	2
http://hadoopdev12.cm6:60030/	1
http://hadoopdev14.cm6:60030/	1
http://hadoopdev15.cm6:60030/	1
http://hadoopdev6.cm6:60030/	1
http://hadoopdev13.cm6:60030/	1
http://hadoopdev11.cm6:60030/	1
http://hadoopdev10.cm6:60030/	1

http://hadoopdev19.cm6:60030/	1
http://hadoopdev8.cm6:60030/	1
http://hadoopdev17.cm6:60030/	1
http://hadoopdev12.cm6:60030/	1
http://hadoopdev5.cm6:60030/	1
http://hadoopdev22.cm6:60030/	1
http://hadoopdev11.cm6:60030/	1
http://hadoopdev7.cm6:60030/	1
http://hadoopdev21.cm6:60030/	2
http://hadoopdev16.cm6:60030/	1
http://hadoopdev10.cm6:60030/	1

http://hadoopdev3.cm6:60030/	1
http://hadoopdev20.cm6:60030/	1
http://hadoopdev18.cm6:60030/	1
http://hadoopdev4.cm6:60030/	1
http://hadoopdev12.cm6:60030/	1
http://hadoopdev15.cm6:60030/	1
http://hadoopdev14.cm6:60030/	2
http://hadoopdev6.cm6:60030/	1
http://hadoopdev13.cm6:60030/	1
http://hadoopdev11.cm6:60030/	1
http://hadoopdev10.cm6:60030/	1

from the above logs, we can also find that some regions needn't move, but they moved. follow into 'org.apache.hadoop.hbase.master.DefaultLoadBalancer.balanceCluster()', I found that 'maxToTake' is error calculated. ", ,,1,,,,,
HBASE-8462,"Client supplied timestamps should not be allowed to be negative, otherwise unpredictable results will follow. Especially, since we are encoding the ts using Bytes.Bytes(long), negative timestamps are sorted after positive ones. Plus, the new PB messages define ts' as uint64. 

Credit goes to Huned Lokhandwala for reporting this.", ,1,,,,,,
HBASE-8482,"I've been looking into this test failure because I thought it particular to my rpc hackery.

What I see is like the subject:

{code}
java.lang.AssertionError: expected:<[]> but was:<[EXPIRED_TABLE_LOCK]>
{code}

and later in same unit test:

{code}
java.lang.AssertionError: expected:<[EXPIRED_TABLE_LOCK]> but was:<[EXPIRED_TABLE_LOCK, EXPIRED_TABLE_LOCK]>
{code}

The test creates a write lock and then expires it.  In subject failure, we are expiring the lock ahead of the time it should be.  Easier for me to reproduce is that the second write lock we put in place is not allowed to happen because of the presence of the first lock EVEN THOUGH IT HAS BEEN JUDGED EXPIRED:

{code}
ERROR: Table lock acquire attempt found:[tableName=foo, lockOwner=localhost,60000,1, threadId=387, purpose=testCheckTableLocks, isShared=false, createTime=129898749]
2013-05-02 00:34:42,715 INFO  [Thread-183] lock.ZKInterProcessLockBase(431): Lock is held by: write-testing utility0000000000
ERROR: Table lock acquire attempt found:[tableName=foo, lockOwner=localhost,60000,1, threadId=349, purpose=testCheckTableLocks, isShared=false, createTime=28506852]
{code}

Above, you see the expired lock and then our hbck lock visitor has it that the second lock is expired because it is held by the first lock.

I can keep looking at this but input would be appreciated.

It failed in recent trunk build https://builds.apache.org/view/H-L/view/HBase/job/HBase-TRUNK/4090/testReport/junit/org.apache.hadoop.hbase.util/TestHBaseFsck/testCheckTableLocks/", ,,,,,,,
HBASE-8483,"If one thread calls deleteStaleConnection while other threads are using connection, can leak ZK connections.", ,,,,,,,
HBASE-8492,"From a failed 0.94 build on EC2 Jenkins:

{noformat}
Failed tests:   testClusterStatus(org.apache.hadoop.hbase.rest.client.TestRemoteAdmin): Number of requests from cluster status and returned status did not match up.  expected:<4> but was:<7>
{noformat}

TestRemoteAdmin#testClusterStatus should not assume status does not change from when the client gets it directly and when the client asks again via REST. ", ,,,,,,,
HBASE-8494,"From a failed 0.94 build on EC2 Jenkins:

{noformat}
Failed tests:   testClusterStatus(org.apache.hadoop.hbase.rest.client.TestRemoteAdmin): Number of requests from cluster status and returned status did not match up.  expected:<4> but was:<7>
{noformat}

TestRemoteAdmin#testClusterStatus should not assume status does not change from when the client gets it directly and when the client asks again via REST. ", ,,,,,,,
HBASE-8498,"From dev list, there's IOException during log splitting on hadoop-2.0.3. We need to understand whether this is caused by EOF, and ignore it if it is.", ,,,,,,,
HBASE-8501,"The ZooKeeperMainServerArg return String like this:<IP1>,<IP2>,<IP3>:<port>, but zookeeper client connection String shoud like this :<IP1>:<port>,<IP1>:<port>,<IP1>:<port>, if changed the zookeeper client port, the zkcli can't connection the first and the second ip.", ,1,,,,,,
HBASE-8502,"  Exact HBase version: 0.92.1-cdh4.1.2

A couple of days ago I encountered a RIT problem with a single region.
After an hbck run it started trying to assign a region which has been 
bouncing between OFFLINE/PENDING_OPEN/OPENING for two days afterwards.

This was due to a split gone wrong in some way, which led to several 
reference files being left in the region-directory despite the two relevant HFiles being copies successfully to the daughter.

I will try to give as many details as possible, but unfortunately I was
unable to find any information about the split itself.

Short thread about this issue on the users-ML: http://mail-archives.apache.org/mod_mbox/hbase-user/201305.mbox/%3C5182758B.1060306@neofonie.de%3E

===

Parent region: 5b9c16898a371de58f31f0bdf86b1f8b
Daughter region in question: 79c619508659018ff3ef0887611eb8f7

Rough sequence from the logs seems to be the following:

===
* Received request to open region:
documents,7128586022887322720,1363696791400.79c619508659018ff3ef0887611eb8f7.

* Setting up tabledescriptor config now ...

* Opening of region {NAME =>
'documents,7128586022887322720,1363696791400.79c619508659018ff3ef0887611eb8f7.',
     STARTKEY => '7128586022887322720',
     ENDKEY => '7130716361635801616',
     ENCODED => 79c619508659018ff3ef0887611eb8f7,} failed, marking as 
FAILED_OPEN in ZK

* File does not exist: 
/hbase/documents/5b9c16898a371de58f31f0bdf86b1f8b/d/0707b1ec4c6b41cf9174e0d2a1785fe9 
[...]
===

What happened, was that somehow (and that's the question here) the daughters
region folder contained some left-over reference files were causing the 
RegionServer to look-up the parent region, which already was deleted.

original contents of /hbase/documents/79c619508659018ff3ef0887611eb8f7/d:
==
0707b1ec4c6b41cf9174e0d2a1785fe9.5b9c16898a371de58f31f0bdf86b1f8b
47511faae81b4452afd3ca206e28346f.5b9c16898a371de58f31f0bdf86b1f8b
4f01ecd052ce464d81e79a62ea227d6b
4f01ecd052ce464d81e79a62ea227d6b.5b9c16898a371de58f31f0bdf86b1f8b
eb7dbb09701d4353be24ca82481c4a7e
== 

I attached the full FileNotFound Exception.

Please let me know if I can provide more information or help otherwise.", ,,1,,,,,
HBASE-8504,"In CatalogJanitor, we clean up the parent regions whose daughters does not have any more references to their parent regions. In doing so, we do two Delete's one for removing the split daughter columns, and the other for removing the row. 

The first one seems unnecessary, and causes NPE from concurrent MetaScanner. 

Stack trace:
{code}
2013-05-07 04:49:40,828|machine|INFO|Exception in thread ""main"" java.lang.NullPointerException
2013-05-07 04:49:40,828|machine|INFO|at org.apache.hadoop.hbase.util.Writables.getWritable(Writables.java:103)
2013-05-07 04:49:40,828|machine|INFO|at org.apache.hadoop.hbase.util.Writables.getHRegionInfo(Writables.java:147)
2013-05-07 04:49:40,829|machine|INFO|at org.apache.hadoop.hbase.client.MetaScanner$BlockingMetaScannerVisitor.processRow(MetaScanner.java:406)
2013-05-07 04:49:40,829|machine|INFO|at org.apache.hadoop.hbase.client.MetaScanner$TableMetaScannerVisitor.processRow(MetaScanner.java:487)
2013-05-07 04:49:40,830|machine|INFO|at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:224)
2013-05-07 04:49:40,830|machine|INFO|at org.apache.hadoop.hbase.client.MetaScanner.access$000(MetaScanner.java:54)
2013-05-07 04:49:40,830|machine|INFO|at org.apache.hadoop.hbase.client.MetaScanner$1.connect(MetaScanner.java:133)
2013-05-07 04:49:40,831|machine|INFO|at org.apache.hadoop.hbase.client.MetaScanner$1.connect(MetaScanner.java:130)
2013-05-07 04:49:40,831|machine|INFO|at org.apache.hadoop.hbase.client.HConnectionManager.execute(HConnectionManager.java:384)
2013-05-07 04:49:40,831|machine|INFO|at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:130)
2013-05-07 04:49:40,832|machine|INFO|at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:105)
2013-05-07 04:49:40,832|machine|INFO|at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:83)
2013-05-07 04:49:40,832|machine|INFO|at org.apache.hadoop.hbase.client.MetaScanner.allTableRegions(MetaScanner.java:323)
2013-05-07 04:49:40,833|machine|INFO|at org.apache.hadoop.hbase.client.HTable.getRegionLocations(HTable.java:485)
2013-05-07 04:49:40,833|machine|INFO|at org.apache.hadoop.hbase.client.HTable.getStartEndKeys(HTable.java:438)
{code}

Master is doing the CatalogJanitor concurrently:
{code}
2013-05-07 04:49:40,636 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted daughters references, qualifier=splitA and qualifier=splitB, from parent IntegrationTestBigLinkedList,\x07\xFB\x98\xB7a\x89\xF5\xE6,1367898577620.4ef1329ff0e8911db998ac8ccd32108d.
2013-05-07 04:49:40,666 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted region IntegrationTestBigLinkedList,\x07\xFB\x98\xB7a\x89\xF5\xE6,1367898577620.4ef1329ff0e8911db998ac8ccd32108d. from META
2013-05-07 04:49:40,690 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted daughters references, qualifier=splitA and qualifier=splitB, from parent IntegrationTestBigLinkedList,\x0B\xF8n\xEA\xD3\xAA\xA9\x92,1367898577620.b502376df2623cb0be3f0c1664d799a6.
2013-05-07 04:49:40,716 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted region IntegrationTestBigLinkedList,\x0B\xF8n\xEA\xD3\xAA\xA9\x92,1367898577620.b502376df2623cb0be3f0c1664d799a6. from META
2013-05-07 04:49:40,742 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted daughters references, qualifier=splitA and qualifier=splitB, from parent IntegrationTestBigLinkedList,\x17\xF5\x11\xB9\xE3\xDB)\x0C,1367898541729.ec2df58fafb823cec6e793ba35e2241d.
{code}

This is critical for 0.94, but not for 0.95 and trunk due to HBASE-7721. ", ,,,,,,,
HBASE-8505,"In CatalogJanitor, we clean up the parent regions whose daughters does not have any more references to their parent regions. In doing so, we do two Delete's one for removing the split daughter columns, and the other for removing the row. 

The first one seems unnecessary, and causes NPE from concurrent MetaScanner. 

Stack trace:
{code}
2013-05-07 04:49:40,828|machine|INFO|Exception in thread ""main"" java.lang.NullPointerException
2013-05-07 04:49:40,828|machine|INFO|at org.apache.hadoop.hbase.util.Writables.getWritable(Writables.java:103)
2013-05-07 04:49:40,828|machine|INFO|at org.apache.hadoop.hbase.util.Writables.getHRegionInfo(Writables.java:147)
2013-05-07 04:49:40,829|machine|INFO|at org.apache.hadoop.hbase.client.MetaScanner$BlockingMetaScannerVisitor.processRow(MetaScanner.java:406)
2013-05-07 04:49:40,829|machine|INFO|at org.apache.hadoop.hbase.client.MetaScanner$TableMetaScannerVisitor.processRow(MetaScanner.java:487)
2013-05-07 04:49:40,830|machine|INFO|at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:224)
2013-05-07 04:49:40,830|machine|INFO|at org.apache.hadoop.hbase.client.MetaScanner.access$000(MetaScanner.java:54)
2013-05-07 04:49:40,830|machine|INFO|at org.apache.hadoop.hbase.client.MetaScanner$1.connect(MetaScanner.java:133)
2013-05-07 04:49:40,831|machine|INFO|at org.apache.hadoop.hbase.client.MetaScanner$1.connect(MetaScanner.java:130)
2013-05-07 04:49:40,831|machine|INFO|at org.apache.hadoop.hbase.client.HConnectionManager.execute(HConnectionManager.java:384)
2013-05-07 04:49:40,831|machine|INFO|at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:130)
2013-05-07 04:49:40,832|machine|INFO|at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:105)
2013-05-07 04:49:40,832|machine|INFO|at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:83)
2013-05-07 04:49:40,832|machine|INFO|at org.apache.hadoop.hbase.client.MetaScanner.allTableRegions(MetaScanner.java:323)
2013-05-07 04:49:40,833|machine|INFO|at org.apache.hadoop.hbase.client.HTable.getRegionLocations(HTable.java:485)
2013-05-07 04:49:40,833|machine|INFO|at org.apache.hadoop.hbase.client.HTable.getStartEndKeys(HTable.java:438)
{code}

Master is doing the CatalogJanitor concurrently:
{code}
2013-05-07 04:49:40,636 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted daughters references, qualifier=splitA and qualifier=splitB, from parent IntegrationTestBigLinkedList,\x07\xFB\x98\xB7a\x89\xF5\xE6,1367898577620.4ef1329ff0e8911db998ac8ccd32108d.
2013-05-07 04:49:40,666 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted region IntegrationTestBigLinkedList,\x07\xFB\x98\xB7a\x89\xF5\xE6,1367898577620.4ef1329ff0e8911db998ac8ccd32108d. from META
2013-05-07 04:49:40,690 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted daughters references, qualifier=splitA and qualifier=splitB, from parent IntegrationTestBigLinkedList,\x0B\xF8n\xEA\xD3\xAA\xA9\x92,1367898577620.b502376df2623cb0be3f0c1664d799a6.
2013-05-07 04:49:40,716 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted region IntegrationTestBigLinkedList,\x0B\xF8n\xEA\xD3\xAA\xA9\x92,1367898577620.b502376df2623cb0be3f0c1664d799a6. from META
2013-05-07 04:49:40,742 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted daughters references, qualifier=splitA and qualifier=splitB, from parent IntegrationTestBigLinkedList,\x17\xF5\x11\xB9\xE3\xDB)\x0C,1367898541729.ec2df58fafb823cec6e793ba35e2241d.
{code}

This is critical for 0.94, but not for 0.95 and trunk due to HBASE-7721. ", ,,,,,,,
HBASE-8509,"As you can see below, we don't pass data down when parent folder doesn't exists.

{code}
    } catch(KeeperException.NoNodeException nne) {
      createWithParents(zkw, getParent(znode));
      createWithParents(zkw, znode);
{code}", ,,,,,,,
HBASE-8517,"I have a cluster that runs IT tests.  Last night after all tests were done I noticed that the balancer was thrashing regions around.

The number of regions on each machine is not correct.
The balancer seems to value the cost of moving a region way too little.
{code}
2013-05-09 16:34:58,920 DEBUG [IPC Server handler 4 on 60000] org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer: Finished computing new load balance plan.  Computation took 5367ms to try 8910 different iterations.  Found a solution that moves 37 regions; Going from a computed cost of 56.50254222730425 to a new cost of 11.214035466575254
2013-05-09 16:37:48,715 DEBUG [IPC Server handler 7 on 60000] org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer: Finished computing new load balance plan.  Computation took 4735ms to try 8910 different iterations.  Found a solution that moves 38 regions; Going from a computed cost of 56.612624531830996 to a new cost of 11.275763861636982
2013-05-09 16:38:11,398 DEBUG [IPC Server handler 6 on 60000] org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer: Finished computing new load balance plan.  Computation took 4502ms to try 8910 different iterations.  Found a solution that moves 39 regions; Going from a computed cost of 56.50048461413552 to a new cost of 11.225352339003237
{code}

Each of those balancer runs were triggered when there was no load on the cluster.
", ,,1,,,,,
HBASE-8518,"The ZooKeeperMainServerArg return String like this:<IP1>,<IP2>,<IP3>:<port>, but zookeeper client connection String shoud like this :<IP1>:<port>,<IP1>:<port>,<IP1>:<port>, if changed the zookeeper client port, the zkcli can't connection the first and the second ip.", ,1,,,,,,
HBASE-8519,"The problem happens if primary master dies after becoming master but before it completes initialization and calls clusterStatusTracker.setClusterUp(),
The backup master will try to become the master, but will shutdown itself promptly because it sees 'the cluster is not up'.

This is the backup master log:

2013-05-09 15:08:05,568 INFO org.apache.hadoop.hbase.master.metrics.MasterMetrics: Initialized
2013-05-09 15:08:05,573 DEBUG org.apache.hadoop.hbase.master.HMaster: HMaster started in backup mode.  Stalling until master znode is written.
2013-05-09 15:08:05,589 INFO org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper: Node /hbase/master already exists and this is not a retry
2013-05-09 15:08:05,590 INFO org.apache.hadoop.hbase.master.ActiveMasterManager: Adding ZNode for /hbase/backup-masters/xxx.com,60000,1368137285373 in backup master directory
2013-05-09 15:08:05,595 INFO org.apache.hadoop.hbase.master.ActiveMasterManager: Another master is the active master, xxx.com,60000,1368137283107; waiting to become the next active master
2013-05-09 15:09:45,006 DEBUG org.apache.hadoop.hbase.master.ActiveMasterManager: No master available. Notifying waiting threads
2013-05-09 15:09:45,006 INFO org.apache.hadoop.hbase.master.HMaster: Cluster went down before this master became active
2013-05-09 15:09:45,006 DEBUG org.apache.hadoop.hbase.master.HMaster: Stopping service threads
2013-05-09 15:09:45,006 INFO org.apache.hadoop.ipc.HBaseServer: Stopping server on 60000
 
In ActiveMasterManager::blockUntilBecomingActiveMaster()
{code}
  ..
  if (!clusterStatusTracker.isClusterUp()) {
          this.master.stop(
            ""Cluster went down before this master became active"");
        }
  ..
{code}", ,,,,,,,
HBASE-8520,"The problem happens if primary master dies after becoming master but before it completes initialization and calls clusterStatusTracker.setClusterUp(),
The backup master will try to become the master, but will shutdown itself promptly because it sees 'the cluster is not up'.

This is the backup master log:

2013-05-09 15:08:05,568 INFO org.apache.hadoop.hbase.master.metrics.MasterMetrics: Initialized
2013-05-09 15:08:05,573 DEBUG org.apache.hadoop.hbase.master.HMaster: HMaster started in backup mode.  Stalling until master znode is written.
2013-05-09 15:08:05,589 INFO org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper: Node /hbase/master already exists and this is not a retry
2013-05-09 15:08:05,590 INFO org.apache.hadoop.hbase.master.ActiveMasterManager: Adding ZNode for /hbase/backup-masters/xxx.com,60000,1368137285373 in backup master directory
2013-05-09 15:08:05,595 INFO org.apache.hadoop.hbase.master.ActiveMasterManager: Another master is the active master, xxx.com,60000,1368137283107; waiting to become the next active master
2013-05-09 15:09:45,006 DEBUG org.apache.hadoop.hbase.master.ActiveMasterManager: No master available. Notifying waiting threads
2013-05-09 15:09:45,006 INFO org.apache.hadoop.hbase.master.HMaster: Cluster went down before this master became active
2013-05-09 15:09:45,006 DEBUG org.apache.hadoop.hbase.master.HMaster: Stopping service threads
2013-05-09 15:09:45,006 INFO org.apache.hadoop.ipc.HBaseServer: Stopping server on 60000
 
In ActiveMasterManager::blockUntilBecomingActiveMaster()
{code}
  ..
  if (!clusterStatusTracker.isClusterUp()) {
          this.master.stop(
            ""Cluster went down before this master became active"");
        }
  ..
{code}", ,,,,,,,
HBASE-8521,"Let's say you have a pre-built HFile that contains a cell:

('rowkey1', 'family1', 'qual1', 1234L, 'value1')

We bulk load this first HFile. Now, let's create a second HFile that contains a cell that overwrites the first:

('rowkey1', 'family1', 'qual1', 1234L, 'value2')

That gets bulk loaded into the table, but the value that HBase bubbles up is still 'value1'.

It seems that there's no way to overwrite a cell for a particular timestamp without an explicit put operation. This seems to be the case even after minor and major compactions happen.

My guess is that this is pretty closely related to the sequence number work being done on the compaction algorithm via HBASE-7842, but I'm not sure if one of would fix the other.", ,1,,,,,,
HBASE-8528,"Let's say you have a pre-built HFile that contains a cell:

('rowkey1', 'family1', 'qual1', 1234L, 'value1')

We bulk load this first HFile. Now, let's create a second HFile that contains a cell that overwrites the first:

('rowkey1', 'family1', 'qual1', 1234L, 'value2')

That gets bulk loaded into the table, but the value that HBase bubbles up is still 'value1'.

It seems that there's no way to overwrite a cell for a particular timestamp without an explicit put operation. This seems to be the case even after minor and major compactions happen.

My guess is that this is pretty closely related to the sequence number work being done on the compaction algorithm via HBASE-7842, but I'm not sure if one of would fix the other.", ,1,,,,,,
HBASE-8531,"TestZooKeeper fails on occasion.  I caught a good example recently.  See below failure stack trace.

It took me a while.  I thought the issue had to do w/ our recent ipc refactorings but it looks like a problem we have always had.  In short, MetaScanner is not handling DoNotRetryIOEs -- it is letting them out.  DNRIOEs when scanning are supposed to force a reset of the scan.  HTable#next catches these and does the necessary scanner reset up.  MetaScanner is running some subset of what HTable does when it is scanning except the part where it catches a DNRIOE and redoes the scan.  Odd.

TestZooKeeper failed in this instance because the test kills a regionserver at same time as we are trying to create a table.  In create table we do a meta scan using MetaScanner passing a Visitor.  The scan starts and gets a RegionServerStoppedException (This is NOT a DNRIOE -- it should be -- but later we convert it into one up in ScannerCallable).

DNRIOEs are thrown to the upper layers to handle....

Let me look into having MetaScanner just use HTable scanning.  It makes an instance just to find where to start the scan... let me try using this instance for actually scanning.

TODO: Do this convertion everywhere a DNRIOE could come out.

Here is the stack trace

{code}

org.apache.hadoop.hbase.exceptions.DoNotRetryIOException: Reset scanner
	at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:209)
	at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:52)
	at org.apache.hadoop.hbase.client.ServerCallable.withRetries(ServerCallable.java:170)
	at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:212)
	at org.apache.hadoop.hbase.client.MetaScanner.access$000(MetaScanner.java:52)
	at org.apache.hadoop.hbase.client.MetaScanner$1.connect(MetaScanner.java:131)
	at org.apache.hadoop.hbase.client.MetaScanner$1.connect(MetaScanner.java:128)
	at org.apache.hadoop.hbase.client.HConnectionManager.execute(HConnectionManager.java:398)
	at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:128)
	at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:103)
	at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:81)
	at org.apache.hadoop.hbase.client.HBaseAdmin.createTable(HBaseAdmin.java:448)
	at org.apache.hadoop.hbase.client.HBaseAdmin.createTable(HBaseAdmin.java:348)
	at org.apache.hadoop.hbase.TestZooKeeper.testSanity(TestZooKeeper.java:242)
	at org.apache.hadoop.hbase.TestZooKeeper.testRegionServerSessionExpired(TestZooKeeper.java:203)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.hadoop.hbase.exceptions.RegionServerStoppedException: org.apache.hadoop.hbase.exceptions.RegionServerStoppedException: Server p0116.mtv.cloudera.com,60679,1368057284663 not running, aborting
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:95)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:79)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.getRemoteException(ProtobufUtil.java:227)
	at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:175)
	... 43 more
Caused by: org.apache.hadoop.hbase.ipc.RemoteWithExtrasException: org.apache.hadoop.hbase.exceptions.RegionServerStoppedException: Server p0116.mtv.cloudera.com,60679,1368057284663 not running, aborting
	at org.apache.hadoop.hbase.regionserver.HRegionServer.checkOpen(HRegionServer.java:2310)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.scan(HRegionServer.java:2874)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:20577)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2103)
	at org.apache.hadoop.hbase.ipc.RpcServer$Handler.run(RpcServer.java:1810)

	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1336)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1532)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1587)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.scan(ClientProtos.java:21012)
	at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:147)
	... 43 more

{code}", ,,,,,,,
HBASE-8533,"Let's say you have a pre-built HFile that contains a cell:

('rowkey1', 'family1', 'qual1', 1234L, 'value1')

We bulk load this first HFile. Now, let's create a second HFile that contains a cell that overwrites the first:

('rowkey1', 'family1', 'qual1', 1234L, 'value2')

That gets bulk loaded into the table, but the value that HBase bubbles up is still 'value1'.

It seems that there's no way to overwrite a cell for a particular timestamp without an explicit put operation. This seems to be the case even after minor and major compactions happen.

My guess is that this is pretty closely related to the sequence number work being done on the compaction algorithm via HBASE-7842, but I'm not sure if one of would fix the other.", ,1,,,,,,
HBASE-8536,"When I ran TestHBaseFsck, I sometimes saw:
{code}
testNoVersionFile(org.apache.hadoop.hbase.util.TestHBaseFsck)  Time elapsed: 0.003 sec  <<< ERROR!
java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.hbase.master.TableLockManager$ZKTableLockManager.reapAllExpiredLocks(TableLockManager.java:433)
        at org.apache.hadoop.hbase.util.hbck.TableLockChecker.fixExpiredTableLocks(TableLockChecker.java:83)
        at org.apache.hadoop.hbase.util.HBaseFsck.checkAndFixTableLocks(HBaseFsck.java:2483)
        at org.apache.hadoop.hbase.util.HBaseFsck.onlineHbck(HBaseFsck.java:460)
        at org.apache.hadoop.hbase.util.hbck.HbckTestingUtil.doFsck(HbckTestingUtil.java:65)
        at org.apache.hadoop.hbase.util.hbck.HbckTestingUtil.doFsck(HbckTestingUtil.java:41)
        at org.apache.hadoop.hbase.util.hbck.HbckTestingUtil.doFsck(HbckTestingUtil.java:36)
        at org.apache.hadoop.hbase.util.TestHBaseFsck.testNoVersionFile(TestHBaseFsck.java:1086)
...
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessLockBase.reapExpiredLocks(ZKInterProcessLockBase.java:393)
        at org.apache.hadoop.hbase.master.TableLockManager$ZKTableLockManager.reapAllExpiredLocks(TableLockManager.java:427)
        ... 37 more
{code}
This was due to null being returned from ZKUtil.listChildrenNoWatch() at line 384:
{code}
      children = ZKUtil.listChildrenNoWatch(zkWatcher, parentLockNode);
{code}", ,,,,,,,
HBASE-8538,"To avoid compatibility issues with older versions HBaseAdmin#isTableEnabled returning true even if the table state is null. Its also returning true even a table is not present. We should confirm table existence from .META. before checking in zk. If table not present or deleted, then It will throw TableNotFoundException.", ,,,,,,,
HBASE-8539,"When Master tries to recover from zookeeper session expired exceptions, we don't clean old registered listener instances. Therefore, it may end up we have two(or more) listeners to double handling same events. Attached a screen shot from debugger to show the issue.

I considered to limit one listener per class while I think that would limit the listener usage so I choose to clear exiting listeners during recovery for the fix.

(This issue is unrelated to the issue HBASE-8365 because I verified there is no dup-listeners when HBASE-8365 happened)", ,,,,,,,
HBASE-8540,"On a cluster with ""hbase.snapshot.enabled"" true, the /hbase/.hbase-snapshot directory will not exists until a first snapshot will get created.

meanwhile the SnapshotFileCache (snapshot cleaner) will keep print out as error that the snapshot dir doesn't exists every N seconds (""hbase.master.hfilecleaner.plugins.snapshot.period"")
{code}
LOG.error(""Snapshot directory: "" + snapshotDir + "" doesn't exist"");
{code}", ,,,,,,,
HBASE-8542,"On a cluster with ""hbase.snapshot.enabled"" true, the /hbase/.hbase-snapshot directory will not exists until a first snapshot will get created.

meanwhile the SnapshotFileCache (snapshot cleaner) will keep print out as error that the snapshot dir doesn't exists every N seconds (""hbase.master.hfilecleaner.plugins.snapshot.period"")
{code}
LOG.error(""Snapshot directory: "" + snapshotDir + "" doesn't exist"");
{code}", ,,,,,,,
HBASE-8545,"Support the meta region server is down, and the SSH tries to re-assign it.  This could happen:

1. AM plans to assign meta to a region server (R_old);
2. Now R_old is dead, the new region server (R_new) starts up on the same host, port, but gets a different start code;
3. AM sends the open region request to R_new and the Meta is opened on it;
4. AM gets ZK event, but it is from a different region server instance (R_new), not the expected one (R_old), so it sends a close region request to R_new;
5. Now, the meta is stuck in transition and won't be assigned.

This won't happen to a user region since the SSH for R_old will find out the user region stuck in transition and re-assign it.  For meta, it is a little different.  AM checks if a dead region server carries the meta based on the ZK info, which is changed to the new region server R_new at step 3 by the open region handler.

The fix I was thinking about is:
1. In checking if a region server carries a region, uses the region transition information if it exists (which is the source of truth, to master), if not, checks the ZK data as before;
2. In open region handler, when transition assign zk node from offline to opening, make sure the current region server is the expected one (ZK#transitionNode, existing code doesn't check the target server name).", ,,,,,,,
HBASE-8547,"In one test, one of the region servers received the following on 0.94. 

Note HalfStoreFileReader in the stack trace. I think the root cause is that after the region is split, the mid point can be in the middle of the block (for store files that the mid point is not chosen from). Each half store file tries to load the half block and put it in the block cache. Since IdLock is instantiated per store file reader, they do not share the same IdLock instance, thus does not lock against each other effectively. 

{code}
2013-05-12 01:30:37,733 ERROR org.apache.hadoop.hbase.regionserver.HRegionServer:
java.lang.RuntimeException: Cached an already cached block
  at org.apache.hadoop.hbase.io.hfile.LruBlockCache.cacheBlock(LruBlockCache.java:279)
  at org.apache.hadoop.hbase.io.hfile.HFileReaderV2.readBlock(HFileReaderV2.java:353)
  at org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader.loadDataBlockWithScanInfo(HFileBlockIndex.java:254)
  at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.seekTo(HFileReaderV2.java:480)
  at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.seekTo(HFileReaderV2.java:501)
  at org.apache.hadoop.hbase.io.HalfStoreFileReader$1.seekTo(HalfStoreFileReader.java:237)
  at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seekAtOrAfter(StoreFileScanner.java:226)
  at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seek(StoreFileScanner.java:145)
  at org.apache.hadoop.hbase.regionserver.StoreFileScanner.enforceSeek(StoreFileScanner.java:351)
  at org.apache.hadoop.hbase.regionserver.KeyValueHeap.pollRealKV(KeyValueHeap.java:354)
  at org.apache.hadoop.hbase.regionserver.KeyValueHeap.generalizedSeek(KeyValueHeap.java:312)
  at org.apache.hadoop.hbase.regionserver.KeyValueHeap.requestSeek(KeyValueHeap.java:277)
  at org.apache.hadoop.hbase.regionserver.StoreScanner.reseek(StoreScanner.java:543)
  at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:411)
  at org.apache.hadoop.hbase.regionserver.KeyValueHeap.next(KeyValueHeap.java:143)
  at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.populateResult(HRegion.java:3829)
  at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.nextInternal(HRegion.java:3896)
  at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.nextRaw(HRegion.java:3778)
  at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.nextRaw(HRegion.java:3770)
  at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:2643)
  at sun.reflect.GeneratedMethodAccessor25.invoke(Unknown Source)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
  at java.lang.reflect.Method.invoke(Method.java:597)
  at org.apache.hadoop.hbase.ipc.SecureRpcEngine$Server.call(SecureRpcEngine.java:308)
  at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1426)
{code}

I can see two possible fixes: 
 # Allow this kind of rare cases in LruBlockCache by not throwing an exception. 
 # Move the lock instances to upper layer (possibly in CacheConfig), and let half hfile readers share the same IdLock implementation. 
", ,,,,,,,
HBASE-8548,"postOpen hook is called twice when a region is initializing:

Once at the end of the body of the  initializeRegionInternals() method of the HRegion class.
Once at the end initializeRegionStores() method of the HRegion class; initializeRegionStores() is called inside initializeRegionInternals() and as such causes the postOpen hook to be called twice.", ,,1,,,,,
HBASE-8555,"say, ther're 10 rows, column value is i%2:
row0 0
row1 1
row2 0
row3 1
row4 0
row5 1
row6 0
row7 1
row8 0
row9 1

1: filter : row filter > row4   ===> row5 row6 row7 row8 row9
2: subFilterList:  row filter <= row4 && column==0    ===> row0 row2 row4
3.1 filterlist[expected]   filter || subFilterList  ===> row0 row2 row4 row5 row6 row7 row8 row9
3.2 filterlist[BUGON!]  subFilterList || filter ===> row0 row1 row2 row3 row4 row5 row6 row7 row8 row9
(Please refer to the new testNestedFilterListWithSCVF case)

It was found when i managed to transform the following SQL into HBase scan statement: 
select xxx from xxx where (pk <= xxx and column1 = xxx) or pk > xxx

My finding is that we had an assumption for filter methods call sequence:
e.g. filterRowKey() should be called before filterKeyValue().
and the orignial filterList.filterRowKey impl broke it due to fast short-circuit returning.", ,,,,,,,
HBASE-8558,"I run jstack at client host. The result is below.
""hbase-tablepool-60-thread-34"" daemon prio=10 tid=0x00007f1e65a48000 nid=0x5173 runnable [0x00000000579cc000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
        - locked <0x0000000758cb0780> (a sun.nio.ch.Util$2)
        - locked <0x0000000758cb0770> (a java.util.Collections$UnmodifiableSet)
        - locked <0x0000000758cb0548> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
        at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:336)
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:158)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:153)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:114)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
        - locked <0x0000000754e978a0> (a java.io.BufferedOutputStream)
        at java.io.DataOutputStream.flush(DataOutputStream.java:106)
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.sendParam(HBaseClient.java:620)
        - locked <0x0000000754e97880> (a java.io.DataOutputStream)
        at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:975)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:86)
        at $Proxy13.multi(Unknown Source)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$3$1.call(HConnectionManager.java:1395)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$3$1.call(HConnectionManager.java:1393)
        at org.apache.hadoop.hbase.client.ServerCallable.withoutRetries(ServerCallable.java:210)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$3.call(HConnectionManager.java:1402)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$3.call(HConnectionManager.java:1390)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)


This thread have hung for one hours

Meanwhile other thread try to close connection

""IPC Client (1983049639) connection to dump002030.cm6.tbsite.net/10.246.2.30:30020 from admin"" daemon prio=10 tid=0x00007f1e70674800 nid=0x3d76 waiting for monitor entry [0x000000004bc0f000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
        - waiting to lock <0x0000000754e978a0> (a java.io.BufferedOutputStream)
        at java.io.DataOutputStream.flush(DataOutputStream.java:106)
        at java.io.FilterOutputStream.close(FilterOutputStream.java:140)
        at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)
        at org.apache.hadoop.io.IOUtils.closeStream(IOUtils.java:254)
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.close(HBaseClient.java:715)
        - locked <0x0000000754e7b818> (a org.apache.hadoop.hbase.ipc.HBaseClient$Connection)
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.run(HBaseClient.java:587)



dump002030.cm6.tbsite.net is dead regionserver.

I read  hbase sourececode, discover connection.out doesn't set timeout 

        this.out = new DataOutputStream
            (new BufferedOutputStream(NetUtils.getOutputStream(socket)));

I see this mean epoll_wait will block indefinitely. 
", ,,,,,,,
HBASE-8559,"postOpen hook is called twice when a region is initializing:

Once at the end of the body of the  initializeRegionInternals() method of the HRegion class.
Once at the end initializeRegionStores() method of the HRegion class; initializeRegionStores() is called inside initializeRegionInternals() and as such causes the postOpen hook to be called twice.", ,,1,,,,,
HBASE-8560,I'm looking at this too... [~jeffreyz] you are too?, ,,,,,,,
HBASE-8563,"Whenever a RegionScanner is created via HRegion.getScanner(), the read request count is incremented. Since get is implemented as a scan internally. Each Get request is counted twice. Scans will have an extra count as well.", ,,,,,,,
HBASE-8564,TestMetricsRegionServer has failed a few times on trunk.  It's passing depends upon test ordering so will fail sometimes., ,1,,,,,,
HBASE-8566,"after successfully setup the replcation. put some rows into 'usertable' , 

At Master cluster
$truncate 'usertable'

The truncate(or mass delete from user perspective) request isn't sent over to slave cluster. 

From internal, the truncate is 'disable', 'drop' and 'create'. Such operations are not designed for replication. However, from external/user perspective, this is a 'delete everything' operation, which should be part of the replication. 

This JIRA is to add this support
---------------------------
additional information. I did a few loads using YCSB into 'usertable', with different # of rows(from 1000 to 100000). And did truncate a couple times in between. Then the slave cluster began to throw errors:
{code:title=count failed on slave cluster|borderStyle=solid}

ERROR: org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=7, exceptions:
Thu May 16 15:00:13 PDT 2013, org.apache.hadoop.hbase.client.ScannerCallable@4c474c47, java.net.ConnectException: Connection refused
Thu May 16 15:00:32 PDT 2013, org.apache.hadoop.hbase.client.ScannerCallable@4c474c47, org.apache.hadoop.hbase.client.NoServerForRegionException: Unable to find region for usertable,,99999999999999 after 7 tries.
Thu May 16 15:00:51 PDT 2013, org.apache.hadoop.hbase.client.ScannerCallable@4c474c47, org.apache.hadoop.hbase.client.NoServerForRegionException: Unable to find region for usertable,,99999999999999 after 7 tries.
Thu May 16 15:01:11 PDT 2013, org.apache.hadoop.hbase.client.ScannerCallable@4c474c47, org.apache.hadoop.hbase.client.NoServerForRegionException: Unable to find region for usertable,,99999999999999 after 7 tries.
{code}
The regionserver log of slave cluster throws :
{code:title=regionserver log of slave cluster|borderStyle=solid}
2013-05-16 14:59:59,655 ERROR org.apache.hadoop.hbase.replication.regionserver.ReplicationSink: Unable to accept edit because:
java.io.IOException: java.lang.InterruptedException
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSink.batch(ReplicationSink.java:220)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSink.replicateEntries(ReplicationSink.java:154)
        at org.apache.hadoop.hbase.replication.regionserver.Replication.replicateLogEntries(Replication.java:140)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.replicateLogEntries(HRegionServer.java:3797)
        at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:37)
        at java.lang.reflect.Method.invoke(Method.java:611)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1426)
Caused by: java.lang.InterruptedException
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:853)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.processBatchCallback(HConnectionManager.java:1507)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.processBatch(HConnectionManager.java:1400)
        at org.apache.hadoop.hbase.client.HTable.batch(HTable.java:699)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSink.batch(ReplicationSink.java:217)
        ... 8 more
{code}
", ,,,,,,,
HBASE-8568,"The test case only start 1 RS. After the only RS aborted, META region can't be re-assigned. 
Since distributedLogReplay(hbase-7006) depends on a region get re-assigned firstly, the test case failed timed out while old recovered.edits splitting doesn't have the issue.", ,1,,,,,,
HBASE-8573,"Basically we tried to recover lease of a wal file while the file doesn't exist anymore. We keep retrying recover the file(it always fails for sure because the file is gone). Then the test case failed with time out

Here is a recent run failure log:
https://builds.apache.org/job/HBase-0.95/204/testReport/junit/org.apache.hadoop.hbase/TestZooKeeper/testLogSplittingAfterMasterRecoveryDueToZKExpiry/

Related exception:
{code}
2013-05-18 05:53:03,918 ERROR [IPC Server handler 2 on 36965] security.UserGroupInformation(1152): PriviledgedActionException as:jenkins.hfs.2 cause:java.io.FileNotFoundException: File not found /user/jenkins/hbase/.logs/hemera.apache.org,42628,1368856131392-splitting/hemera.apache.org,42628,1368856131392
2013-05-18 05:53:03,918 WARN  [SplitLogWorker-hemera.apache.org,47651,1368856143179] util.FSHDFSUtils(80): Got IOException on attempt 241 to recover lease for file hdfs://localhost:36965/user/jenkins/hbase/.logs/hemera.apache.org,42628,1368856131392-splitting/hemera.apache.org,42628,1368856131392, retrying.
java.io.FileNotFoundException: java.io.FileNotFoundException: File not found /user/jenkins/hbase/.logs/hemera.apache.org,42628,1368856131392-splitting/hemera.apache.org,42628,1368856131392
	at sun.reflect.GeneratedConstructorAccessor21.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:95)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:57)
	at org.apache.hadoop.hdfs.DFSClient.recoverLease(DFSClient.java:885)
	at org.apache.hadoop.hdfs.DistributedFileSystem.recoverLease(DistributedFileSystem.java:177)
	at org.apache.hadoop.hbase.util.FSHDFSUtils.recoverFileLease(FSHDFSUtils.java:71)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.getReader(HLogSplitter.java:821)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:504)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:455)
	at org.apache.hadoop.hbase.regionserver.SplitLogWorker$1.exec(SplitLogWorker.java:132)
	at org.apache.hadoop.hbase.regionserver.SplitLogWorker.grabTask(SplitLogWorker.java:337)
	at org.apache.hadoop.hbase.regionserver.SplitLogWorker.taskLoop(SplitLogWorker.java:225)
	at org.apache.hadoop.hbase.regionserver.SplitLogWorker.run(SplitLogWorker.java:192)
{code}", ,,,,,,,
HBASE-8575,"Basically we tried to recover lease of a wal file while the file doesn't exist anymore. We keep retrying recover the file(it always fails for sure because the file is gone). Then the test case failed with time out

Here is a recent run failure log:
https://builds.apache.org/job/HBase-0.95/204/testReport/junit/org.apache.hadoop.hbase/TestZooKeeper/testLogSplittingAfterMasterRecoveryDueToZKExpiry/

Related exception:
{code}
2013-05-18 05:53:03,918 ERROR [IPC Server handler 2 on 36965] security.UserGroupInformation(1152): PriviledgedActionException as:jenkins.hfs.2 cause:java.io.FileNotFoundException: File not found /user/jenkins/hbase/.logs/hemera.apache.org,42628,1368856131392-splitting/hemera.apache.org,42628,1368856131392
2013-05-18 05:53:03,918 WARN  [SplitLogWorker-hemera.apache.org,47651,1368856143179] util.FSHDFSUtils(80): Got IOException on attempt 241 to recover lease for file hdfs://localhost:36965/user/jenkins/hbase/.logs/hemera.apache.org,42628,1368856131392-splitting/hemera.apache.org,42628,1368856131392, retrying.
java.io.FileNotFoundException: java.io.FileNotFoundException: File not found /user/jenkins/hbase/.logs/hemera.apache.org,42628,1368856131392-splitting/hemera.apache.org,42628,1368856131392
	at sun.reflect.GeneratedConstructorAccessor21.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:95)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:57)
	at org.apache.hadoop.hdfs.DFSClient.recoverLease(DFSClient.java:885)
	at org.apache.hadoop.hdfs.DistributedFileSystem.recoverLease(DistributedFileSystem.java:177)
	at org.apache.hadoop.hbase.util.FSHDFSUtils.recoverFileLease(FSHDFSUtils.java:71)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.getReader(HLogSplitter.java:821)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:504)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:455)
	at org.apache.hadoop.hbase.regionserver.SplitLogWorker$1.exec(SplitLogWorker.java:132)
	at org.apache.hadoop.hbase.regionserver.SplitLogWorker.grabTask(SplitLogWorker.java:337)
	at org.apache.hadoop.hbase.regionserver.SplitLogWorker.taskLoop(SplitLogWorker.java:225)
	at org.apache.hadoop.hbase.regionserver.SplitLogWorker.run(SplitLogWorker.java:192)
{code}", ,,,,,,,
HBASE-8577,"Basically we tried to recover lease of a wal file while the file doesn't exist anymore. We keep retrying recover the file(it always fails for sure because the file is gone). Then the test case failed with time out

Here is a recent run failure log:
https://builds.apache.org/job/HBase-0.95/204/testReport/junit/org.apache.hadoop.hbase/TestZooKeeper/testLogSplittingAfterMasterRecoveryDueToZKExpiry/

Related exception:
{code}
2013-05-18 05:53:03,918 ERROR [IPC Server handler 2 on 36965] security.UserGroupInformation(1152): PriviledgedActionException as:jenkins.hfs.2 cause:java.io.FileNotFoundException: File not found /user/jenkins/hbase/.logs/hemera.apache.org,42628,1368856131392-splitting/hemera.apache.org,42628,1368856131392
2013-05-18 05:53:03,918 WARN  [SplitLogWorker-hemera.apache.org,47651,1368856143179] util.FSHDFSUtils(80): Got IOException on attempt 241 to recover lease for file hdfs://localhost:36965/user/jenkins/hbase/.logs/hemera.apache.org,42628,1368856131392-splitting/hemera.apache.org,42628,1368856131392, retrying.
java.io.FileNotFoundException: java.io.FileNotFoundException: File not found /user/jenkins/hbase/.logs/hemera.apache.org,42628,1368856131392-splitting/hemera.apache.org,42628,1368856131392
	at sun.reflect.GeneratedConstructorAccessor21.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:95)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:57)
	at org.apache.hadoop.hdfs.DFSClient.recoverLease(DFSClient.java:885)
	at org.apache.hadoop.hdfs.DistributedFileSystem.recoverLease(DistributedFileSystem.java:177)
	at org.apache.hadoop.hbase.util.FSHDFSUtils.recoverFileLease(FSHDFSUtils.java:71)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.getReader(HLogSplitter.java:821)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:504)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:455)
	at org.apache.hadoop.hbase.regionserver.SplitLogWorker$1.exec(SplitLogWorker.java:132)
	at org.apache.hadoop.hbase.regionserver.SplitLogWorker.grabTask(SplitLogWorker.java:337)
	at org.apache.hadoop.hbase.regionserver.SplitLogWorker.taskLoop(SplitLogWorker.java:225)
	at org.apache.hadoop.hbase.regionserver.SplitLogWorker.run(SplitLogWorker.java:192)
{code}", ,,,,,,,
HBASE-8579,"It failed here http://54.241.6.143/job/HBase-0.95/291/

Looks like basic rpc timeout (timeout set to 1 second and complaint was that it had waited 1.6 seconds).  I suppose this could happen on loaded server.  Let me try upping timeouts for now.", ,,,,,,,
HBASE-8580,jd was poking and noticed that we were not passing the operation timeout down as rpc timeout as we used to in 0.94.  Let me fix., ,,,,,,,
HBASE-8581,jd was poking and noticed that we were not passing the operation timeout down as rpc timeout as we used to in 0.94.  Let me fix., ,,,,,,,
HBASE-8582,"Running test suite on hadoop 2.0 I saw the following test failure:
{code}
testErrorReporter(org.apache.hadoop.hbase.util.TestHBaseFsck)  Time elapsed: 0.003 sec  <<< ERROR!
java.lang.NullPointerException
        at org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessLockBase.visitLocks(ZKInterProcessLockBase.java:426)
        at org.apache.hadoop.hbase.master.TableLockManager$ZKTableLockManager.visitAllLocks(TableLockManager.java:386)
        at org.apache.hadoop.hbase.util.hbck.TableLockChecker.checkTableLocks(TableLockChecker.java:76)
        at org.apache.hadoop.hbase.util.HBaseFsck.checkAndFixTableLocks(HBaseFsck.java:2480)
        at org.apache.hadoop.hbase.util.HBaseFsck.onlineHbck(HBaseFsck.java:460)
        at org.apache.hadoop.hbase.util.hbck.HbckTestingUtil.doFsck(HbckTestingUtil.java:65)
        at org.apache.hadoop.hbase.util.hbck.HbckTestingUtil.doFsck(HbckTestingUtil.java:41)
        at org.apache.hadoop.hbase.util.hbck.HbckTestingUtil.doFsck(HbckTestingUtil.java:36)
        at org.apache.hadoop.hbase.util.TestHBaseFsck.testErrorReporter(TestHBaseFsck.java:1868)
{code}
Here is related code:
{code}
  public void visitLocks(MetadataHandler handler) throws IOException {
    List<String> children;
    try {
      children = ZKUtil.listChildrenNoWatch(zkWatcher, parentLockNode);
{code}
Looks like children was null.", ,,,,,,,
HBASE-8585,"In test output, I noticed the following:
{code}
2013-05-21 03:39:37,015 DEBUG [Thread-1446-EventThread] zookeeper.ZooKeeperWatcher(307): master:35255-0x13ec52b4ce30000 Received ZooKeeper Event, type=NodeDataChanged, state=SyncConnected, path=/hbase/region-in-transition/8b6ce00aafd483a6ed7a76425009ebf8
2013-05-21 03:39:37,015 ERROR [Thread-1446-EventThread] zookeeper.ClientCnxn$EventThread(521): Error while calling watcher
java.util.concurrent.RejectedExecutionException
        at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:1768)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
        at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:78)
        at org.apache.hadoop.hbase.master.AssignmentManager.zkEventWorkersSubmit(AssignmentManager.java:1136)
        at org.apache.hadoop.hbase.master.AssignmentManager.handleAssignmentEvent(AssignmentManager.java:1306)
        at org.apache.hadoop.hbase.master.AssignmentManager.nodeDataChanged(AssignmentManager.java:1095)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:338)
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:519)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:495)
{code}
By default, ThreadPoolExecutor.AbortPolicy is used for zkEventWorkers, leading to the above exception.

We should examine the calls to zkEventWorkersSubmit() and devise proper handling for RejectedExecutionException.
", ,,,,,,,
HBASE-8590,"This was discovered after HBASE-8505 went in, which introduces a test sporadically triggering this bug. 

From comments at HBASE-8505: 
From the logs at https://builds.apache.org/job/HBase-0.94-security/ws/trunk/target/surefire-reports/org.apache.hadoop.hbase.client.TestMetaScanner-output.txt, I think I understand what is going on: 
BlockingMetaScannerVisitor blocks and wait for the split daughter to appear when it sees a parent region (HBASE-5986). CatalogJanitor on the other hand will order the regions in a (kind-of) topological sort (based on parent child relation) so that it will guarantee parents are not GC'd before daughters.
What is happening in this issue is not related to the patch in this jira, but the test triggers this extremely rare case by running concurrent catalogjanitor, splits and metascanners. We have parent, splita and splitb regions, and catalogjanitor decides to delete parent first and splitb in one run. While there is a concurrent metascanner which will go over the parent, and sees that it is split, but before being able to read the split daughter, catalog janitor will delete both the parent and the child, which will lead to metascanner blocking until timeout and failing the test.
On solution might be to also check whether the parent is still there in BlockingMetaScannerVisitor while we are blocking for the daughter.
Good thing is that with HBASE-7721, we don't need any of this in trunk.", ,,,,,,,
HBASE-8597,"A region is opened by a server, major compaction is performed, that triggers a split, and the region is closed and split. There's no indication of memstore flush for this region.
After that, LogRoller repeatedly tries to request the flush of this region due to large number of HLogs, but fails to flush it for hours because the region is not in online regions.
It seems that what's happening is that when we append entries to WAL we add the first entry after we flush/open some region to ""earliest unflushed seqNums per region"" map in FSHLog. However, compaction now adds compaction record to WAL, which also affects this map. If the compaction record is the first entry for this region to go into some WAL, and there are no writes to the region after that, there will be no memstore flush and the entry will never be removed. 
In fact ""flushing"" for compaction record doesn't make sense, there's no  preservation of the record outside WAL; so, we probably should not add it to ""latest unflushed"" map.", ,1,,,,,,
HBASE-8599,"A region is opened by a server, major compaction is performed, that triggers a split, and the region is closed and split. There's no indication of memstore flush for this region.
After that, LogRoller repeatedly tries to request the flush of this region due to large number of HLogs, but fails to flush it for hours because the region is not in online regions.
It seems that what's happening is that when we append entries to WAL we add the first entry after we flush/open some region to ""earliest unflushed seqNums per region"" map in FSHLog. However, compaction now adds compaction record to WAL, which also affects this map. If the compaction record is the first entry for this region to go into some WAL, and there are no writes to the region after that, there will be no memstore flush and the entry will never be removed. 
In fact ""flushing"" for compaction record doesn't make sense, there's no  preservation of the record outside WAL; so, we probably should not add it to ""latest unflushed"" map.", ,1,,,,,,
HBASE-8604,"Running test suite on hadoop 2.0, I saw the following test failure:
{code}
testFavoredNodes(org.apache.hadoop.hbase.regionserver.TestRegionFavoredNodes)  Time elapsed: 0.106 sec  <<< ERROR!
org.apache.hadoop.hbase.exceptions.DroppedSnapshotException: region: table,rrr,1369355298031.1fdb1b446b02b497f0869a08adad7745.
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1568)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1429)
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:1347)
        at org.apache.hadoop.hbase.MiniHBaseCluster.flushcache(MiniHBaseCluster.java:531)
        at org.apache.hadoop.hbase.HBaseTestingUtility.flush(HBaseTestingUtility.java:961)
        at org.apache.hadoop.hbase.regionserver.TestRegionFavoredNodes.testFavoredNodes(TestRegionFavoredNodes.java:132)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
        at org.junit.runners.Suite.runChild(Suite.java:127)
        at org.junit.runners.Suite.runChild(Suite.java:26)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:293)
        at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:268)
        at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:427)
        at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:791)
        at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:733)
        at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:671)
        at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:799)
        at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:75)
        at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:704)
        at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1813)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1543)
        ... 33 more
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1306)
        at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:283)
{code}
DistributedFileSystem#create() which supports favoredNodes parameter threw NullPointerException.
We should handle the NullPointerException and fall back to conventional DistributedFileSystem#create()", ,,,,,,,
HBASE-8614,"Running test suite on hadoop 2.0, I saw the following test failure:
{code}
testFavoredNodes(org.apache.hadoop.hbase.regionserver.TestRegionFavoredNodes)  Time elapsed: 0.106 sec  <<< ERROR!
org.apache.hadoop.hbase.exceptions.DroppedSnapshotException: region: table,rrr,1369355298031.1fdb1b446b02b497f0869a08adad7745.
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1568)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1429)
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:1347)
        at org.apache.hadoop.hbase.MiniHBaseCluster.flushcache(MiniHBaseCluster.java:531)
        at org.apache.hadoop.hbase.HBaseTestingUtility.flush(HBaseTestingUtility.java:961)
        at org.apache.hadoop.hbase.regionserver.TestRegionFavoredNodes.testFavoredNodes(TestRegionFavoredNodes.java:132)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
        at org.junit.runners.Suite.runChild(Suite.java:127)
        at org.junit.runners.Suite.runChild(Suite.java:26)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:293)
        at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:268)
        at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:427)
        at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:791)
        at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:733)
        at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:671)
        at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:799)
        at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:75)
        at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:704)
        at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1813)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1543)
        ... 33 more
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1306)
        at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:283)
{code}
DistributedFileSystem#create() which supports favoredNodes parameter threw NullPointerException.
We should handle the NullPointerException and fall back to conventional DistributedFileSystem#create()", ,,,,,,,
HBASE-8617,"A region is opened by a server, major compaction is performed, that triggers a split, and the region is closed and split. There's no indication of memstore flush for this region.
After that, LogRoller repeatedly tries to request the flush of this region due to large number of HLogs, but fails to flush it for hours because the region is not in online regions.
It seems that what's happening is that when we append entries to WAL we add the first entry after we flush/open some region to ""earliest unflushed seqNums per region"" map in FSHLog. However, compaction now adds compaction record to WAL, which also affects this map. If the compaction record is the first entry for this region to go into some WAL, and there are no writes to the region after that, there will be no memstore flush and the entry will never be removed. 
In fact ""flushing"" for compaction record doesn't make sense, there's no  preservation of the record outside WAL; so, we probably should not add it to ""latest unflushed"" map.", ,1,1,,,,,
HBASE-8618,"In our rolling upgrade testing, running ImportTsv failed in the job submission phase when the master was down. This was when the master was failing over to the backup master. In this case, a retry would have been helpful and made sure that the job would get submitted.

A good solution would be to refresh the master information before placing the call to getHTableDescriptor.

Command:
{code} sudo -u hbase hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=HBASE_ROW_KEY,c1,c2,c3 -Dimporttsv.bulk.output=/user/hbase/storeFiles2_2/import2_table1369439156 import2_table1369439156 /user/hbase/tsv2{code}

Here is the stack trace:

{code} 13/05/24 16:55:49 INFO compress.CodecPool: Got brand-new compressor [.deflate]
16:45:44  Exception in thread ""main"" java.lang.reflect.UndeclaredThrowableException
16:45:44  	at $Proxy7.getHTableDescriptors(Unknown Source)
16:45:44  	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHTableDescriptor(HConnectionManager.java:1861)
16:45:44  	at org.apache.hadoop.hbase.client.HTable.getTableDescriptor(HTable.java:440)
16:45:44  	at org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.configureCompression(HFileOutputFormat.java:458)
16:45:44  	at org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.configureIncrementalLoad(HFileOutputFormat.java:375)
16:45:44  	at org.apache.hadoop.hbase.mapreduce.ImportTsv.createSubmittableJob(ImportTsv.java:280)
16:45:44  	at org.apache.hadoop.hbase.mapreduce.ImportTsv.main(ImportTsv.java:424)
16:45:44  Caused by: java.io.IOException: Call to hbase-rolling-6.ent.cloudera.com/10.20.186.99:22001 failed on local exception: java.io.EOFException
16:45:44  	at org.apache.hadoop.hbase.ipc.HBaseClient.wrapException(HBaseClient.java:1030)
16:45:44  	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:999)
16:45:44  	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:86)
16:45:44  	... 7 more
16:45:44  Caused by: java.io.EOFException
16:45:44  	at java.io.DataInputStream.readInt(DataInputStream.java:375)
16:45:44  	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.receiveResponse(HBaseClient.java:646)
16:45:44  	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.run(HBaseClient.java:580){code}", ,,,,,,,
HBASE-8619,"In our rolling upgrade testing, running ImportTsv failed in the job submission phase when the master was down. This was when the master was failing over to the backup master. In this case, a retry would have been helpful and made sure that the job would get submitted.

A good solution would be to refresh the master information before placing the call to getHTableDescriptor.

Command:
{code} sudo -u hbase hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=HBASE_ROW_KEY,c1,c2,c3 -Dimporttsv.bulk.output=/user/hbase/storeFiles2_2/import2_table1369439156 import2_table1369439156 /user/hbase/tsv2{code}

Here is the stack trace:

{code} 13/05/24 16:55:49 INFO compress.CodecPool: Got brand-new compressor [.deflate]
16:45:44  Exception in thread ""main"" java.lang.reflect.UndeclaredThrowableException
16:45:44  	at $Proxy7.getHTableDescriptors(Unknown Source)
16:45:44  	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHTableDescriptor(HConnectionManager.java:1861)
16:45:44  	at org.apache.hadoop.hbase.client.HTable.getTableDescriptor(HTable.java:440)
16:45:44  	at org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.configureCompression(HFileOutputFormat.java:458)
16:45:44  	at org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.configureIncrementalLoad(HFileOutputFormat.java:375)
16:45:44  	at org.apache.hadoop.hbase.mapreduce.ImportTsv.createSubmittableJob(ImportTsv.java:280)
16:45:44  	at org.apache.hadoop.hbase.mapreduce.ImportTsv.main(ImportTsv.java:424)
16:45:44  Caused by: java.io.IOException: Call to hbase-rolling-6.ent.cloudera.com/10.20.186.99:22001 failed on local exception: java.io.EOFException
16:45:44  	at org.apache.hadoop.hbase.ipc.HBaseClient.wrapException(HBaseClient.java:1030)
16:45:44  	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:999)
16:45:44  	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:86)
16:45:44  	... 7 more
16:45:44  Caused by: java.io.EOFException
16:45:44  	at java.io.DataInputStream.readInt(DataInputStream.java:375)
16:45:44  	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.receiveResponse(HBaseClient.java:646)
16:45:44  	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.run(HBaseClient.java:580){code}", ,,,,,,,
HBASE-8626,"When RowMutations have a Delete followed by Put to same column family or columns or rows, only the Delete is happening while the Put is ignored so atomicity of RowMutations is broken for such cases.

Attached is a unit test where the following tests are failing:
- testDeleteCFThenPutInSameCF: Delete a column family and then Put to same column family.
- testDeleteColumnThenPutSameColumn: Delete a column and then Put to same column.
- testDeleteRowThenPutSameRow: Delete a row and then Put to same row
", ,1,,,,,,
HBASE-8627,"When meta table region is not assigned to any RS, HBCK run will get exception. I can see code added in checkMetaRegion() to solve this issue but it wont work. It still refers to ROOT region!", ,,,,,,,
HBASE-8631,"We have a separate wal for meta region. While log splitting logic haven't taken the advantage of this and splitlogworker still picks a wal file randomly. Imaging if we have multiple region servers including meta RS fails about the same time while meta wal is recovered last, all failed regions have to wait meta recovered and then can be online again. 

The open JIRA is to let splitlogworker to pick a meta wal file firstly and then others.", ,,1,,,,,
HBASE-8637,IntegrationTestBigLinkedListWithChaosMonkey creates a table named IntegrationTestBigLinkedListWithChaosMonkey but when inserting data it doesn't insert any data into it., ,1,,,,,,
HBASE-8639,"Hi, I am running a app on top of phoenix which will fork say around 100+ thread to call htable.getscanner(scan) to do parallel scan ( say each scan is actually targeting one Region), And each scan will only match a few result and return thus will be very fast.

under this case, I found that the htable.getscanner(scan) op itself runs pretty slow. by profiling with jvisualvm. I found 90% of app time is cost on org.apache.hadoop.net.DNS.getDefaultHost. Which been invoked by scannnercallable.checkIfRegionServerIsRemote. 

The root cause is that DNS.getDefaultHost involves synchronized methods in java.net.Inet4AddressImpl which have the 100+ thread to lock and wait upon each other. each call to DNS.getDefaultHost cost around 30ms, while in another case, I run single thread to call 100K times DNS.getDefaultHost , each cost leas than 0.06ms.

By hacking the code and remove the call to checkIfRegionServerIsRemote, my app runs 5 times faster, say, 50K op in my app cost 200+ seconds instead of 1000+ seconds.

by check the code further, I found this checkIfRegionServerIsRemote seems just for use of metrics collection. ( or maybe retry logic?) I am wondering that could this been removed or switch to some other implementation? so that cases like mine which run large number of small scan with multi threads could performance way better?
", ,,1,,,,,
HBASE-8640,"We are starting rpc server with default interface hostname or configured ipc address
{code}
    this.rpcServer = HBaseRPC.getServer(this,
      new Class<?>[]{HMasterInterface.class, HMasterRegionInterface.class},
        initialIsa.getHostName(), // This is bindAddress if set else it's hostname
        initialIsa.getPort(),
        numHandlers,
        0, // we dont use high priority handlers in master
        conf.getBoolean(""hbase.rpc.verbose"", false), conf,
        0); // this is a DNC w/o high priority handlers
{code}

But we are initialzing servername with default hostname always master znode also have this hostname.
{code}
    String hostname = Strings.domainNamePointerToHostName(DNS.getDefaultHost(
      conf.get(""hbase.master.dns.interface"", ""default""),
      conf.get(""hbase.master.dns.nameserver"", ""default"")));
	...
    this.serverName = new ServerName(hostname,
      this.isa.getPort(), System.currentTimeMillis());
{code}

If both default interface hostname and configured ipc address are not same clients will get MasterNotRunningException.", ,1,,,,,,
HBASE-8646,"Looking at master:60010/master-status#compactStas , I noticed that 'Num. Compacting KVs' column stays unchanged at non-zero value(s).

In DefaultCompactor#compact(), we have this at the beginning:
{code}
    this.progress = new CompactionProgress(fd.maxKeyCount);
{code}
But progress.totalCompactingKVs is not reset at the end of compact().", ,,,,,,,
HBASE-8652,"Looking at master:60010/master-status#compactStas , I noticed that 'Num. Compacting KVs' column stays unchanged at non-zero value(s).

In DefaultCompactor#compact(), we have this at the beginning:
{code}
    this.progress = new CompactionProgress(fd.maxKeyCount);
{code}
But progress.totalCompactingKVs is not reset at the end of compact().", ,,,,,,,
HBASE-8653,"Putting it in .1, feel free to move to .2.
We have observed some compaction errors where the code was creating a new HDFS block, and the file would not exist. Upon investigation, we found the .tmp directory delete request on namenode from master IP shortly before that. There are no specific logs on master, but one thing running at that time was CatalogJanitor. CatalogJanitor calls HRegionFileSystem::openRegionFromFileSystem with readOnly == true (in fact, everyone does); if readOnly is true, HRegionFileSystem nukes the .tmp directory.
We didn't go thru details on how it arrived there (or if there may have been other culprit), but it appears that deleting stuff if (readOnly) is not the intended behavior and it should be if (!readOnly). Given that readOnly is not really used (or rather is always true except some inconsequential usage in test) perhaps entire cleanup should be removed.", ,,,,,,,
HBASE-8656,"In SecureClient.java, rpc responses will be processed by receiveResponse() which looks like:
{code}
try {
        int id = in.readInt();                    // try to read an id

        if (LOG.isDebugEnabled())
          LOG.debug(getName() + "" got value #"" + id);

        Call call = calls.remove(id);

        int state = in.readInt();     // read call status
        if (LOG.isDebugEnabled()) {
          LOG.debug(""call #""+id+"" state is "" + state);
        }
        if (state == Status.SUCCESS.state) {
          Writable value = ReflectionUtils.newInstance(valueClass, conf);
          value.readFields(in);                 // read value
          if (LOG.isDebugEnabled()) {
            LOG.debug(""call #""+id+"", response is:\n""+value.toString());
          }
          // it's possible that this call may have been cleaned up due to a RPC
          // timeout, so check if it still exists before setting the value.
          if (call != null) {
            call.setValue(value);
          }
        } else if (state == Status.ERROR.state) {
          if (call != null) {
            call.setException(new RemoteException(WritableUtils.readString(in), WritableUtils
                .readString(in)));
          }
        } else if (state == Status.FATAL.state) {
          // Close the connection
          markClosed(new RemoteException(WritableUtils.readString(in),
                                         WritableUtils.readString(in)));
        }
      } catch (IOException e) {
        if (e instanceof SocketTimeoutException && remoteId.rpcTimeout > 0) {
          // Clean up open calls but don't treat this as a fatal condition,
          // since we expect certain responses to not make it by the specified
          // {@link ConnectionId#rpcTimeout}.
          closeException = e;
        } else {
          // Since the server did not respond within the default ping interval
          // time, treat this as a fatal condition and close this connection
          markClosed(e);
        }
      } finally {
        if (remoteId.rpcTimeout > 0) {
          cleanupCalls(remoteId.rpcTimeout);
        }
      }
    }
{code}
In above code, in the try block, the call will be firstly removed from call map by:
{code}
        Call call = calls.remove(id);
{code}
There may be two cases leading the call couldn't be notified and the invoking thread will wait forever. 
Firstly, if the returned status is Status.FATAL.state by:
{code}
        int state = in.readInt();     // read call status
{code}
The code will come into:
{code}
} else if (state == Status.FATAL.state) {
          // Close the connection
          markClosed(new RemoteException(WritableUtils.readString(in),
                                         WritableUtils.readString(in)));
        }
{code}
Here, the SecureConnection is marked as closed and all rpc calls in call map of this connection will be notified to receive an exception. However, the current rpc call has been removed from the call map, it won't be notified.
Secondly, after the call has been removed by:
{code}
        Call call = calls.remove(id);
{code}
If we encounter any exception before the 'try' block finished, the code will come into 'catch' and 'finally' block, neither 'catch' block nor 'finally' block will notify the rpc call because it has been removed from call map.
Compared with receiveResponse() in HBaseClient.java, it may be better to get the rpc call from call map and remove it at the end of the 'try' block.",1,,,,,,,
HBASE-8659,"LruBlockCache logs too much.

{code}
grep -c . hbase-hbase-regionserver-.....log 
77539
grep -c LruBlockCache  hbase-hbase-regionserver-......log 
64459
{code}
", ,,,,,,,
HBASE-8660,"REST interface is accessed over http, which is most likely done from outside of a HBase cluster, perhaps over internet. It will be great if we can make it secure. To be secure, we need to:
1. authenticate the caller,
2. check authorization if needed,
3. make the connection secure (https)",1,,,,,,,
HBASE-8662,"Currently, our client API uses a fixed user: the current user. It should accept a user passed in, if authenticated.",1,,,,,,,
HBASE-8665,"Note that this can be solved by bumping up the number of compaction threads but still it seems like this priority ""inversion"" should be dealt with.
There's a store with 1 big file and 3 flushes (1 2 3 4) sitting around and minding its own business when it decides to compact. Compaction (2 3 4) is created and put in queue, it's low priority, so it doesn't get out of the queue for some time - other stores are compacting. Meanwhile more files are flushed and at (1 2 3 4 5 6 7) it decides to compact (5 6 7). This compaction now has higher priority than the first one. After that if the load is high it enters vicious cycle of compacting and compacting files as they arrive, with store being blocked on and off, with the (2 3 4) compaction staying in queue for up to ~20 minutes (that I've seen).

I wonder why we do thing thing where we queue compaction and compact separately. Perhaps we should take snapshot of all store priorities, then do select in order and execute the first compaction we find. This will need starvation safeguard too but should probably be better.

Btw, exploring compaction policy may be more prone to this, as it can select files from the middle, not just beginning, which, given the treatment of already selected files that was not changed from the old ratio-based one (all files with lower seqNums than the ones selected are also ineligible for further selection), will make more files ineligible (e.g. imagine with 10 blocking files, with 8 present (1-8), (6 7 8) being selected and getting stuck). Today I see the case that would also apply to old policy, but yesterday I saw file distribution something like this: 4,5g, 2,1g, 295,9m, 113,3m, 68,0m, 67,8m, 1,1g, 295,1m, 100,4m, unfortunately w/o enough logs to figure out how it resulted.", ,,1,,,,,
HBASE-8666,"In distributedLogReplay mode when Meta recovery had experienced chained failures(recovery failed multiple times in a row), META region can't be fully recovered during master starts up.
 ", ,,,,,,,
HBASE-8667,"While testing HBASE-8640 fix found that master and regionserver running on different interfaces are not communicating properly.
I have two interfaces 1) lo 2) eth0 in my machine and default hostname interface is lo.
I have configured master ipc address to ip of eth0 interface.

Started master and regionserver on the same machine.
1) master rpc server bound to eth0 and RS rpc server bound to lo
2) Since rpc client is not binding to any ip address, when RS is reporting RS startup its getting registered with eth0 ip address(but actually it should register localhost)

Here are RS logs:
{code}
2013-05-31 06:05:28,608 WARN  [regionserver60020] org.apache.hadoop.hbase.regionserver.HRegionServer: reportForDuty failed; sleeping and then retrying.
2013-05-31 06:05:31,609 INFO  [regionserver60020] org.apache.hadoop.hbase.regionserver.HRegionServer: Attempting connect to Master server at 192.168.0.100,60000,1369960497008
2013-05-31 06:05:31,609 INFO  [regionserver60020] org.apache.hadoop.hbase.regionserver.HRegionServer: Telling master at 192.168.0.100,60000,1369960497008 that we are up with port=60020, startcode=1369960502544
2013-05-31 06:05:31,618 DEBUG [regionserver60020] org.apache.hadoop.hbase.regionserver.HRegionServer: Config from master: hbase.rootdir=hdfs://localhost:2851/hbase
2013-05-31 06:05:31,618 DEBUG [regionserver60020] org.apache.hadoop.hbase.regionserver.HRegionServer: Config from master: fs.default.name=hdfs://localhost:2851
2013-05-31 06:05:31,618 INFO  [regionserver60020] org.apache.hadoop.hbase.regionserver.HRegionServer: Master passed us a different hostname to use; was=localhost, but now=192.168.0.100
{code}

Here are master logs:
{code}
2013-05-31 06:05:31,615 INFO  [IPC Server handler 9 on 60000] org.apache.hadoop.hbase.master.ServerManager: Registering server=192.168.0.100,60020,1369960502544
{code}

Since master has wrong rpc server address of RS, META is not getting assigned.
{code}
2013-05-31 06:05:34,362 DEBUG [master-192.168.0.100,60000,1369960497008] org.apache.hadoop.hbase.master.AssignmentManager: No previous transition plan was found (or we are ignoring an existing plan) for .META.,,1.1028785192 so generated a random one; hri=.META.,,1.1028785192, src=, dest=192.168.0.100,60020,1369960502544; 1 (online=1, available=1) available servers, forceNewPlan=false
-----
org.apache.hadoop.hbase.master.AssignmentManager: Failed assignment of .META.,,1.1028785192 to 192.168.0.100,60020,1369960502544, trying to assign elsewhere instead; try=1 of 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:592)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupConnection(RpcClient.java:549)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:813)
	at org.apache.hadoop.hbase.ipc.RpcClient.getConnection(RpcClient.java:1422)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1315)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1532)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1587)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.openRegion(AdminProtos.java:15039)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionOpen(ServerManager.java:627)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1826)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1453)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1432)
	at org.apache.hadoop.hbase.master.handler.ClosedRegionHandler.process(ClosedRegionHandler.java:104)
	at org.apache.hadoop.hbase.master.AssignmentManager.addToRITandCallClose(AssignmentManager.java:699)
	at org.apache.hadoop.hbase.master.AssignmentManager.processRegionsInTransition(AssignmentManager.java:584)
	at org.apache.hadoop.hbase.master.AssignmentManager.processRegionInTransition(AssignmentManager.java:517)
	at org.apache.hadoop.hbase.master.AssignmentManager.processRegionInTransitionAndBlockUntilAssigned(AssignmentManager.java:473)
	at org.apache.hadoop.hbase.master.HMaster.assignMeta(HMaster.java:917)
	at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:803)
	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:547)
	at java.lang.Thread.run(Thread.java:636)
{code}", ,1,,,,,,
HBASE-8675,"In our product cluster, because of the net problem to kerberos server, the ZooKeeperWatcher in active hmaster fails to Auth , gets a connection Event of AUTH_FAILED  and loose the master lock. But the zookeeper watcher ignores the event, so the old active hmaster keeps to be active. After the net problem is fixed, the backup hmaster gets the master lock and becomes active. There are two two active hmasters in the cluster.

2013-05-30 09:44:21,004 ERROR org.apache.zookeeper.client.ZooKeeperSaslClient: An error: (java.security.PrivilegedActionException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: krb1.xiaomi.net)]) occurred when evaluating Zookeeper Quorum Member's  received SASL token. Zookeeper Client will go to AUTH_FAILED state.

2013-05-30 09:54:07,755 WARN org.apache.hadoop.hbase.zookeeper.ZKUtil: hconnection-0x3e10d98be405bc Unable to set watcher on znode /hbase/master
org.apache.zookeeper.KeeperException$AuthFailedException: KeeperErrorCode = AuthFailed for /hbase/master
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:123)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1036)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.exists(RecoverableZooKeeper.java:166)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.watchAndCheckExists(ZKUtil.java:231)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.start(ZooKeeperNodeTracker.java:76)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.ensureZookeeperTrackers(HConnectionManager.java:595)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:850)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:825)
        at org.apache.hadoop.hbase.client.HTable.finishSetup(HTable.java:286)
        at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:201)
        at org.apache.hadoop.hbase.catalog.MetaReader.getHTable(MetaReader.java:200)
        at org.apache.hadoop.hbase.catalog.MetaReader.getMetaHTable(MetaReader.java:226)
        at org.apache.hadoop.hbase.catalog.MetaReader.fullScan(MetaReader.java:705)
        at org.apache.hadoop.hbase.catalog.MetaReader.fullScan(MetaReader.java:183)
        at org.apache.hadoop.hbase.catalog.MetaReader.fullScan(MetaReader.java:168)
        at org.apache.hadoop.hbase.master.CatalogJanitor.getSplitParents(CatalogJanitor.java:123)
        at org.apache.hadoop.hbase.master.CatalogJanitor.scan(CatalogJanitor.java:134)
        at org.apache.hadoop.hbase.master.CatalogJanitor.chore(CatalogJanitor.java:92)
        at org.apache.hadoop.hbase.Chore.run(Chore.java:67)
        at java.lang.Thread.run(Thread.java:662)


I want to just abort the hmaster server if AuthFailed or SaslAuthenticated. Any better idea about this issue? 
For ZookeeperWatcher is used in many classes, will the aborting will bring more problems? Any more problems we need consider? 
",1,,,,,,,
HBASE-8678,"First, I put a cell using put interface, but I don't specify timestamp. Then I delete the the same row, specify a timestamp of 1L. Unfortunately, the former cell is 

deleted. We should know this cell can not be deleted in this case. (Using original Client API)

Code like this;
public static void main(String[] args) throws Exception {
        Cluster cluster = new Cluster();
        cluster.add(""10.28.171.38"", 8080);
        Client client = new Client(cluster);
        RemoteHTable table = new RemoteHTable(client, ""demotime"");
                
        Put put = new Put(Bytes.toBytes(""row21""));
        put.add(""info"".getBytes(), ""name"".getBytes(), ""huanggang"".getBytes());
        table.put(put);
        
        Delete delete = new Delete(Bytes.toBytes(""row21""));
        delete.setTimestamp(1L);
        table.delete(delete);
    }", ,,,,,,,
HBASE-8680,"The JIRA is to check in changes to address performance issues found during my performance testing as following:
1) When WALEdits belongs to a region which split/merged later, replay incurs extra waitUntilRegionOnline RPC call
2) Using a single shared connection for replaying achieves better performance. Everytime creating a new connection, it incurs 4+ seconds to establish a connection to ZK.
3) other small modifications to mitigate excessive logging", ,,1,,,,,
HBASE-8684,"With the default configuration passed to a RegionCoprocessor environment, you cannot reach an HTable on the same cluster (at least in 0.94 - no verified yet in 0.96/8). The reason is in the usage of CompoundConfiguration (backported to 0.94 in HBASE-8176) when loading a table coprocessor we just do (RegionCoprocessorHost, ln 182):
{code}
Configuration newConf = new Configuration(conf);
// set per-table cfspec properties
{code}
but the conf we pass in a CompoundConfiguration, which means the copy constructor from Configuration doesn't work at all. 

So, we really need two things:
 1. A test that we can reach another HTable in the same cluster via a coprocessor by default
 2. Fixing the configuration creation in RegionCoprocessorHost to support (1)", ,1,,,,,,
HBASE-8692,"Some users are concerned about having table schema exposed to every user and would like it protected, similar to the rest of the admin operations for schema. 
This used to be hopeless because META would leak HTableDescriptors in HRegionInfo, but that is no longer the case in 0.94+.
Consider adding CP hooks in the master for intercepting HMasterInterface#getHTableDescriptors and HMasterInterface#getHTableDescriptors(List<String>). Add support in the AccessController for only allowing GLOBAL ADMIN to the first method. Add support in the AccessController for allowing access to the descriptors for the table names in the list of the second method only if the user has TABLE ADMIN privilege for all of the listed table names.
Then, fix the code in HBaseAdmin (and elsewhere) that expects to be able to enumerate all table descriptors e.g. in deleteTable. A TABLE ADMIN can delete a table but wont have GLOBAL ADMIN privilege to enumerate the total list. So a minor fixup is needed here, and in other places like this which make the same assumption.",1,,,,,,,
HBASE-8698,"MetaScanner.metaScan() creates an instance of HTable per call. The constructur used creates a new ThreadPoolExecutor. The executor itself will not create a thread unless it's pool is used. I am not sure if the HTable instance in question ever uses it's pool. But if so, this could become a big performance issue. Logging an issue at Lars's request. mail list chain below.

------------------------------------- 

Indeed. That is bad.
I cannot see a clean fix immediately, but we need to look at this.

Mind filing a ticket, Kireet?

-- Lars



________________________________
 From: Kireet <kireet-Teh5dPVPL8nQT0dZR+AlfA@public.gmane.org>
To: public-user-50Pas4EWwPEyzMRdD/IqWQ-wOFGN7rlS/M9smdsby/KFg@public.gmane.org 
Sent: Friday, May 31, 2013 11:58 AM
Subject: Re: HConnectionManager$HConnectionImplementation.locateRegionInMeta
 



Even if I initiate the call via a pooled htable, the MetaScanner seems 
to use a concrete HTable instance. The constructor invoked seems to 
create a java ThreadPoolExecutor. I am not 100% sure but I think as long 
as nothing is submitted to the ThreadPoolExecutor it won't create any 
threads. I just wanted to confirm this was the case. I do see the 
connection is shared.

--Kireet



On 5/30/13 7:38 PM, Ted Yu wrote:
> HTablePool$**PooledHTable is a wrapper around HTable.
>
> Here is how HTable obtains a connection:
>
>     public HTable(Configuration conf, final byte[] tableName, final
> ExecutorService pool)
>         throws IOException {
>       this.connection = HConnectionManager.getConnection(conf);
>
> Meaning the connection is a shared one based on certain key/value pairs
> from conf.
>
> bq. So every call to batch will create a new thread?
>
> I don't think so.
>
> On Thu, May 30, 2013 at 11:28 AM, Kireet <kireet-Teh5dPVPL8nQT0dZR+AlfA-XMD5yJDbdMReXY1tMh2IBg@public.gmane.org> wrote:
>
>>
>>
>> Thanks, will give it a shot. So I should download 0.94.7 (latest stable)
>> and run the patch tool on top with the backport? This is a little new to me.
>>
>> Also, I was looking at the stack below. From my reading of the code, the
>> HTable.batch() call will always cause the prefetch call to occur, which
>> will cause a new HTable object to get created. The constructor used in
>> creating a new thread pool. So every call to batch will create a new
>> thread? Or the HTable's thread pool never gets used as the pool is only
>> used for writes? I think I am missing something but just want to confirm.
>>
>> Thanks
>> Kireet
", ,,1,,,,,
HBASE-8699,"Here is current code of FSHDFSUtils#isFileClosed():
{code}
  boolean isFileClosed(final DistributedFileSystem dfs, final Path p) {
    try {
      Method m = dfs.getClass().getMethod(""isFileClosed"", new Class<?>[] {String.class});
      return (Boolean) m.invoke(dfs, p.toString());
{code}
We look for isFileClosed method with parameter type of String.

However, from hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java (branch-2):
{code}
  public boolean isFileClosed(Path src) throws IOException {
{code}
The parameter type is of Path.
This means we would get NoSuchMethodException.", ,,,,,,,
HBASE-8700,"The test can fail due to random number collision, claiming there are unreferenced elements for obvious reasons (we rewrite some link). Original Accumulo test has one-stage generation so it doesn't count unreferenced elements as failures, only undefined ones. With 200m longs out of half-long range the probability of collision is approx 0.2%.

Moreover, without some way to debug, it's hard to debug what keys should be looked at in such cases", ,,,,,,,
HBASE-8701,"This issue happens in distributedLogReplay mode when recovering multiple puts of the same key + version(timestamp). After replay, the value is nondeterministic of the key

h5. The original concern situation raised from [~eclark]:

For all edits the rowkey is the same.
There's a log with: [ A (ts = 0), B (ts = 0) ]
Replay the first half of the log.
A user puts in C (ts = 0)
Memstore has to flush
A new Hfile will be created with [ C, A ] and MaxSequenceId = C's seqid.
Replay the rest of the Log.
Flush

The issue will happen in similar situation like Put(key, t=T) in WAL1 and Put(key,t=T) in WAL2

h5. Below is the option(proposed by Ted) I'd like to use:

a) During replay, we pass original wal sequence number of each edit to the receiving RS
b) In receiving RS, we store negative original sequence number of wal edits into mvcc field of KVs of wal edits
c) Add handling of negative MVCC in KVScannerComparator and KVComparator   
d) In receiving RS, write original sequence number into an optional field of wal file for chained RS failure situation 
e) When opening a region, we add a safety bumper(a large number) in order for the new sequence number of a newly opened region not to collide with old sequence numbers. 

In the future, when we stores sequence number along with KVs, we can adjust the above solution a little bit by avoiding to overload MVCC field.

h5. The other alternative options are listed below for references:

Option one
a) disallow writes during recovery
b) during replay, we pass original wal sequence ids
c) hold flush till all wals of a recovering region are replayed. Memstore should hold because we only recover unflushed wal edits. For edits with same key + version, whichever with larger sequence Id wins.

Option two
a) During replay, we pass original wal sequence ids
b) for each wal edit, we store each edit's original sequence id along with its key. 
c) during scanning, we use the original sequence id if it's present otherwise its store file sequence Id
d) compaction can just leave put with max sequence id

Please let me know if you have better ideas.", ,,,,,,,
HBASE-8723,"Currently any IT tests that have chaos monkey fail because we are not recovering regions before the number of RPC reties is exhausted.

We should set that default higher.", ,1,,,,,,
HBASE-8724,"On 0.94, ExportSnapshot uses ""hbase.tmp.dir"" as the job's staging directory on hdfs. However, ""hbase.tmp.dir"" is by definition a local directory, thus should not be used as an hdfs directory for the job. 

Trunk uses JobUtil.getStagingDir() which gets the staging dir from JobSubmissionFiles class in Hadoop, so trunk is fine. 

We've discovered this since it fails the test on windows, but this is not windows-specific as per above (like specifying hbase.tmp.dir as /var/hbase/tmp/ etc)", ,1,,,,,,
HBASE-8729,"In a test, half cluster(in terms of region servers) was down and some log replay had incurred chained RS failures(receiving RS of a log replay failed again). 

Since by default, we only allow 3 concurrent SSH handlers(controlled by {code}this.executorService.startExecutorService(ExecutorType.MASTER_SERVER_OPERATIONS,conf.getInt(""hbase.master.executor.serverops.threads"", 3));{code}).

If all 3 SSH handlers are doing logReplay(blocking call) and one of receiving RS fails again then logReplay will hang because regions of the newly failed RS can't be re-assigned to another live RS(no ssh handler will be processed due to max threads setting) and existing log replay will keep routing replay traffic to the dead RS.

The fix is to submit logReplay work into a separate type of executor queue in order not to block SSH region assignment so that logReplay can route traffic to a live RS after retries and move forward. ", ,,,,,,,
HBASE-8732,Getting an error when opening a scanner on a file that has no encoding., ,,,,,,,
HBASE-8741,"Currently, when opening a region, we find the maximum sequence ID from all its HFiles and then set the LogSequenceId of the log (in case the later is at a small value). This works good in recovered.edits case as we are not writing to the region until we have replayed all of its previous edits. 
With distributed log replay, if we want to enable writes while a region is under recovery, we need to make sure that the logSequenceId > maximum logSequenceId of the old regionserver. Otherwise, we might have a situation where new edits have same (or smaller) sequenceIds. 

We can store region level information in the WALTrailer, than this scenario could be avoided by:
a) reading the trailer of the ""last completed"" file, i.e., last wal file which has a trailer and,
b) completely reading the last wal file (this file would not have the trailer, so it needs to be read completely).

In future, if we switch to multi wal file, we could read the trailer for all completed WAL files, and reading the remaining incomplete files.", ,,,,,,,
HBASE-8749,"We recently saw the following running 0.95.1 on hadoop 2.0:
{code}
2013-06-11 21:51:22,199 ERROR [IPC Server handler 18 on 60000] master.HMaster: Region server ip-10-138-2-28.ec2.internal,60020,1370887927492 reported a fatal error:
ABORTING region server ip-10-138-2-28.ec2.internal,60020,1370887927492: IOE in log roller
Cause:
java.io.FileNotFoundException: File/Directory /hbase/.oldlogs/ip-10-138-2-28.ec2.internal%2C60020%2C1370887927492.1370986265468 does not exist.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setTimesInt(FSNamesystem.java:1488)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setTimes(FSNamesystem.java:1453)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setTimes(NameNodeRpcServer.java:798)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setTimes(ClientNamenodeProtocolServerSideTranslatorPB.java:704)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:43194)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:454)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:910)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1694)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1690)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1367)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1688)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:90)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:57)
	at org.apache.hadoop.hdfs.DFSClient.setTimes(DFSClient.java:2097)
	at org.apache.hadoop.hdfs.DistributedFileSystem.setTimes(DistributedFileSystem.java:813)
	at org.apache.hadoop.hbase.util.FSUtils.renameAndSetModifyTime(FSUtils.java:1596)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog.archiveLogFile(FSHLog.java:705)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog.cleanOldLogs(FSHLog.java:595)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog.rollWriter(FSHLog.java:536)
	at org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:96)
	at java.lang.Thread.run(Thread.java:722)
Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File/Directory /hbase/.oldlogs/ip-10-138-2-28.ec2.internal%2C60020%2C1370887927492.1370986265468 does not exist.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setTimesInt(FSNamesystem.java:1488)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setTimes(FSNamesystem.java:1453)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setTimes(NameNodeRpcServer.java:798)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setTimes(ClientNamenodeProtocolServerSideTranslatorPB.java:704)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:43194)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:454)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:910)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1694)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1690)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1367)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1688)

	at org.apache.hadoop.ipc.Client.call(Client.java:1164)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:202)
	at com.sun.proxy.$Proxy11.setTimes(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor27.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:164)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:83)
	at com.sun.proxy.$Proxy11.setTimes(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.setTimes(ClientNamenodeProtocolTranslatorPB.java:685)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:266)
	at com.sun.proxy.$Proxy12.setTimes(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.setTimes(DFSClient.java:2095)
	... 7 more

2013-06-11 21:51:32,976 INFO  [main-EventThread] zookeeper.RegionServerTracker: RegionServer ephemeral node deleted, processing expiration [ip-10-138-2-28.ec2.internal,60020,1370887927492]
{code}
One scenario that could have led to the above exception was that the LogCleaner thread deleted the log file which had just been moved to .oldlogs dir before the call to fs.setTimes() executed.
When fs.setTimes() ran, the file was no longer there.", ,,,,,,,
HBASE-8750,"While running log replay on a one node setup, I killed meta regionserver. The MetaSSH tries to assign the meta table, but it failed as there was no other regionservers to assign to. But the meta server znode was already updated to null. When the assignment fails, the metaSSH is retried. But from the next iteration, it will not try to assign the meta region, but keeps on waiting for meta region. This keeps on going even after regionserver is brought up again.
", ,,,,,,,
HBASE-8757,"In secure HBase, I got the following exception.  Actually it is a valid event: SaslAuthenticated 

{noformat}
2013-06-17 16:32:29,959 ERROR [2049101312@qtp-1851406627-0-EventThread] zookeeper.ClientCnxn: Error while calling watcher
java.lang.IllegalStateException: Received event is not valid.
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:406)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:316)
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:519)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:495)
{noformat}
", ,,,,,,,
HBASE-8760,"Right after a region split but before the daughter regions are compacted, we have two daughter regions containing Reference files to the parent hfiles.

If we take snapshot right at the moment, the snapshot will succeed, but it will only contain the daughter Reference files. Since there is no hold on the parent hfiles, they will be deleted by the HFile Cleaner after they are no longer needed by the daughter regions soon after.

A minimum we need to do is the keep these parent hfiles from being deleted. ", ,,,,,,,
HBASE-8762,"There are two implications to calling HTable.get with a list of one Get.
1. The overhead of processBatch is paid unnecessarily, which is not insignificant.
2. The get requests show up as a 'multi' when reviewing RPC handlers, when the request should just be a single Get.  It seems likely that there are other places in logs/ui it shows up as a multi as well.

To give some context to the overhead, here are some timings performed by a member of our team:

In a very simple test, of reading the same key 100 times, taking the time it took, and then repeating this 10 times (1000 total gets), the times are as follows (excluding the actual first iteration as there was considerable HBase warm-up times on the JVM for establishing connections):
||Iteration||Batch (in ms)||Single (in ms)||
|1|2255|815| 
|2|1545|823| 
|3|1427|742| 
|4|1451|721| 
|5|1480|775| 
|6|1379|735| 
|7|1657|775| 
|8|1392|804|

While I can see the argument that callers should use the single Get method signature, the cost implications are somewhat surprising and it's very easy to be smart in this case.  We simply need to have HTable.get(List<Get>) delegate to HTable.get(<Get>) if the list has one Get.", ,,1,,,,,
HBASE-8776,, ,1,,,,,,
HBASE-8778,"There are two implications to calling HTable.get with a list of one Get.
1. The overhead of processBatch is paid unnecessarily, which is not insignificant.
2. The get requests show up as a 'multi' when reviewing RPC handlers, when the request should just be a single Get.  It seems likely that there are other places in logs/ui it shows up as a multi as well.

To give some context to the overhead, here are some timings performed by a member of our team:

In a very simple test, of reading the same key 100 times, taking the time it took, and then repeating this 10 times (1000 total gets), the times are as follows (excluding the actual first iteration as there was considerable HBase warm-up times on the JVM for establishing connections):
||Iteration||Batch (in ms)||Single (in ms)||
|1|2255|815| 
|2|1545|823| 
|3|1427|742| 
|4|1451|721| 
|5|1480|775| 
|6|1379|735| 
|7|1657|775| 
|8|1392|804|

While I can see the argument that callers should use the single Get method signature, the cost implications are somewhat surprising and it's very easy to be smart in this case.  We simply need to have HTable.get(List<Get>) delegate to HTable.get(<Get>) if the list has one Get.", ,,1,,,,,
HBASE-8790,"The Hbase cluster is a fresh start with one regionserver.
When we stop hbase, an unhandled NullPointerException is throwed in the regionserver.
The regionserver's log is as follows:

2013-06-21 10:21:11,284 INFO  [regionserver61020] regionserver.HRegionServer: Closing user regions
2013-06-21 10:21:14,288 DEBUG [regionserver61020] regionserver.HRegionServer: Waiting on 1028785192
2013-06-21 10:21:14,290 FATAL [regionserver61020] regionserver.HRegionServer: ABORTING region server HOSTNAME_TEST,61020,1371781086817
: Unhandled: null
java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.HRegionServer.tryRegionServerReport(HRegionServer.java:988)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:832)
        at java.lang.Thread.run(Thread.java:662)
2013-06-21 10:21:14,292 FATAL [regionserver61020] regionserver.HRegionServer: RegionServer abort: loaded coprocessors are: [org.apache
.hadoop.hbase.coprocessor.MultiRowMutationEndpoint]
2013-06-21 10:21:14,293 INFO  [regionserver61020] regionserver.HRegionServer: STOPPED: Unhandled: null
2013-06-21 10:21:14,293 INFO  [regionserver61020] ipc.RpcServer: Stopping server on 61020

It seems that after closing user regions, the rssStub is null.

update:
we found that if setting hbase.client.ipc.pool.type to RoundRobinPool(or other pool type) and hbase.client.ipc.pool.size to 10(possibly other values) in hbase-site.xml, the regionserver is continuously attempting connect to master. and if we stop hbase, the above NullPointerException occurred. With hbase.client.ipc.pool.size set to 1, the cluster can be completely stopped.", ,1,,,,,,
HBASE-8797,"When two regions are merged online, they are closed but master doesn't know they should stay closed during the merge.  If master moves them by mistake, for example, load balancer kicks in, the merge could be messed up.", ,,,,,,,
HBASE-8803,"When there is many regions in a cluster, rolling_restart can take hours because region_mover is moving the regions one by one.", ,,1,,,,,
HBASE-8806,"When there is many regions in a cluster, rolling_restart can take hours because region_mover is moving the regions one by one.", ,,1,,,,,
HBASE-8811,"In rest.RowResource.update(), this code keeps executing a request if a misspelled check= parameter is provided.
{noformat}
    if (CHECK_PUT.equalsIgnoreCase(check)) {
      return checkAndPut(model);
    } else if (CHECK_DELETE.equalsIgnoreCase(check)) {
      return checkAndDelete(model);
    } else if (check != null && check.length() > 0) {
      LOG.warn(""Unknown check value: "" + check + "", ignored"");
    }
{noformat}

By my reading of the code, this results in the provided cell value that was intended as a check instead being treated as a mutation, which is sure to destroy user data.  Thus the priority of this bug, as it can cause corruption.

I suggest that a better reaction than a warning would be, approximately:

{noformat}
return Response.status(Response.Status.BAD_REQUEST)
        .type(MIMETYPE_TEXT).entity(""Invalid check value '"" + check + ""'"")
        .build();
{noformat}
", ,,,,,,,
HBASE-8814,"{code}
2013-06-27 14:12:54,472 INFO  [RS:1;BLRY2R009039160:49833-splits-1372322572806] regionserver.SplitRequest(92): Running rollback/cleanup of failed split of testSplitShouldNotThrowNPEEvenARegionHasEmptySplitFiles,,1372322556662.276e00da1420119e2f91f3a4c4c41d78.; java.util.concurrent.ExecutionException: java.lang.NullPointerException
java.io.IOException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at org.apache.hadoop.hbase.regionserver.SplitTransaction.splitStoreFiles(SplitTransaction.java:602)
	at org.apache.hadoop.hbase.regionserver.SplitTransaction.createDaughters(SplitTransaction.java:297)
	at org.apache.hadoop.hbase.regionserver.SplitTransaction.execute(SplitTransaction.java:466)
	at org.apache.hadoop.hbase.regionserver.SplitRequest.run(SplitRequest.java:82)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.hadoop.hbase.regionserver.SplitTransaction.splitStoreFiles(SplitTransaction.java:596)
	... 6 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hbase.regionserver.HRegionFileSystem.splitStoreFile(HRegionFileSystem.java:539)
	at org.apache.hadoop.hbase.regionserver.SplitTransaction.splitStoreFile(SplitTransaction.java:610)
	at org.apache.hadoop.hbase.regionserver.SplitTransaction.access$1(SplitTransaction.java:607)
	at org.apache.hadoop.hbase.regionserver.SplitTransaction$StoreFileSplitter.call(SplitTransaction.java:633)
	at org.apache.hadoop.hbase.regionserver.SplitTransaction$StoreFileSplitter.call(SplitTransaction.java:1)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	... 3 more
{code}

If a storefile is empty(can be because of puts and deletes) then first and lastkey of the file will be empty. Then we will get first or last key as null. Then we will end up in NPE when we will check splitkey in the range or not.
{code}
    if (top) {
      //check if larger than last key.
      KeyValue splitKey = KeyValue.createFirstOnRow(splitRow);
      byte[] lastKey = f.createReader().getLastKey();      
      if (f.getReader().getComparator().compare(splitKey.getBuffer(), 
          splitKey.getKeyOffset(), splitKey.getKeyLength(), lastKey, 0, lastKey.length) > 0) {
        return null;
      }
    } else {
      //check if smaller than first key
      KeyValue splitKey = KeyValue.createLastOnRow(splitRow);
      byte[] firstKey = f.createReader().getFirstKey();
      if (f.getReader().getComparator().compare(splitKey.getBuffer(), 
          splitKey.getKeyOffset(), splitKey.getKeyLength(), firstKey, 0, firstKey.length) < 0) {
        return null;
      }      
    }
{code}", ,,,,,,,
HBASE-8838,"FileLink assumes that underlying files can be moved while there are open readers. Windows does not allow the file which has active readers to be moved unless the file is opened in shared mode. JDK6 and 7 open the file in exclusive mode by default, so moving the file while there are readers is not possible. 
Hadoop-on-windows copies the file, if it cannot perform the rename. This will cause duplication, and the original file not being deleted. 
This affects the unit test TestFileLink. On local mode over NTFS, reading from snapshot files and replication might cause issues, but I don't think it has any other implications on hbase+hdfs on windows.",1,,,,,,,
HBASE-8853,It seems we broke something when we changed the client code (likely nio stuff). Here is a fix and a unit test., ,,,,,,,
HBASE-8855,"It looks like TestTableInputFormatScan1 and TestTableInputFormatScan2 never complete and surefire doesn't complain about it. Sure, you may see this:

{noformat}
Tests run: 6, Failures: 5, Errors: 1, Skipped: 0, Time elapsed: 269.036 sec <<< FAILURE!
org.apache.maven.surefire.util.SurefireReflectionException: ...
org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:74)
Caused by: java.lang.OutOfMemoryError: PermGen space
org.apache.maven.surefire.booter.SurefireBooterForkException: Error occurred in starting fork, check output in log
...
  testScanEmptyToBBA(org.apache.hadoop.hbase.mapreduce.TestTableInputFormatScan1)
  testScanEmptyToBBB(org.apache.hadoop.hbase.mapreduce.TestTableInputFormatScan1)
  testScanEmptyToOPP(org.apache.hadoop.hbase.mapreduce.TestTableInputFormatScan1)
  testScanEmptyToEmpty(org.apache.hadoop.hbase.mapreduce.TestTableInputFormatScan1)
{noformat}

But then:

{noformat}
[INFO] HBase - Server .................................... SUCCESS [10:59.929s]
{noformat}

This is on my machine. On our local jenkins it's leaking and the processes never die. And this is only with Hadoop 2. It also looks like other tests are failing with PermGen space.", ,,,,,,,
HBASE-8856,"I am using DelimitedKeyPrefixRegionSplitPolicy on my table.  When I attempted to split the table from the web interface, it gives the error below.

I believe the problem is org.apache.hadoop.hbase.regionserver.DelimitedKeyPrefixRegionSplitPolicy.getSplitPoint:75 - the call from super.getSplitPoint() (line 71) returns null and this condition is not checked.

-------

{code}
HTTP ERROR 500

Problem accessing /table.jsp. Reason:

    java.io.IOException: java.lang.NullPointerException: array
	at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:204)
	at com.google.common.primitives.Bytes.indexOf(Bytes.java:115)
	at org.apache.hadoop.hbase.regionserver.DelimitedKeyPrefixRegionSplitPolicy.getSplitPoint(DelimitedKeyPrefixRegionSplitPolicy.java:75)
	at org.apache.hadoop.hbase.regionserver.HRegion.checkSplit(HRegion.java:5697)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.splitRegion(HRegionServer.java:3279)
	at sun.reflect.GeneratedMethodAccessor60.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:320)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1426)

Caused by:

org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.NullPointerException: array
	at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:204)
	at com.google.common.primitives.Bytes.indexOf(Bytes.java:115)
	at org.apache.hadoop.hbase.regionserver.DelimitedKeyPrefixRegionSplitPolicy.getSplitPoint(DelimitedKeyPrefixRegionSplitPolicy.java:75)
	at org.apache.hadoop.hbase.regionserver.HRegion.checkSplit(HRegion.java:5697)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.splitRegion(HRegionServer.java:3279)
	at sun.reflect.GeneratedMethodAccessor60.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:320)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1426)

	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:995)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:86)
	at com.sun.proxy.$Proxy11.splitRegion(Unknown Source)
	at org.apache.hadoop.hbase.client.HBaseAdmin.split(HBaseAdmin.java:1620)
	at org.apache.hadoop.hbase.client.HBaseAdmin.split(HBaseAdmin.java:1608)
	at org.apache.hadoop.hbase.client.HBaseAdmin.split(HBaseAdmin.java:1565)
	at org.apache.hadoop.hbase.client.HBaseAdmin.split(HBaseAdmin.java:1552)
	at org.apache.hadoop.hbase.generated.master.table_jsp._jspService(table_jsp.java:94)
	at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:98)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
	at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:835)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
{code}", ,,,,,,,
HBASE-8871,"I have this stack when I start a fresh region server. 5% of the time I would say (per region server).

{code}
2013-07-04 12:00:22,609 FATAL [regionserver60020] regionserver.HRegionServer: ABORTING region server ip-10-137-7-67.ec2.internal,60020,1372939221819: Initialization of RS failed.  Hence aborting RS.
java.util.ConcurrentModificationException
	at java.util.Hashtable$Enumerator.next(Hashtable.java:1200)
	at org.apache.hadoop.conf.Configuration.iterator(Configuration.java:1820)
	at org.apache.hadoop.hbase.zookeeper.ZKConfig.makeZKProps(ZKConfig.java:92)
	at org.apache.hadoop.hbase.zookeeper.ZKConfig.getZKQuorumServersString(ZKConfig.java:267)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.<init>(ZooKeeperWatcher.java:158)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.<init>(ZooKeeperWatcher.java:133)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.initializeZooKeeper(HRegionServer.java:667)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.preRegistrationInitialization(HRegionServer.java:647)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:778)
	at java.lang.Thread.run(Thread.java:722)
2013-07-04 12:00:22,614 FATAL [regionserver60020] regionserver.HRegionServer: RegionServer abort: loaded coprocessors are: []
2013-07-04 12:00:22,614 INFO  [regionserver60020] regionserver.HRegionServer: STOPPED: Initialization of RS failed.  Hence aborting RS.
2013-07-04 12:00:22,616 FATAL [regionserver60020] regionserver.HRegionServer: ABORTING region server ip-10-137-7-67.ec2.internal,60020,1372939221819: Unhandled: null
java.lang.NullPointerException
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:798)
	at java.lang.Thread.run(Thread.java:722)
2013-07-04 12:00:22,616 FATAL [regionserver60020] regionserver.HRegionServer: RegionServer abort: loaded coprocessors are: []
2013-07-04 12:00:22,617 INFO  [regionserver60020] regionserver.HRegionServer: STOPPED: Unhandled: null
2013-07-04 12:00:22,767 INFO  [main] regionserver.ShutdownHook: Installed shutdown hook thread: Shutdownhook:regionserver60020
2013-07-04 12:00:22,768 ERROR [main] regionserver.HRegionServerCommandLine: Region server exiting
java.lang.RuntimeException: HRegionServer Aborted
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:66)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:85)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:78)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:2309)
2013-07-04 12:00:22,770 INFO  [Thread-4] regionserver.ShutdownHook: Shutdown hook starting; hbase.shutdown.hook=true; fsShutdownHook=org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer@21f0dbb9
{code}

There is one bug here in the region server: we should not start the snapshot if the machine is supposed to stop:

{code}
      // start the snapshot handler, since the server is ready to run
      this.snapshotManager.start();
{code}

and the root cause is here in ZKConfig:
{code}
    for (Entry<String, String> entry : conf) { // <=== BUG
      String key = entry.getKey();
      if (key.startsWith(HConstants.ZK_CFG_PROPERTY_PREFIX)) {
        String zkKey = key.substring(HConstants.ZK_CFG_PROPERTY_PREFIX_LEN);
        String value = entry.getValue();
        // If the value has variables substitutions, need to do a get.
        if (value.contains(VARIABLE_START)) {
          value = conf.get(key);
        }
        zkProperties.put(zkKey, value);
      }
{code}", ,1,,,,,,
HBASE-8874,"While combining puts of same row in map phase we are using below logic in PutCombiner#reduce. In for loop first time we will add one Put object to puts map. Next time onwards we are just overriding key values of a family with key values of the same family in other put. So we are mostly writing one Put object to map output and remaining will be skipped(data loss).
{code}
    Map<byte[], Put> puts = new TreeMap<byte[], Put>(Bytes.BYTES_COMPARATOR);
    for (Put p : vals) {
      cnt++;
      if (!puts.containsKey(p.getRow())) {
        puts.put(p.getRow(), p);
      } else {
        puts.get(p.getRow()).getFamilyMap().putAll(p.getFamilyMap());
      }
    }
{code}

We need to change logic similar as below because we are sure the rowkey of all the puts will be same.
{code}
    Put finalPut = null;
    Map<byte[], List<? extends Cell>> familyMap = null;
    for (Put p : vals) {
     cnt++;
      if (finalPut==null) {
        finalPut = p;
        familyMap = finalPut.getFamilyMap();
      } else {
        for (Entry<byte[], List<? extends Cell>> entry : p.getFamilyMap().entrySet()) {
          List<? extends Cell> list = familyMap.get(entry.getKey());
          if (list == null) {
            familyMap.put(entry.getKey(), entry.getValue());
          } else {
            (((List<KeyValue>)list)).addAll((List<KeyValue>)entry.getValue());
          }
        }
      }
    }
    context.write(row, finalPut);
{code}

Also need to implement TODOs mentioned by Nick 
{code}
    // TODO: would be better if we knew <code>K row</code> and Put rowkey were
    // identical. Then this whole Put buffering business goes away.
    // TODO: Could use HeapSize to create an upper bound on the memory size of
    // the puts map and flush some portion of the content while looping. This
    // flush could result in multiple Puts for a single rowkey. That is
    // acceptable because Combiner is run as an optimization and it's not
    // critical that all Puts are grouped perfectly.
{code}

", ,,,,,,,
HBASE-8877,"HBASE-8806 revealed performance problems with batch mutations failing to reacquire the same row locks.  It looks like HBASE-8806 will use a less intrusive change for 0.94 to have batch mutations track their own row locks and not attempt to reacquire them.  Another approach will be to support reentrant row locks directly.  This allows simplifying a great deal of calling code to no longer track and pass around lock ids.

One affect this change will have is changing the RegionObserver coprocessor's methods preBatchMutate and postBatchMutate from taking a {{MiniBatchOperationInProgress<Pair<Mutation, Integer>> miniBatchOp}} to taking a {{MiniBatchOperationInProgress<Mutation> miniBatchOp}}.  I don't believe CPs should be relying on these lock ids, but that's a potential incompatibility.", ,,1,,,,,
HBASE-8883,"From https://builds.apache.org/job/HBase-TRUNK/4218/testReport/org.apache.hadoop.hbase.client/TestAdmin/testCloseRegionWhenServerNameIsEmpty/

{code}
java.lang.ArrayIndexOutOfBoundsException: -1
	at java.util.concurrent.CopyOnWriteArrayList.get(CopyOnWriteArrayList.java:343)
	at java.util.Collections$UnmodifiableList.get(Collections.java:1152)
	at org.apache.hadoop.hbase.HBaseTestingUtility.getRSForFirstRegionInTable(HBaseTestingUtility.java:1649)
	at org.apache.hadoop.hbase.client.TestAdmin.testCloseRegionWhenServerNameIsEmpty(TestAdmin.java:1401)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
{code}", ,,,,,,,
HBASE-8888,"Follow on from hbase-8776.

Need to fix retries and timeouts.  We cut them down so much hbase-it tests fail.

From https://issues.apache.org/jira/browse/HBASE-8776?focusedCommentId=13698762&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13698762 @nkeywal says:

{code}
I would like to change
hbase.client.retries.number -> 30 (instead of 14 or 20 today)
hbase.client.pause -> 500 (instead of 100 or 1000 today).
Context: see HBASE-6295.
As well, would it make sense to remove all the hbase-site.xml and hbase-defaults.xml to rely only on the defaults in the code. This would trigger another set of issues, as sometimes the defaults are duplicated and different. But these are bugs as well. Imho, this duplication is confusing and it leads to unreliable behavior as we don't really know what are the setting actually used.
{code}

Regards removing hbase-site.xml from everywhere to rely on defaults in code, over in hbase-8776 I tried removing them and way too many tests failed.  Looks like it'd be tough removing them.", ,1,1,,,,,
HBASE-8894,"HBASE-8806 revealed performance problems with batch mutations failing to reacquire the same row locks.  It looks like HBASE-8806 will use a less intrusive change for 0.94 to have batch mutations track their own row locks and not attempt to reacquire them.  Another approach will be to support reentrant row locks directly.  This allows simplifying a great deal of calling code to no longer track and pass around lock ids.

One affect this change will have is changing the RegionObserver coprocessor's methods preBatchMutate and postBatchMutate from taking a {{MiniBatchOperationInProgress<Pair<Mutation, Integer>> miniBatchOp}} to taking a {{MiniBatchOperationInProgress<Mutation> miniBatchOp}}.  I don't believe CPs should be relying on these lock ids, but that's a potential incompatibility.", ,,1,,,,,
HBASE-8910,"Here's a case where TestReplicationDisableInactivePeer fails while re-starting the second master:

http://54.241.6.143/job/HBase-0.95-Hadoop-2/574/org.apache.hbase$hbase-server/testReport/junit/org.apache.hadoop.hbase.replication/TestReplicationDisableInactivePeer/testDisableInactivePeer/

The reason is that when we first shutdown the master, it comes back to life thinking it just lost its session:

{noformat}
2013-07-07 04:27:03,989 FATAL [pool-1-thread-1-EventThread] master.HMaster(2062): Master server abort: loaded coprocessors are: [org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint]
2013-07-07 04:27:03,989 INFO  [pool-1-thread-1-EventThread] master.HMaster(2155): Primary Master trying to recover from ZooKeeper session expiry.
{noformat}

And after that it tries to assign .META. fails since the RS are down.

One way I think we can prevent this is by skipping recovering the session if we are stopping.", ,,,,,,,
HBASE-8912,"AM throws this exception which subsequently causes the master to abort: 
{code}
java.lang.IllegalStateException: Unexpected state : testRetrying,jjj,1372891751115.9b828792311001062a5ff4b1038fe33b. state=PENDING_OPEN, ts=1372891751912, server=hemera.apache.org,39064,1372891746132 .. Cannot transit it to OFFLINE.
	at org.apache.hadoop.hbase.master.AssignmentManager.setOfflineInZooKeeper(AssignmentManager.java:1879)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1688)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1424)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1399)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1394)
	at org.apache.hadoop.hbase.master.handler.ClosedRegionHandler.process(ClosedRegionHandler.java:105)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:175)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
{code}

This exception trace is from the failing test TestMetaReaderEditor which is failing pretty frequently, but looking at the test code, I think this is not a test-only issue, but affects the main code path. 

https://builds.apache.org/job/HBase-0.94/1036/testReport/junit/org.apache.hadoop.hbase.catalog/TestMetaReaderEditor/testRetrying/
", ,,,,,,,
HBASE-8920,"Failed here:

https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/607/testReport/org.apache.hadoop.hbase.master/TestZKBasedOpenCloseRegion/testReOpenRegion/

and


https://builds.apache.org/job/HBase-TRUNK/4232/testReport/org.apache.hadoop.hbase.master/TestZKBasedOpenCloseRegion/testReOpenRegion/

In both cases, we fail for the same reason.  The previous test closes a region.  The failing test starts but we are reopening the region we'd just closed as we are wont to do.  This is the region that gets chosen by the failing test to practise reopen on only  it is not yet fully open so in comes the close and it gets stomped on by the ongoing open.

Let me post some log.", ,,,,,,,
HBASE-8924,On a real cluster the master won't come up if the sleep time between killing and starting is too short., ,1,,,,,,
HBASE-8930,"1- Fill row with some columns
2- Get row with some columns less than universe - Use filter to print kvs
3- Filter prints not requested columns

Filter (AllwaysNextColFilter) always return ReturnCode.INCLUDE_AND_NEXT_COL and prints KV's qualifier

SUFFIX_0 = 0
SUFFIX_1 = 1
SUFFIX_4 = 4
SUFFIX_6 = 6

P= Persisted
R= Requested
E= Evaluated
X= Returned
| 5580 | 5581 | 5584 | 5586 | 5590 | 5591 | 5594 | 5596 | 5600 | 5601 | 5604 | 5606 |... 
|      |  P   |   P  |      |      |  P   |   P  |      |      |  P   |   P  |      |...
|      |  R   |   R  |   R  |      |  R   |   R  |   R  |      |      |      |      |...
|      |  E   |   E  |      |      |  E   |   E  |      |      |  {color:red}E{color}   |      |      |...
|      |  X   |   X  |      |      |  X   |   X  |      |      |      |      |      |


{code:title=ExtraColumnTest.java|borderStyle=solid}

    @Test
    public void testFilter() throws Exception {

        Configuration config = HBaseConfiguration.create();
        config.set(""hbase.zookeeper.quorum"", ""myZK"");

        HTable hTable = new HTable(config, ""testTable"");
        byte[] cf = Bytes.toBytes(""cf"");
        byte[] row = Bytes.toBytes(""row"");


        byte[] col1 = new QualifierConverter().objectToByteArray(new Qualifier((short) 558, (byte) SUFFIX_1));
        byte[] col2 = new QualifierConverter().objectToByteArray(new Qualifier((short) 559, (byte) SUFFIX_1));
        byte[] col3 = new QualifierConverter().objectToByteArray(new Qualifier((short) 560, (byte) SUFFIX_1));
        byte[] col4 = new QualifierConverter().objectToByteArray(new Qualifier((short) 561, (byte) SUFFIX_1));
        byte[] col5 = new QualifierConverter().objectToByteArray(new Qualifier((short) 562, (byte) SUFFIX_1));
        byte[] col6 = new QualifierConverter().objectToByteArray(new Qualifier((short) 563, (byte) SUFFIX_1));


        byte[] col1g = new QualifierConverter().objectToByteArray(new Qualifier((short) 558, (byte) SUFFIX_6));
        byte[] col2g = new QualifierConverter().objectToByteArray(new Qualifier((short) 559, (byte) SUFFIX_6));

        byte[] col1v = new QualifierConverter().objectToByteArray(new Qualifier((short) 558, (byte) SUFFIX_4));
        byte[] col2v = new QualifierConverter().objectToByteArray(new Qualifier((short) 559, (byte) SUFFIX_4));
        byte[] col3v = new QualifierConverter().objectToByteArray(new Qualifier((short) 560, (byte) SUFFIX_4));
        byte[] col4v = new QualifierConverter().objectToByteArray(new Qualifier((short) 561, (byte) SUFFIX_4));
        byte[] col5v = new QualifierConverter().objectToByteArray(new Qualifier((short) 562, (byte) SUFFIX_4));
        byte[] col6v = new QualifierConverter().objectToByteArray(new Qualifier((short) 563, (byte) SUFFIX_4));

        // =========== INSERTION =============//
        Put put = new Put(row);
        put.add(cf, col1, Bytes.toBytes((short) 1));
        put.add(cf, col2, Bytes.toBytes((short) 1));
        put.add(cf, col3, Bytes.toBytes((short) 3));
        put.add(cf, col4, Bytes.toBytes((short) 3));
        put.add(cf, col5, Bytes.toBytes((short) 3));
        put.add(cf, col6, Bytes.toBytes((short) 3));
        hTable.put(put);

        put = new Put(row);
        put.add(cf, col1v, Bytes.toBytes((short) 10));
        put.add(cf, col2v, Bytes.toBytes((short) 10));
        put.add(cf, col3v, Bytes.toBytes((short) 10));
        put.add(cf, col4v, Bytes.toBytes((short) 10));
        put.add(cf, col5v, Bytes.toBytes((short) 10));
        put.add(cf, col6v, Bytes.toBytes((short) 10));

        hTable.put(put);
        hTable.flushCommits();

        //==============READING=================//
        Filter allwaysNextColFilter = new AllwaysNextColFilter();

        Get get = new Get(row);
        get.addColumn(cf, col1); //5581
        get.addColumn(cf, col1v); //5584
        get.addColumn(cf, col1g); //5586
        get.addColumn(cf, col2); //5591
        get.addColumn(cf, col2v); //5594        
        get.addColumn(cf, col2g); //5596
        
        get.setFilter(allwaysNextColFilter);
        get.setMaxVersions(1);
        System.out.println(get);

        Scan scan = new Scan(get);

        ResultScanner scanner = hTable.getScanner(scan);
        Iterator<Result> iterator = scanner.iterator();

        System.out.println(""SCAN"");
        while (iterator.hasNext()) {
            Result next = iterator.next();
            for (KeyValue kv : next.list()) {
                System.out.println(new QualifierConverter().byteArrayToObject(kv.getQualifier()));
            }
        }
    }

}

{code}

Requested 5581 5584 5586 5591 5594 5596
NOT REQUESTED: 5561

Sysout Filter
{noformat}

\x00\x00\x1A\xBE\x00\x05^:\x00\x00\xA0X\x00\x00=\x1A/H0:\x02.\x01/1373577819267/Put/vlen=2/ts=2
Qualifier{date=558, type=SUFFIX_1}
\x00\x00\x1A\xBE\x00\x05^:\x00\x00\xA0X\x00\x00=\x1A/H0:\x02.\x02/1373577819272/Put/vlen=2/ts=3
Qualifier{date=558, type=SUFFIX_4}
\x00\x00\x1A\xBE\x00\x05^:\x00\x00\xA0X\x00\x00=\x1A/H0:\x02/\x01/1373577819267/Put/vlen=2/ts=2
ualifier{date=559, type=SUFFIX_1}
\x00\x00\x1A\xBE\x00\x05^:\x00\x00\xA0X\x00\x00=\x1A/H0:\x02/\x02/1373577819272/Put/vlen=2/ts=3
Qualifier{date=559, type=SUFFIX_4}

\x00\x00\x1A\xBE\x00\x05^:\x00\x00\xA0X\x00\x00=\x1A/H0:\x020\x01/1373577819267/Put/vlen=2/ts=2
Qualifier{date=560, type=SUFFIX_1} (DATE 5601 NOT REQUESTED BUT EVALUATED)
{noformat}

Sysout ExtraColumnTest
{noformat}
{""timeRange"":[0,9223372036854775807],""totalColumns"":6,""cacheBlocks"":true,""families"":{""H0"":[""\\x02.\\x01"",""\\x02.\\x02"",""\\x02.\\x06"",""\\x02/\\x01""]},""maxVersions"":1,""filter"":""AllwaysNextColFilter"",""row"":""\\x00\\x00\\x1A\\xBE\\x00\\x05^:\\x00\\x00\\xA0X\\x00\\x00=\\x1A""}
SCAN
Qualifier{date=558, type=SUFFIX_1}
Qualifier{date=558, type=SUFFIX_4}
Qualifier{date=559, type=SUFFIX_1}
Qualifier{date=559, type=SUFFIX_4}
{noformat}", ,,1,,,,,
HBASE-8933,"Here:

http://54.241.6.143/job/HBase-0.95/org.apache.hbase$hbase-server/596/testReport/org.apache.hadoop.hbase.regionserver/TestSplitTransactionOnCluster/testSplitRegionWithNoStoreFiles/

and here:

http://54.241.6.143/job/HBase-0.95-Hadoop-2/org.apache.hbase$hbase-server/597/testReport/org.apache.hadoop.hbase.regionserver/TestSplitTransactionOnCluster/testRSSplitEphemeralsDisappearButDaughtersAreOnlinedAfterShutdownHandling/

and here:

http://54.241.6.143/job/HBase-0.95/org.apache.hbase$hbase-server/598/testReport/org.apache.hadoop.hbase.regionserver/TestSplitTransactionOnCluster/testRSSplitEphemeralsDisappearButDaughtersAreOnlinedAfterShutdownHandling/

and here:

http://54.241.6.143/job/HBase-TRUNK-Hadoop-2/org.apache.hbase$hbase-server/395/testReport/org.apache.hadoop.hbase.regionserver/TestSplitTransactionOnCluster/testRSSplitEphemeralsDisappearButDaughtersAreOnlinedAfterShutdownHandling/

... etc.

Looking, the move operation just evaporates.  The region we want to move has not finished opening yet so when move comes into the master it does not proceed (because region is currently in RIT).", ,,,,,,,
HBASE-8935,"We observe occasional failures (4-5k undefined and unreferenced nodes in the list, running) when the cluster is stressed while running IntegrationTestBigLinkedList on 0.94. Unfortunately debug logging is disabled.
If Verify is rerun after the fact, the data is there, so it's not data loss. All the missing keys come from very narrow range from the very beginning of  one region; most of this region is not affected.
In the case I'm looking at, region range is
{code}\xD0\x0C`=%\xA2\xD3\xA8\x0A\xF8\x917\x05\x94\x1D> .. \xD4\x0Bx\xAF\x88\x0C\xF4""7\x9A\x9F{\xCE\x0E\x8A{code}
and problematic renage
{code}\xD0\x0C`QLD\xED2\xD5c\x8D\xDB5\x01\xD2H] ... \xD0\x0D\x89)\x11\x0E8\xC5\x08o\xD7\xE3$\xB3\xAAu{code}
There are no splits and compactions on the region during MR job; there are no compactions after that could have affected the data, although there is one flush that happened long after, and region was moved and reopened (before I ran verify manually that showed that data is in HBase).
One thing that happened was that the scanners used by all the map tasks had lease expirations during the MR job that had missing data, some of them twice.
First scanner expiration for each task I looked at was exactly 1 minute after ""Processing split"" log message, when the scanner is opened.
Only the tasks where the scanners have expired twice (2 of them) logged any errors; presumably one expiration in each task happened after the reading was finished, because everything was slow and scanner wasn't closed in time - no errors or exceptions are logged in the tasks where scanner only expired once.
The job I ran afterwards had no scanner expirations so it does close them correctly in normal case...
The task that was reading the problematic range had one scanner expiration and didn't log any errors.
It's a little bit special (or it may be a total red herring) in that in other tasks, scanner expired while the task was splitting partial outputs (which may mean end of input reading?), whereas in the task that lost data spilling started long (~40s) after the scanner expired.
However there's one another such task (spilling started 25s after scanner expired) and it didn't log any errors and didn't lose data.
At this point I am not sure about the root cause but I suspect it might be related to scanner expiration handling, given the patterns.
One thing for sure is that there isn't enough logging... so I will start with adding that.

 ", ,,,,,,,
HBASE-8939,"We have hanging tests.  Here's a few from this morning's review:

{code}
durruti:0.95 stack$ ./dev-support/findHangingTest.sh  https://builds.apache.org/job/hbase-0.95-on-hadoop2/176/consoleText
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 3300k    0 3300k    0     0   508k      0 --:--:--  0:00:06 --:--:--  621k

Hanging test: Running org.apache.hadoop.hbase.TestIOFencing
Hanging test: Running org.apache.hadoop.hbase.regionserver.wal.TestLogRolling
{code}

And...
{code}
durruti:0.95 stack$ ./dev-support/findHangingTest.sh http://54.241.6.143/job/HBase-TRUNK-Hadoop-2/396/consoleText
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  779k    0  779k    0     0   538k      0 --:--:--  0:00:01 --:--:--  559k
Hanging test: Running org.apache.hadoop.hbase.TestIOFencing
Hanging test: Running org.apache.hadoop.hbase.coprocessor.TestRegionServerCoprocessorExceptionWithAbort
Hanging test: Running org.apache.hadoop.hbase.client.TestFromClientSide3
{code}


and....


{code}
durruti:0.95 stack$ ./dev-support/findHangingTest.sh  http://54.241.6.143/job/HBase-0.95/607/consoleText
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  445k    0  445k    0     0   490k      0 --:--:-- --:--:-- --:--:--  522k
Hanging test: Running org.apache.hadoop.hbase.replication.TestReplicationDisableInactivePeer
Hanging test: Running org.apache.hadoop.hbase.master.TestAssignmentManager
Hanging test: Running org.apache.hadoop.hbase.util.TestHBaseFsck
Hanging test: Running org.apache.hadoop.hbase.regionserver.TestStoreFileBlockCacheSummary
Hanging test: Running org.apache.hadoop.hbase.IntegrationTestDataIngestSlowDeterministic

{code}


and...


{code}

durruti:0.95 stack$ ./dev-support/findHangingTest.sh  http://54.241.6.143/job/HBase-0.95-Hadoop-2/607/consoleText
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  781k    0  781k    0     0   240k      0 --:--:--  0:00:03 --:--:--  244k
Hanging test: Running org.apache.hadoop.hbase.coprocessor.TestCoprocessorEndpoint
Hanging test: Running org.apache.hadoop.hbase.client.TestFromClientSide
Hanging test: Running org.apache.hadoop.hbase.TestIOFencing
Hanging test: Running org.apache.hadoop.hbase.master.TestMasterFailoverBalancerPersistence
Hanging test: Running org.apache.hadoop.hbase.master.TestDistributedLogSplitting

{code}", ,,,,,,,
HBASE-8940,"From http://54.241.6.143/job/HBase-TRUNK-Hadoop-2/org.apache.hbase$hbase-server/395/testReport/org.apache.hadoop.hbase.regionserver/TestRegionMergeTransactionOnCluster/testWholesomeMerge/ :
{code}
013-07-11 09:33:44,154 INFO  [AM.ZK.Worker-pool-2-thread-2] master.RegionStates(309): Offlined 3ffefd878a234031675de6b2c70b2ead from ip-10-174-118-204.us-west-1.compute.internal,60498,1373535184820
2013-07-11 09:33:44,154 INFO  [AM.ZK.Worker-pool-2-thread-2] master.AssignmentManager$4(1223): The master has opened testWholesomeMerge,testRow0020,1373535210125.3ffefd878a234031675de6b2c70b2ead. that was online on ip-10-174-118-204.us-west-1.compute.internal,59210,1373535184884
2013-07-11 09:33:44,182 DEBUG [RS_OPEN_REGION-ip-10-174-118-204:59210-1] zookeeper.ZKAssign(862): regionserver:59210-0x13fcd13a20c0002 Successfully transitioned node 3ffefd878a234031675de6b2c70b2ead from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2013-07-11 09:33:44,182 INFO  [MASTER_TABLE_OPERATIONS-ip-10-174-118-204:39405-0] handler.DispatchMergingRegionHandler(154): Failed send MERGE REGIONS RPC to server ip-10-174-118-204.us-west-1.compute.internal,59210,1373535184884 for region testWholesomeMerge,,1373535210124.efcb10dcfa250e31bfd50dc6c7049f32.,testWholesomeMerge,testRow0020,1373535210125.3ffefd878a234031675de6b2c70b2ead., focible=false, org.apache.hadoop.hbase.exceptions.RegionOpeningException: Region is being opened: 3ffefd878a234031675de6b2c70b2ead
	at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionByEncodedName(HRegionServer.java:2566)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:3862)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.mergeRegions(HRegionServer.java:3649)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$2.callBlockingMethod(AdminProtos.java:14400)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2124)
	at org.apache.hadoop.hbase.ipc.RpcServer$Handler.run(RpcServer.java:1831)

2013-07-11 09:33:44,182 DEBUG [RS_OPEN_REGION-ip-10-174-118-204:59210-1] handler.OpenRegionHandler(373): region transitioned to opened in zookeeper: {ENCODED => 3ffefd878a234031675de6b2c70b2ead, NAME => 'testWholesomeMerge,testRow0020,1373535210125.3ffefd878a234031675de6b2c70b2ead.', STARTKEY => 'testRow0020', ENDKEY => 'testRow0040'}, server: ip-10-174-118-204.us-west-1.compute.internal,59210,1373535184884
2013-07-11 09:33:44,183 DEBUG [RS_OPEN_REGION-ip-10-174-118-204:59210-1] handler.OpenRegionHandler(186): Opened testWholesomeMerge,testRow0020,1373535210125.3ffefd878a234031675de6b2c70b2ead. on server:ip-10-174-118-204.us-west-1.compute.internal,59210,1373535184884
{code}
We can see that MASTER_TABLE_OPERATIONS thread couldn't get region 3ffefd878a234031675de6b2c70b2ead because RS_OPEN_REGION thread finished region opening 1 millisecond later.

One solution is to retry operation when receiving RegionOpeningException", ,,,,,,,
HBASE-8949,"While initializing mapreduce job we are not configuring hbase.mapreduce.hfileoutputformat.blocksize, so hfiles are always creating with 64kb (default)block size even though tables has different block size.
We need to configure it with block size from table descriptor.", ,1,,,,,,
HBASE-8952,"If the connection to the client is closed unexpectedly and ""at the wrong time"", the code will attempt to keep reading from the socket in a busy loop.

This bug seems to be present in all released versions of HBase, including the tip of the 0.94 and 0.95 branches, however I only ran into it while porting AsyncHBase to 0.95", ,,,,,,,
HBASE-8960,"If the connection to the client is closed unexpectedly and ""at the wrong time"", the code will attempt to keep reading from the socket in a busy loop.

This bug seems to be present in all released versions of HBase, including the tip of the 0.94 and 0.95 branches, however I only ran into it while porting AsyncHBase to 0.95", ,,,,,,,
HBASE-8983,Cf; mailing list http://search-hadoop.com/m/wurpu1s8Fhs/liochon&subj=Re+Connection+reference+counting, ,,,,,,,
HBASE-9001,"https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/624/testReport/junit/org.apache.hadoop.hbase.thrift/TestThriftServerCmdLine/testRunThriftServer_0_/

It seems stuck here:

{code}
2013-07-19 03:52:03,158 INFO  [Thread-131] thrift.TestThriftServerCmdLine(132): Starting HBase Thrift server with command line: -hsha -port 56708 start
2013-07-19 03:52:03,174 INFO  [ThriftServer-cmdline] thrift.ThriftServerRunner$ImplType(208): Using thrift server type hsha
2013-07-19 03:52:03,205 WARN  [ThriftServer-cmdline] conf.Configuration(817): fs.default.name is deprecated. Instead, use fs.defaultFS
2013-07-19 03:52:03,206 WARN  [ThriftServer-cmdline] conf.Configuration(817): mapreduce.job.counters.limit is deprecated. Instead, use mapreduce.job.counters.max
2013-07-19 03:52:03,207 WARN  [ThriftServer-cmdline] conf.Configuration(817): io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum
2013-07-19 03:54:03,156 INFO  [pool-1-thread-1] hbase.ResourceChecker(171): after: thrift.TestThriftServerCmdLine#testRunThriftServer[0] Thread=146 (was 155), OpenFileDescriptor=295 (was 311), MaxFileDescriptor=4096 (was 4096), SystemLoadAverage=293 (was 240) - SystemLoadAverage LEAK? -, ProcessCount=145 (was 143) - ProcessCount LEAK? -, AvailableMemoryMB=779 (was 1263), ConnectionCount=4 (was 4)
2013-07-19 03:54:03,157 DEBUG [pool-1-thread-1] thrift.TestThriftServerCmdLine(107): implType=-hsha, specifyFramed=false, specifyBindIP=false, specifyCompact=true
{code}

My guess is that we didn't get scheduled because load was almost 300 on this box at the time?

Let me up the timeout of two minutes.", ,1,,,,,,
HBASE-9003,"This is the problem: {{TableMapReduceUtil#addDependencyJars}} relies on {{org.apache.hadoop.util.JarFinder}} if available to call {{getJar()}}. However {{getJar()}} uses File.createTempFile() to create a temporary file under {{hadoop.tmp.dir}}{{/target/test-dir}}. Due HADOOP-9737 the created jar and its content is not purged after the JVM is destroyed. Since most configurations point {{hadoop.tmp.dir}} under {{/tmp}} the generated jar files get purged by {{tmpwatch}} or a similar tool, but boxes that have {{hadoop.tmp.dir}} pointing to a different location not monitored by {{tmpwatch}} will pile up a collection of jars causing all kind of issues. Since {{JarFinder#getJar}} is not a public API from Hadoop (see [~tucu00] comment on HADOOP-9737) we shouldn't use that as part of {{TableMapReduceUtil}} in order to avoid this kind of issues.", ,1,,,,,,
HBASE-9006,"The protobuf definition provides a default value:

{code}
// This is sent on connection setup after the connection preamble is sent.
message ConnectionHeader {
  [...]
  optional string cellBlockCodecClass = 3 [default = ""org.apache.hadoop.hbase.codec.KeyValueCodec""];
  // Compressor we will use if cell block is compressed.  Server will throw exception if not supported.
  // Class must implement hadoop's CompressionCodec Interface
  [...]
}
{code}

Yet if one doesn't explicitly set a value, the code was rejecting the connection.", ,1,,,,,,
HBASE-9011,"The protobuf definition provides a default value:

{code}
// This is sent on connection setup after the connection preamble is sent.
message ConnectionHeader {
  [...]
  optional string cellBlockCodecClass = 3 [default = ""org.apache.hadoop.hbase.codec.KeyValueCodec""];
  // Compressor we will use if cell block is compressed.  Server will throw exception if not supported.
  // Class must implement hadoop's CompressionCodec Interface
  [...]
}
{code}

Yet if one doesn't explicitly set a value, the code was rejecting the connection.", ,1,,,,,,
HBASE-9019,https://builds.apache.org/job/PreCommit-HBASE-Build/6428//testReport/org.apache.hadoop.hbase.regionserver.wal/TestHLogSplit/testIOEOnOutputThread/, ,,,,,,,
HBASE-9022,https://builds.apache.org/job/PreCommit-HBASE-Build/6428//testReport/org.apache.hadoop.hbase.regionserver.wal/TestHLogSplit/testIOEOnOutputThread/, ,,,,,,,
HBASE-9026,"The protobuf definition provides a default value:

{code}
// This is sent on connection setup after the connection preamble is sent.
message ConnectionHeader {
  [...]
  optional string cellBlockCodecClass = 3 [default = ""org.apache.hadoop.hbase.codec.KeyValueCodec""];
  // Compressor we will use if cell block is compressed.  Server will throw exception if not supported.
  // Class must implement hadoop's CompressionCodec Interface
  [...]
}
{code}

Yet if one doesn't explicitly set a value, the code was rejecting the connection.", ,1,,,,,,
HBASE-9031,"The attached patch addresses few issues.

# We need only (3*this.length) capacity in ByteBuffer and not (3*this.bytes.length).
# Do not calculate (offset + length) at every iteration.
# No test is required at every iteration to add space (' ') before every byte other than the first one. Uses {{sb.substring(1)}} instead.
# Finally and most importantly (the original issue of this report), downcast the promoted int (the parameter to {{Integer.toHexString()}}) to byte range.

Without #4, the byte array \{54,125,64, -1, -45\} is transformed to ""36 7d 40 ffffffff ffffffd3"" instead of ""36 7d 40 ff d3"".", ,,,,,,,
HBASE-9032,"This applies only to 0.94 (and earlier) branch.

If the Result object was constructed using either of Result(KeyValue[]) or Result(List<KeyValue>), calling Result.getBytes() returns null instead of the serialized ImmutableBytesWritable object.", ,,,,,,,
HBASE-9038,"If you enable replication, and get a compaction requested, you'll see this in the logs:

{noformat}
2013-07-24 15:16:38,831 ERROR [regionserver60020-smallCompactions-1374704194254] regionserver.CompactSplitThread: Compaction failed Request = regionName=TestTable,00000000000000000000057204,1374704192994.6bb7c58d1e6cc99fbfe04592e44fbc35., storeName=info, fileCount=4, fileSize=335.1m (115.4m, 115.4m, 69.0m, 35.2m), priority=6, time=1374704194257521000
java.lang.NullPointerException
	at org.apache.hadoop.hbase.replication.regionserver.Replication.visitLogEntryBeforeWrite(Replication.java:215)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog.doWrite(FSHLog.java:1204)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog.append(FSHLog.java:890)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog.append(FSHLog.java:840)
	at org.apache.hadoop.hbase.regionserver.wal.HLogUtil.writeCompactionMarker(HLogUtil.java:262)
	at org.apache.hadoop.hbase.regionserver.HStore.writeCompactionWalRecord(HStore.java:1026)
	at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:973)
	at org.apache.hadoop.hbase.regionserver.HRegion.compact(HRegion.java:1278)
	at org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner.run(CompactSplitThread.java:465)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:680)

{noformat}

It's a simple case of filtering METAFAMILY like this:

bq. if (kv.matchingFamily(WALEdit.METAFAMILY)) continue;

and add a unit test.", ,,,,,,,
HBASE-9041,"Assigning Matteo to take a look (give back to me if you don't have time boss).

Failed here: https://builds.apache.org/job/HBase-TRUNK/4293/testReport/org.apache.hadoop.hbase.snapshot/TestFlushSnapshotFromClient/testConcurrentSnapshottingAttempts/

Yesterday, it failed in a different place and for different reason: https://builds.apache.org/view/H-L/view/HBase/job/hbase-0.95/352/testReport/junit/org.apache.hadoop.hbase.snapshot/TestFlushSnapshotFromClient/testFlushTableSnapshot/

The latter test fail was noted on tail of HBASE-8984.  There I speculate that its the 'load' of 400.  I don't think the load reporting is correct.  Will dig in on that.", ,,,,,,,
HBASE-9044,"{noformat}
hbase(main):003:0> merge_region '92ade79eb3ac390b09e2b19c74b56dad', '92ade79eb3ac390b09e2b19c74b56dad'
2013-07-25 13:21:23,991 ERROR [main] client.HBaseAdmin: Unexpected exception: com.google.protobuf.ServiceException: org.apache.hadoop.hbase.ipc.RemoteWithExtrasException(java.lang.NullPointerException): java.lang.NullPointerException
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2108)
	at org.apache.hadoop.hbase.ipc.RpcServer$CallRunner.run(RpcServer.java:1809)
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.consumerLoop(SimpleRpcScheduler.java:165)
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.access$000(SimpleRpcScheduler.java:41)
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler$1.run(SimpleRpcScheduler.java:113)
	at java.lang.Thread.run(Thread.java:662)
 from calling HMaster.dispatchMergingRegions
0 row(s) in 0.0160 seconds
{noformat}

I also saw the following in the master log:

{noformat}
2013-07-25 13:25:12,683 ERROR [MASTER_TABLE_OPERATIONS-a1220:36000-0] executor.EventHandler: Caught throwable while processing event C_M_MERGE_REGION
java.lang.NullPointerException
        at org.apache.hadoop.hbase.master.handler.DispatchMergingRegionHandler.process(DispatchMergingRegionHandler.java:104)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:130)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}", ,,,,,,,
HBASE-9048,"HCM uses SoftValueSortedMap to cache region locations.  Under load, some soft referred values may be GCed.  So we should check if the value is null.  Otherwise, NPE will be thrown:

{code}
        for (Map<byte[], HRegionLocation> tableLocations :
            cachedRegionLocations.values()) {
          for (Entry<byte[], HRegionLocation> e : tableLocations.entrySet()) {
       ===> if (serverName.equals(e.getValue().getServerName())) {
              tableLocations.remove(e.getKey());
              deletedSomething = true;
            }
          }
        }
{code}", ,,,,,,,
HBASE-9049,"Currently, sever callables are instantiated via direct calls. Instead, we can use a single factory and that allows more specialized callable implementations, for instance, using a circuit-breaker pattern (or the Hystrix implementation!) to minimize attempts to contact the server.", ,,1,,,,,
HBASE-9050,"In HBaseClient#call, we have

{code}
    connection.sendParam(call);                 // send the parameter
    boolean interrupted = false;
    //noinspection SynchronizationOnLocalVariableOrMethodParameter
    synchronized (call) {
      while (!call.done) {
        try {
          call.wait();                           // wait for the result
{code}
sendParam could do nothing if the connection is closed right after the call is added into the queue.  Since the connection is closed, we won't get any response, therefore, we won't get any notify call.  So we will keep waiting here for something won't happen.", ,,,,,,,
HBASE-9052,"If a region is split/merged, before it's removed from meta, you can still assign it from the HBase shell. It's better to prevent this from happening.", ,,,,,,,
HBASE-9055,"Currently HBaseAdmin#isTableEnabled() returns true for a table which doesn't exist.

We should check table existence.", ,,,,,,,
HBASE-9058,"Currently HBaseAdmin#isTableEnabled() returns true for a table which doesn't exist.

We should check table existence.", ,,,,,,,
HBASE-9060,"Here is the stack trace:

hbase org.apache.hadoop.hbase.snapshot.ExportSnapshot -snapshot table1_snapshot -copy-to hdfs:///myhbase%2Cbackup/table1_snapshot
 
{code}
13/07/26 18:09:50 INFO mapred.JobClient:  map 0% reduce 0%
13/07/26 18:09:58 INFO mapred.JobClient: Task Id : attempt_201307261804_0002_m_000001_0, Status : FAILED
java.util.MissingFormatArgumentException: Format specifier ') from family1/table1=3567d8ac6cfee83dfe81c346f139fb9c-c5bc120475a54d188f30d4b621d505b1 to hdfs:/myhbase%2C'
        at java.util.Formatter.getArgument(Formatter.java:592)
        at java.util.Formatter.format(Formatter.java:561)
        at java.util.Formatter.format(Formatter.java:510)
        at java.lang.String.format(String.java:1977)
        at org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportMapper.copyData(ExportSnapshot.java:274)
        at org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportMapper.copyFile(ExportSnapshot.java:204)
        at org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportMapper.map(ExportSnapshot.java:149)
        at org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportMapper.map(ExportSnapshot.java:98)
{code}

The problem is this code in copyData():
{code}
final String statusMessage = ""copied %s/"" + StringUtils.humanReadableInt(inputFileSize) +
                                   "" (%.3f%%) from "" + inputPath + "" to "" + outputPath;
{code}

Since we don't know what the path may contain that may confuse the formatter, we need to pull that part out of the format string.

Also the percentage completion math seems to be wrong in the same code.", ,1,,,,,,
HBASE-9072,"http://54.241.6.143/job/HBase-TRUNK-Hadoop-2/org.apache.hbase$hbase-server/428/testReport/junit/org.apache.hadoop.hbase.master.cleaner/TestSnapshotFromMaster/testSnapshotHFileArchiving/

{code}
Failed 4600 actions: SocketTimeoutException: 4600 times, 
Stacktrace
	at org.apache.hadoop.hbase.client.HTable.doPut(HTable.java:794)
	at org.apache.hadoop.hbase.client.HTable.put(HTable.java:756)
	at org.apache.hadoop.hbase.HBaseTestingUtility.loadTable(HBaseTestingUtility.java:1312)
	at org.apache.hadoop.hbase.master.cleaner.TestSnapshotFromMaster.testSnapshotHFileArchiving(TestSnapshotFromMaster.java:297)
org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 4600 actions: SocketTimeoutException: 4600 times, 
	at org.apache.hadoop.hbase.client.AsyncProcess$BatchErrors.makeException(AsyncProcess.java:157)
	at org.apache.hadoop.hbase.client.AsyncProcess$BatchErrors.access$500(AsyncProcess.java:145)
	at org.apache.hadoop.hbase.client.AsyncProcess.getErrors(AsyncProcess.java:700)
	at org.apache.hadoop.hbase.client.HTable.backgroundFlushCommits(HTable.java:827)
	at org.apache.hadoop.hbase.client.HTable.doPut(HTable.java:794)
	at org.apache.hadoop.hbase.client.HTable.put(HTable.java:756)
	at org.apache.hadoop.hbase.HBaseTestingUtility.loadTable(HBaseTestingUtility.java:1312)
	at org.apache.hadoop.hbase.master.cleaner.TestSnapshotFromMaster.testSnapshotHFileArchiving(TestSnapshotFromMaster.java:297)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
{code}

[~mbertozzi] Opinion?", ,,,,,,,
HBASE-9079,"I hit a weird issue/bug and am able to reproduce the error consistently. The problem arises when FilterList has two filters where each implements the getNextKeyHint method.

The way the current implementation works is, StoreScanner will call matcher.getNextKeyHint() whenever it gets a SEEK_NEXT_USING_HINT. This in turn will call filter.getNextKeyHint() which at this stage is of type FilterList. The implementation in FilterList iterates through all the filters and keeps the max KeyValue that it sees. All is fine if you wrap filters in FilterList in which only one of them implements getNextKeyHint. but if multiple of them implement then that's where things get weird.

For example:
- create two filters: one is FuzzyRowFilter and second is ColumnRangeFilter. Both of them implement getNextKeyHint
- wrap them in FilterList with MUST_PASS_ALL
- FuzzyRowFilter will seek to the correct first row and then pass it to ColumnRangeFilter which will return the SEEK_NEXT_USING_HINT code.
- Now in FilterList when getNextKeyHint is called, it calls the one on FuzzyRow first which basically says what the next row should be. While in reality we want the ColumnRangeFilter to give the seek hint.
- The above behavior skips data that should be returned, which I have verified by using a RowFilter with RegexStringComparator.

I updated the FilterList to maintain state on which filter returns the SEEK_NEXT_USING_HINT and in getNextKeyHint, I invoke the method on the saved filter and reset that state. I tested it with my current queries and it works fine but I need to run the entire test suite to make sure I have not introduced any regression. In addition to that I need to figure out what should be the behavior when the opeation is MUST_PASS_ONE, but I doubt it should be any different.

Is my understanding of it being a bug correct ? Or am I trivializing it and ignoring something very important ? If it's tough to wrap your head around the explanation, then I can open a JIRA and upload a patch against 0.94 head.", ,,,,,,,
HBASE-9080,"Region servers are picked in round robin fashion when a table is re-enabled.

This results in poor data locality.

We should use retain assignment when re-enabling table(s).

For initial discussion, see HBASE-6143
Since patches for 0.94 and trunk may turn out to be very different, this JIRA is for fixing the issue in 0.94 branch.

Thanks Lars, Elliot, Vladmir and Stack for providing opinions on this subject.", ,,1,,,,,
HBASE-9082,"Let me up the retries so more likely tests will fail.

I went over a bunch of those w/ retries == 1 but most times it is what is wanted... a fail fast.  In snapshot test it is set to 1 retry only but seems like it could do w/ more retrying since a bunch of them load data first and it can take a while for stuff to come on line on loaded server.  See this failure:

http://54.241.6.143/job/HBase-0.95-Hadoop-2/org.apache.hbase$hbase-server/720/testReport/org.apache.hadoop.hbase.client/TestSnapshotFromClient/testSnapshotDeletionWithRegex/

Says....

org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 4600 actions: SocketTimeoutException: 4600 times, 

It is in the loadTable at start of the test.

", ,,,,,,,
HBASE-9084,"I was running the following test over a Distributed Cluster:
bin/hbase org.apache.hadoop.hbase.IntegrationTestsDriver IntegrationTestDataIngestSlowDeterministic

The IntegrationTestingUtility.restoreCluster() is called in the teardown phase of the test.
For a distributed cluster, it ends up calling DistributedHBaseCluster.restoreClusterStatus, which does the task 
of restoring the cluster back to original state.

The restore steps done here, does not solve one specific case:
When the initial HBase Master is currently down, and the current HBase Master is different from the initial one.

You get into this flow:

    //check whether current master has changed
    if (!ServerName.isSameHostnameAndPort(initial.getMaster(), current.getMaster())) {
	.............
    }

In the above code path, the current backup masters are stopped, and the current active master is also stopped.
At this point, for the aforementioned usecase, none of the Hbase Masters would be available, hence the subsequent
attempts to do any operation over the cluster would fail, resulting in Test Failure.", ,,,,,,,
HBASE-9085,"I was running the following test over a Distributed Cluster:
bin/hbase org.apache.hadoop.hbase.IntegrationTestsDriver IntegrationTestDataIngestSlowDeterministic

The IntegrationTestingUtility.restoreCluster() is called in the teardown phase of the test.
For a distributed cluster, it ends up calling DistributedHBaseCluster.restoreClusterStatus, which does the task 
of restoring the cluster back to original state.

The restore steps done here, does not solve one specific case:
When the initial HBase Master is currently down, and the current HBase Master is different from the initial one.

You get into this flow:

    //check whether current master has changed
    if (!ServerName.isSameHostnameAndPort(initial.getMaster(), current.getMaster())) {
	.............
    }

In the above code path, the current backup masters are stopped, and the current active master is also stopped.
At this point, for the aforementioned usecase, none of the Hbase Masters would be available, hence the subsequent
attempts to do any operation over the cluster would fail, resulting in Test Failure.", ,,,,,,,
HBASE-9086,"Region servers are picked in round robin fashion when a table is re-enabled.

This results in poor data locality.

We should use retain assignment when re-enabling table(s).

For initial discussion, see HBASE-6143
Since patches for 0.94 and trunk may turn out to be very different, this JIRA is for fixing the issue in 0.94 branch.

Thanks Lars, Elliot, Vladmir and Stack for providing opinions on this subject.", ,,1,,,,,
HBASE-9087,"I'm having a lot of handlers (90 - 300 aprox) being blocked when reading rows. They are blocked during changedReaderObserver registration.

Lars Hofhansl suggests to change the implementation of changedReaderObserver from CopyOnWriteList to ConcurrentHashMap.

Here is a stack trace: 

""IPC Server handler 99 on 60020"" daemon prio=10 tid=0x0000000041c84000 nid=0x2244 waiting on condition [0x00007ff51fefd000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000c5c13ae8> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:842)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1178)
        at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:186)
        at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:262)
        at java.util.concurrent.CopyOnWriteArrayList.addIfAbsent(CopyOnWriteArrayList.java:553)
        at java.util.concurrent.CopyOnWriteArraySet.add(CopyOnWriteArraySet.java:221)
        at org.apache.hadoop.hbase.regionserver.Store.addChangedReaderObserver(Store.java:1085)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.<init>(StoreScanner.java:138)
        at org.apache.hadoop.hbase.regionserver.Store.getScanner(Store.java:2077)
        at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.<init>(HRegion.java:3755)
        at org.apache.hadoop.hbase.regionserver.HRegion.instantiateRegionScanner(HRegion.java:1804)
        at org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:1796)
        at org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:1771)
        at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:4776)
        at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:4750)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:2152)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.java:3700)
        at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:320)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1426)
      ", ,,1,,,,,
HBASE-9092,"Looked into failed test: http://54.241.6.143/job/HBase-0.95/org.apache.hbase$hbase-server/721/testReport/

In this test run, several tests in TestAssignmentManagerOnCluster failed.  Most of them timed out because the first failure testOpenFailedUnrecoverable used too much resource in deleting the table.

http://54.241.6.143/job/HBase-0.95/org.apache.hbase$hbase-server/721/testReport/org.apache.hadoop.hbase.master/TestAssignmentManagerOnCluster/testOpenFailedUnrecoverable/

The reason testOpenFailedUnrecoverable failed is that the second openRegion call was ignored since the previous open call was still going on and stayed in OpenRegionHandler#doCleanUpOnFailedOpen for too long (perhaps thread scheduling issue).  The second openRegion call was skipped since the region was still in the middle of opening.  However, the failed_open event was already processed by master.  Therefore the region stuck in transition and the delete table went no where.  It is a similar issue as we ran into before while for that time, the region was closing.", ,,,,,,,
HBASE-9093,"The writeToWal is used by downstream projects like Flume to disable writes to WAL to improve performance when durability is not strictly required. But renaming this method to setDurability forces us to use reflection to support hbase versions < 95 - which in turn hits performance, as this method needs to be called on every single write. I recommend adding the old method back as deprecated and removing it once hbase-95/96 becomes the popular version used in prod.", ,,1,,,,,
HBASE-9095,"While debugging a case where a region was getting opened on a RegionServer and then closed soon after (and then never re-opened anywhere thereafter), it seemed like the processing in handleRegion to do with deletion of ZK nodes should be non-asynchronous. This achieves two things:
1. The synchronous deletion prevents more than one processing on the same event data twice. Assuming that we do get more than one notification (on let's say, region OPENED event), the subsequent processing(s) in handleRegion for the same znode would end up with a zookeeper node not found exception. The return value of the data read would be null and that's already handled. If it is asynchronous, it leads to issues like - master opens a region on a certain RegionServer and soon after it sends that RegionServer a close for the same region, and then the znode is deleted.

2. The deletion is currently handled in an executor service. This is problematic since by design the events for a given region should be processed in order. By delegating a part of the processing to executor service we are somewhat violating this contract since there is no guarantee of the ordering in the executor service executions...

Thanks to [~jeffreyz] and [~enis] for the discussions on this issue.", ,,,,,,,
HBASE-9098,"In HLogSplitter:locateRegionAndRefreshLastFlushedSequenceId(HConnection, byte[], byte[], String), we talk to the replayee regionserver to figure out whether a region is in recovery or not. We should look at ZK only for this piece of information (since that is the source of truth for recovery otherwise).", ,1,,,,,,
HBASE-9099,The symptom is the first region assignment submitted in SSH is in progress while when am.waitOnRegionToClearRegionsInTransition times out we will re-submitted another SSH which will invoke another region assignment for the region. It will cause the region get stuck in RIT status., ,,,,,,,
HBASE-9110,"I use Hbase Java API and I try to append values Bytes.toBytes(""one two"") and Bytes.toBytes("" three"") in 3 columns.
Only for 2 out of these 3 columns the result is ""one two three"".

*Output from the hbase shell:*
{noformat} 
hbase(main):008:0* scan ""mytesttable""
ROW                                    COLUMN+CELL                                                                                                    
 mytestRowKey                          column=TestA:dlbytes, timestamp=1375436156140, value=one two three                                             
 mytestRowKey                          column=TestA:tbytes, timestamp=1375436156140, value=one two three                                              
 mytestRowKey                          column=TestA:ulbytes, timestamp=1375436156140, value= three                                                    
1 row(s) in 0.0280 seconds
{noformat}

*My test code:*
{code:title=Database.java|borderStyle=solid}
import static org.junit.Assert.*;

import java.io.IOException;
 
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.HColumnDescriptor;
import org.apache.hadoop.hbase.HTableDescriptor;
import org.apache.hadoop.hbase.client.HBaseAdmin;
import org.apache.hadoop.hbase.client.HTableInterface;
import org.apache.hadoop.hbase.client.HTablePool;
import org.apache.hadoop.hbase.client.Append;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.util.Bytes;
import org.junit.Test;

...

    @Test
    public void testAppend() throws IOException {
        byte [] rowKey = Bytes.toBytes(""mytestRowKey"");
        byte [] column1 = Bytes.toBytes(""ulbytes"");
        byte [] column2 = Bytes.toBytes(""dlbytes"");
        byte [] column3 = Bytes.toBytes(""tbytes"");
        String part11 = ""one two"";
        String part12 = "" three"";
        String cFamily = ""TestA"";
        String TABLE = ""mytesttable"";
        Configuration conf = HBaseConfiguration.create();

        HTablePool pool = new HTablePool(conf, 10);
        HBaseAdmin admin = new HBaseAdmin(conf);
        
        if(admin.tableExists(TABLE)){
            admin.disableTable(TABLE);
            admin.deleteTable(TABLE);
        }
        
        HTableDescriptor tableDescriptor = new HTableDescriptor(TABLE);
        HColumnDescriptor hcd = new HColumnDescriptor(cFamily);
        hcd.setMaxVersions(1);
        tableDescriptor.addFamily(hcd);
        admin.createTable(tableDescriptor);

        HTableInterface table = pool.getTable(TABLE);
        
        Append a = new Append(rowKey);
        a.setReturnResults(false);
        a.add(Bytes.toBytes(cFamily), column1, Bytes.toBytes(part11));
        a.add(Bytes.toBytes(cFamily), column2, Bytes.toBytes(part11));
        a.add(Bytes.toBytes(cFamily), column3, Bytes.toBytes(part11));
        table.append(a);
        a = new Append(rowKey);
        a.add(Bytes.toBytes(cFamily), column1, Bytes.toBytes(part12));
        a.add(Bytes.toBytes(cFamily), column2, Bytes.toBytes(part12));
        a.add(Bytes.toBytes(cFamily), column3, Bytes.toBytes(part12));
        Result result = table.append(a);

        byte [] resultForColumn1 = result.getValue(Bytes.toBytes(cFamily), column1);
        byte [] resultForColumn2 = result.getValue(Bytes.toBytes(cFamily), column2);
        byte [] resultForColumn3 = result.getValue(Bytes.toBytes(cFamily), column3);
        if (resultForColumn1 == null || resultForColumn2 == null || resultForColumn3 == null)
            System.out.println(""The DB table contains these values but they are never given back, strange..."");
        else {
            assertEquals(0, Bytes.compareTo(Bytes.toBytes(part11 + part12),
                    resultForColumn1));
            assertEquals(0, Bytes.compareTo(Bytes.toBytes(part11 + part12),
                    resultForColumn2));
            assertEquals(0, Bytes.compareTo(Bytes.toBytes(part11 + part12),
                    resultForColumn3));
        }
        HTable t = new HTable(conf, TABLE);   
        Get getOperation = new Get(rowKey);
        getOperation.addColumn(Bytes.toBytes(cFamily), column1);
        Result res = t.get(getOperation);           
        assertEquals(0, Bytes.compareTo(Bytes.toBytes(part11 + part12), res.getValue(Bytes.toBytes(cFamily), column1)));
      }
{code} ", ,,,,,,,
HBASE-9115,"I use Hbase Java API and I try to append values Bytes.toBytes(""one two"") and Bytes.toBytes("" three"") in 3 columns.
Only for 2 out of these 3 columns the result is ""one two three"".

*Output from the hbase shell:*
{noformat} 
hbase(main):008:0* scan ""mytesttable""
ROW                                    COLUMN+CELL                                                                                                    
 mytestRowKey                          column=TestA:dlbytes, timestamp=1375436156140, value=one two three                                             
 mytestRowKey                          column=TestA:tbytes, timestamp=1375436156140, value=one two three                                              
 mytestRowKey                          column=TestA:ulbytes, timestamp=1375436156140, value= three                                                    
1 row(s) in 0.0280 seconds
{noformat}

*My test code:*
{code:title=Database.java|borderStyle=solid}
import static org.junit.Assert.*;

import java.io.IOException;
 
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.HColumnDescriptor;
import org.apache.hadoop.hbase.HTableDescriptor;
import org.apache.hadoop.hbase.client.HBaseAdmin;
import org.apache.hadoop.hbase.client.HTableInterface;
import org.apache.hadoop.hbase.client.HTablePool;
import org.apache.hadoop.hbase.client.Append;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.util.Bytes;
import org.junit.Test;

...

    @Test
    public void testAppend() throws IOException {
        byte [] rowKey = Bytes.toBytes(""mytestRowKey"");
        byte [] column1 = Bytes.toBytes(""ulbytes"");
        byte [] column2 = Bytes.toBytes(""dlbytes"");
        byte [] column3 = Bytes.toBytes(""tbytes"");
        String part11 = ""one two"";
        String part12 = "" three"";
        String cFamily = ""TestA"";
        String TABLE = ""mytesttable"";
        Configuration conf = HBaseConfiguration.create();

        HTablePool pool = new HTablePool(conf, 10);
        HBaseAdmin admin = new HBaseAdmin(conf);
        
        if(admin.tableExists(TABLE)){
            admin.disableTable(TABLE);
            admin.deleteTable(TABLE);
        }
        
        HTableDescriptor tableDescriptor = new HTableDescriptor(TABLE);
        HColumnDescriptor hcd = new HColumnDescriptor(cFamily);
        hcd.setMaxVersions(1);
        tableDescriptor.addFamily(hcd);
        admin.createTable(tableDescriptor);

        HTableInterface table = pool.getTable(TABLE);
        
        Append a = new Append(rowKey);
        a.setReturnResults(false);
        a.add(Bytes.toBytes(cFamily), column1, Bytes.toBytes(part11));
        a.add(Bytes.toBytes(cFamily), column2, Bytes.toBytes(part11));
        a.add(Bytes.toBytes(cFamily), column3, Bytes.toBytes(part11));
        table.append(a);
        a = new Append(rowKey);
        a.add(Bytes.toBytes(cFamily), column1, Bytes.toBytes(part12));
        a.add(Bytes.toBytes(cFamily), column2, Bytes.toBytes(part12));
        a.add(Bytes.toBytes(cFamily), column3, Bytes.toBytes(part12));
        Result result = table.append(a);

        byte [] resultForColumn1 = result.getValue(Bytes.toBytes(cFamily), column1);
        byte [] resultForColumn2 = result.getValue(Bytes.toBytes(cFamily), column2);
        byte [] resultForColumn3 = result.getValue(Bytes.toBytes(cFamily), column3);
        if (resultForColumn1 == null || resultForColumn2 == null || resultForColumn3 == null)
            System.out.println(""The DB table contains these values but they are never given back, strange..."");
        else {
            assertEquals(0, Bytes.compareTo(Bytes.toBytes(part11 + part12),
                    resultForColumn1));
            assertEquals(0, Bytes.compareTo(Bytes.toBytes(part11 + part12),
                    resultForColumn2));
            assertEquals(0, Bytes.compareTo(Bytes.toBytes(part11 + part12),
                    resultForColumn3));
        }
        HTable t = new HTable(conf, TABLE);   
        Get getOperation = new Get(rowKey);
        getOperation.addColumn(Bytes.toBytes(cFamily), column1);
        Result res = t.get(getOperation);           
        assertEquals(0, Bytes.compareTo(Bytes.toBytes(part11 + part12), res.getValue(Bytes.toBytes(cFamily), column1)));
      }
{code} ", ,1,,,,,,
HBASE-9129,"Running YCSB workload-e on 0.95 basically hangs the whole cluster.

YCSB opens a scanner for 100 rows.

# YCSB Calls next
# Pre-fetching starts
# YCSB Closes scanner
# Pre-fetching re-adds the scanner.

So the end result is:

{noformat}

""scan-prefetch-2-thread-45"" daemon prio=10 tid=0x00007f7e406ec800 nid=0x40bc runnable [0x00007f75ffefd000]
   java.lang.Thread.State: RUNNABLE
        at org.apache.hadoop.hbase.regionserver.Leases$Lease.equals(Leases.java:272)
        at java.util.PriorityQueue.indexOf(PriorityQueue.java:342)
        at java.util.PriorityQueue.remove(PriorityQueue.java:360)
        at java.util.concurrent.DelayQueue.remove(DelayQueue.java:476)
        at org.apache.hadoop.hbase.regionserver.Leases.removeLease(Leases.java:232)
        - locked <0x00007f774455a660> (a java.util.concurrent.DelayQueue)
        at org.apache.hadoop.hbase.regionserver.RegionScannerHolder$ScanPrefetcher.call(RegionScannerHolder.java:269)
        at org.apache.hadoop.hbase.regionserver.RegionScannerHolder$ScanPrefetcher.call(RegionScannerHolder.java:260)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)

""scan-prefetch-2-thread-44"" daemon prio=10 tid=0x00007f7e4c1ba800 nid=0x40bb waiting on condition [0x00007f7605b79000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00007f774455ad78> (a java.util.concurrent.SynchronousQueue$TransferStack)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:359)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:942)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)

""scan-prefetch-2-thread-43"" daemon prio=10 tid=0x00007f7e38cbc800 nid=0x40ba waiting on condition [0x00007f7609ab8000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00007f774455ad78> (a java.util.concurrent.SynchronousQueue$TransferStack)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:359)
        at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:942)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722){noformat}

", ,,,,,,,
HBASE-9136,"With reference to the mail sent in the dev list,
http://comments.gmane.org/gmane.comp.java.hadoop.hbase.devel/38984
We should have a provision such that the codec on the server side could be different from the one on the client side. This would help to remove the tags for security usecases. 
This JIRA is aimed to provide that capability in the codec itself.",1,,,,,,,
HBASE-9140,"Running YCSB progress got stuck for quite a while and then a Null Pointer Exception was thrown:
{noformat}
 230 sec: 1266253 operations; 0 current ops/sec;  
 240 sec: 1266253 operations; 0 current ops/sec;  
 250 sec: 1266253 operations; 0 current ops/sec;  
 260 sec: 1266253 operations; 0 current ops/sec;  
 270 sec: 1266253 operations; 0 current ops/sec;  
 280 sec: 1266253 operations; 0 current ops/sec;  
 290 sec: 1266253 operations; 0 current ops/sec;  
 300 sec: 1266253 operations; 0 current ops/sec;  
 310 sec: 1266253 operations; 0 current ops/sec;  
 320 sec: 1266253 operations; 0 current ops/sec;  
 330 sec: 1266253 operations; 0 current ops/sec;  
 340 sec: 1266253 operations; 0 current ops/sec;  
 350 sec: 1266253 operations; 0 current ops/sec;  
 360 sec: 1266253 operations; 0 current ops/sec;  
 370 sec: 1266253 operations; 0 current ops/sec;  
 380 sec: 1266253 operations; 0 current ops/sec;  
 390 sec: 1266253 operations; 0 current ops/sec;  
Exception in thread ""Thread-26"" java.lang.NullPointerException
	at org.apache.hadoop.hbase.client.AsyncProcess.findDestLocation(AsyncProcess.java:288)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:233)
	at org.apache.hadoop.hbase.client.HTable.backgroundFlushCommits(HTable.java:811)
	at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:1192)
	at com.yahoo.ycsb.db.HBaseClient.cleanup(HBaseClient.java:106)
	at com.yahoo.ycsb.DBWrapper.cleanup(DBWrapper.java:73)
	at com.yahoo.ycsb.ClientThread.run(Client.java:307)
 400 sec: 1266253 operations; 0 current ops/sec; [UPDATE AverageLatency(us)=1341720] [INSERT AverageLatency(us)=197941421.07] [CLEANUP AverageLatency(us)=1342113] 
Exception in thread ""Thread-16"" java.lang.NullPointerException
	at org.apache.hadoop.hbase.client.AsyncProcess.findDestLocation(AsyncProcess.java:288)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:233)
	at org.apache.hadoop.hbase.client.HTable.backgroundFlushCommits(HTable.java:811)
	at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:1192)
	at com.yahoo.ycsb.db.HBaseClient.cleanup(HBaseClient.java:106)
	at com.yahoo.ycsb.DBWrapper.cleanup(DBWrapper.java:73)
	at com.yahoo.ycsb.ClientThread.run(Client.java:307)
 410 sec: 1266253 operations; 0 current ops/sec;  [INSERT AverageLatency(us)=208462580.75]  
{noformat}", ,,,,,,,
HBASE-9151,"When meta server znode deleted and meta in FAILED_OPEN state, then hbck cannot fix it. This scenario can come when all region servers stopped by stop command and didnt start any RS within 10 secs(with default configurations). 
{code}
  public void assignMeta() throws KeeperException {
    MetaRegionTracker.deleteMetaLocation(this.watcher);
    assign(HRegionInfo.FIRST_META_REGIONINFO, true);
  }
{code}
", ,,,,,,,
HBASE-9157,"In one of integration test, I observed that a thread keeps spinning error logs ""Unexpected exception handling blockUntilAvailable"" due to KeeperException.ConnectionLossException. Below is the related code:

{code}    
    while (!finished) {
      try {
        data = ZKUtil.getData(zkw, znode);
      } catch(KeeperException e) {
        LOG.warn(""Unexpected exception handling blockUntilAvailable"", e);
      }

      if (data == null && (System.currentTimeMillis() +
        HConstants.SOCKET_RETRY_WAIT_MS < endTime)) {
        Thread.sleep(HConstants.SOCKET_RETRY_WAIT_MS);
      } else {
        finished = true;
      }
    }
{code}

ConnectionLossException might be recoverable but SessionExpiredException and AuthFailed are not recoverable errors, the while loop can't break.", ,,,,,,,
HBASE-9158,"While studying the code for HBASE-7709, I found a serious bug in the current cyclic replication code. The problem is here in HRegion.doMiniBatchMutation:
{code}
      Mutation first = batchOp.operations[firstIndex].getFirst();
      txid = this.log.appendNoSync(regionInfo, this.htableDescriptor.getName(),
               walEdit, first.getClusterId(), now, this.htableDescriptor);
{code}
Now note that edits replicated from remote cluster and local edits might interleave in the WAL, we might also receive edit from multiple remote clusters. Hence that <walEdit> might have edits from many clusters in it, but all are just labeled with the clusterId of the first Mutation.

Fixing this in doMiniBatchMutation seems tricky to do efficiently (imagine we get a batch with cluster1, cluster2, cluster1, cluster2, ..., in that case each edit would have to be its own batch). The coprocessor handling would also be difficult.

The other option is create batches of Puts grouped by the cluster id in ReplicationSink.replicateEntries(...), this is not as general, but equally correct. This is the approach I would favor.

Lastly this is very hard to verify in a unittest.
", ,1,,,,,,
HBASE-9163,"If callTimeout is not integer.max and throwable is not SocketTimeoutException, we set the callTimeout to a negative value since endTime is not set yet.  Therefore, the next call will always throw SocketTimeoutException.

{noformat}
    if (this.callTimeout != HConstants.DEFAULT_HBASE_CLIENT_OPERATION_TIMEOUT)
      if (throwable instanceof SocketTimeoutException
          || (this.endTime - this.startTime > this.callTimeout)) {
        throw (SocketTimeoutException) (SocketTimeoutException) new SocketTimeoutException(
            ""Call to access row '"" + Bytes.toString(row) + ""' on table '""
                + Bytes.toString(tableName)
                + ""' failed on socket timeout exception: "" + throwable)
            .initCause(throwable);
      } else {
   ===>     this.callTimeout = ((int) (this.endTime - this.startTime));
      }
{noformat}", ,,,,,,,
HBASE-9167,"If callTimeout is not integer.max and throwable is not SocketTimeoutException, we set the callTimeout to a negative value since endTime is not set yet.  Therefore, the next call will always throw SocketTimeoutException.

{noformat}
    if (this.callTimeout != HConstants.DEFAULT_HBASE_CLIENT_OPERATION_TIMEOUT)
      if (throwable instanceof SocketTimeoutException
          || (this.endTime - this.startTime > this.callTimeout)) {
        throw (SocketTimeoutException) (SocketTimeoutException) new SocketTimeoutException(
            ""Call to access row '"" + Bytes.toString(row) + ""' on table '""
                + Bytes.toString(tableName)
                + ""' failed on socket timeout exception: "" + throwable)
            .initCause(throwable);
      } else {
   ===>     this.callTimeout = ((int) (this.endTime - this.startTime));
      }
{noformat}", ,,,,,,,
HBASE-9187,"While studying the code for HBASE-7709, I found a serious bug in the current cyclic replication code. The problem is here in HRegion.doMiniBatchMutation:
{code}
      Mutation first = batchOp.operations[firstIndex].getFirst();
      txid = this.log.appendNoSync(regionInfo, this.htableDescriptor.getName(),
               walEdit, first.getClusterId(), now, this.htableDescriptor);
{code}
Now note that edits replicated from remote cluster and local edits might interleave in the WAL, we might also receive edit from multiple remote clusters. Hence that <walEdit> might have edits from many clusters in it, but all are just labeled with the clusterId of the first Mutation.

Fixing this in doMiniBatchMutation seems tricky to do efficiently (imagine we get a batch with cluster1, cluster2, cluster1, cluster2, ..., in that case each edit would have to be its own batch). The coprocessor handling would also be difficult.

The other option is create batches of Puts grouped by the cluster id in ReplicationSink.replicateEntries(...), this is not as general, but equally correct. This is the approach I would favor.

Lastly this is very hard to verify in a unittest.
", ,1,,,,,,
HBASE-9198,"Just realized that, similar to HBASE-9050, trunk has the same issue too.", ,,,,,,,
HBASE-9202,"See https://builds.apache.org/job/hbase-0.95/434/testReport/junit/org.apache.hadoop.hbase.thrift/TestThriftServerCmdLine/testRunThriftServer_18_/

Looking in log it looks like it is making progress still.  There is something going on in here but not enough log:

{code}
2013-08-12 18:14:43,163 INFO  [Thread-258] thrift.TestThriftServerCmdLine(132): Starting HBase Thrift server with command line: -threadpool -port 49160 -bind minerva.apache.org start
2013-08-12 18:14:43,164 INFO  [ThriftServer-cmdline] thrift.ThriftServerRunner$ImplType(209): Using thrift server type threadpool
2013-08-12 18:14:58,174 DEBUG [NamespaceJanitor-minerva:44563] client.ClientScanner(218): Finished region={ENCODED => abf65547b0b2d46a6cc9bfbd20f80d68, NAME => 'hbase:namespace,,1376331267651.abf65547b0b2d46a6cc9bfbd20f80d68.', STARTKEY => '', ENDKEY => ''}
2013-08-12 18:15:28,174 DEBUG [NamespaceJanitor-minerva:44563] client.ClientScanner(218): Finished region={ENCODED => abf65547b0b2d46a6cc9bfbd20f80d68, NAME => 'hbase:namespace,,1376331267651.abf65547b0b2d46a6cc9bfbd20f80d68.', STARTKEY => '', ENDKEY => ''}
2013-08-12 18:15:58,175 DEBUG [NamespaceJanitor-minerva:44563] client.ClientScanner(218): Finished region={ENCODED => abf65547b0b2d46a6cc9bfbd20f80d68, NAME => 'hbase:namespace,,1376331267651.abf65547b0b2d46a6cc9bfbd20f80d68.', STARTKEY => '', ENDKEY => ''}
2013-08-12 18:16:28,173 DEBUG [NamespaceJanitor-minerva:44563] client.ClientScanner(218): Finished region={ENCODED => abf65547b0b2d46a6cc9bfbd20f80d68, NAME => 'hbase:namespace,,1376331267651.abf65547b0b2d46a6cc9bfbd20f80d68.', STARTKEY => '', ENDKEY => ''}
2013-08-12 18:16:58,174 DEBUG [NamespaceJanitor-minerva:44563] client.ClientScanner(218): Finished region={ENCODED => abf65547b0b2d46a6cc9bfbd20f80d68, NAME => 'hbase:namespace,,1376331267651.abf65547b0b2d46a6cc9bfbd20f80d68.', STARTKEY => '', ENDKEY => ''}
2013-08-12 18:17:28,174 DEBUG [NamespaceJanitor-minerva:44563] client.ClientScanner(218): Finished region={ENCODED => abf65547b0b2d46a6cc9bfbd20f80d68, NAME => 'hbase:namespace,,1376331267651.abf65547b0b2d46a6cc9bfbd20f80d68.', STARTKEY => '', ENDKEY => ''}
2013-08-12 18:17:58,174 DEBUG [NamespaceJanitor-minerva:44563] client.ClientScanner(218): Finished region={ENCODED => abf65547b0b2d46a6cc9bfbd20f80d68, NAME => 'hbase:namespace,,1376331267651.abf65547b0b2d46a6cc9bfbd20f80d68.', STARTKEY => '', ENDKEY => ''}
2013-08-12 18:18:28,174 DEBUG [NamespaceJanitor-minerva:44563] client.ClientScanner(218): Finished region={ENCODED => abf65547b0b2d46a6cc9bfbd20f80d68, NAME => 'hbase:namespace,,1376331267651.abf65547b0b2d46a6cc9bfbd20f80d68.', STARTKEY => '', ENDKEY => ''}
2013-08-12 18:18:58,175 DEBUG [NamespaceJanitor-minerva:44563] client.ClientScanner(218): Finished region={ENCODED => abf65547b0b2d46a6cc9bfbd20f80d68, NAME => 'hbase:namespace,,1376331267651.abf65547b0b2d46a6cc9bfbd20f80d68.', STARTKEY => '', ENDKEY => ''}
2013-08-12 18:19:24,564 DEBUG [LRU Statistics #0] hfile.LruBlockCache(728): Stats: total=3.17 MB, free=672.41 MB, max=675.58 MB, blocks=0, accesses=0, hits=0, hitRatio=0, cachingAccesses=0, cachingHits=0, cachingHitsRatio=0,evictions=0, evicted=0, evictedPerRun=NaN
2013-08-12 18:19:28,175 DEBUG [NamespaceJanitor-minerva:44563] client.ClientScanner(218): Finished region={ENCODED => abf65547b0b2d46a6cc9bfbd20f80d68, NAME => 'hbase:namespace,,1376331267651.abf65547b0b2d46a6cc9bfbd20f80d68.', STARTKEY => '', ENDKEY => ''}
{code}

We seem to stall.

Let me just up the timeout for now.  Its been happening frequently enough that this test timeouts.", ,1,,,,,,
HBASE-9203,"See https://builds.apache.org/job/hbase-0.95/434/testReport/junit/org.apache.hadoop.hbase.thrift/TestThriftServerCmdLine/testRunThriftServer_18_/

Looking in log it looks like it is making progress still.  There is something going on in here but not enough log:

{code}
2013-08-12 18:14:43,163 INFO  [Thread-258] thrift.TestThriftServerCmdLine(132): Starting HBase Thrift server with command line: -threadpool -port 49160 -bind minerva.apache.org start
2013-08-12 18:14:43,164 INFO  [ThriftServer-cmdline] thrift.ThriftServerRunner$ImplType(209): Using thrift server type threadpool
2013-08-12 18:14:58,174 DEBUG [NamespaceJanitor-minerva:44563] client.ClientScanner(218): Finished region={ENCODED => abf65547b0b2d46a6cc9bfbd20f80d68, NAME => 'hbase:namespace,,1376331267651.abf65547b0b2d46a6cc9bfbd20f80d68.', STARTKEY => '', ENDKEY => ''}
2013-08-12 18:15:28,174 DEBUG [NamespaceJanitor-minerva:44563] client.ClientScanner(218): Finished region={ENCODED => abf65547b0b2d46a6cc9bfbd20f80d68, NAME => 'hbase:namespace,,1376331267651.abf65547b0b2d46a6cc9bfbd20f80d68.', STARTKEY => '', ENDKEY => ''}
2013-08-12 18:15:58,175 DEBUG [NamespaceJanitor-minerva:44563] client.ClientScanner(218): Finished region={ENCODED => abf65547b0b2d46a6cc9bfbd20f80d68, NAME => 'hbase:namespace,,1376331267651.abf65547b0b2d46a6cc9bfbd20f80d68.', STARTKEY => '', ENDKEY => ''}
2013-08-12 18:16:28,173 DEBUG [NamespaceJanitor-minerva:44563] client.ClientScanner(218): Finished region={ENCODED => abf65547b0b2d46a6cc9bfbd20f80d68, NAME => 'hbase:namespace,,1376331267651.abf65547b0b2d46a6cc9bfbd20f80d68.', STARTKEY => '', ENDKEY => ''}
2013-08-12 18:16:58,174 DEBUG [NamespaceJanitor-minerva:44563] client.ClientScanner(218): Finished region={ENCODED => abf65547b0b2d46a6cc9bfbd20f80d68, NAME => 'hbase:namespace,,1376331267651.abf65547b0b2d46a6cc9bfbd20f80d68.', STARTKEY => '', ENDKEY => ''}
2013-08-12 18:17:28,174 DEBUG [NamespaceJanitor-minerva:44563] client.ClientScanner(218): Finished region={ENCODED => abf65547b0b2d46a6cc9bfbd20f80d68, NAME => 'hbase:namespace,,1376331267651.abf65547b0b2d46a6cc9bfbd20f80d68.', STARTKEY => '', ENDKEY => ''}
2013-08-12 18:17:58,174 DEBUG [NamespaceJanitor-minerva:44563] client.ClientScanner(218): Finished region={ENCODED => abf65547b0b2d46a6cc9bfbd20f80d68, NAME => 'hbase:namespace,,1376331267651.abf65547b0b2d46a6cc9bfbd20f80d68.', STARTKEY => '', ENDKEY => ''}
2013-08-12 18:18:28,174 DEBUG [NamespaceJanitor-minerva:44563] client.ClientScanner(218): Finished region={ENCODED => abf65547b0b2d46a6cc9bfbd20f80d68, NAME => 'hbase:namespace,,1376331267651.abf65547b0b2d46a6cc9bfbd20f80d68.', STARTKEY => '', ENDKEY => ''}
2013-08-12 18:18:58,175 DEBUG [NamespaceJanitor-minerva:44563] client.ClientScanner(218): Finished region={ENCODED => abf65547b0b2d46a6cc9bfbd20f80d68, NAME => 'hbase:namespace,,1376331267651.abf65547b0b2d46a6cc9bfbd20f80d68.', STARTKEY => '', ENDKEY => ''}
2013-08-12 18:19:24,564 DEBUG [LRU Statistics #0] hfile.LruBlockCache(728): Stats: total=3.17 MB, free=672.41 MB, max=675.58 MB, blocks=0, accesses=0, hits=0, hitRatio=0, cachingAccesses=0, cachingHits=0, cachingHitsRatio=0,evictions=0, evicted=0, evictedPerRun=NaN
2013-08-12 18:19:28,175 DEBUG [NamespaceJanitor-minerva:44563] client.ClientScanner(218): Finished region={ENCODED => abf65547b0b2d46a6cc9bfbd20f80d68, NAME => 'hbase:namespace,,1376331267651.abf65547b0b2d46a6cc9bfbd20f80d68.', STARTKEY => '', ENDKEY => ''}
{code}

We seem to stall.

Let me just up the timeout for now.  Its been happening frequently enough that this test timeouts.", ,,1,,,,,
HBASE-9207,"An offline split parent region can be assigned by using 'assign' from shell.

This may trigger a compaction on the split parent region that should be offline, breaking the references of the two daughters.

The easy way to test this is:
 * disable compactions
 * create a table and insert some row
 * split
 * shutdown the master (otherwise there's a state checked inside the AssignmentManager)
 * start the master
 * assign the region marked as OFFLINE => true, SPLIT => true (the compaction on region startup will probably break the reference of the two  daughters)
", ,,,,,,,
HBASE-9208,"See https://builds.apache.org/job/hbase-0.95/434/testReport/junit/org.apache.hadoop.hbase.thrift/TestThriftServerCmdLine/testRunThriftServer_18_/

Looking in log it looks like it is making progress still.  There is something going on in here but not enough log:

{code}
2013-08-12 18:14:43,163 INFO  [Thread-258] thrift.TestThriftServerCmdLine(132): Starting HBase Thrift server with command line: -threadpool -port 49160 -bind minerva.apache.org start
2013-08-12 18:14:43,164 INFO  [ThriftServer-cmdline] thrift.ThriftServerRunner$ImplType(209): Using thrift server type threadpool
2013-08-12 18:14:58,174 DEBUG [NamespaceJanitor-minerva:44563] client.ClientScanner(218): Finished region={ENCODED => abf65547b0b2d46a6cc9bfbd20f80d68, NAME => 'hbase:namespace,,1376331267651.abf65547b0b2d46a6cc9bfbd20f80d68.', STARTKEY => '', ENDKEY => ''}
2013-08-12 18:15:28,174 DEBUG [NamespaceJanitor-minerva:44563] client.ClientScanner(218): Finished region={ENCODED => abf65547b0b2d46a6cc9bfbd20f80d68, NAME => 'hbase:namespace,,1376331267651.abf65547b0b2d46a6cc9bfbd20f80d68.', STARTKEY => '', ENDKEY => ''}
2013-08-12 18:15:58,175 DEBUG [NamespaceJanitor-minerva:44563] client.ClientScanner(218): Finished region={ENCODED => abf65547b0b2d46a6cc9bfbd20f80d68, NAME => 'hbase:namespace,,1376331267651.abf65547b0b2d46a6cc9bfbd20f80d68.', STARTKEY => '', ENDKEY => ''}
2013-08-12 18:16:28,173 DEBUG [NamespaceJanitor-minerva:44563] client.ClientScanner(218): Finished region={ENCODED => abf65547b0b2d46a6cc9bfbd20f80d68, NAME => 'hbase:namespace,,1376331267651.abf65547b0b2d46a6cc9bfbd20f80d68.', STARTKEY => '', ENDKEY => ''}
2013-08-12 18:16:58,174 DEBUG [NamespaceJanitor-minerva:44563] client.ClientScanner(218): Finished region={ENCODED => abf65547b0b2d46a6cc9bfbd20f80d68, NAME => 'hbase:namespace,,1376331267651.abf65547b0b2d46a6cc9bfbd20f80d68.', STARTKEY => '', ENDKEY => ''}
2013-08-12 18:17:28,174 DEBUG [NamespaceJanitor-minerva:44563] client.ClientScanner(218): Finished region={ENCODED => abf65547b0b2d46a6cc9bfbd20f80d68, NAME => 'hbase:namespace,,1376331267651.abf65547b0b2d46a6cc9bfbd20f80d68.', STARTKEY => '', ENDKEY => ''}
2013-08-12 18:17:58,174 DEBUG [NamespaceJanitor-minerva:44563] client.ClientScanner(218): Finished region={ENCODED => abf65547b0b2d46a6cc9bfbd20f80d68, NAME => 'hbase:namespace,,1376331267651.abf65547b0b2d46a6cc9bfbd20f80d68.', STARTKEY => '', ENDKEY => ''}
2013-08-12 18:18:28,174 DEBUG [NamespaceJanitor-minerva:44563] client.ClientScanner(218): Finished region={ENCODED => abf65547b0b2d46a6cc9bfbd20f80d68, NAME => 'hbase:namespace,,1376331267651.abf65547b0b2d46a6cc9bfbd20f80d68.', STARTKEY => '', ENDKEY => ''}
2013-08-12 18:18:58,175 DEBUG [NamespaceJanitor-minerva:44563] client.ClientScanner(218): Finished region={ENCODED => abf65547b0b2d46a6cc9bfbd20f80d68, NAME => 'hbase:namespace,,1376331267651.abf65547b0b2d46a6cc9bfbd20f80d68.', STARTKEY => '', ENDKEY => ''}
2013-08-12 18:19:24,564 DEBUG [LRU Statistics #0] hfile.LruBlockCache(728): Stats: total=3.17 MB, free=672.41 MB, max=675.58 MB, blocks=0, accesses=0, hits=0, hitRatio=0, cachingAccesses=0, cachingHits=0, cachingHitsRatio=0,evictions=0, evicted=0, evictedPerRun=NaN
2013-08-12 18:19:28,175 DEBUG [NamespaceJanitor-minerva:44563] client.ClientScanner(218): Finished region={ENCODED => abf65547b0b2d46a6cc9bfbd20f80d68, NAME => 'hbase:namespace,,1376331267651.abf65547b0b2d46a6cc9bfbd20f80d68.', STARTKEY => '', ENDKEY => ''}
{code}

We seem to stall.

Let me just up the timeout for now.  Its been happening frequently enough that this test timeouts.", ,,1,,,,,
HBASE-9211,"Not sure where this is coming from but since today if I try to create a table that already exists in the shell I get:

bq. ERROR: undefined method `message' for nil:NilClass

instead of the normal exception.", ,,,,,,,
HBASE-9221,"Sometimes, users will want to provide their own User class depending on the type of security they will support in their local environment. For instance, running Hadoop1 vs Hadoop2 vs CDH means potentially different ways of getting the UserGroupInformation. 
This issue abstracts out the mechanism by which we obtain an o.a.h.hbase.security.User to a UserProvider. This UserProvider can then be extented as a Hadoop 1/2 shim as well as supporting custom authentication code.",1,1,,,,,,
HBASE-9233,"HConnectionManager.isTableAvailable() the SERVER_QUALIFIER for each region, including the offline split parent.

In case of hbck META recovery or snapshots clone/restore an offline parent will never be assigned (since should stay offline). In this case HBaseAdmin.waitUntilTableIsEnabled() used by enable() and cloneSnapshot() will wait until the timeout is expired.", ,,,,,,,
HBASE-9250,"{code}
2013-08-16 09:33:56,742 FATAL [regionserver60020.leaseChecker] regionserver.Leases: Unexpected exception killed leases thread
java.lang.IllegalArgumentException: timeout value is negative
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hbase.regionserver.Leases.run(Leases.java:91)
	at java.lang.Thread.run(Thread.java:722){code}", ,,,,,,,
HBASE-9251,"I was trying the RC0 of 0.95.2. and was on the shell trying to list tables in a namespace. I tried listing tables in the 'hbase' namespace and in a namespace I created. The listing fails:

{noformat}
hbase(main):008:0> list_namespace_tables 'hbase'
TABLE

ERROR: org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=7, exceptions:
Fri Aug 16 17:47:17 UTC 2013, org.apache.hadoop.hbase.client.RpcRetryingCaller@2313b44d, org.apache.hadoop.hbase.ipc.RemoteWithExtrasException(java.io.IOException): java.io.IOException: Illegal character <58> at 5. User-space table qualifiers can only contain 'alphanumeric characters': i.e. [a-zA-Z_0-9-.]: hbase:meta
        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2194)
        at org.apache.hadoop.hbase.ipc.RpcServer$Handler.run(RpcServer.java:1861)
Caused by: java.lang.IllegalArgumentException: Illegal character <58> at 5. User-space table qualifiers can only contain 'alphanumeric characters': i.e. [a-zA-Z_0-9-.]: hbase:meta
        at org.apache.hadoop.hbase.TableName.isLegalTableQualifierName(TableName.java:155)
        at org.apache.hadoop.hbase.TableName.isLegalTableQualifierName(TableName.java:125)
        at org.apache.hadoop.hbase.TableName.finishValueOf(TableName.java:249)
        at org.apache.hadoop.hbase.TableName.valueOf(TableName.java:242)
        at org.apache.hadoop.hbase.master.HMaster.listTableNamesByNamespace(HMaster.java:3130)
        at org.apache.hadoop.hbase.master.HMaster.listTableNamesByNamespace(HMaster.java:3063)
        at org.apache.hadoop.hbase.protobuf.generated.MasterAdminProtos$MasterAdminService$2.callBlockingMethod(MasterAdminProtos.java:27764)
        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2156)
        ... 1 more
{noformat}", ,,,,,,,
HBASE-9257,"The test failed in our test env with the following stack. The reason is that we use the following pattern to check errors:

{code}
    while (!ap.hasError()) {
      Thread.sleep(1);
    }
    ...
    Assert.assertEquals(1, ap.getErrors().actions.size());

{code}

While in our product code, we set errors in different order
{code}
      ...
      this.hasError.set(true);
      errors.add(throwable, row, location);
      ...
{code}
Therefore, a client code may not get error actions when hasError is true due to thread scheduling.

{code}
Test Error Stack:

java.lang.AssertionError: expected:<1> but was:<0>
    at org.junit.Assert.fail(Assert.java:88)
    at org.junit.Assert.failNotEquals(Assert.java:743)
    at org.junit.Assert.assertEquals(Assert.java:118)
    at org.junit.Assert.assertEquals(Assert.java:555)
    at org.junit.Assert.assertEquals(Assert.java:542)
    at org.apache.hadoop.hbase.client.TestAsyncProcess.testFailAndSuccess(TestAsyncProcess.java:308)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
    at org.junit.runners.Suite.runChild(Suite.java:127)
    at org.junit.runners.Suite.runChild(Suite.java:26)
    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
{code}", ,,,,,,,
HBASE-9267,"I trying out 0.95.2, I left it running over the weekend (8 RS, average load between 12 and 3 regions) and right now the balancer runs for 12 mins:

bq. 2013-08-19 21:54:45,534 DEBUG [jdec2hbase0403-1.vpc.cloudera.com,60000,1376689696384-BalancerChore] org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer: Could not find a better load balance plan.  Tried 0 different configurations in 777309ms, and did not find anything with a computed cost less than 36.32576937689094

It seems it slowly crept up there, yesterday it was doing:

bq. 2013-08-18 20:53:17,232 DEBUG [jdec2hbase0403-1.vpc.cloudera.com,60000,1376689696384-BalancerChore] org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer: Could not find a better load balance plan.  Tried 0 different configurations in 257374ms, and did not find anything with a computed cost less than 36.3251082542424

And originally it was doing 1 minute.

In the jstack I see a 1000 of these and jstack doesn't want to show me the whole thing:

bq.  at java.util.SubList$1.nextIndex(AbstractList.java:713)",,1,1,,,,,
HBASE-9268,"Got this testing the 0.95.2 RC.

I killed -STOP a region server and let it stay like that while running PE. The clients didn't find the new region locations and in the jstack were stuck doing RPC. Eventually I killed -CONT and the client printed these:

bq. Exception in thread ""TestClient-6"" java.lang.RuntimeException: org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 128 actions: IOException: 90 times, SocketTimeoutException: 38 times,", ,,,,,,,
HBASE-9269,"{code}
2013-08-19 17:24:47,592 INFO  [main] mapreduce.Job: Task Id : attempt_1370384262467_4376_m_000001_0, Status : FAILED
Error: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:128)
	at org.apache.hadoop.mapred.JobConf.getOutputKeyComparator(JobConf.java:808)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.init(MapTask.java:970)
	at org.apache.hadoop.mapred.MapTask.createSortingCollector(MapTask.java:390)
	at org.apache.hadoop.mapred.MapTask.access$100(MapTask.java:79)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.<init>(MapTask.java:669)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:741)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:339)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:158)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1478)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:153)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:126)
	... 12 more
{code}", ,,,,,,,
HBASE-9274,"Some of our jenkins CI machines have been failing out with /tmp/hbase-<user>

This can be shown by executing the following command before and after the namespaces patch.

{code}
# several tests are dropping stuff in the archive dir, just pick one
mvn clean test -Dtest=TestEncodedSeekers
find /tmp/hbase-jon/hbase/
{code}

/tmp/hbase-jon after test run before patch applied
{code}
$ find /tmp/hbase-jon/
/tmp/hbase-jon/
/tmp/hbase-jon/local
/tmp/hbase-jon/local/jars
{code}

after namespaces patch applied
{code}
/tmp/hbase-jon/
/tmp/hbase-jon/local
/tmp/hbase-jon/local/jars
/tmp/hbase-jon/hbase
/tmp/hbase-jon/hbase/.archive
/tmp/hbase-jon/hbase/.archive/.data
/tmp/hbase-jon/hbase/.archive/.data/default
/tmp/hbase-jon/hbase/.archive/.data/default/encodedSeekersTable
/tmp/hbase-jon/hbase/.archive/.data/default/encodedSeekersTable/c6ec51dca2a9fe4c2279006345d62b35
/tmp/hbase-jon/hbase/.archive/.data/default/encodedSeekersTable/c6ec51dca2a9fe4c2279006345d62b35/encodedSeekersCF
/tmp/hbase-jon/hbase/.archive/.data/default/encodedSeekersTable/c6ec51dca2a9fe4c2279006345d62b35/encodedSeekersCF/8e76a87806b94483851158366f7d5c17
/tmp/hbase-jon/hbase/.archive/.data/default/encodedSeekersTable/c6ec51dca2a9fe4c2279006345d62b35/encodedSeekersCF/494c07dbf08940749696bb0f9278401e
/tmp/hbase-jon/hbase/.archive/.data/default/encodedSeekersTable/c6ec51dca2a9fe4c2279006345d62b35/encodedSeekersCF/.8e76a87806b94483851158366f7d5c1
7.crc     
....
{code}", ,,,,,,,
HBASE-9278,"In upgrading to 0.96, there might be some meta/root table edits. Currently, we are just killing SplitLogWorker thread in case it sees any META, or ROOT waledit, which blocks log splitting/replaying of remaining WALs.
{code}
2013-08-20 15:45:16,998 ERROR regionserver.SplitLogWorker (SplitLogWorker.java:run(210)) - unexpected error 
java.lang.IllegalArgumentException: .META. no longer exists. The table has been renamed to hbase:meta
        at org.apache.hadoop.hbase.TableName.valueOf(TableName.java:269)
        at org.apache.hadoop.hbase.TableName.valueOf(TableName.java:261)
        at org.apache.hadoop.hbase.regionserver.wal.HLogKey.readFields(HLogKey.java:338)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1898)
        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1938)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.readNext(SequenceFileLogReader.java:215)
        at org.apache.hadoop.hbase.regionserver.wal.ReaderBase.next(ReaderBase.java:98)
        at org.apache.hadoop.hbase.regionserver.wal.ReaderBase.next(ReaderBase.java:85)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.getNextLogLine(HLogSplitter.java:582)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:292)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:209)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker$1.exec(SplitLogWorker.java:138)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.grabTask(SplitLogWorker.java:358)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.taskLoop(SplitLogWorker.java:245)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.run(SplitLogWorker.java:205)
        at java.lang.Thread.run(Thread.java:662)
2013-08-20 15:45:16,999 INFO  regionserver.SplitLogWorker (SplitLogWorker.java:run(212)) - SplitLogWorker localhost,60020,1377035111898 exiting
{code}
", ,,,,,,,
HBASE-9285,"User hrt_qa has been given 'C' permission.
{code}
create 'te', {NAME => 'f1', VERSIONS => 5}
...
hbase(main):003:0> list
TABLE
hbase:acl
hbase:namespace
te
6 row(s) in 0.0570 seconds

hbase(main):004:0> scan 'te'
ROW                                      COLUMN+CELL
2013-08-21 02:21:00,921 DEBUG [main] token.AuthenticationTokenSelector: No matching token found
2013-08-21 02:21:00,921 DEBUG [main] security.HBaseSaslRpcClient: Creating SASL GSSAPI client. Server's Kerberos principal name is hbase/hor16n13.gq1.ygridcore.net@HORTON.YGRIDCORE.NET
2013-08-21 02:21:00,923 DEBUG [main] security.HBaseSaslRpcClient: Have sent token of size 582 from initSASLContext.
2013-08-21 02:21:00,926 DEBUG [main] security.HBaseSaslRpcClient: Will read input token of size 0 for processing by initSASLContext
2013-08-21 02:21:00,926 DEBUG [main] security.HBaseSaslRpcClient: Will send token of size 0 from initSASLContext.
2013-08-21 02:21:00,926 DEBUG [main] security.HBaseSaslRpcClient: Will read input token of size 53 for processing by initSASLContext
2013-08-21 02:21:00,927 DEBUG [main] security.HBaseSaslRpcClient: Will send token of size 53 from initSASLContext.
2013-08-21 02:21:00,927 DEBUG [main] security.HBaseSaslRpcClient: SASL client context established. Negotiated QoP: auth
2013-08-21 02:21:00,935 WARN  [main] client.RpcRetryingCaller: Call exception, tries=0, retries=7, retryTime=-14ms
org.apache.hadoop.hbase.security.AccessDeniedException: org.apache.hadoop.hbase.security.AccessDeniedException: Insufficient permissions for user 'hrt_qa' for scanner open on table te
	at org.apache.hadoop.hbase.security.access.AccessController.preScannerOpen(AccessController.java:1116)
	at org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preScannerOpen(RegionCoprocessorHost.java:1294)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.scan(HRegionServer.java:3007)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:26847)
...
Caused by: org.apache.hadoop.hbase.ipc.RemoteWithExtrasException(org.apache.hadoop.hbase.security.AccessDeniedException): org.apache.hadoop.hbase.security.AccessDeniedException: Insufficient permissions for user 'hrt_qa' for scanner open on table te
	at org.apache.hadoop.hbase.security.access.AccessController.preScannerOpen(AccessController.java:1116)
	at org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preScannerOpen(RegionCoprocessorHost.java:1294)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.scan(HRegionServer.java:3007)
{code}
Here was related entries in hbase:acl table:
{code}
hbase(main):001:0> scan 'hbase:acl'
ROW                                      COLUMN+CELL
 hbase:acl                               column=l:hrt_qa, timestamp=1377045996685, value=C
 te                                      column=l:hrt_qa, timestamp=1377051648649, value=RWXCA
{code}",1,,,,,,,
HBASE-9298,"Make ns checker run every 5 minutes instead of every 30 seconds.

Also fix a bit more logging.  Can make the asyncprocess messages shorter by removing redundnate info like tablename.

Did pass removing 'region' and 'server' and META qualifiers where they are not needed.", ,1,1,,,,,
HBASE-9303,"Take snapshot of a table ('tablethree' in the log).
Put some data in the table and split the table.
Restore snapshot.
Table cannot be enabled due to:
{code}
Thu Aug 22 19:37:20 UTC 2013, org.apache.hadoop.hbase.client.RpcRetryingCaller@47a6ac39, org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region is not online: c32e63d8c8a1a94b68966645b956d86d
	at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionByEncodedName(HRegionServer.java:2557)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:3921)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.scan(HRegionServer.java:2996)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:26847)
{code}", ,,,,,,,
HBASE-9307,"I found this while trying out 0.95.2

I couldn't shut my cluster down because one region was still compacting, but it already had been doing that for an hour and it only had 80MBs of data.

Debugging I saw that it was a post-split compaction, specifically for the top part of the HFiles, and that it was just spinning on the first entry in the file.

Eventually I saw that the anonymous HFileScanner inside HalfStoreFileReader.getScanner (that's so dirty) wasn't handling HConstants.INDEX_KEY_MAGIC in seekTo() when calling this:

bq. this.delegate.seekTo(splitkey)

Instead it would treat this as if the split key wasn't in the file, but still seek back to the beginning of the file *then read on from there*.

During the compaction it would see a KV that's not even part of the region, and it just tries to find the correct block over and over again.

This came from HBASE-7845.", ,,1,,,,,
HBASE-9321,"I've been running tests on clusters with ""lots"" of regions, about 400, and I'm seeing weird contention in the client.

This one I see a lot, hundreds and sometimes thousands of threads are blocked like this:

{noformat}
""htable-pool4-t74"" daemon prio=10 tid=0x00007f2254114000 nid=0x2a99 waiting for monitor entry [0x00007f21f9e94000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:466)
	- waiting to lock <0x00000000fb5ad000> (a java.lang.Class for org.apache.hadoop.security.UserGroupInformation)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1013)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1407)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1634)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1691)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.multi(ClientProtos.java:27339)
	at org.apache.hadoop.hbase.client.MultiServerCallable.call(MultiServerCallable.java:105)
	at org.apache.hadoop.hbase.client.MultiServerCallable.call(MultiServerCallable.java:43)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:183)
{noformat}

While the holder is doing this:

{noformat}
""htable-pool17-t55"" daemon prio=10 tid=0x00007f2244408000 nid=0x2a98 runnable [0x00007f21f9f95000]
   java.lang.Thread.State: RUNNABLE
	at java.security.AccessController.getStackAccessControlContext(Native Method)
	at java.security.AccessController.getContext(AccessController.java:487)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:466)
	- locked <0x00000000fb5ad000> (a java.lang.Class for org.apache.hadoop.security.UserGroupInformation)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1013)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1407)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1634)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1691)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.multi(ClientProtos.java:27339)
	at org.apache.hadoop.hbase.client.MultiServerCallable.call(MultiServerCallable.java:105)
	at org.apache.hadoop.hbase.client.MultiServerCallable.call(MultiServerCallable.java:43)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:183)
{noformat}", ,,1,,,,,
HBASE-9333,"I've been running tests on clusters with ""lots"" of regions, about 400, and I'm seeing weird contention in the client.

This one I see a lot, hundreds and sometimes thousands of threads are blocked like this:

{noformat}
""htable-pool4-t74"" daemon prio=10 tid=0x00007f2254114000 nid=0x2a99 waiting for monitor entry [0x00007f21f9e94000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:466)
	- waiting to lock <0x00000000fb5ad000> (a java.lang.Class for org.apache.hadoop.security.UserGroupInformation)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1013)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1407)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1634)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1691)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.multi(ClientProtos.java:27339)
	at org.apache.hadoop.hbase.client.MultiServerCallable.call(MultiServerCallable.java:105)
	at org.apache.hadoop.hbase.client.MultiServerCallable.call(MultiServerCallable.java:43)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:183)
{noformat}

While the holder is doing this:

{noformat}
""htable-pool17-t55"" daemon prio=10 tid=0x00007f2244408000 nid=0x2a98 runnable [0x00007f21f9f95000]
   java.lang.Thread.State: RUNNABLE
	at java.security.AccessController.getStackAccessControlContext(Native Method)
	at java.security.AccessController.getContext(AccessController.java:487)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:466)
	- locked <0x00000000fb5ad000> (a java.lang.Class for org.apache.hadoop.security.UserGroupInformation)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1013)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1407)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1634)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1691)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.multi(ClientProtos.java:27339)
	at org.apache.hadoop.hbase.client.MultiServerCallable.call(MultiServerCallable.java:105)
	at org.apache.hadoop.hbase.client.MultiServerCallable.call(MultiServerCallable.java:43)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:183)
{noformat}", ,1,1,,,,,
HBASE-9343,"I've been running tests on clusters with ""lots"" of regions, about 400, and I'm seeing weird contention in the client.

This one I see a lot, hundreds and sometimes thousands of threads are blocked like this:

{noformat}
""htable-pool4-t74"" daemon prio=10 tid=0x00007f2254114000 nid=0x2a99 waiting for monitor entry [0x00007f21f9e94000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:466)
	- waiting to lock <0x00000000fb5ad000> (a java.lang.Class for org.apache.hadoop.security.UserGroupInformation)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1013)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1407)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1634)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1691)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.multi(ClientProtos.java:27339)
	at org.apache.hadoop.hbase.client.MultiServerCallable.call(MultiServerCallable.java:105)
	at org.apache.hadoop.hbase.client.MultiServerCallable.call(MultiServerCallable.java:43)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:183)
{noformat}

While the holder is doing this:

{noformat}
""htable-pool17-t55"" daemon prio=10 tid=0x00007f2244408000 nid=0x2a98 runnable [0x00007f21f9f95000]
   java.lang.Thread.State: RUNNABLE
	at java.security.AccessController.getStackAccessControlContext(Native Method)
	at java.security.AccessController.getContext(AccessController.java:487)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:466)
	- locked <0x00000000fb5ad000> (a java.lang.Class for org.apache.hadoop.security.UserGroupInformation)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1013)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1407)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1634)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1691)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.multi(ClientProtos.java:27339)
	at org.apache.hadoop.hbase.client.MultiServerCallable.call(MultiServerCallable.java:105)
	at org.apache.hadoop.hbase.client.MultiServerCallable.call(MultiServerCallable.java:43)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:183)
{noformat}", ,,1,,,,,
HBASE-9344,"We ran into a situation where due to a Kerberos configuration problem one of our region server could not connect to ZK when opening a region. Instead of shutting down it continue to try to reconnect. Eventually the master would assign the region to another region server. Each time that region server was assigned a region it would sit there for 5 mins with the region offline. It would have been better if the region server had shut itself down.

This is in the logs:

{quote}
2013-08-16 17:31:35,999 WARN org.apache.hadoop.hbase.zookeeper.ZKUtil: hconnection-0x2407b842ff2012d-0x2407b842ff2012d-0x2407b842ff2012d Unable to set watcher on znode (/hbase/hbaseid)
org.apache.zookeeper.KeeperException$AuthFailedException: KeeperErrorCode = AuthFailed for /hbase/hbaseid
    at org.apache.zookeeper.KeeperException.create(KeeperException.java:123)
    at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
    at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1041)
    at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.exists(RecoverableZooKeeper.java:172)
    at org.apache.hadoop.hbase.zookeeper.ZKUtil.checkExists(ZKUtil.java:450)
    at org.apache.hadoop.hbase.zookeeper.ClusterId.readClusterIdZNode(ClusterId.java:61)
    at org.apache.hadoop.hbase.zookeeper.ClusterId.getId(ClusterId.java:50)
    at org.apache.hadoop.hbase.zookeeper.ClusterId.hasId(ClusterId.java:44)
    at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.ensureZookeeperTrackers(HConnectionManager.java:616)
    at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:882)
    at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:857)
    at org.apache.hadoop.hbase.client.HTable.finishSetup(HTable.java:233)
    at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:173)
    at org.apache.hadoop.hbase.catalog.MetaReader.getHTable(MetaReader.java:201)
    at org.apache.hadoop.hbase.catalog.MetaReader.getMetaHTable(MetaReader.java:227)
    at org.apache.hadoop.hbase.catalog.MetaReader.getCatalogHTable(MetaReader.java:214)
    at org.apache.hadoop.hbase.catalog.MetaEditor.putToCatalogTable(MetaEditor.java:91)
    at org.apache.hadoop.hbase.catalog.MetaEditor.updateLocation(MetaEditor.java:296)
    at org.apache.hadoop.hbase.catalog.MetaEditor.updateRegionLocation(MetaEditor.java:276)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.postOpenDeployTasks(HRegionServer.java:1828)
    at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler$PostOpenDeployTasksThread.run(OpenRegionHandler.java:240)
{quote}

I think the RS should shut itself down instead.", ,1,,,,,,
HBASE-9346,"If META don't have the same region boundaries as the stores files, writes and read might go to the wrong place. We need to provide a way to check that withing HBCK.", ,,,,,,,
HBASE-9349,"Found this in a run of TestWALObserver:

{noformat}
java.lang.NullPointerException
	at org.apache.hadoop.hbase.master.HMaster.shutdown(HMaster.java:1510)
	at org.apache.hadoop.hbase.util.JVMClusterUtil.shutdown(JVMClusterUtil.java:226)
	at org.apache.hadoop.hbase.LocalHBaseCluster.shutdown(LocalHBaseCluster.java:424)
	at org.apache.hadoop.hbase.MiniHBaseCluster.shutdown(MiniHBaseCluster.java:417)
	at org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniHBaseCluster(HBaseTestingUtility.java:607)
	at org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster(HBaseTestingUtility.java:583)
	at org.apache.hadoop.hbase.coprocessor.TestWALObserver.teardownAfterClass(TestWALObserver.java:111)
{noformat}

if the active master in the minicluster is terminated before fully initialized. Needs null checks in HMaster#shutdown.", ,,,,,,,
HBASE-9353,"Here is related code:
{code}
  public static void addRegionToMeta(CatalogTracker catalogTracker, HRegionInfo regionInfo,
      HRegionInfo splitA, HRegionInfo splitB) throws IOException {
    addRegionToMeta(MetaReader.getMetaHTable(catalogTracker), regionInfo, splitA, splitB);
  }
{code}
HTable returned by MetaReader#getMetaHTable() is not closed in MetaEditor#addRegionToMeta()", ,,1,,,,,
HBASE-9364,"When a GET request is issue for a table row with multiple columns and columns have empty qualifier like f1: ,  results for empty qualifiers is being ignored. ", ,,,,,,,
HBASE-9373,"In the hbase shell, when a user tries to get a value related to a column, hbase returns only the latest value. But using the REST API returns HColumnDescriptor.DEFAULT_VERSIONS versions by default. 

The behavior should be consistent with the hbase shell.", ,,,,,,,
HBASE-9374,"Per this [thread|http://mail-archives.apache.org/mod_mbox/hbase-dev/201308.mbox/%3cCANZa=GuLO0jTLs1fF+5_NRDczO+M=SSqjeAGVEeiCY8injbP8w@mail.gmail.com%3e] from the dev list.

{quote}
It appears that as of HBASE-1936, we now require that client applications have write access to hbase.local.dir. This is because ProtobufUtil instantiates a DyanamicClassLoader as part of static initialization. This classloader is used for instantiating Comparators, Filters, and Exceptions.
{quote}

Client applications do not need to use DynamicClassLoader and so should not require this write access.",1,,,,,,,
HBASE-9375,"In the hbase shell, when a user tries to get a value related to a column, hbase returns only the latest value. But using the REST API returns HColumnDescriptor.DEFAULT_VERSIONS versions by default. 

The behavior should be consistent with the hbase shell.", ,,,,,,,
HBASE-9380,"We are not closing the StoreFile.Reader upon memstore flush.
", ,,1,,,,,
HBASE-9384,"We define hbase.rootdir as follows in hbase-default.xml: 
{code}
  <property>
    <name>hbase.tmp.dir</name>
    <value>${java.io.tmpdir}/hbase-${user.name}</value>
  </property>
  <property>
    <name>hbase.rootdir</name>
    <value>file://${hbase.tmp.dir}/hbase</value>
  </property>
{code}
This causes an
java.lang.IllegalArgumentException: Wrong FS: file://C:\Users\Administrator\AppData\Local\Temp\2\/hbase-Administrator/hbase, expected: file:/// 
on windows. 

", ,1,,,,,,
HBASE-9387,"I observed test timeout running against hadoop 2.1.0 with distributed log replay turned on.
Looks like region state for 1588230740 became inconsistent between master and the surviving region server:
{code}
2013-08-29 22:15:34,180 INFO  [AM.ZK.Worker-pool2-t4] master.RegionStates(299): Onlined 1588230740 on kiyo.gq1.ygridcore.net,57016,1377814510039
...
2013-08-29 22:15:34,587 DEBUG [Thread-221] client.HConnectionManager$HConnectionImplementation(1269): locateRegionInMeta parentTable=hbase:meta, metaLocation={region=hbase:meta,,1.1588230740, hostname=kiyo.gq1.ygridcore.net,57016,1377814510039, seqNum=0}, attempt=2 of 35 failed; retrying after sleep of 302 because: org.apache.hadoop.hbase.exceptions.RegionOpeningException: Region is being opened: 1588230740
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionByEncodedName(HRegionServer.java:2574)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:3949)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:2733)
        at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:26965)
        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2063)
        at org.apache.hadoop.hbase.ipc.RpcServer$CallRunner.run(RpcServer.java:1800)
        at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.consumerLoop(SimpleRpcScheduler.java:165)
        at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.access$000(SimpleRpcScheduler.java:41)
{code}", ,,,,,,,
HBASE-9390,"See the patch to reproduce the issue: If we activate log replay we don't have the events on WAL restore.

Pinging [~jeffreyz], we discussed this offline.", ,,,,,,,
HBASE-9393,"HBase dose not close a dead connection with the datanode.
This resulting in over 60K CLOSE_WAIT and at some point HBase can not connect to the datanode because too many mapped sockets from one host to another on the same port.

The example below is with low CLOSE_WAIT count because we had to restart hbase to solve the porblem, later in time it will incease to 60-100K sockets on CLOSE_WAIT

[root@hd2-region3 ~]# netstat -nap |grep CLOSE_WAIT |grep 21592 |wc -l
13156
[root@hd2-region3 ~]# ps -ef |grep 21592
root     17255 17219  0 12:26 pts/0    00:00:00 grep 21592
hbase    21592     1 17 Aug29 ?        03:29:06 /usr/java/jdk1.6.0_26/bin/java -XX:OnOutOfMemoryError=kill -9 %p -Xmx8000m -ea -XX:+UseConcMarkSweepGC -XX:+CMSIncrementalMode -Dhbase.log.dir=/var/log/hbase -Dhbase.log.file=hbase-hbase-regionserver-hd2-region3.swnet.corp.log ...


", ,,,,,,,
HBASE-9397,"Snapshots with the same name (but on different tables) are allowed to proceed concurrently.
This seems to be loop hole created by allowing multiple snapshots (on different tables) to run concurrently.
There are two checks in SnapshotManager, but fail to catch this particular case.
In isSnapshotCompleted(), we only check the completed snapshot directory.
In isTakingSnapshot(), we only check for the same table name.

The end result is the concurrently running snapshots with the same name are overlapping and messing up each other. For example, cleaning up the other's snapshot working directory in .hbase-snapshot/.tmp/snapshot-name.

{code}
2013-08-29 18:25:13,443 ERROR org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler: Failed taking snapshot { ss=mysnapshot table=TestTable type=FLUSH } due to exception:Couldn't read snapshot info from:hdfs://hdtest009:9000/hbase/.hbase-snapshot/.tmp/mysnapshot/.snapshotinfo
org.apache.hadoop.hbase.snapshot.CorruptedSnapshotException: Couldn't read snapshot info from:hdfs://hdtest009:9000/hbase/.hbase-snapshot/.tmp/mysnapshot/.snapshotinfo
        at org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils.readSnapshotInfo(SnapshotDescriptionUtils.java:321)
        at org.apache.hadoop.hbase.master.snapshot.MasterSnapshotVerifier.verifySnapshotDescription(MasterSnapshotVerifier.java:123)
{code}", ,,,,,,,
HBASE-9399,"I've been running tests on clusters with ""lots"" of regions, about 400, and I'm seeing weird contention in the client.

This one I see a lot, hundreds and sometimes thousands of threads are blocked like this:

{noformat}
""htable-pool4-t74"" daemon prio=10 tid=0x00007f2254114000 nid=0x2a99 waiting for monitor entry [0x00007f21f9e94000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:466)
	- waiting to lock <0x00000000fb5ad000> (a java.lang.Class for org.apache.hadoop.security.UserGroupInformation)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1013)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1407)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1634)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1691)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.multi(ClientProtos.java:27339)
	at org.apache.hadoop.hbase.client.MultiServerCallable.call(MultiServerCallable.java:105)
	at org.apache.hadoop.hbase.client.MultiServerCallable.call(MultiServerCallable.java:43)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:183)
{noformat}

While the holder is doing this:

{noformat}
""htable-pool17-t55"" daemon prio=10 tid=0x00007f2244408000 nid=0x2a98 runnable [0x00007f21f9f95000]
   java.lang.Thread.State: RUNNABLE
	at java.security.AccessController.getStackAccessControlContext(Native Method)
	at java.security.AccessController.getContext(AccessController.java:487)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:466)
	- locked <0x00000000fb5ad000> (a java.lang.Class for org.apache.hadoop.security.UserGroupInformation)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1013)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1407)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1634)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1691)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.multi(ClientProtos.java:27339)
	at org.apache.hadoop.hbase.client.MultiServerCallable.call(MultiServerCallable.java:105)
	at org.apache.hadoop.hbase.client.MultiServerCallable.call(MultiServerCallable.java:43)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:183)
{noformat}", ,1,1,,,,,
HBASE-9401,"We are not closing the StoreFile.Reader upon memstore flush.
", ,,1,,,,,
HBASE-9410,Multiple concurrent executions of coprocessor endpoints slow down drastically. It is compounded further when there are more Htable connection setups happening., ,1,1,,,,,
HBASE-9411,"Here is related code:
{code}
    /* Decrement the outstanding RPC count */
    protected void decRpcCount() {
      rpcCount--;
    }

    /* Increment the outstanding RPC count */
    protected void incRpcCount() {
      rpcCount++;
    }
{code}
Even though rpcCount is volatile, in non atomic operations (increment / decrement) different threads may get unexpected result.

See http://stackoverflow.com/questions/7805192/is-a-volatile-int-in-java-thread-safe", ,,,,,,,
HBASE-9423,Distributed log splitting manager is started before we wait for HDFS to be out of safe mode.  This leads to log splitting failure since HDFS can't replicate blocks to write the recovered edits., ,,,,,,,
HBASE-9425,"This bug was introduced via HBASE-6677 ""Random ZooKeeper port in test can overrun max port"".

If 2181 is occupied but you start a LocalHBaseCluster (let's say you untar hbase and start it right away) you'll get this:

{noformat}
13/09/03 10:38:13 INFO server.NIOServerCnxnFactory: binding to port 0.0.0.0/0.0.0.0:2181
13/09/03 10:38:13 INFO server.NIOServerCnxnFactory: binding to port 0.0.0.0/0.0.0.0:2181
13/09/03 10:38:13 INFO server.NIOServerCnxnFactory: binding to port 0.0.0.0/0.0.0.0:2181
...
13/09/03 10:38:44 INFO server.NIOServerCnxnFactory: binding to port 0.0.0.0/0.0.0.0:2181
13/09/03 10:38:44 INFO server.NIOServerCnxnFactory: binding to port 0.0.0.0/0.0.0.0:2181
13/09/03 10:38:44 ERROR master.HMasterCommandLine: Master exiting
java.io.IOException: Too many open files
        at sun.nio.ch.EPollArrayWrapper.epollCreate(Native Method)
        at sun.nio.ch.EPollArrayWrapper.<init>(EPollArrayWrapper.java:87)
        at sun.nio.ch.EPollSelectorImpl.<init>(EPollSelectorImpl.java:68)
        at sun.nio.ch.EPollSelectorProvider.openSelector(EPollSelectorProvider.java:36)
        at java.nio.channels.Selector.open(Selector.java:227)
        at org.apache.zookeeper.server.NIOServerCnxnFactory.<init>(NIOServerCnxnFactory.java:61)
        at org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.startup(MiniZooKeeperCluster.java:165)
        at org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.startup(MiniZooKeeperCluster.java:131)
        at org.apache.hadoop.hbase.master.HMasterCommandLine.startMaster(HMasterCommandLine.java:164)
        at org.apache.hadoop.hbase.master.HMasterCommandLine.run(HMasterCommandLine.java:134)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:78)
        at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:2812)
{noformat}

The reason is that MiniZookeeperCluster.selectClientPort returns 2181 if defaultClientPort is greater than 0, which it always is when starting a LocalHBaseCluster.", ,1,,,,,,
HBASE-9428,"I found this issue after debugging a performance problem on an OpenTSDB cluster, it was basically unusable after an upgrade from 0.94.2 to 0.94.6. It was caused by HBASE-7279 (ping [~lhofhansl]).

The easiest way to see it is to run a simple 1 client PE:

{noformat}
$ ./bin/hbase org.apache.hadoop.hbase.PerformanceEvaluation sequentialWrite 1
{noformat}

Then in the shell do a filter scan (flush the table first and make sure if fits in your blockcache if you want stable numbers).

Pre HBASE-7279:
{noformat}
hbase(main):028:0> scan 'TestTable', {FILTER => ""(RowFilter (=, 'regexstring:0000055872') )""}
ROW                                                 COLUMN+CELL                                                                                                                                         
 0000055872                                         column=info:data, timestamp=1378248850191, value=(blanked)                                                                                                                                    
1 row(s) in 1.2780 seconds
{noformat}

Post HBASE-7279

{noformat}
hbase(main):037:0* scan 'TestTable', {FILTER => ""(RowFilter (=, 'regexstring:0000055872') )""}
ROW                                                 COLUMN+CELL                                                                                                                                         
 0000055872                                         column=info:data, timestamp=1378248850191, value=(blanked)                                                                                                                                      
1 row(s) in 24.2940 seconds
{noformat}

I tried a bunch of 0.94, up to 0.94.11, and the tip of 0.96. They are all slow like this.

It seems that since that jira went in we do a lot more row matching, and running the regex gets super expensive.", ,,1,,,,,
HBASE-9430,"{code}
expected += ClassSize.estimateBase(ConcurrentSkipListMap.class, false);
expected += ClassSize.estimateBase(ConcurrentSkipListMap.class, false);
expected += ClassSize.estimateBase(CopyOnWriteArraySet.class, false);
expected += ClassSize.estimateBase(CopyOnWriteArrayList.class, false);
{code}

We need to consider the heap requirement for KeyValueSkipListSet.
From where CopyOnWriteArraySet & CopyOnWriteArrayList is coming into picture?  I am not able to follow.
Also we need to consider the heap for TimeRangeTracker", ,,,,,,,
HBASE-9434,"It seems not to be like this before. Anyway, we should fix it.

{noformat}
13/09/04 09:23:31 INFO util.HBaseFsck: Trying to sildeline reference filehdfs://test-1.cloud.cloudera.com:8020/hbase/data/default/usertable/5dbfd775b15a383caab6131b992e1d37/f/864fa006a4814f1f9b42e2bd34ea6543.6bbe5472a333772ec4b2c5c87de503e0 to hdfs://test-1.cloud.cloudera.com:8020/864fa006a4814f1f9b42e2bd34ea6543.6bbe5472a333772ec4b2c5c87de503e0
Exception in thread ""main"" org.apache.hadoop.security.AccessControlException: Permission denied: user=hbase, access=WRITE, inode=""/"":hdfs:supergroup:drwxr-xr-x
        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:234)
{noformat}", ,,,,,,,
HBASE-9445,"Currently, taking a snapshot will not create the family directory under a region if the family does not have any files in it. 

Subsequent verification fails because of this. There is some logic in the SnapshotTestingUtils.confirmSnapshotValid() to deal with empty family directories, but I think we should create the family directories regardless of whether there are any hfiles referencing them. 

{code}
2013-09-05 11:07:21,566 DEBUG [Thread-208] util.FSUtils(1687): |-data/
2013-09-05 11:07:21,567 DEBUG [Thread-208] util.FSUtils(1687): |----default/
2013-09-05 11:07:21,568 DEBUG [Thread-208] util.FSUtils(1687): |-------test/
2013-09-05 11:07:21,569 DEBUG [Thread-208] util.FSUtils(1687): |----------.tabledesc/
2013-09-05 11:07:21,570 DEBUG [Thread-208] util.FSUtils(1690): |-------------.tableinfo.0000000001
2013-09-05 11:07:21,570 DEBUG [Thread-208] util.FSUtils(1687): |----------.tmp/
2013-09-05 11:07:21,571 DEBUG [Thread-208] util.FSUtils(1687): |----------accd6e55887057888de758df44dacda7/
2013-09-05 11:07:21,572 DEBUG [Thread-208] util.FSUtils(1690): |-------------.regioninfo
2013-09-05 11:07:21,572 DEBUG [Thread-208] util.FSUtils(1687): |-------------fam/

2013-09-05 11:07:21,555 DEBUG [Thread-208] util.FSUtils(1687): |-.hbase-snapshot/
2013-09-05 11:07:21,556 DEBUG [Thread-208] util.FSUtils(1687): |----.tmp/
2013-09-05 11:07:21,557 DEBUG [Thread-208] util.FSUtils(1687): |----offlineTableSnapshot/
2013-09-05 11:07:21,558 DEBUG [Thread-208] util.FSUtils(1690): |-------.snapshotinfo
2013-09-05 11:07:21,558 DEBUG [Thread-208] util.FSUtils(1687): |-------.tabledesc/
2013-09-05 11:07:21,558 DEBUG [Thread-208] util.FSUtils(1690): |----------.tableinfo.0000000001
2013-09-05 11:07:21,559 DEBUG [Thread-208] util.FSUtils(1687): |-------.tmp/
2013-09-05 11:07:21,559 DEBUG [Thread-208] util.FSUtils(1687): |-------accd6e55887057888de758df44dacda7/
2013-09-05 11:07:21,560 DEBUG [Thread-208] util.FSUtils(1690): |----------.regioninfo
{code}

I think this is important for 0.96.0. ", ,,,,,,,
HBASE-9451,"While running tests described in HBASE-9338, ran into this problem. The hbase.status.listener.class was set to org.apache.hadoop.hbase.client.ClusterStatusListener$MultiCastListener.
1. I had the meta server coming down
2. The metaSSH got triggered. The call chain:
   2.1 verifyAndAssignMetaWithRetries
   2.2 verifyMetaRegionLocation
   2.3 waitForMetaServerConnection
   2.4 getMetaServerConnection
   2.5 getCachedConnection
   2.6 HConnectionManager.getAdmin(serverName, false)
   2.7 isDeadServer(serverName) -> This is hardcoded to return 'false' when the clusterStatusListener field is null. If clusterStatusListener is not null (in my test), then it could return true in certain cases (and in this case, indeed it should return true since the server is down). I am trying to understand why it's hardcoded to 'false' for former case.
3. When isDeadServer returns true, the method HConnectionManager.getAdmin(ServerName, boolean) throws RegionServerStoppedException.
4. Finally, after the retries are over verifyAndAssignMetaWithRetries gives up and the master aborts.

The methods in the above call chain don't handle RegionServerStoppedException. Maybe something to look at... ", ,,,,,,,
HBASE-9456,"The flow:
1. Cluster is up, meta is assigned to some server
2. Master is killed
3. Master is brought up, it is initializing. It learns about the Meta server (in assignMeta).
4. Server holding meta is killed
5. Meta never gets reassigned since the SSH wasn't enabled", ,,,,,,,
HBASE-9457,"In the region server holding the system table is killed while master is starting, master will hang there waiting for system table to be assigned which won't happen.", ,,,,,,,
HBASE-9465,"I found this issue after debugging a performance problem on an OpenTSDB cluster, it was basically unusable after an upgrade from 0.94.2 to 0.94.6. It was caused by HBASE-7279 (ping [~lhofhansl]).

The easiest way to see it is to run a simple 1 client PE:

{noformat}
$ ./bin/hbase org.apache.hadoop.hbase.PerformanceEvaluation sequentialWrite 1
{noformat}

Then in the shell do a filter scan (flush the table first and make sure if fits in your blockcache if you want stable numbers).

Pre HBASE-7279:
{noformat}
hbase(main):028:0> scan 'TestTable', {FILTER => ""(RowFilter (=, 'regexstring:0000055872') )""}
ROW                                                 COLUMN+CELL                                                                                                                                         
 0000055872                                         column=info:data, timestamp=1378248850191, value=(blanked)                                                                                                                                    
1 row(s) in 1.2780 seconds
{noformat}

Post HBASE-7279

{noformat}
hbase(main):037:0* scan 'TestTable', {FILTER => ""(RowFilter (=, 'regexstring:0000055872') )""}
ROW                                                 COLUMN+CELL                                                                                                                                         
 0000055872                                         column=info:data, timestamp=1378248850191, value=(blanked)                                                                                                                                      
1 row(s) in 24.2940 seconds
{noformat}

I tried a bunch of 0.94, up to 0.94.11, and the tip of 0.96. They are all slow like this.

It seems that since that jira went in we do a lot more row matching, and running the regex gets super expensive.", ,,1,,,,,
HBASE-9467,"I found this issue after debugging a performance problem on an OpenTSDB cluster, it was basically unusable after an upgrade from 0.94.2 to 0.94.6. It was caused by HBASE-7279 (ping [~lhofhansl]).

The easiest way to see it is to run a simple 1 client PE:

{noformat}
$ ./bin/hbase org.apache.hadoop.hbase.PerformanceEvaluation sequentialWrite 1
{noformat}

Then in the shell do a filter scan (flush the table first and make sure if fits in your blockcache if you want stable numbers).

Pre HBASE-7279:
{noformat}
hbase(main):028:0> scan 'TestTable', {FILTER => ""(RowFilter (=, 'regexstring:0000055872') )""}
ROW                                                 COLUMN+CELL                                                                                                                                         
 0000055872                                         column=info:data, timestamp=1378248850191, value=(blanked)                                                                                                                                    
1 row(s) in 1.2780 seconds
{noformat}

Post HBASE-7279

{noformat}
hbase(main):037:0* scan 'TestTable', {FILTER => ""(RowFilter (=, 'regexstring:0000055872') )""}
ROW                                                 COLUMN+CELL                                                                                                                                         
 0000055872                                         column=info:data, timestamp=1378248850191, value=(blanked)                                                                                                                                      
1 row(s) in 24.2940 seconds
{noformat}

I tried a bunch of 0.94, up to 0.94.11, and the tip of 0.96. They are all slow like this.

It seems that since that jira went in we do a lot more row matching, and running the regex gets super expensive.", ,,1,,,,,
HBASE-9468,"When the active master's zk session expires, it'll try to recover zk session, but without turn off its RpcServer. What if a previous backup master has already become the now active master, and some client tries to send request to this expired master by using the cached master info? Any problem here?", ,1,,,,,,
HBASE-9480,"Came across this issue (HBASE-9338 test):
1. Client issues a request to move a region from ServerA to ServerB
2. ServerA is compacting that region and doesn't close region immediately. In fact, it takes a while to complete the request.
3. The master in the meantime, sends another close request.
4. ServerA sends it a NotServingRegionException
5. Master handles the exception, deletes the znode, and invokes regionOffline for the said region.
6. ServerA fails to operate on ZK in the CloseRegionHandler since the node is deleted.

The region is permanently offline.


There are potentially other situations where when a RegionServer is offline and the client asks for a region move off from that server, the master makes the region offline.
", ,,,,,,,
HBASE-9481,"In integration tests, we found SSH got aborted with following stack trace:
{code}
13/09/07 18:10:00 ERROR executor.EventHandler: Caught throwable while processing event M_SERVER_SHUTDOWN
java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
        at java.util.HashMap$ValueIterator.next(HashMap.java:822)
        at org.apache.hadoop.hbase.master.RegionStates.serverOffline(RegionStates.java:378)
        at org.apache.hadoop.hbase.master.AssignmentManager.processServerShutdown(AssignmentManager.java:3143)
        at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:207)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:131)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}", ,,,,,,,
HBASE-9482,"We should recommend and not enforce secure Hadoop underneath as a requirement to run secure HBase.

Few of our customers have HBase clusters which expose only HBase services to outside the physical network and no other services (including ssh) are accessible from outside of such cluster.

However they are forced to setup secure Hadoop and incur the penalty of security overhead at filesystem layer even if they do not need to.

The following code tests for both secure HBase and secure Hadoop.

{code:title=org.apache.hadoop.hbase.security.User|borderStyle=solid}
  /**
   * Returns whether or not secure authentication is enabled for HBase.  Note that
   * HBase security requires HDFS security to provide any guarantees, so this requires that
   * both <code>hbase.security.authentication</code> and <code>hadoop.security.authentication</code>
   * are set to <code>kerberos</code>.
   */
  public static boolean isHBaseSecurityEnabled(Configuration conf) {
    return ""kerberos"".equalsIgnoreCase(conf.get(HBASE_SECURITY_CONF_KEY)) &&
        ""kerberos"".equalsIgnoreCase(
            conf.get(CommonConfigurationKeys.HADOOP_SECURITY_AUTHENTICATION));
  }
{code}

What is worse that if {{""hadoop.security.authentication""}} is not set to {{""kerberos""}} (undocumented at http://hbase.apache.org/book/security.html), all other configuration have no impact and HBase RPCs silently switch back to unsecured mode.",1,,,,,,,
HBASE-9485,HBase extends OutputCommitter which turns recovery off. Meaning all completed maps are lost on RM restart and job starts from scratch. FileOutputCommitter implements recovery so we should look at that to see what is potentially needed for recovery., ,,,,,,,
HBASE-9486,"When running 
{code}
hbase org.apache.hadoop.hbase.test.IntegrationTestBigLinkedList --monkey slowDeterministic
{code}
One task failed with the following stack trace:
{code}
2013-09-10 01:56:03,115 WARN [htable-pool1-t134] org.apache.hadoop.hbase.client.AsyncProcess: Attempt #35/35 failed for 3 ops on server02,60020,1378776046122 NOT resubmitting.region=IntegrationTestBigLinkedList,\xA6\x10\x9C\x85,1378776439065.766ab62aa30fa94c9014f09738698922., hostname=server02,60020,1378776046122, seqNum=16146143
2013-09-10 01:56:03,115 WARN [htable-pool1-t119] org.apache.hadoop.hbase.client.AsyncProcess: Attempt #35/35 failed for 6 ops on server02,60020,1378775896233 NOT resubmitting.region=IntegrationTestBigLinkedList,\x9D\x95>\xDB\xCB\xD5\xE2\xAD\x7F\xCB\x1D\xBCN~\xF2U,1378774537592.b2534e273feecba91db43496efa1cd12., hostname=server02,60020,1378775896233, seqNum=14890994
2013-09-10 01:56:03,655 WARN [htable-pool1-t119] org.apache.hadoop.hbase.client.AsyncProcess: Attempt #35/35 failed for 9 ops on server01,60020,1378775896233 NOT resubmitting.region=IntegrationTestBigLinkedList,\xB8\x0B.\x8C\x12Px\x88>\x10\xA4\x07\x9FJ\x97\xD0,1378775167749.7c0f1c17bc5f02e41e02939187304976., hostname=server01,60020,1378775896233, seqNum=15863492
2013-09-10 01:56:03,818 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.NullPointerException
	at org.apache.hadoop.hbase.client.AsyncProcess.findDestLocation(AsyncProcess.java:289)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:234)
	at org.apache.hadoop.hbase.client.HTable.backgroundFlushCommits(HTable.java:894)
	at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:1275)
	at org.apache.hadoop.hbase.client.HTable.close(HTable.java:1313)
	at org.apache.hadoop.hbase.test.IntegrationTestBigLinkedList$Generator$GeneratorMapper.cleanup(IntegrationTestBigLinkedList.java:352)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:148)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:763)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:339)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:162)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1477)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157)
{code}

Seems worth investigating. ", ,,,,,,,
HBASE-9488,"I found this issue after debugging a performance problem on an OpenTSDB cluster, it was basically unusable after an upgrade from 0.94.2 to 0.94.6. It was caused by HBASE-7279 (ping [~lhofhansl]).

The easiest way to see it is to run a simple 1 client PE:

{noformat}
$ ./bin/hbase org.apache.hadoop.hbase.PerformanceEvaluation sequentialWrite 1
{noformat}

Then in the shell do a filter scan (flush the table first and make sure if fits in your blockcache if you want stable numbers).

Pre HBASE-7279:
{noformat}
hbase(main):028:0> scan 'TestTable', {FILTER => ""(RowFilter (=, 'regexstring:0000055872') )""}
ROW                                                 COLUMN+CELL                                                                                                                                         
 0000055872                                         column=info:data, timestamp=1378248850191, value=(blanked)                                                                                                                                    
1 row(s) in 1.2780 seconds
{noformat}

Post HBASE-7279

{noformat}
hbase(main):037:0* scan 'TestTable', {FILTER => ""(RowFilter (=, 'regexstring:0000055872') )""}
ROW                                                 COLUMN+CELL                                                                                                                                         
 0000055872                                         column=info:data, timestamp=1378248850191, value=(blanked)                                                                                                                                      
1 row(s) in 24.2940 seconds
{noformat}

I tried a bunch of 0.94, up to 0.94.11, and the tip of 0.96. They are all slow like this.

It seems that since that jira went in we do a lot more row matching, and running the regex gets super expensive.", ,,1,,,,,
HBASE-9497,"In pre-0.96, .META. has .tableinfo files which refer to .META. On startup, master tries to read it and aborts since the table name has changed. The .META. .tableinfo files are not being created in 0.94.x (fixed for 96 in HBASE-6971; but this can be reproduced when migrating from 0.92 -> 0.94 -> 0.96. Our old users would be affected by this.
{code}
java.lang.IllegalArgumentException: .META. no longer exists. The table has been renamed to hbase:meta
	at org.apache.hadoop.hbase.TableName.valueOf(TableName.java:291)
	at org.apache.hadoop.hbase.TableName.valueOf(TableName.java:283)
	at org.apache.hadoop.hbase.HTableDescriptor.readFields(HTableDescriptor.java:960)
	at org.apache.hadoop.hbase.util.Writables.getWritable(Writables.java:131)
	at org.apache.hadoop.hbase.util.Writables.getWritable(Writables.java:101)
	at org.apache.hadoop.hbase.HTableDescriptor.parseFrom(HTableDescriptor.java:1407)
	at org.apache.hadoop.hbase.util.FSTableDescriptors.readTableDescriptor(FSTableDescriptors.java:521)
	at org.apache.hadoop.hbase.util.FSTableDescriptors.createTableDescriptorForTableDirectory(FSTableDescriptors.java:707)
	at org.apache.hadoop.hbase.util.FSTableDescriptors.createTableDescriptor(FSTableDescriptors.java:683)
	at org.apache.hadoop.hbase.util.FSTableDescriptors.createTableDescriptor(FSTableDescriptors.java:670)
	at org.apache.hadoop.hbase.master.MasterFileSystem.checkRootDir(MasterFileSystem.java:485)
	at org.apache.hadoop.hbase.master.MasterFileSystem.createInitialFileSystemLayout(MasterFileSystem.java:145)
	at org.apache.hadoop.hbase.master.MasterFileSystem.<init>(MasterFileSystem.java:129)
	at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:761)
{code}", ,1,,,,,,
HBASE-9498,"It is caused because master is not up.  The prepare fails:

{code}
2:47:46.573 PM 	ERROR 	com.cloudera.cmon.firehose.AbstractHBasePoller 	

Error polling HBASE-1, error: java.io.IOException: Can't get master address from ZooKeeper; znode data == null
org.apache.hadoop.hbase.MasterNotRunningException: java.io.IOException: Can't get master address from ZooKeeper; znode data == null
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$StubMaker.makeStub(HConnectionManager.java:1641)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$MasterMonitorServiceStubMaker.makeStub(HConnectionManager.java:1667)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getKeepAliveMasterMonitorService(HConnectionManager.java:2152)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.listTables(HConnectionManager.java:2629)
	at org.apache.hadoop.hbase.client.HBaseAdmin.listTables(HBaseAdmin.java:290)
	at com.cloudera.cmf.cdh5client.hbase.HBaseAdminImpl.listTables(HBaseAdminImpl.java:60)
	at com.cloudera.cmon.firehose.HBaseRegionHealthCanaryPoller$CanaryPollerRunnable.run(HBaseRegionHealthCanaryPoller.java:213)
	at com.cloudera.cmon.firehose.HBaseRegionHealthCanaryPoller$CanaryPollerRunnable.run(HBaseRegionHealthCanaryPoller.java:177)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1477)
	at com.cloudera.cmf.cdh5client.security.UserGroupInformationImpl.doAs(UserGroupInformationImpl.java:29)
	at com.cloudera.cmon.firehose.HBaseRegionHealthCanaryPoller.performPoll(HBaseRegionHealthCanaryPoller.java:116)
	at com.cloudera.cmon.firehose.AbstractHBasePoller.handleHbaseService(AbstractHBasePoller.java:339)
	at com.cloudera.cmon.firehose.AbstractHBasePoller.runWithTracking(AbstractHBasePoller.java:215)
	at com.cloudera.cmon.firehose.AbstractHBasePoller.run(AbstractHBasePoller.java:140)
	at com.cloudera.enterprise.PeriodicEnterpriseService$UnexceptionablePeriodicRunnable.doWork(PeriodicEnterpriseService.java:116)
	at com.cloudera.enterprise.PeriodicEnterpriseService$UnexceptionablePeriodicRunnable.run(PeriodicEnterpriseService.java:65)
	at com.cloudera.enterprise.AbstractCDHVersionAwarePeriodicService$4.run(AbstractCDHVersionAwarePeriodicService.java:116)
	at com.cloudera.cmf.cdh5client.CDH5TaskRunner.run(CDH5TaskRunner.java:45)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: Can't get master address from ZooKeeper; znode data == null
	at org.apache.hadoop.hbase.zookeeper.MasterAddressTracker.getMasterAddress(MasterAddressTracker.java:108)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$StubMaker.makeStubNoRetries(HConnectionManager.java:1567)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$StubMaker.makeStub(HConnectionManager.java:1612)
	... 20 more

{code}

Then when done we do the call:

{code}
2:47:51.587 PM 	ERROR 	com.cloudera.cmon.firehose.HBasePoller 	

Encountered exception null
java.lang.NullPointerException
	at org.apache.hadoop.hbase.client.HBaseAdmin$MasterMonitorCallable.close(HBaseAdmin.java:3053)
	at org.apache.hadoop.hbase.client.HBaseAdmin.executeCallable(HBaseAdmin.java:3089)
	at org.apache.hadoop.hbase.client.HBaseAdmin.getClusterStatus(HBaseAdmin.java:2081)
	at com.cloudera.cmf.cdh5client.hbase.HConnectionImpl.getClusterStatus(HConnectionImpl.java:69)
	at com.cloudera.cmon.firehose.HbaseServicePolledStatus.update(HbaseServicePolledStatus.java:137)
	at com.cloudera.cmon.firehose.HBasePoller$1.run(HBasePoller.java:95)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1477)
	at com.cloudera.cmf.cdh5client.security.UserGroupInformationImpl.doAs(UserGroupInformationImpl.java:29)
	at com.cloudera.cmon.firehose.HBasePoller.performPoll(HBasePoller.java:84)
	at com.cloudera.cmon.firehose.AbstractHBasePoller.handleHbaseService(AbstractHBasePoller.java:339)
	at com.cloudera.cmon.firehose.AbstractHBasePoller.runWithTracking(AbstractHBasePoller.java:215)
	at com.cloudera.cmon.firehose.AbstractHBasePoller.run(AbstractHBasePoller.java:140)
	at com.cloudera.enterprise.PeriodicEnterpriseService$UnexceptionablePeriodicRunnable.doWork(PeriodicEnterpriseService.java:116)
	at com.cloudera.enterprise.PeriodicEnterpriseService$UnexceptionablePeriodicRunnable.run(PeriodicEnterpriseService.java:65)
	at com.cloudera.enterprise.AbstractCDHVersionAwarePeriodicService$4.run(AbstractCDHVersionAwarePeriodicService.java:116)
	at com.cloudera.cmf.cdh5client.CDH5TaskRunner.run(CDH5TaskRunner.java:45)
	at java.lang.Thread.run(Thread.java:724)

{code}

Let me fix.  Let me make sure there aren't other prepare/close's that have this issue while at it.", ,,,,,,,
HBASE-9501,"We have a test suite where we turn on distributed log replay for one of test case and turn it off for some test cases. Some times when a cluster isn't clean shut down(there are some RS recovery work left) and cluster restarts, there will be regions in recovery state which is left from previous shut down after the cluster is restarted. ", ,1,1,,,,,
HBASE-9512,"We have a test suite where we turn on distributed log replay for one of test case and turn it off for some test cases. Some times when a cluster isn't clean shut down(there are some RS recovery work left) and cluster restarts, there will be regions in recovery state which is left from previous shut down after the cluster is restarted. ", ,,,,,,,
HBASE-9514,"If a region is assigned before log splitting is done by the server shutdown handler, the edits belonging to this region in the hlogs of the dead server will be lost.

Generally this is not an issue if users don't assign/unassign a region from hbase shell or via hbase admin. These commands are marked for experts only in the hbase shell help too.  However, chaos monkey doesn't care.

If we can prevent from assigning such regions in a bad time, it would make things a little safer.", ,,,,,,,
HBASE-9518,make generating faked key algo more aggressive,1,,,,,,,
HBASE-9519,"we observed a reproducable NPE while scanning special table under special condition in our IntegratedTesting scenario, it was fixed by appling the attached patch.

org.apache.hadoop.hbase.client.ScannerCallable@67ee75a5, java.io.IOException: java.io.IOException: java.lang.NullPointerException
at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:1186)
at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:1175)
at org.apache.hadoop.hbase.regionserver.HRegionServer.openScanner(HRegionServer.java:2391)
at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.apache.hadoop.hbase.ipc.SecureRpcEngine$Server.call(SecureRpcEngine.java:456)
at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1426)
Caused by: java.lang.NullPointerException
at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$EncodedScannerV2.getFirstKeyInBlock(HFileReaderV2.java:1071)
at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.seekBefore(HFileReaderV2.java:547)
at org.apache.hadoop.hbase.io.HalfStoreFileReader$1.seekBefore(HalfStoreFileReader.java:159)
at org.apache.hadoop.hbase.io.HalfStoreFileReader$1.seekBefore(HalfStoreFileReader.java:142)
at org.apache.hadoop.hbase.io.HalfStoreFileReader.getLastKey(HalfStoreFileReader.java:267)
at org.apache.hadoop.hbase.regionserver.StoreFile$Reader.passesKeyRangeFilter(StoreFile.java:1543)
at org.apache.hadoop.hbase.regionserver.StoreFileScanner.shouldUseScanner(StoreFileScanner.java:375)
at org.apache.hadoop.hbase.regionserver.StoreScanner.selectScannersFrom(StoreScanner.java:298)
at org.apache.hadoop.hbase.regionserver.StoreScanner.getScannersNoCompaction(StoreScanner.java:262)
at org.apache.hadoop.hbase.regionserver.StoreScanner.<init>(StoreScanner.java:149)
at org.apache.hadoop.hbase.regionserver.Store.getScanner(Store.java:2122)
at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.<init>(HRegion.java:3460)
at org.apache.hadoop.hbase.regionserver.HRegion.instantiateRegionScanner(HRegion.java:1645)
at org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:1635)
at org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:1610)
at org.apache.hadoop.hbase.regionserver.HRegionServer.openScanner(HRegionServer.java:2377)
... 5 more", ,,,,,,,
HBASE-9522,"In pre-0.96, .META. has .tableinfo files which refer to .META. On startup, master tries to read it and aborts since the table name has changed. The .META. .tableinfo files are not being created in 0.94.x (fixed for 96 in HBASE-6971; but this can be reproduced when migrating from 0.92 -> 0.94 -> 0.96. Our old users would be affected by this.
{code}
java.lang.IllegalArgumentException: .META. no longer exists. The table has been renamed to hbase:meta
	at org.apache.hadoop.hbase.TableName.valueOf(TableName.java:291)
	at org.apache.hadoop.hbase.TableName.valueOf(TableName.java:283)
	at org.apache.hadoop.hbase.HTableDescriptor.readFields(HTableDescriptor.java:960)
	at org.apache.hadoop.hbase.util.Writables.getWritable(Writables.java:131)
	at org.apache.hadoop.hbase.util.Writables.getWritable(Writables.java:101)
	at org.apache.hadoop.hbase.HTableDescriptor.parseFrom(HTableDescriptor.java:1407)
	at org.apache.hadoop.hbase.util.FSTableDescriptors.readTableDescriptor(FSTableDescriptors.java:521)
	at org.apache.hadoop.hbase.util.FSTableDescriptors.createTableDescriptorForTableDirectory(FSTableDescriptors.java:707)
	at org.apache.hadoop.hbase.util.FSTableDescriptors.createTableDescriptor(FSTableDescriptors.java:683)
	at org.apache.hadoop.hbase.util.FSTableDescriptors.createTableDescriptor(FSTableDescriptors.java:670)
	at org.apache.hadoop.hbase.master.MasterFileSystem.checkRootDir(MasterFileSystem.java:485)
	at org.apache.hadoop.hbase.master.MasterFileSystem.createInitialFileSystemLayout(MasterFileSystem.java:145)
	at org.apache.hadoop.hbase.master.MasterFileSystem.<init>(MasterFileSystem.java:129)
	at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:761)
{code}", ,1,,,,,,
HBASE-9524,"I ran into a situation where the CM issued a move for a region just after a region was split. The master went bonkers since the master honored the CM request, and assigned the split region, but subsequently all the region state assumptions on this (split)region was messed up. I started seeing log lines lines like ""THIS SHOULD NOT HAPPEN"". Also, it created other problems - a compaction on original region happened on the new assignee, and then the daughter regions started seeing issues to do with store files missing, etc., etc. 

I will upload the logs shortly.", ,,,,,,,
HBASE-9525,"I ran into a situation where the CM issued a move for a region just after a region was split. The master went bonkers since the master honored the CM request, and assigned the split region, but subsequently all the region state assumptions on this (split)region was messed up. I started seeing log lines lines like ""THIS SHOULD NOT HAPPEN"". Also, it created other problems - a compaction on original region happened on the new assignee, and then the daughter regions started seeing issues to do with store files missing, etc., etc. 

I will upload the logs shortly.", ,,,,,,,
HBASE-9526,"Observed behavior:

Against trunk, using MRv1 with hadoop 1.0.4, r1393290, I am able to run MRv1 jobs (e.g. pi 2 4).

However, when I use it to run MR over HBase jobs, they fail with the stack trace below.

From the trace, the issue seems to be that it cannot find a class that the netty jar contains. This would make sense, given that the dependency jars that we use for the MapReduce job are hard-coded, and that the netty jar is not one of them.

https://github.com/apache/hbase/blob/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java#L519

Strangely, this is only an issue in trunk, not 0.95, even though the code hasn't changed.

Command:
{code}/bin/hbase org.apache.hadoop.hbase.mapreduce.RowCounter sampletable{code}

TT logs (attached)

Output from console running job:
{code}13/09/13 16:02:58 INFO mapred.JobClient: Task Id : attempt_201309131601_0002_m_000000_2, Status : FAILED
java.io.IOException: Cannot create a record reader because of a previous error. Please look at the previous logs lines from the task's full log for more details.
	at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.createRecordReader(TableInputFormatBase.java:119)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.<init>(MapTask.java:489)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:731)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1149)
	at org.apache.hadoop.mapred.Child.main(Child.java:249)

13/09/13 16:03:09 INFO mapred.JobClient: Job complete: job_201309131601_0002
13/09/13 16:03:09 INFO mapred.JobClient: Counters: 7
13/09/13 16:03:09 INFO mapred.JobClient:   Job Counters 
13/09/13 16:03:09 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=29913
13/09/13 16:03:09 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0
13/09/13 16:03:09 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0
13/09/13 16:03:09 INFO mapred.JobClient:     Launched map tasks=4
13/09/13 16:03:09 INFO mapred.JobClient:     Data-local map tasks=4
13/09/13 16:03:09 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=0
13/09/13 16:03:09 INFO mapred.JobClient:     Failed map tasks=1{code}

Expected behavior:
As a stopgap, the netty jar should be included in that list. More generally, there should be a more elegant way to include the jars that are needed.", ,,,,,,,
HBASE-9533,"Observed behavior:

Against trunk, using MRv1 with hadoop 1.0.4, r1393290, I am able to run MRv1 jobs (e.g. pi 2 4).

However, when I use it to run MR over HBase jobs, they fail with the stack trace below.

From the trace, the issue seems to be that it cannot find a class that the netty jar contains. This would make sense, given that the dependency jars that we use for the MapReduce job are hard-coded, and that the netty jar is not one of them.

https://github.com/apache/hbase/blob/trunk/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java#L519

Strangely, this is only an issue in trunk, not 0.95, even though the code hasn't changed.

Command:
{code}/bin/hbase org.apache.hadoop.hbase.mapreduce.RowCounter sampletable{code}

TT logs (attached)

Output from console running job:
{code}13/09/13 16:02:58 INFO mapred.JobClient: Task Id : attempt_201309131601_0002_m_000000_2, Status : FAILED
java.io.IOException: Cannot create a record reader because of a previous error. Please look at the previous logs lines from the task's full log for more details.
	at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.createRecordReader(TableInputFormatBase.java:119)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.<init>(MapTask.java:489)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:731)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1149)
	at org.apache.hadoop.mapred.Child.main(Child.java:249)

13/09/13 16:03:09 INFO mapred.JobClient: Job complete: job_201309131601_0002
13/09/13 16:03:09 INFO mapred.JobClient: Counters: 7
13/09/13 16:03:09 INFO mapred.JobClient:   Job Counters 
13/09/13 16:03:09 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=29913
13/09/13 16:03:09 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0
13/09/13 16:03:09 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0
13/09/13 16:03:09 INFO mapred.JobClient:     Launched map tasks=4
13/09/13 16:03:09 INFO mapred.JobClient:     Data-local map tasks=4
13/09/13 16:03:09 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=0
13/09/13 16:03:09 INFO mapred.JobClient:     Failed map tasks=1{code}

Expected behavior:
As a stopgap, the netty jar should be included in that list. More generally, there should be a more elegant way to include the jars that are needed.", ,1,,,,,,
HBASE-9534,"Coprocessors currently create a full HTable when they want to write. However, we know that coprocessors must run from within an HBase server (either master or RS). For the master, its rare that we are going to be doing performance sensitive operations, but RS calls could be very time-intensive. 

Therefore, we should be able to tell when a call from a CP attempts to talk to the RS on which it lives and just short-circuit to calling that RS, rather than going the long way around (which does the full marshalling/unmarshalling of data, as well as going over the loopback interface).", ,,1,,,,,
HBASE-9535,"Coprocessors currently create a full HTable when they want to write. However, we know that coprocessors must run from within an HBase server (either master or RS). For the master, its rare that we are going to be doing performance sensitive operations, but RS calls could be very time-intensive. 

Therefore, we should be able to tell when a call from a CP attempts to talk to the RS on which it lives and just short-circuit to calling that RS, rather than going the long way around (which does the full marshalling/unmarshalling of data, as well as going over the loopback interface).", ,,1,,,,,
HBASE-9539,"When checking for HFileV1 before upgrading to 96, the snapshot file links tries to read from post-namespace locations. The migration script needs to be run on 94 cluster, and it requires reading the old (94) layout to check for HFileV1.

{code}
Got exception while reading trailer for file: hdfs://xxx:41020/cops/cluster_collection_events_snapshot/2086db948c484be62dcd76c170fe0b17/meta/cluster_collection_event=42037b88dbc34abff6cbfbb1fde2c900-c24b358ddd2f4429a7287258142841a2
java.io.FileNotFoundException: Unable to open link: org.apache.hadoop.hbase.io.HFileLink locations=[hdfs://xxx:41020/hbase-96/data/default/cluster_collection_event/42037b88dbc34abff6cbfbb1fde2c900/meta/c24b358ddd2f4429a7287258142841a2, hdfs://xxx:41020/hbase-96/.tmp/data/default/cluster_collection_event/42037b88dbc34abff6cbfbb1fde2c900/meta/c24b358ddd2f4429a7287258142841a2, hdfs://xxx:41020/hbase-96/archive/data/default/cluster_collection_event/42037b88dbc34abff6cbfbb1fde2c900/meta/c24b358ddd2f4429a7287258142841a2]
{code}", ,1,,,,,,
HBASE-9541,"Benot found an npe migrating asynchbase to 0.96 exercising icv:

{code}
2013-09-14 23:27:17,305 WARN  [RpcServer.handler=10,port=60934]
ipc.RpcServer: RpcServer.handler=10,port=60934: caught:
java.lang.NullPointerException
        at org.apache.hadoop.hbase.ipc.RpcServer.channelWrite(RpcServer.java:2346)
        at org.apache.hadoop.hbase.ipc.RpcServer$Responder.processResponse(RpcServer.java:985)
        at org.apache.hadoop.hbase.ipc.RpcServer$Responder.doRespond(RpcServer.java:1062)
        at org.apache.hadoop.hbase.ipc.RpcServer$Call.sendResponseIfReady(RpcServer.java:478)
        at org.apache.hadoop.hbase.ipc.RpcServer$Handler.run(RpcServer.java:1875)
{code}

This is stalling progress on asynchbase migration.", ,1,,,,,,
HBASE-9552,A null check was missed. This leads to NPEs in the client tool when the tool generates the favored nodes assignment and tries to update the RegionServers with the information., ,,,,,,,
HBASE-9554,"From http://54.241.6.143/job/HBase-TRUNK/org.apache.hbase$hbase-server/496/testReport/org.apache.hadoop.hbase.util.hbck/TestOfflineMetaRebuildOverlap/testMetaRebuildOverlapFail/ :
{code}
java.lang.Exception: test timed out after 120000 milliseconds
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:148)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:94)
	at org.apache.hadoop.hbase.client.HBaseAdmin.executeCallable(HBaseAdmin.java:3153)
	at org.apache.hadoop.hbase.client.HBaseAdmin.unassign(HBaseAdmin.java:1714)
	at org.apache.hadoop.hbase.util.hbck.OfflineMetaRebuildTestCore.wipeOutMeta(OfflineMetaRebuildTestCore.java:242)
	at org.apache.hadoop.hbase.util.hbck.TestOfflineMetaRebuildOverlap.testMetaRebuildOverlapFail(TestOfflineMetaRebuildOverlap.java:54)
...
2013-09-17 00:59:52,928 ERROR [FifoRpcScheduler.handler1-thread-2] ipc.RpcServer(2016): Unexpected throwable object 
java.lang.NullPointerException
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2301)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2381)
	at org.apache.hadoop.hbase.master.HMaster.unassignRegion(HMaster.java:2499)
	at org.apache.hadoop.hbase.protobuf.generated.MasterAdminProtos$MasterAdminService$2.callBlockingMethod(MasterAdminProtos.java:32854)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:1979)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:90)
	at org.apache.hadoop.hbase.ipc.FifoRpcScheduler$1.run(FifoRpcScheduler.java:73)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
{code}", ,,,,,,,
HBASE-9562,"When checking for HFileV1 before upgrading to 96, the snapshot file links tries to read from post-namespace locations. The migration script needs to be run on 94 cluster, and it requires reading the old (94) layout to check for HFileV1.

{code}
Got exception while reading trailer for file: hdfs://xxx:41020/cops/cluster_collection_events_snapshot/2086db948c484be62dcd76c170fe0b17/meta/cluster_collection_event=42037b88dbc34abff6cbfbb1fde2c900-c24b358ddd2f4429a7287258142841a2
java.io.FileNotFoundException: Unable to open link: org.apache.hadoop.hbase.io.HFileLink locations=[hdfs://xxx:41020/hbase-96/data/default/cluster_collection_event/42037b88dbc34abff6cbfbb1fde2c900/meta/c24b358ddd2f4429a7287258142841a2, hdfs://xxx:41020/hbase-96/.tmp/data/default/cluster_collection_event/42037b88dbc34abff6cbfbb1fde2c900/meta/c24b358ddd2f4429a7287258142841a2, hdfs://xxx:41020/hbase-96/archive/data/default/cluster_collection_event/42037b88dbc34abff6cbfbb1fde2c900/meta/c24b358ddd2f4429a7287258142841a2]
{code}", ,1,,,,,,
HBASE-9563,I've seen this several times where a master didn't autorestart because zk cleaner failed.  We should still restart the daemon even if it's not possible to clean the zk nodes., ,,,,,,,
HBASE-9578,"HBASE-7544 will protect key and value data on the server from accidental leakage by way of improperly disposed disks, improper direct filesystem access, or incorrect HDFS permissions. There are also use cases where sensitive data stored in a table or column family by a given user or application should be protected from all others, and the combination of transparent server-side storage encryption and transport security (SASL auth-conf) is still not sufficient. These instances call for a client side per-cell encryption feature, given the following additional observations:
The scope of transmission, distribution, and storage of private key material should be as limited as possible. The server is a centralized target (even in the case of an HBase cluster) where the scope of damage from a compromise is magnified if user key material also resides there or can be intercepted after compromise. Where keys are stored in hardware devices, e.g. smartcards, getting the keys to the server may be not possible anyway.
A client system is far more likely than a contended shared server resource to have necessary available CPU cycles for per-operation cryptographic overheads.
For some cases we might not care so much about the second item, but the first is very important.
I have an implementation of per cell client side encryption as an encrypting HTable wrapper which I could contribute if there is interest.
This JIRA is also about brainstorming how to do better than that.",1,,,,,,,
HBASE-9582,"I have this Scan


Scan scan = new Scan();
scan.setCaching(50);
scan.setCacheBlocks(false);
scan.setMaxVersions();
scan.setTimeRange(Long.valueOf(args[7] + ""000""),Long.valueOf(args[8] + ""000""));
SingleColumnValueFilter filter = new SingleColumnValueFilter(Bytes.toBytes(args[1]),Bytes.toBytes(args[2]),CompareFilter.CompareOp.EQUAL,new BinaryComparator(Bytes.toBytes(args[3])));
filter.setFilterIfMissing(true);
scan.setFilter(filter);


It works without any warns and errors in command line. But when regionservers CPU is high loaded, Scan with the same parameters (Column, value, timestamps) gives different results. For example
first time - Map output records=571374   
second time -  Map output records=777620
third time - Map output records=776099

Regionservers log includes such WARNs:

2013-09-19 13:29:44,827 WARN org.apache.hadoop.ipc.HBaseServer: (responseTooSlow): {""processingtimems"":30759,""call"":""next(-308003858163246780, 10), rpc version=1, client version=29, methodsFingerPrint=-1368823753"",""client"":""10.10.54.22:53361"",""starttimems"":1379582954067,""queuetimems"":1,""class"":""HRegionServer"",""responsesize"":51343,""method"":""next""}

and these ERRORs:
2013-09-19 13:26:18,202 ERROR org.apache.hadoop.hbase.regionserver.HRegionServer: 
org.apache.hadoop.hbase.ipc.CallerDisconnectedException: Aborting call next(-9095740742796333934, 10), rpc version=1, client version=29, methodsFingerPrint=-1368823753 from 10.10.54.22:32914 after 60059 ms, since caller disconnected                                                                                  
        at org.apache.hadoop.hbase.ipc.HBaseServer$Call.throwExceptionIfCallerDisconnected(HBaseServer.java:436)                                             
        at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.nextInternal(HRegion.java:3723)                                                    
        at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.nextRaw(HRegion.java:3643)                                                         
        at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.nextRaw(HRegion.java:3635)                                                         
        at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:2483)                                                                  
        at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)                                                                                      
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)                                                             
        at java.lang.reflect.Method.invoke(Method.java:597)                                                                                                  
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:320)                                                             
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1426)


    
When regionservers CPU is not loaded, Scan gives same results every times.
In this case regionservers log doesn't include any WARNs.

Why does it happen? I want to be sure that Scan give me all the data that I request no matter how CPU is using now.
", ,1,,,,,,
HBASE-9584,"If the coprocessor isn't on the server hosting root/meta/the target HTable, it will fall back to doing the usual HConnection lookup. 

However, that call then calls the overloaded getHRegionConnection methods, which HBASE-9534 previously made unsupported for Coprocessors. The initial idea was to limit the number of paths that we could lookup a region, but it was just incorrectly done.", ,,,,,,,
HBASE-9593,"In some of our tests we found that regionserer always showing online in master UI but its actually dead.
If region server went down in the middle following steps then the region server always showing in master online servers list.
1) register to master
2) create  ephemeral znode

Since no notification from zookeeper, master is not removing the expired server from online servers list.
Assignments will fail if the RS is selected as destination server.
Some cases ROOT or META also wont be assigned if the RS is randomly selected every time need to wait for timeout.

Here are the logs:
1) HOST-10-18-40-153 is registered to master
{code}
2013-09-19 19:47:41,123 DEBUG org.apache.hadoop.hbase.master.ServerManager: STARTUP: Server HOST-10-18-40-153,61020,1379600260255 came back up, removed it from the dead servers list
2013-09-19 19:47:41,123 INFO org.apache.hadoop.hbase.master.ServerManager: Registering server=HOST-10-18-40-153,61020,1379600260255
{code}
{code}
2013-09-19 19:47:41,119 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Connected to master at HOST-10-18-40-153/10.18.40.153:61000
2013-09-19 19:47:41,119 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Telling master at HOST-10-18-40-153,61000,1379600055284 that we are up with port=61020, startcode=1379600260255
{code}
2) Terminated before creating ephemeral node.
{code}
Thu Sep 19 19:47:41 IST 2013 Terminating regionserver
{code}
3) The RS can be selected for assignment and they will fail.
{code}
2013-09-19 19:47:54,049 WARN org.apache.hadoop.hbase.master.AssignmentManager: Failed assignment of -ROOT-,,0.70236052 to HOST-10-18-40-153,61020,1379600260255, trying to assign elsewhere instead; retry=0
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupConnection(HBaseClient.java:390)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupIOstreams(HBaseClient.java:436)
	at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:1127)
	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:974)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:86)
	at $Proxy15.openRegion(Unknown Source)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionOpen(ServerManager.java:533)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1734)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1431)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1406)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1401)
	at org.apache.hadoop.hbase.master.AssignmentManager.assignRoot(AssignmentManager.java:2374)
	at org.apache.hadoop.hbase.master.handler.MetaServerShutdownHandler.verifyAndAssignRoot(MetaServerShutdownHandler.java:136)
	at org.apache.hadoop.hbase.master.handler.MetaServerShutdownHandler.verifyAndAssignRootWithRetries(MetaServerShutdownHandler.java:160)
	at org.apache.hadoop.hbase.master.handler.MetaServerShutdownHandler.process(MetaServerShutdownHandler.java:82)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:175)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
2013-09-19 19:47:54,050 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Found an existing plan for -ROOT-,,0.70236052 destination server is HOST-10-18-40-153,61020,1379600260255
2013-09-19 19:47:54,050 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: No previous transition plan was found (or we are ignoring an existing plan) for -ROOT-,,0.70236052 so generated a random one; hri=-ROOT-,,0.70236052, src=, dest=HOST-10-18-40-153,61020,1379600260255; 1 (online=1, available=1) available servers
2013-09-19 19:47:54,050 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:61000-0x14135a277ff017d Creating (or updating) unassigned node for 70236052 with OFFLINE state
2013-09-19 19:47:54,070 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=M_ZK_REGION_OFFLINE, server=HOST-10-18-40-153,61000,1379600055284, region=70236052/-ROOT-
2013-09-19 19:47:54,071 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Found an existing plan for -ROOT-,,0.70236052 destination server is HOST-10-18-40-153,61020,1379600260255
2013-09-19 19:47:54,071 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Using pre-existing plan for region -ROOT-,,0.70236052; plan=hri=-ROOT-,,0.70236052, src=, dest=HOST-10-18-40-153,61020,1379600260255
2013-09-19 19:47:54,071 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region -ROOT-,,0.70236052 to HOST-10-18-40-153,61020,1379600260255
2013-09-19 19:47:54,072 WARN org.apache.hadoop.hbase.master.AssignmentManager: Failed assignment of -ROOT-,,0.70236052 to HOST-10-18-40-153,61020,1379600260255, trying to assign elsewhere instead; retry=1
org.apache.hadoop.hbase.ipc.HBaseClient$FailedServerException: This server is in the failed servers list: HOST-10-18-40-153/10.18.40.153:61020
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupIOstreams(HBaseClient.java:425)
	at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:1127)
	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:974)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:86)
	at $Proxy15.openRegion(Unknown Source)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionOpen(ServerManager.java:533)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1734)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1431)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1406)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1401)
	at org.apache.hadoop.hbase.master.AssignmentManager.assignRoot(AssignmentManager.java:2374)
	at org.apache.hadoop.hbase.master.handler.MetaServerShutdownHandler.verifyAndAssignRoot(MetaServerShutdownHandler.java:136)
	at org.apache.hadoop.hbase.master.handler.MetaServerShutdownHandler.verifyAndAssignRootWithRetries(MetaServerShutdownHandler.java:160)
	at org.apache.hadoop.hbase.master.handler.MetaServerShutdownHandler.process(MetaServerShutdownHandler.java:82)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:175)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
2013-09-19 19:47:54,072 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Found an existing plan for -ROOT-,,0.70236052 destination server is HOST-10-18-40-153,61020,1379600260255
{code}", ,,,,,,,
HBASE-9598,"In SplitLogManager#resubmit():
{code}
    if (directive != FORCE) {
      task.unforcedResubmits++;
    }
{code}
task.unforcedResubmits is volatile but the above increment doesn't have proper synchronization.", ,,,,,,,
HBASE-9602,"It looks like we cannot show the master's web ui at start time when there are logs to split because we can't reach the namespace regions.

So it means that you can't see how things are progressing without tailing the log while waiting on your cluster to boot up. This wasn't the case in 0.94

See this jstack:

{noformat}
""606214580@qtp-2001431298-3"" prio=10 tid=0x00007f6ac8040000 nid=0x7b1 in Object.wait() [0x00007f6aa82bf000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000000bc0c1460> (a org.apache.hadoop.hbase.ipc.RpcClient$Call)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1416)
	- locked <0x00000000bc0c1460> (a org.apache.hadoop.hbase.ipc.RpcClient$Call)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1634)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1691)
	at org.apache.hadoop.hbase.protobuf.generated.MasterAdminProtos$MasterAdminService$BlockingStub.listTableDescriptorsByNamespace(MasterAdminProtos.java:35031)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$5.listTableDescriptorsByNamespace(HConnectionManager.java:2181)
	at org.apache.hadoop.hbase.client.HBaseAdmin$22.call(HBaseAdmin.java:2265)
	at org.apache.hadoop.hbase.client.HBaseAdmin$22.call(HBaseAdmin.java:2262)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:116)
	- locked <0x00000000c09baf20> (a org.apache.hadoop.hbase.client.RpcRetryingCaller)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:94)
	- locked <0x00000000c09baf20> (a org.apache.hadoop.hbase.client.RpcRetryingCaller)
	at org.apache.hadoop.hbase.client.HBaseAdmin.executeCallable(HBaseAdmin.java:3155)
	at org.apache.hadoop.hbase.client.HBaseAdmin.listTableDescriptorsByNamespace(HBaseAdmin.java:2261)
	at org.apache.hadoop.hbase.tmpl.master.MasterStatusTmplImpl.__jamon_innerUnit__catalogTables(MasterStatusTmplImpl.java:461)
	at org.apache.hadoop.hbase.tmpl.master.MasterStatusTmplImpl.renderNoFlush(MasterStatusTmplImpl.java:270)
	at org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.renderNoFlush(MasterStatusTmpl.java:382)
	at org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.render(MasterStatusTmpl.java:372)
	at org.apache.hadoop.hbase.master.MasterStatusServlet.doGet(MasterStatusServlet.java:95)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
	at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:850)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
...
""RpcServer.handler=28,port=60000"" daemon prio=10 tid=0x00007f6ad08a5800 nid=0x77e waiting on condition [0x00007f6aa97d5000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:148)
	- locked <0x00000000c0909178> (a org.apache.hadoop.hbase.client.RpcRetryingCaller)
	at org.apache.hadoop.hbase.client.HTable.get(HTable.java:760)
	at org.apache.hadoop.hbase.master.TableNamespaceManager.get(TableNamespaceManager.java:171)
	at org.apache.hadoop.hbase.master.TableNamespaceManager.getNamespaceTable(TableNamespaceManager.java:119)
	- locked <0x00000000c1958890> (a org.apache.hadoop.hbase.master.TableNamespaceManager)
	at org.apache.hadoop.hbase.master.TableNamespaceManager.get(TableNamespaceManager.java:155)
	- locked <0x00000000c1958890> (a org.apache.hadoop.hbase.master.TableNamespaceManager)
	at org.apache.hadoop.hbase.master.HMaster.getNamespaceDescriptor(HMaster.java:3088)
	at org.apache.hadoop.hbase.master.HMaster.listTableDescriptorsByNamespace(HMaster.java:3102)
	at org.apache.hadoop.hbase.master.HMaster.listTableDescriptorsByNamespace(HMaster.java:3012)
	at org.apache.hadoop.hbase.protobuf.generated.MasterAdminProtos$MasterAdminService$2.callBlockingMethod(MasterAdminProtos.java:32908)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2146)
	at org.apache.hadoop.hbase.ipc.RpcServer$Handler.run(RpcServer.java:1851)
{noformat}", ,,,,,,,
HBASE-9603,the done field of IsRestoreSnapshotDoneRequest is set to true which cause the restoreSnapshot() to not wait until the restore is done. resulting in an async behavior., ,1,,,,,,
HBASE-9606,"Coprocessors currently create a full HTable when they want to write. However, we know that coprocessors must run from within an HBase server (either master or RS). For the master, its rare that we are going to be doing performance sensitive operations, but RS calls could be very time-intensive. 

Therefore, we should be able to tell when a call from a CP attempts to talk to the RS on which it lives and just short-circuit to calling that RS, rather than going the long way around (which does the full marshalling/unmarshalling of data, as well as going over the loopback interface).", ,,1,,,,,
HBASE-9607,"Take snapshot s1 of table t1 which has some data
Drop t1
Clone snapshot s1 to t1
Disable t1
Restore s1 to t1
Enable t1

At this moment, scan 't1' returns nothing.
", ,,,,,,,
HBASE-9629,"Take snapshot s1 of table t1 which has some data
Drop t1
Clone snapshot s1 to t1
Disable t1
Restore s1 to t1
Enable t1

At this moment, scan 't1' returns nothing.
", ,,,,,,,
HBASE-9634,"{noformat}

HBase Table few regions are not getting recovered from the 'Transition'/'OFFLINE state'

Test Procedure:
1. Setup Non HA Hadoop Cluster with two nodes (Node1-XX.XX.XX.XX,  Node2-YY.YY.YY.YY)
2. Install Zookeeper & HRegionServer in Node-1
3. Install HMaster & HRegionServer in Node-2
4. From Node2 create HBase Table ( table name 't1' with one column family 'cf1' )
5. Perform addrecord 99649 rows 
6. Perform kill and restart of Node1 Region Server & Node2 Region Server in a loop for 10-20 times

2013-09-23 18:28:06,610 INFO org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Opening of region {NAME => 't1,row507465,1379937224590.2d9fad2aee78103f928d8c7fe16ba6cd.', STARTKEY => 'row507465', ENDKEY => 'row508987', ENCODED => 2d9fad2aee78103f928d8c7fe16ba6cd,} failed, marking as FAILED_OPEN in ZK

2013-09-23 18:46:12,160 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Instantiated t1,row507465,1379937224590.2d9fad2aee78103f928d8c7fe16ba6cd.
2013-09-23 18:46:12,160 ERROR org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Failed open of region=t1,row507465,1379937224590.2d9fad2aee78103f928d8c7fe16ba6cd., starting to roll back the global memstore size.

{noformat}  ", ,1,,,,,,
HBASE-9635,"{noformat}
HBase Table regions are not getting assigned to the new region server for a period of 30 minutes (when the existing region server not able to handle the load)


Procedure:
1. Setup Non HA Hadoop Cluster with two nodes (Node1-XX.XX.XX.XX,  Node2-YY.YY.YY.YY)
2. Install Zookeeper & HRegionServer in Node-1
3. Install HMaster & HRegionServer in Node-2
4. From Node2 create HBase Table ( table name 't1' with one column family 'cf1' )
5. Perform addrecord 99649 rows 
6. kill both the node Region Server and limit the Node1 Region Server FD to 600
7. Start only the Node1 Region server ==> so that FD exhaust can happen in Node1 Region Server
8. After some 5-10 minuites start the Node2 Region Server

===> Huge number of regions of table 't1' are in OPENING state, which are not getting re assigned to the Node2 region server which is free. 

===> When the new region server comes up then the master should detect and allocate the open failed regions to the region server (here it is staying the OPENINING state for 30 minutes which will have huge impcat user app which makes use of this table)



2013-09-23 18:46:12,160 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Instantiated t1,row507465,1379937224590.2d9fad2aee78103f928d8c7fe16ba6cd.
2013-09-23 18:46:12,160 ERROR org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Failed open of region=t1,row507465,1379937224590.2d9fad2aee78103f928d8c7fe16ba6cd., starting to roll back the global memstore size.

2013-09-23 18:50:55,284 WARN org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_hb_rs_HOST-XX.XX.XX.XX,61020,1379940823286_-641204614_48] for 309 seconds.  Will retry shortly ...
java.io.IOException: Failed on local exception: java.net.SocketException: Too many open files; Host Details : local host is: ""HOST-XX.XX.XX.XX/XX.XX.XX.XX""; destination host is: ""HOST-XX.XX.XX.XX"":8020;
        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
        at org.apache.hadoop.ipc.Client.call(Client.java:1351)
        at org.apache.hadoop.ipc.Client.call(Client.java:1300)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
        at $Proxy13.renewLease(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:188)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
        at $Proxy13.renewLease(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:522)
        at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:679)
        at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
        at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
        at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
        at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.net.SocketException: Too many open files
        at sun.nio.ch.Net.socket0(Native Method)
        at sun.nio.ch.Net.socket(Net.java:97)
        at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:84)
        at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:37)
        at java.nio.channels.SocketChannel.open(SocketChannel.java:105)
        at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62)
        at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:523)
        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:642)
        at org.apache.hadoop.ipc.Client$Connection.access$2600(Client.java:314)
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1399)
        at org.apache.hadoop.ipc.Client.call(Client.java:1318)
        ... 16 more
2013-09-23 18:50:56,285 WARN org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_hb_rs_HOST-XX.XX.XX.XX,61020,1379940823286_-641204614_48] for 310 seconds.  Will retry shortly ...
java.io.IOException: Failed on local exception: java.net.SocketException: Too many open files; Host Details : local host is: ""HOST-XX.XX.XX.XX/XX.XX.XX.XX""; destination host is: ""HOST-XX.XX.XX.XX"":8020;
        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
        at org.apache.hadoop.ipc.Client.call(Client.java:1351)
        at org.apache.hadoop.ipc.Client.call(Client.java:1300)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
       at $Proxy13.renewLease(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:188)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
        at $Proxy13.renewLease(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:522)
        at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:679)
        at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
        at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
        at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
        at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.net.SocketException: Too many open files
        at sun.nio.ch.Net.socket0(Native Method)
        at sun.nio.ch.Net.socket(Net.java:97)
        at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:84)
        at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:37)
        at java.nio.channels.SocketChannel.open(SocketChannel.java:105)
        at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62)
        at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:523)
        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:642)
        at org.apache.hadoop.ipc.Client$Connection.access$2600(Client.java:314)
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1399)
        at org.apache.hadoop.ipc.Client.call(Client.java:1318)
        ... 16 more
2013-09-23 18:50:57,287 WARN org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_hb_rs_HOST-XX.XX.XX.XX,61020,1379940823286_-641204614_48] for 311 seconds.  Will retry shortly ...
java.io.IOException: Failed on local exception: java.net.SocketException: Too many open files; Host Details : local host is: ""HOST-XX.XX.XX.XX/XX.XX.XX.XX""; destination host is: ""HOST-XX.XX.XX.XX"":8020;







{noformat}
", ,1,1,,,,,
HBASE-9639,"When running a bulk load on a secure environment and loading data into the first region of a table, the request to load the HFile set is dispatched to all Regions for the table. This is reproduced consistently by running IntegrationTestBulkLoad on a secure cluster. The load fails with an exception that looks like:

{noformat}
2013-08-30 07:37:22,993 INFO  [main] mapreduce.LoadIncrementalHFiles: Split occured while grouping HFiles, retry attempt 1 with 3 files remaining to group or split
2013-08-30 07:37:22,999 ERROR [main] mapreduce.LoadIncrementalHFiles: IOException during splitting
java.util.concurrent.ExecutionException: java.io.FileNotFoundException: File does not exist: /user/hbase/test-data/c45ddfe9-ee30-4d32-8042-928db12b1cee/IntegrationTestBulkLoad-0/L/bf41ea13997b4e228d05e67ba7b1b686
at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)
at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:51)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsUpdateTimes(FSNamesystem.java:1489)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1438)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1418)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1392)
at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:438)
at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:269)
at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:59566)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2048)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:396)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1477)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2042)
at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
at java.util.concurrent.FutureTask.get(FutureTask.java:83)
at org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.groupOrSplitPhase(LoadIncrementalHFiles.java:403)
at org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.doBulkLoad(LoadIncrementalHFiles.java:284)
at org.apache.hadoop.hbase.mapreduce.IntegrationTestBulkLoad.runLinkedListMRJob(IntegrationTestBulkLoad.java:200)
at org.apache.hadoop.hbase.mapreduce.IntegrationTestBulkLoad.testBulkLoad(IntegrationTestBulkLoad.java:133)
{noformat}",1,,,,,,,
HBASE-9645,"I upgrade my hbase cluster to 0.94.10 three weeks ago, and this case happened several days after that. I change the bug's priority to 'Critical' because every  time it happens, a regionserver halt down. All of them have the same log:
{noformat}
ERROR org.apache.hadoop.hbase.regionserver.wal.HLog: Logic Error Snapshot seq id from earlier flush still present! for region c0d88db4ce3606842fbec9d34c38f707 overwritten oldseq=80114270537with new seq=80115066829
{noformat}
I check the code finding that it locates at HLog.startCacheFlush method. The 'lastSeqWritten' has been locked. Maybe something wrong happened outside the HLog that change it by mistake.", ,,,,,,,
HBASE-9648,"There's a shortcut in compaction selection that causes the selection of expired store files to quickly delete.
However, there's also the code that ensures we write at least one file to preserve seqnum. This new empty file is ""expired"", because it has no data, presumably.
So it's collected again, etc.
This affects 94, probably also 96.", ,,,,,,,
HBASE-9649,"Should instead ""simply"" display ""null""...

{code}
/usr/lib/hbase/bin/hbase org.apache.hadoop.hbase.io.hfile.HFile -m -s -v -f
/hbase/compound3/5ab5fdfcf2aff2633e1d6d5089c96aa2/d/fca0882dc7624342a8f4fce4b89420ff
13/09/24 12:33:40 INFO util.ChecksumType: Checksum using
org.apache.hadoop.util.PureJavaCrc32
Scanning ->
/hbase/compound3/5ab5fdfcf2aff2633e1d6d5089c96aa2/d/fca0882dc7624342a8f4fce4b89420ff
13/09/24 12:33:41 INFO hfile.CacheConfig: Allocating LruBlockCache with
maximum size 2.9g
13/09/24 12:33:41 ERROR metrics.SchemaMetrics: Inconsistent configuration.
Previous configuration for using table name in metrics: true, new
configuration: false
13/09/24 12:33:41 WARN snappy.LoadSnappy: Snappy native library is available
13/09/24 12:33:41 INFO util.NativeCodeLoader: Loaded the native-hadoop
library
13/09/24 12:33:41 INFO snappy.LoadSnappy: Snappy native library loaded
13/09/24 12:33:41 INFO compress.CodecPool: Got brand-new decompressor
Block index size as per heapsize: 336
Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.hadoop.hbase.KeyValue.keyToString(KeyValue.java:716)
        at
org.apache.hadoop.hbase.io.hfile.AbstractHFileReader.toStringFirstKey(AbstractHFileReader.java:138)
        at
org.apache.hadoop.hbase.io.hfile.AbstractHFileReader.toString(AbstractHFileReader.java:149)
        at
org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter.printMeta(HFilePrettyPrinter.java:318)
        at
org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter.processFile(HFilePrettyPrinter.java:234)
        at
org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter.run(HFilePrettyPrinter.java:189)
        at org.apache.hadoop.hbase.io.hfile.HFile.main(HFile.java:756)

{code}", ,,,,,,,
HBASE-9657,"In FSHLog#syncer(), we have this comment:
{code}
      // TODO: preserving the old behavior for now, but this check is strange. It's not
      //       protected by any locks here, so for all we know rolling locks might start
      //       as soon as we enter the ""if"". Is this best-effort optimization check?
      if (!this.logRollRunning) {
        checkLowReplication();
{code}
The implication is that checkLowReplication() may be running when FSHLog#rollWriter() is also running.", ,,,,,,,
HBASE-9665,"In summary, a server dies and its regions are re-assigned. While right before SSH, balancer is starting assign one region on the server to somewhere. 

The balancer assignment got preempted by the SSH assignment:
{code}
2013-09-25 11:55:32,854 INFO Priority.RpcServer.handler=7,port=60020 regionserver.HRegionServer: Received CLOSE for the region:6deb1bfefe8cbdb443084efe919fdeb7 , which we are already trying to OPEN. Cancelling OPENING.
{code}

The SSH assignment(by GeneralBulkAssigner) failed too due to:
{code}
2013-09-25 11:55:32,927 WARN  [RS_OPEN_REGION-hor15n09:60020-2] zookeeper.ZKAssign: regionserver:60020-0x14153d449d30ad0 Attempt to transition the unassigned node for 6deb1bfefe8cbdb443084efe919fdeb7 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING failed, the server that tried to transition was hor15n09.gq1.ygridcore.net,60020,1380109280320 not the expected hor15n07.gq1.ygridcore.net,60020,1380109890414
{code}

In the end, the region 6deb1bfefe8cbdb443084efe919fdeb7 is lost.


Below is the master log, you can see both balancer and SSH try to assign the region around the same time:

{code}
2013-09-25 11:55:32,731 INFO  [MASTER_SERVER_OPERATIONS-hor15n05:60000-4] master.RegionStates: Transitioning {6deb1bfefe8cbdb443084efe919fdeb7 state=PENDING_CLOSE, ts=1380110132710, server=hor15n12.gq1.ygridcore.net,60020,1380109596307} will be handled by SSH for hor15n12.gq1.ygridcore.net,60020,1380109596307

...

2013-09-25 11:55:32,849 INFO  [hor15n05.gq1.ygridcore.net,60000,1380108611483-BalancerChore] master.RegionStates: Transitioned {6deb1bfefe8cbdb443084efe919fdeb7 state=OFFLINE, ts=1380110132768, server=null} to {6deb1bfefe8cbdb443084efe919fdeb7 state=PENDING_OPEN, ts=1380110132849, server=hor15n07.gq1.ygridcore.net,60020,1380109890414}

...

2013-09-25 11:55:32,898 INFO  [hor15n05.gq1.ygridcore.net,60000,1380108611483-GeneralBulkAssigner-1] master.RegionStates: Transitioned {6deb1bfefe8cbdb443084efe919fdeb7 state=OFFLINE, ts=1380110132861, server=null} to {6deb1bfefe8cbdb443084efe919fdeb7 state=PENDING_OPEN, ts=1380110132898, server=hor15n09.gq1.ygridcore.net,60020,1380109280320}
{code}

Since SSH force region assignment while it doesn't recreate offline znode, the later region opening would fail with the following error. I'm suggesting to recreate offline znode when we force a region assignment(forceNewPlan=true) with low impact.

{code}
2013-09-25 11:55:32,927 WARN  [RS_OPEN_REGION-hor15n09:60020-2] zookeeper.ZKAssign: regionserver:60020-0x14153d449d30ad0 Attempt to transition the unassigned node for 6deb1bfefe8cbdb443084efe919fdeb7 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING failed, the server that tried to transition was hor15n09.gq1.ygridcore.net,60020,1380109280320 not the expected hor15n07.gq1.ygridcore.net,60020,1380109890414
{code}

", ,,,,,,,
HBASE-9668,"Coprocessors currently create a full HTable when they want to write. However, we know that coprocessors must run from within an HBase server (either master or RS). For the master, its rare that we are going to be doing performance sensitive operations, but RS calls could be very time-intensive. 

Therefore, we should be able to tell when a call from a CP attempts to talk to the RS on which it lives and just short-circuit to calling that RS, rather than going the long way around (which does the full marshalling/unmarshalling of data, as well as going over the loopback interface).", ,,1,,,,,
HBASE-9676,"hbase.client.max.total.tasks allows to control the number of tasks in progress. But when adding tasks, we take only into account the number of tasks currently running, not the one we're creating.", ,1,,,,,,
HBASE-9687,"First line below was the command, launched from gateway machine:
{code}
$ hbase org.apache.hadoop.hbase.snapshot.ExportSnapshot -Dfs.default.name=hdfs://hor17n25:8020/ -Dhbase.rootdir=hdfs://hor17n25:8020/apps/hbase/data/ -snapshot snapshot_tableone -copy-to hdfs://hor15n02:8020/apps/hbase/data/ -mappers 1

13/09/30 17:23:56 WARN conf.Configuration: hbase-site.xml:an attempt to override final parameter: dfs.support.append; Ignoring.
13/09/30 17:23:57 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
13/09/30 17:23:57 INFO util.FSVisitor: No logs under directory:hdfs://hor17n25:8020/apps/hbase/data/.hbase-snapshot/snapshot_tableone/WALs
13/09/30 17:23:57 WARN conf.Configuration: hbase-site.xml:an attempt to override final parameter: dfs.support.append; Ignoring.
13/09/30 17:23:57 INFO client.RMProxy: Connecting to ResourceManager at hor17n25/:8032
13/09/30 17:23:57 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
13/09/30 17:23:57 INFO compress.CodecPool: Got brand-new compressor .deflate
13/09/30 17:23:57 WARN conf.Configuration: hbase-site.xml:an attempt to override final parameter: dfs.support.append; Ignoring.
13/09/30 17:23:57 INFO client.RMProxy: Connecting to ResourceManager at hor17n25/:8032
13/09/30 17:23:58 INFO input.FileInputFormat: Total input paths to process : 1
13/09/30 17:23:58 INFO mapreduce.JobSubmitter: number of splits:1
13/09/30 17:23:58 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name
13/09/30 17:23:58 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
13/09/30 17:23:58 INFO Configuration.deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS
13/09/30 17:23:58 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
13/09/30 17:23:58 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
13/09/30 17:23:58 INFO Configuration.deprecation: mapreduce.map.class is deprecated. Instead, use mapreduce.job.map.class
13/09/30 17:23:58 INFO Configuration.deprecation: mapred.job.name is deprecated. Instead, use mapreduce.job.name
13/09/30 17:23:58 INFO Configuration.deprecation: mapreduce.inputformat.class is deprecated. Instead, use mapreduce.job.inputformat.class
13/09/30 17:23:58 INFO Configuration.deprecation: mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir
13/09/30 17:23:58 INFO Configuration.deprecation: mapreduce.outputformat.class is deprecated. Instead, use mapreduce.job.outputformat.class
13/09/30 17:23:58 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
13/09/30 17:23:58 INFO Configuration.deprecation: mapred.working.dir is deprecated. Instead, use mapreduce.job.working.dir
13/09/30 17:23:58 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1380335340676_0003
13/09/30 17:23:58 INFO impl.YarnClientImpl: Submitted application application_1380335340676_0003 to ResourceManager at hor17n25/:8032
13/09/30 17:23:58 INFO mapreduce.Job: The url to track the job: http://hor17n25:8088/proxy/application_1380335340676_0003/
13/09/30 17:23:58 INFO mapreduce.Job: Running job: job_1380335340676_0003
13/09/30 17:24:03 INFO mapreduce.Job: Job job_1380335340676_0003 running in uber mode : false
13/09/30 17:24:03 INFO mapreduce.Job: map 0% reduce 0%
13/09/30 17:24:06 INFO mapreduce.Job: Task Id : attempt_1380335340676_0003_m_000000_0, Status : FAILED
Error: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.TableName
at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
at java.security.AccessController.doPrivileged(Native Method)
at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
at org.apache.hadoop.hbase.io.HFileLink.getReferencedTableName(HFileLink.java:245)
at org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportMapper.getOutputPath(ExportSnapshot.java:154)
at org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportMapper.map(ExportSnapshot.java:137)
at org.apache.hadoop.hbase.snapshot.ExportSnapshot$ExportMapper.map(ExportSnapshot.java:89)
at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:763)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:339)
at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:162)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:396)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1483)
at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157)
{code}
resourcemanager runs on hor17n25 where there is no HBase installation.

The root cause is that .ExportSnapshot#runCopyJob() doesn't specify dependent jars.", ,1,,,,,,
HBASE-9696,"The online merge znode uses the new region to be created.  When master restarts, the new region is still unknown if the merging is not completed. Therefore the znode is ignored, which should not.  That means the two merging regions could be moved around.  This could cause some data loss if we are not luck.", ,,,,,,,
HBASE-9698,"hbck does not handle the case where all the regions of a table is gone. To reproduce this: 
1. create table with one region 
2. delete the region directory
3. run hbck --repair, which removes the region from meta as well 
4. now, the meta won't contain the region, but the table dir in hdfs will be there. 

", ,1,,,,,,
HBASE-9703,"HBase client code assumes ZooKeeper is secured, as long as there is a java.security.auth.login.config property being set.  When HBase client is embedded in other java program with other security configuration, it can produce wrong assumption that ZooKeeper is secured.  Ideally, isSecureZooKeeper method should detect Jaas configuration specifically for ZooKeeper to ensure that client program doesn't have a false positive detection.",,,,,,,,
HBASE-9706,"HBase client code assumes ZooKeeper is secured, as long as there is a java.security.auth.login.config property being set.  When HBase client is embedded in other java program with other security configuration, it can produce wrong assumption that ZooKeeper is secured.  Ideally, isSecureZooKeeper method should detect Jaas configuration specifically for ZooKeeper to ensure that client program doesn't have a false positive detection.",1,,,,,,,
HBASE-9721,"On a test cluster, this following events happened with ITBLL and CM leading to meta being unavailable until master is restarted. 

An RS carrying meta died, and master assigned the region to one of the RSs. 
{code}
2013-10-03 23:30:06,611 INFO  [MASTER_META_SERVER_OPERATIONS-gs-hdp2-secure-1380781860-hbase-12:60000-1] master.AssignmentManager: Assigning hbase:meta,,1.1588230740 to gs-hdp2-secure-1380781860-hbase-5.cs1cloud.internal,60020,1380842900820
2013-10-03 23:30:06,611 INFO  [MASTER_META_SERVER_OPERATIONS-gs-hdp2-secure-1380781860-hbase-12:60000-1] master.RegionStates: Transitioned {1588230740 state=OFFLINE, ts=1380843006601, server=null} to {1588230740 state=PENDING_OPEN, ts=1380843006611, server=gs-hdp2-secure-1380781860-hbase-5.cs1cloud.internal,60020,1380842900820}
2013-10-03 23:30:06,611 DEBUG [MASTER_META_SERVER_OPERATIONS-gs-hdp2-secure-1380781860-hbase-12:60000-1] master.ServerManager: New admin connection to gs-hdp2-secure-1380781860-hbase-5.cs1cloud.internal,60020,1380842900820
{code}

At the same time, the RS that meta recently got assigned also died (due to CM), and restarted: 
{code}
2013-10-03 23:30:07,636 DEBUG [RpcServer.handler=17,port=60000] master.ServerManager: REPORT: Server gs-hdp2-secure-1380781860-hbase-8.cs1cloud.internal,60020,1380843002494 came back up, removed it from the dead servers list
2013-10-03 23:30:08,769 INFO  [RpcServer.handler=18,port=60000] master.ServerManager: Triggering server recovery; existingServer gs-hdp2-secure-1380781860-hbase-5.cs1cloud.internal,60020,1380842900820 looks stale, new server:gs-hdp2-secure-1380781860-hbase-5.cs1cloud.internal,60020,1380843006362
2013-10-03 23:30:08,771 DEBUG [RpcServer.handler=18,port=60000] master.AssignmentManager: Checking region=hbase:meta,,1.1588230740, zk server=gs-hdp2-secure-1380781860-hbase-5.cs1cloud.internal,60020,1380842900820 current=gs-hdp2-secure-1380781860-hbase-5.cs1cloud.internal,60020,1380842900820, matches=true
2013-10-03 23:30:08,771 DEBUG [RpcServer.handler=18,port=60000] master.ServerManager: Added=gs-hdp2-secure-1380781860-hbase-5.cs1cloud.internal,60020,1380842900820 to dead servers, submitted shutdown handler to be executed meta=true
2013-10-03 23:30:08,771 INFO  [RpcServer.handler=18,port=60000] master.ServerManager: Registering server=gs-hdp2-secure-1380781860-hbase-5.cs1cloud.internal,60020,1380843006362
2013-10-03 23:30:08,772 INFO  [MASTER_META_SERVER_OPERATIONS-gs-hdp2-secure-1380781860-hbase-12:60000-2] handler.MetaServerShutdownHandler: Splitting hbase:meta logs for gs-hdp2-secure-1380781860-hbase-5.cs1cloud.internal,60020,1380842900820
{code}

AM/SSH sees that the RS that died was carrying meta, but the assignment RPC request was still not sent:
{code}
2013-10-03 23:30:08,791 DEBUG [MASTER_META_SERVER_OPERATIONS-gs-hdp2-secure-1380781860-hbase-12:60000-2] master.AssignmentManager: Checking region=hbase:meta,,1.1588230740, zk server=gs-hdp2-secure-1380781860-hbase-5.cs1cloud.internal,60020,1380842900820 current=gs-hdp2-secure-1380781860-hbase-5.cs1cloud.internal,60020,1380842900820, matches=true
2013-10-03 23:30:08,791 INFO  [MASTER_META_SERVER_OPERATIONS-gs-hdp2-secure-1380781860-hbase-12:60000-2] handler.MetaServerShutdownHandler: Server gs-hdp2-secure-1380781860-hbase-5.cs1cloud.internal,60020,1380842900820 was carrying META. Trying to assign.
2013-10-03 23:30:08,791 DEBUG [MASTER_META_SERVER_OPERATIONS-gs-hdp2-secure-1380781860-hbase-12:60000-2] master.RegionStates: Offline 1588230740 with current state=PENDING_OPEN, expected state=OFFLINE/SPLITTING/MERGING
2013-10-03 23:30:08,791 INFO  [MASTER_META_SERVER_OPERATIONS-gs-hdp2-secure-1380781860-hbase-12:60000-2] master.RegionStates: Transitioned {1588230740 state=PENDING_OPEN, ts=1380843006611, server=gs-hdp2-secure-1380781860-hbase-5.cs1cloud.internal,60020,1380842900820} to {1588230740 state=OFFLINE, ts=1380843008791, server=null}
2013-10-03 23:30:09,809 INFO  [MASTER_META_SERVER_OPERATIONS-gs-hdp2-secure-1380781860-hbase-12:60000-2] zookeeper.ZooKeeperNodeTracker: Unsetting hbase:meta region location in ZooKeeper
{code}

Our first attempt at the assign rpc fails, because the new server is now starting. The second attempt though succeeds: 
{code}
2013-10-03 23:30:10,621 INFO  [MASTER_META_SERVER_OPERATIONS-gs-hdp2-secure-1380781860-hbase-12:60000-1] master.AssignmentManager: Assigning hbase:meta,,1.1588230740 to gs-hdp2-secure-1380781860-hbase-5.cs1cloud.internal,60020,1380842900820
2013-10-03 23:30:10,621 INFO  [MASTER_META_SERVER_OPERATIONS-gs-hdp2-secure-1380781860-hbase-12:60000-1] master.RegionStates: Transitioned {1588230740 state=OFFLINE, ts=1380843008791, server=null} to {1588230740 state=PENDING_OPEN, ts=1380843010621, server=gs-hdp2-secure-1380781860-hbase-5.cs1cloud.internal,60020,1380842900820}
2013-10-03 23:30:10,621 DEBUG [MASTER_META_SERVER_OPERATIONS-gs-hdp2-secure-1380781860-hbase-12:60000-1] master.ServerManager: New admin connection to gs-hdp2-secure-1380781860-hbase-5.cs1cloud.internal,60020,1380842900820
2013-10-03 23:30:10,622 DEBUG [RpcServer.handler=22,port=60000] master.ServerManager: REPORT: Server gs-hdp2-secure-1380781860-hbase-5.cs1cloud.internal,60020,1380843006362 came back up, removed it from the dead servers list
2013-10-03 23:30:10,934 DEBUG [MASTER_META_SERVER_OPERATIONS-gs-hdp2-secure-1380781860-hbase-12:60000-2] master.AssignmentManager: Skip assigning {ENCODED => 1588230740, NAME => 'hbase:meta,,1', STARTKEY => '', ENDKEY => ''}, it is already {1588230740 state=PENDING_OPEN, ts=1380843010621, server=gs-hdp2-secure-1380781860-hbase-5.cs1cloud.internal,60020,1380842900820}
{code}

Note that the start time for the server does not match (1380842900820 is old , 1380843006362 is new). 

The region server got the rpc to open the region, but failed to change the zk state, because ServerNames is not matching: 
{code}
2013-10-03 23:30:10,601 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open hbase:meta,,1.1588230740
2013-10-03 23:30:10,897 DEBUG [RS_OPEN_META-gs-hdp2-secure-1380781860-hbase-5:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1417d489d9b0bd6 Transitioning 1588230740 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2013-10-03 23:30:10,918 WARN  [RS_OPEN_META-gs-hdp2-secure-1380781860-hbase-5:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1417d489d9b0bd6 Attempt to transition the unassigned node for 1588230740 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING failed, the server that tried to transition was gs-hdp2-secure-1380781860-hbase-5.cs1cloud.internal,60020,1380843006362 not the expected gs-hdp2-secure-1380781860-hbase-5.cs1cloud.internal,60020,1380842900820
2013-10-03 23:30:10,918 WARN  [RS_OPEN_META-gs-hdp2-secure-1380781860-hbase-5:60020-0] handler.OpenRegionHandler: Failed transition from OFFLINE to OPENING for region=1588230740
2013-10-03 23:30:10,919 WARN  [RS_OPEN_META-gs-hdp2-secure-1380781860-hbase-5:60020-0] handler.OpenRegionHandler: Region was hijacked? Opening cancelled for encodedName=1588230740
2013-10-03 23:30:10,919 INFO  [RS_OPEN_META-gs-hdp2-secure-1380781860-hbase-5:60020-0] handler.OpenRegionHandler: Opening of region {ENCODED => 1588230740, NAME => 'hbase:meta,,1', STARTKEY => '', ENDKEY => ''} failed, transitioning from OFFLINE to FAILED_OPEN in ZK, expecting version 0
2013-10-03 23:30:10,919 DEBUG [RS_OPEN_META-gs-hdp2-secure-1380781860-hbase-5:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1417d489d9b0bd6 Transitioning 1588230740 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_FAILED_OPEN
2013-10-03 23:30:10,921 WARN  [RS_OPEN_META-gs-hdp2-secure-1380781860-hbase-5:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1417d489d9b0bd6 Attempt to transition the unassigned node for 1588230740 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_FAILED_OPEN failed, the server that tried to transition was gs-hdp2-secure-1380781860-hbase-5.cs1cloud.internal,60020,1380843006362 not the expected gs-hdp2-secure-1380781860-hbase-5.cs1cloud.internal,60020,1380842900820
2013-10-03 23:30:10,921 WARN  [RS_OPEN_META-gs-hdp2-secure-1380781860-hbase-5:60020-0] handler.OpenRegionHandler: Unable to mark region {ENCODED => 1588230740, NAME => 'hbase:meta,,1', STARTKEY => '', ENDKEY => ''} as FAILED_OPEN. It's likely that the master already timed out this open attempt, and thus another RS already has the region.
{code}

It seems that the RS behaved correct by not being able to open the region by transitioning the zk assignment node. However, the master fails to timeout the assignment even though the meta region is reported in RIT: 
{code}
2013-10-04 00:14:50,658 DEBUG [gs-hdp2-secure-1380781860-hbase-12.cs1cloud.internal,60000,1380842679864-BalancerChore] master.HMaster: Not running balancer because 1 region(s) in transition: {1588230740={1588230740 state=PENDING_OPEN, ts=1380843010621, server=gs-hdp2-secure-1380781860-hbase-5.cs1cloud.internal,60020,1380842900820}}
{code}
", ,,,,,,,
HBASE-9724,"We ran into a case with ITLoadAndVerify with CM which left a region in RIT state, because the region failed a region SPLIT, but AM did not process this correctly. 

Region server attempting SPLIT and rollover: 
{code}
2013-10-06 16:42:56,959 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Splitting IntegrationTestLoadAndVerify,/(k\xCA\x1A\xF2\x86\xB6,1381100194019.9608be62536a61444766da669e67d39e.
...
2013-10-06 16:43:01,480 INFO  [regionserver60020-splits-1381102351302] regionserver.SplitTransaction: Starting split of region IntegrationTestLoadAndVerify,/(k\xCA\x1A\xF2\x86\xB6,1381100194019.9608be62536a61444766da669e67d39e.
2013-10-06 16:43:01,480 DEBUG [regionserver60020-splits-1381102351302] regionserver.SplitTransaction: regionserver:60020-0x1418eb9f38301bb Creating ephemeral node for 9608be62536a61444766da669e67d39e in SPLITTING state
2013-10-06 16:43:01,570 DEBUG [regionserver60020-splits-1381102351302] zookeeper.ZKAssign: regionserver:60020-0x1418eb9f38301bb Transitioning 9608be62536a61444766da669e67d39e from RS_ZK_REGION_SPLITTING to RS_ZK_REGION_SPLITTING
2013-10-06 16:43:01,607 DEBUG [regionserver60020-splits-1381102351302] zookeeper.ZKAssign: regionserver:60020-0x1418eb9f38301bb Transitioned node 9608be62536a61444766da669e67d39e from RS_ZK_REGION_SPLITTING to RS_ZK_REGION_SPLITTING
2013-10-06 16:43:01,607 DEBUG [regionserver60020-splits-1381102351302] zookeeper.ZKAssign: regionserver:60020-0x1418eb9f38301bb Transitioning 9608be62536a61444766da669e67d39e from RS_ZK_REGION_SPLITTING to RS_ZK_REGION_SPLITTING
2013-10-06 16:43:01,629 DEBUG [regionserver60020-splits-1381102351302] zookeeper.ZKAssign: regionserver:60020-0x1418eb9f38301bb Transitioned node 9608be62536a61444766da669e67d39e from RS_ZK_REGION_SPLITTING to RS_ZK_REGION_SPLITTING
2013-10-06 16:43:02,047 DEBUG [regionserver60020-splits-1381102351302] regionserver.HRegion: Closing IntegrationTestLoadAndVerify,/(k\xCA\x1A\xF2\x86\xB6,1381100194019.9608be62536a61444766da669e67d39e.: disabling compactions & flushes
2013-10-06 16:43:02,047 DEBUG [regionserver60020-splits-1381102351302] regionserver.HRegion: Updates disabled for region IntegrationTestLoadAndVerify,/(k\xCA\x1A\xF2\x86\xB6,1381100194019.9608be62536a61444766da669e67d39e.
2013-10-06 16:43:02,047 DEBUG [regionserver60020-splits-1381102351302] regionserver.HRegion: Started memstore flush for IntegrationTestLoadAndVerify,/(k\xCA\x1A\xF2\x86\xB6,1381100194019.9608be62536a61444766da669e67d39e., current region memstore size 4.4 M
2013-10-06 16:43:03,945 INFO  [regionserver60020-splits-1381102351302] regionserver.HRegion: Finished memstore flush of ~4.4 M/4607072, currentsize=0/0 for region IntegrationTestLoadAndVerify,/(k\xCA\x1A\xF2\x86\xB6,1381100194019.9608be62536a61444766da669e67d39e. in 1898ms, sequenceid=6703616, compaction requested=true
2013-10-06 16:43:04,026 INFO  [StoreCloserThread-IntegrationTestLoadAndVerify,/(k\xCA\x1A\xF2\x86\xB6,1381100194019.9608be62536a61444766da669e67d39e.-1] regionserver.HStore: Closed f1
2013-10-06 16:43:04,027 INFO  [regionserver60020-splits-1381102351302] regionserver.HRegion: Closed IntegrationTestLoadAndVerify,/(k\xCA\x1A\xF2\x86\xB6,1381100194019.9608be62536a61444766da669e67d39e.
2013-10-06 16:43:36,938 INFO  [regionserver60020-splits-1381102351302] regionserver.SplitRequest: Running rollback/cleanup of failed split of IntegrationTestLoadAndVerify,/(k\xCA\x1A\xF2\x86\xB6,1381100194019.9608be62536a61444766da669e67d39e.; Took too long to split the files and create the references, aborting split
java.io.IOException: Took too long to split the files and create the references, aborting split
  at org.apache.hadoop.hbase.regionserver.SplitTransaction.splitStoreFiles(SplitTransaction.java:584)
  at org.apache.hadoop.hbase.regionserver.SplitTransaction.createDaughters(SplitTransaction.java:296)
  at org.apache.hadoop.hbase.regionserver.SplitTransaction.execute(SplitTransaction.java:465)
  at org.apache.hadoop.hbase.regionserver.SplitRequest.run(SplitRequest.java:82)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  at java.lang.Thread.run(Thread.java:722)
2013-10-06 16:43:37,482 INFO  [StoreOpener-9608be62536a61444766da669e67d39e-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
2013-10-06 16:43:37,509 DEBUG [StoreOpener-9608be62536a61444766da669e67d39e-1] regionserver.HStore: loaded hdfs://ambari-unsec-1381055459-hbase-1.cs1cloud.internal:8020/apps/hbase/data/data/default/IntegrationTestLoadAndVerify/9608be62536a61444766da669e67d39e/f1/12cecfd69b7948eb8bbf9bacc3d7fbb0, isReference=false, isBulkLoadResult=false, seqid=6703616, majorComp
2013-10-06 16:43:37,549 DEBUG [StoreOpener-9608be62536a61444766da669e67d39e-1] regionserver.HStore: loaded hdfs://ambari-unsec-1381055459-hbase-1.cs1cloud.internal:8020/apps/hbase/data/data/default/IntegrationTestLoadAndVerify/9608be62536a61444766da669e67d39e/f1/6b715040df9b424cbd3d01f36b34677b, isReference=false, isBulkLoadResult=false, seqid=6703217, majorComp
2013-10-06 16:43:37,588 DEBUG [StoreOpener-9608be62536a61444766da669e67d39e-1] regionserver.HStore: loaded hdfs://ambari-unsec-1381055459-hbase-1.cs1cloud.internal:8020/apps/hbase/data/data/default/IntegrationTestLoadAndVerify/9608be62536a61444766da669e67d39e/f1/97cbd5f18ee64e0aaf43f48b1d3fae73, isReference=false, isBulkLoadResult=false, seqid=6681405, majorComp
2013-10-06 16:43:37,630 DEBUG [StoreOpener-9608be62536a61444766da669e67d39e-1] regionserver.HStore: loaded hdfs://ambari-unsec-1381055459-hbase-1.cs1cloud.internal:8020/apps/hbase/data/data/default/IntegrationTestLoadAndVerify/9608be62536a61444766da669e67d39e/f1/a0d33ad9786d4a6fb6fa83b530cb5acd, isReference=false, isBulkLoadResult=false, seqid=6699746, majorComp
2013-10-06 16:43:37,676 DEBUG [StoreOpener-9608be62536a61444766da669e67d39e-1] regionserver.HStore: loaded hdfs://ambari-unsec-1381055459-hbase-1.cs1cloud.internal:8020/apps/hbase/data/data/default/IntegrationTestLoadAndVerify/9608be62536a61444766da669e67d39e/f1/c4ad2148624d42e6b8408b9b257a4c70, isReference=false, isBulkLoadResult=false, seqid=6689091, majorComp
2013-10-06 16:43:37,893 INFO  [regionserver60020-splits-1381102351302] regionserver.HRegionFileSystem: Cleaned up old failed split transaction detritus: hdfs://ambari-unsec-1381055459-hbase-1.cs1cloud.internal:8020/apps/hbase/data/data/default/IntegrationTestLoadAndVerify/9608be62536a61444766da669e67d39e/.splits
2013-10-06 16:43:37,937 INFO  [regionserver60020-splits-1381102351302] regionserver.HRegion: Onlined 9608be62536a61444766da669e67d39e; next sequenceid=6703617
2013-10-06 16:43:38,021 DEBUG [regionserver60020-splits-1381102351302] zookeeper.ZKAssign: regionserver:60020-0x1418eb9f38301bb Deleted unassigned node 9608be62536a61444766da669e67d39e in expected state RS_ZK_REGION_SPLITTING
2013-10-06 16:43:38,021 INFO  [regionserver60020-splits-1381102351302] regionserver.SplitRequest: Successful rollback of failed split of IntegrationTestLoadAndVerify,/(k\xCA\x1A\xF2\x86\xB6,1381100194019.9608be62536a61444766da669e67d39e.
{code}

The RS, 4 mins later will try to do another split, but it will fail to let master know so, because master still thinks the region is in RIT due to the first split. 

{code}
2013-10-06 16:43:01,606 DEBUG [AM.ZK.Worker-pool2-t202] master.AssignmentManager: Handling RS_ZK_REGION_SPLITTING, server=ambari-unsec-1381055459-hbase-5.cs1cloud.internal,60020,1381101889371, region=9608be62536a61444766da669e67d39e, current_state=null
2013-10-06 16:43:01,609 INFO  [AM.ZK.Worker-pool2-t202] master.RegionStates: Transitioned {9608be62536a61444766da669e67d39e state=OPEN, ts=1381102229682, server=ambari-unsec-1381055459-hbase-5.cs1cloud.internal,60020,1381101889371} to {9608be62536a61444766da669e67d39e state=SPLITTING, ts=1381102981609, server=ambari-unsec-1381055459-hbase-5.cs1cloud.internal,600
2013-10-06 16:43:01,634 DEBUG [AM.ZK.Worker-pool2-t203] master.AssignmentManager: Handling RS_ZK_REGION_SPLITTING, server=ambari-unsec-1381055459-hbase-5.cs1cloud.internal,60020,1381101889371, region=9608be62536a61444766da669e67d39e, current_state={9608be62536a61444766da669e67d39e state=SPLITTING, ts=1381102981609, server=ambari-unsec-1381055459-hbase-5.cs1cloud
2013-10-06 16:43:38,019 DEBUG [AM.ZK.Worker-pool2-t204] master.AssignmentManager: Znode IntegrationTestLoadAndVerify,/(k\xCA\x1A\xF2\x86\xB6,1381100194019.9608be62536a61444766da669e67d39e. deleted, state: {9608be62536a61444766da669e67d39e state=SPLITTING, ts=1381102981634, server=ambari-unsec-1381055459-hbase-5.cs1cloud.internal,60020,1381101889371}
2013-10-06 16:44:35,673 DEBUG [AM.ZK.Worker-pool2-t206] master.AssignmentManager: Handling RS_ZK_REGION_SPLITTING, server=ambari-unsec-1381055459-hbase-5.cs1cloud.internal,60020,1381101889371, region=ae22b90a3ddeb30c45c5735f7c855014, current_state=null
2013-10-06 16:44:35,673 INFO  [AM.ZK.Worker-pool2-t206] master.RegionStates: Transitioned {ae22b90a3ddeb30c45c5735f7c855014 state=OPEN, ts=1381102362273, server=ambari-unsec-1381055459-hbase-5.cs1cloud.internal,60020,1381101889371} to {ae22b90a3ddeb30c45c5735f7c855014 state=SPLITTING, ts=1381103075673, server=ambari-unsec-1381055459-hbase-5.cs1cloud.internal,600
2013-10-06 16:44:35,725 DEBUG [AM.ZK.Worker-pool2-t207] master.AssignmentManager: Handling RS_ZK_REGION_SPLITTING, server=ambari-unsec-1381055459-hbase-5.cs1cloud.internal,60020,1381101889371, region=ae22b90a3ddeb30c45c5735f7c855014, current_state={ae22b90a3ddeb30c45c5735f7c855014 state=SPLITTING, ts=1381103075673, server=ambari-unsec-1381055459-hbase-5.cs1cloud
2013-10-06 16:44:38,707 DEBUG [ambari-unsec-1381055459-hbase-1.cs1cloud.internal,60000,1381101554357-BalancerChore] master.HMaster: Not running balancer because 2 region(s) in transition: {9608be62536a61444766da669e67d39e={9608be62536a61444766da669e67d39e state=SPLITTING, ts=1381102981634, server=ambari-unsec-1381055459-hbase-5.cs1cloud.internal,60020,1381101889

{code}

It seems that AM.nodeDeleted() does not revert the in-memory state for the region to be opened. ", ,,,,,,,
HBASE-9736,"{noformat}
HBase Table regions are not getting assigned to the new region server for a period of 30 minutes (when the existing region server not able to handle the load)


Procedure:
1. Setup Non HA Hadoop Cluster with two nodes (Node1-XX.XX.XX.XX,  Node2-YY.YY.YY.YY)
2. Install Zookeeper & HRegionServer in Node-1
3. Install HMaster & HRegionServer in Node-2
4. From Node2 create HBase Table ( table name 't1' with one column family 'cf1' )
5. Perform addrecord 99649 rows 
6. kill both the node Region Server and limit the Node1 Region Server FD to 600
7. Start only the Node1 Region server ==> so that FD exhaust can happen in Node1 Region Server
8. After some 5-10 minuites start the Node2 Region Server

===> Huge number of regions of table 't1' are in OPENING state, which are not getting re assigned to the Node2 region server which is free. 

===> When the new region server comes up then the master should detect and allocate the open failed regions to the region server (here it is staying the OPENINING state for 30 minutes which will have huge impcat user app which makes use of this table)



2013-09-23 18:46:12,160 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Instantiated t1,row507465,1379937224590.2d9fad2aee78103f928d8c7fe16ba6cd.
2013-09-23 18:46:12,160 ERROR org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Failed open of region=t1,row507465,1379937224590.2d9fad2aee78103f928d8c7fe16ba6cd., starting to roll back the global memstore size.

2013-09-23 18:50:55,284 WARN org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_hb_rs_HOST-XX.XX.XX.XX,61020,1379940823286_-641204614_48] for 309 seconds.  Will retry shortly ...
java.io.IOException: Failed on local exception: java.net.SocketException: Too many open files; Host Details : local host is: ""HOST-XX.XX.XX.XX/XX.XX.XX.XX""; destination host is: ""HOST-XX.XX.XX.XX"":8020;
        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
        at org.apache.hadoop.ipc.Client.call(Client.java:1351)
        at org.apache.hadoop.ipc.Client.call(Client.java:1300)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
        at $Proxy13.renewLease(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:188)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
        at $Proxy13.renewLease(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:522)
        at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:679)
        at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
        at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
        at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
        at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.net.SocketException: Too many open files
        at sun.nio.ch.Net.socket0(Native Method)
        at sun.nio.ch.Net.socket(Net.java:97)
        at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:84)
        at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:37)
        at java.nio.channels.SocketChannel.open(SocketChannel.java:105)
        at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62)
        at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:523)
        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:642)
        at org.apache.hadoop.ipc.Client$Connection.access$2600(Client.java:314)
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1399)
        at org.apache.hadoop.ipc.Client.call(Client.java:1318)
        ... 16 more
2013-09-23 18:50:56,285 WARN org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_hb_rs_HOST-XX.XX.XX.XX,61020,1379940823286_-641204614_48] for 310 seconds.  Will retry shortly ...
java.io.IOException: Failed on local exception: java.net.SocketException: Too many open files; Host Details : local host is: ""HOST-XX.XX.XX.XX/XX.XX.XX.XX""; destination host is: ""HOST-XX.XX.XX.XX"":8020;
        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
        at org.apache.hadoop.ipc.Client.call(Client.java:1351)
        at org.apache.hadoop.ipc.Client.call(Client.java:1300)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
       at $Proxy13.renewLease(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:188)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
        at $Proxy13.renewLease(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:522)
        at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:679)
        at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)
        at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)
        at org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)
        at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.net.SocketException: Too many open files
        at sun.nio.ch.Net.socket0(Native Method)
        at sun.nio.ch.Net.socket(Net.java:97)
        at sun.nio.ch.SocketChannelImpl.<init>(SocketChannelImpl.java:84)
        at sun.nio.ch.SelectorProviderImpl.openSocketChannel(SelectorProviderImpl.java:37)
        at java.nio.channels.SocketChannel.open(SocketChannel.java:105)
        at org.apache.hadoop.net.StandardSocketFactory.createSocket(StandardSocketFactory.java:62)
        at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:523)
        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:642)
        at org.apache.hadoop.ipc.Client$Connection.access$2600(Client.java:314)
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1399)
        at org.apache.hadoop.ipc.Client.call(Client.java:1318)
        ... 16 more
2013-09-23 18:50:57,287 WARN org.apache.hadoop.hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_hb_rs_HOST-XX.XX.XX.XX,61020,1379940823286_-641204614_48] for 311 seconds.  Will retry shortly ...
java.io.IOException: Failed on local exception: java.net.SocketException: Too many open files; Host Details : local host is: ""HOST-XX.XX.XX.XX/XX.XX.XX.XX""; destination host is: ""HOST-XX.XX.XX.XX"":8020;







{noformat}
", ,1,1,,,,,
HBASE-9737,"One of our customer was recently hit with OOM error on almost all of the region servers.

Postmortem of the issue reveled that a corrupt HFile had made its way into one of the regions which resulted into the region brought offline immediately which is as per design.

What happened next reveals two issues:\\
\\
* As soon as the region was offlined, Master noticed this and tried to assign the region to another region server which of course failed (again due to the corrupt HFile) and then Master tried to assign this to another and so on. So this region kept bouncing from one server to another and this went unnoticed for few hours and all region servers log were filled with thousands of this message:{noformat}org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Failed open of
region=userdata,50743646010,1378139055806.318c533716869574f10615703269497f.,
starting to roll back the global memstore size.
java.io.IOException: java.io.IOException:
org.apache.hadoop.hbase.io.hfile.CorruptHFileException: Problem reading HFile
Trailer from file
/hbase/userdata/318c533716869574f10615703269497f/data/a3e2ae39f71441ac92a6563479fb976e
        at org.apache.hadoop.hbase.regionserver.HRegion.initializeRegionInternals(HRegion.java:550)
        at org.apache.hadoop.hbase.regionserver.HRegion.initialize(HRegion.java:463)
        at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:3835)
        at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:3783)
        at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.openRegion(OpenRegionHandler.java:332)
        at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:108)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:169)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException:
org.apache.hadoop.hbase.io.hfile.CorruptHFileException: Problem reading HFile
Trailer from file
/hbase/userdata/318c533716869574f10615703269497f/data/a3e2ae39f71441ac92a6563479fb976e
        at org.apache.hadoop.hbase.regionserver.Store.loadStoreFiles(Store.java:404)
        at org.apache.hadoop.hbase.regionserver.Store.<init>(Store.java:257)
        at org.apache.hadoop.hbase.regionserver.HRegion.instantiateHStore(HRegion.java:3017)
        at org.apache.hadoop.hbase.regionserver.HRegion$1.call(HRegion.java:525)
        at org.apache.hadoop.hbase.regionserver.HRegion$1.call(HRegion.java:523)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
{noformat} For situation like this, the region should be marked ""offlined_with_error"" or something similar so that Master does not try to assign it to another server without user fixing the issue. I will create a separate JIRA for that.

* The second problem and the scope of this JIRA is that the function {{org.apache.hadoop.hbase.io.hfile.HFile.pickReaderVersion()}} throws exception without closing the {{FSDataInputStream}} objects even if closeIStream is set to true. This lead to orphan filesystem streams accumulating in region server and it eventually died of OOM.", ,,,,,,,
HBASE-9740,"As described in HBASE-9737, a corrupt HFile in a region could lead to an assignment storm in the cluster since the Master will keep trying to assign the region to each region server one after another and obviously none will succeed.

The region server, upon detecting such a scenario should mark the region as ""RS_ZK_REGION_FAILED_ERROR"" (or something to the effect) in the Zookeeper which should indicate the Master to stop assigning the region until the error has been resolved (via an HBase shell command, probably ""assign""?)", ,,,,,,,
HBASE-9751,It seems that usage of skipKVsNewerThanReadpoint in StoreFileScanner can be greatly reduced or even eliminated all together (HFiles are immutable and no new KVs can be inserted after scanner instance is created). The same is true for MemStoreScanner which checks readpoint on every next() and seek(). Each readpoint check is ThreadLocal.get() and it is quite expensive.  , ,,1,,,,,
HBASE-9753,Brought up by [~vrodionov] on the mailing list. See also HBASE-9751., ,,1,,,,,
HBASE-9754,"Brought up by [~vrodionov] and [~yuzhihong@gmail.com].
Currently we use ThreadLocals to communicate the current readpoint between a RegionScanner and the Store\{File}Scanner's down the stack.
Since ThreadLocals are not cheap we should consider whether it is possible to pass the readpoint through the call stack instead.
", ,,1,,,,,
HBASE-9766,"In the writer V3 includesTags always comes as true only and so writing tags length always even when compaction selection says not to do so.
", ,,,,,,,
HBASE-9767,"Brought up by [~vrodionov] and [~yuzhihong@gmail.com].
Currently we use ThreadLocals to communicate the current readpoint between a RegionScanner and the Store\{File}Scanner's down the stack.
Since ThreadLocals are not cheap we should consider whether it is possible to pass the readpoint through the call stack instead.
", ,,1,,,,,
HBASE-9768,"There may exist two issues in Asyncprocess code as following:

1)  In Htable#backgroundFlushCommits, we have following code:

{code}
      if (ap.hasError()) {
        if (!clearBufferOnFail) {
          // if clearBufferOnFailed is not set, we're supposed to keep the failed operation in the
          //  write buffer. This is a questionable feature kept here for backward compatibility
          writeAsyncBuffer.addAll(ap.getFailedOperations());
        }
        RetriesExhaustedWithDetailsException e = ap.getErrors();
        ap.clearErrors();
        throw e;
      }
{code}

In a rare situation like the following: 
When there are some updates ongoing, a client call Put(internally backgroundFlushCommits get triggered). Then comes the issue:
The first ap.hasError() returns false and the second ap.hasError() returns true. So we could throw exception to caller while writeAsyncBuffer isn't empty.(some updates are still on going).
If a client retry with different values for the same keys, we could end up with nondeterministic state.

2) The following code only update cache for the first row. We should update cache for all the regions inside resultForRS because actions are sent to multiple regions per RS

{code}
          if (failureCount++ == 0) { // We're doing this once per location.
            hConnection.updateCachedLocations(this.tableName, row.getRow(), result, location);
            if (errorsByServer != null) {
              errorsByServer.reportServerError(location);
              canRetry = errorsByServer.canRetryMore();
            }
          }
{code}
", ,,,,,,,
HBASE-9769,"Brought up by [~vrodionov] and [~yuzhihong@gmail.com].
Currently we use ThreadLocals to communicate the current readpoint between a RegionScanner and the Store\{File}Scanner's down the stack.
Since ThreadLocals are not cheap we should consider whether it is possible to pass the readpoint through the call stack instead.
", ,,1,,,,,
HBASE-9773,"Came across this situation (with a version of 0.96 very close to RC5 version created on 10/11):

The sequence of events that happened:

1. The hbck tool couldn't communicate with the RegionServer hosting namespace region due to some security exceptions. hbck INCORRECTLY assumed the region was not deployed.
In output.log (client side):
{noformat}
2013-10-12 10:42:57,067|beaver.machine|INFO|ERROR: Region { meta => hbase:namespace,,1381564449706.a0ac0825ba2d0830614e7f808f31787a., hdfs => hdfs://gs-hdp2-secure-1381559462-hbase-12.cs1cloud.internal:8020/apps/hbase/data/data/hbase/namespace/a0ac0825ba2d0830614e7f808f31787a, deployed =>  } not deployed on any region server.
2013-10-12 10:42:57,067|beaver.machine|INFO|Trying to fix unassigned region...
{noformat}

2. This led to the hbck tool trying to tell the master to ""assign"" the region.
In master log (hbase-hbase-master-gs-hdp2-secure-1381559462-hbase-12.log):
{noformat}
2013-10-12 10:52:35,960 INFO  [RpcServer.handler=4,port=60000] master.HMaster: Client=hbase//172.18.145.105 assign hbase:namespace,,1381564449706.a0ac0825ba2d0830614e7f808f31787a.
{noformat}

3. The master went through the steps - sent a CLOSE to the RegionServer hosting namespace region.
From master log:
{noformat}
2013-10-12 10:52:35,981 DEBUG [RpcServer.handler=4,port=60000] master.AssignmentManager: Sent CLOSE to gs-hdp2-secure-1381559462-hbase-1.cs1cloud.internal,60020,1381564439794 for region hbase:namespace,,1381564449706.a0ac0825ba2d0830614e7f808f31787a.
{noformat}

4. The master then tried to assign the namespace region to a region server, and in the process ABORTED:
From master log:
{noformat}
2013-10-12 10:52:36,025 DEBUG [RpcServer.handler=4,port=60000] master.AssignmentManager: No previous transition plan found (or ignoring an existing plan) for hbase:namespace,,1381564449706.a0ac0825ba2d0830614e7f808f31787a.; generated random plan=hri=hbase:namespace,,1381564449706.a0ac0825ba2d0830614e7f808f31787a., src=, dest=gs-hdp2-secure-1381559462-hbase-9.cs1cloud.internal,60020,1381564439807; 4 (online=4, available=4) available servers, forceNewPlan=true
2013-10-12 10:52:36,026 FATAL [RpcServer.handler=4,port=60000] master.HMaster: Master server abort: loaded coprocessors are: [org.apache.hadoop.hbase.security.access.AccessController]
2013-10-12 10:52:36,027 FATAL [RpcServer.handler=4,port=60000] master.HMaster: Unexpected state : {a0ac0825ba2d0830614e7f808f31787a state=OPEN, ts=1381564451344, server=gs-hdp2-secure-1381559462-hbase-1.cs1cloud.internal,60020,1381564439794} .. Cannot transit it to OFFLINE.
java.lang.IllegalStateException: Unexpected state : {a0ac0825ba2d0830614e7f808f31787a state=OPEN, ts=1381564451344, server=gs-hdp2-secure-1381559462-hbase-1.cs1cloud.internal,60020,1381564439794} .. Cannot transit it to OFFLINE.
{noformat}
{code}AssignmentManager.assign(HRegionInfo region, boolean setOfflineInZK, boolean forceNewPlan){code} is the method that does all the above. This was called from the HMaster with true for both the boolean arguments.", ,,,,,,,
HBASE-9775,Testing on larger clusters has not had the desired throughput increases., ,1,,,,,,
HBASE-9777,"Here is the sequence of events (with a version of 0.96 very close to RC5 version created on 10/11):
1. Master assigns regions to some server RS1. One particular region is 300d71b112325d43b99b6148ec7bc5b3
2. RS1 crashes
3. Master tries to bulk-reassign (this has retries as well) the regions to other RSs. Let's say one of them is RS2.
{noformat}
2013-10-14 21:16:22,218 INFO  [hor13n02.gq1.ygridcore.net,60000,1381784464025-GeneralBulkAssigner-0] master.RegionStates: Transitioned {300d71b112325d43b99b6148ec7bc5b3 state=OFFLINE, ts=1381785382125, server=null} to {300d71b112325d43b99b6148ec7bc5b3 state=PENDING_OPEN, ts=1381785382218, server=hor13n04.gq1.ygridcore.net,60020,1381784772417}
{noformat}
4. RS2 crashes
5. The ServerShutdownHandler for RS2 is executed, and it tries to reassign the regions.
{noformat}
2013-10-14 21:16:32,185 INFO  [MASTER_SERVER_OPERATIONS-hor13n02:60000-3] master.RegionStates: Found opening region {300d71b112325d43b99b6148ec7bc5b3 state=PENDING_OPEN, ts=1381785382218, server=hor13n04.gq1.ygridcore.net,60020,1381784772417} to be reassigned by SSH for hor13n04.gq1.ygridcore.net,60020,1381784772417
{noformat}
6. (5) succeeds. The region states are made OPEN.
7. The retry from (3) kicks in 
{noformat}
2013-10-14 21:16:22,222 INFO  [MASTER_SERVER_OPERATIONS-hor13n02:60000-1] master.GeneralBulkAssigner: Failed assigning 52 regions to server hor13n04.gq1.ygridcore.net,60020,1381784772417, reassigning them
{noformat}
8. The retry finds some region state as OPEN, and the master aborts with the stack trace:
{noformat}
2013-10-14 21:16:34,342 FATAL AM.-pool1-t46 master.HMaster: Unexpected state :
{300d71b112325d43b99b6148ec7bc5b3 state=OPEN, ts=1381785392864, server=hor13n08.gq1.ygridcore.net,60020,1381785385596} .. Cannot transit it to OFFLINE.
java.lang.IllegalStateException: Unexpected state : {300d71b112325d43b99b6148ec7bc5b3 state=OPEN, ts=1381785392864, server=hor13n08.gq1.ygridcore.net,60020,1381785385596}
.. Cannot transit it to OFFLINE.
at org.apache.hadoop.hbase.master.AssignmentManager.setOfflineInZooKeeper(AssignmentManager.java:2074)
at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1855)
at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1449)
at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:45)
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
at java.util.concurrent.FutureTask.run(FutureTask.java:138)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:662)
{noformat}", ,,,,,,,
HBASE-9778,"The issue of slow seeking in ExplicitColumnTracker was brought up by [~vrodionov] on the dev list.

My idea here is to avoid the seeking if we know that there aren't many versions to skip.
How do we know? We'll use the column family's VERSIONS setting as a hint. If VERSIONS is set to 1 (or maybe some value < 10) we'll avoid the seek and call SKIP repeatedly.

HBASE-9769 has some initial number for this approach:
Interestingly it depends on which column(s) is (are) selected.

Some numbers: 4m rows, 5 cols each, 1 cf, 10 bytes values, VERSIONS=1, everything filtered at the server with a ValueFilter. Everything measured in seconds.

Without patch:
||Wildcard||Col 1||Col 2||Col 4||Col 5||Col 2+4||
|6.4|8.5|14.3|14.6|11.1|20.3|

With patch:
||Wildcard||Col 1||Col 2||Col 4||Col 5||Col 2+4||
|6.4|8.4|8.9|9.9|6.4|10.0|

Variation here was +- 0.2s.

So with this patch scanning is 2x faster than without in some cases, and never slower. No special hint needed, beyond declaring VERSIONS correctly.
", ,,1,,,,,
HBASE-9783,"h6. 0.94 branch only

Calling {{HTable.mutateRow()}} with non-existent column families result in a NPE on the region server which is wrapped into {{RemoteException}} and returned to the client. Since NPE is not a {{DoNotRetryIOException}}, client keeps repeating it until the number of retries are exhausted.

Negative case, hence creating as minor.", ,,,,,,,
HBASE-9785,"The size reported by Fixed_Overhead variable in HRegion misses out on a boolean variable. TestHeapSize doesn't report it because of the alignment we do to make it multiple of 8. 

The equation for HRegion heap usage from Fixed_Overhead is: 
Fixed_Overhead = align(100 + 42*ref + 1 arr) = align(284) = 288 on a 32 bit vm
(On a 32 bit vm, 1 ref = 4bytes, 1 arr = 16 bytes)

The equation formed using reflection (in Classsize) is:
Expected_Overehead = align(101 + 42*ref + 1arr) = align(285) = 288.
So, the testHeapSize doesn't fail currently.

But if I add a reference (did in last patch in 8741), it starts failing because, now the equations are:
Fixed_Overhead = align(100 + 43*ref + 1 arr) = align(288) = 288
Expected_Overhead = align(101 + 43*ref + 1 arr) = align(289) = 296.", ,,,,,,,
HBASE-9789,"Currently both RegionServer and HMaster log (debug) this
""Received dynamic protocol exec call with protocolName <class>""
on each coprocessor exec call.

We just filled our RegionServer log with 160gb of these since we're making heavy use of coprocessors.

I would like to change this to trace. Any objections?
", ,,1,,,,,
HBASE-9792,"Currently, we update a region's last assignment region server when the region is online.  We should do this sooner, when the region is moved to OPEN state.  CM could kill this region server before we delete the znode and online the region.", ,,,,,,,
HBASE-9793,"The fix for HBASE-9773 could cause double assignment, as [~jeffreyz] pointed out. Let's fix it in a separate jira instead of an addendum since there are different opinions on how to fix it.", ,,,,,,,
HBASE-9796,"Saw this running hbase-it suite on test cluster.  Its tricky.  Needs a little study.  Connection or location became null in this code when we go to clear the caches:

{code}
    if (t instanceof SocketTimeoutException ||
        t instanceof ConnectException ||
        t instanceof RetriesExhaustedException ||
        (location != null && getConnection().isDeadServer(location.getServerName()))) {
      // if thrown these exceptions, we clear all the cache entries that
      // map to that slow/dead server; otherwise, let cache miss and ask
      // hbase:meta again to find the new location
      getConnection().clearCaches(location.getServerName());
{code}

Here is exception seen:

2013-10-17 09:05:44,569 INFO  [Thread-9] actions.Action: Killed region server:a1811.halxg.cloudera.com,60020,1382025707403. Reported num of rs:4
2013-10-17 09:05:44,569 INFO  [Thread-9] actions.Action: Sleeping for:2961
Exception in thread ""HBaseUpdaterThread_3"" java.lang.NullPointerException
	at org.apache.hadoop.hbase.client.RegionServerCallable.throwable(RegionServerCallable.java:120)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:124)
	at org.apache.hadoop.hbase.client.HTable.get(HTable.java:755)
	at org.apache.hadoop.hbase.util.MultiThreadedUpdater$HBaseUpdaterThread.run(MultiThreadedUpdater.java:159)

", ,,,,,,,
HBASE-9807,"In HFileReaderV2.AbstractScannerV2.reseekTo(...) we have this:
{code}
        ByteBuffer bb = getKey();
        compared = reader.getComparator().compare(key, offset,
            length, bb.array(), bb.arrayOffset(), bb.limit());
{code}
{{getKey()}} creates two ByteBuffers in ScannerV2 and makes a deep copy of the key in EncodedScannerV2.", ,,1,,,,,
HBASE-9810,"Recently we encountered such a case in 0.94-version:

Flush is triggered frequently because:

{noformat}DEBUG org.apache.hadoop.hbase.regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=14.4g
{noformat}

But, the real global memstore size is about 1g.

It seems the global memstore size has been calculated wrongly.


Through the logs, I find the following root cause log:
{noformat}
ERROR org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Failed open of region=notifysub2_index,\x83\xDC^\xCD\xA3\x8A<\x
E2\x8E\xE6\xAD!\xDC\xE8t\xED,1379148697072.46be7c2d71c555379278a7494df3015e., starting to roll back the global memstore size.
java.lang.NegativeArraySizeException
        at org.apache.hadoop.hbase.KeyValue.getFamily(KeyValue.java:1096)
        at org.apache.hadoop.hbase.regionserver.HRegion.replayRecoveredEdits(HRegion.java:2933)
        at org.apache.hadoop.hbase.regionserver.HRegion.replayRecoveredEditsIfAny(HRegion.java:2811)
        at org.apache.hadoop.hbase.regionserver.HRegion.initializeRegionInternals(HRegion.java:583)
        at org.apache.hadoop.hbase.regionserver.HRegion.initialize(HRegion.java:499)
        at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:3939)
        at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:3887)
        at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.openRegion(OpenRegionHandler.java:332)
        at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:108)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:169)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}



Browse the code of this part, seems a critial bug about global memstore size when replaying recovered edits.

(RegionServerAccounting#clearRegionReplayEditsSize is called  for each edit file, it means the roll back size is smaller than actual when calling RegionServerAccounting#rollbackRegionReplayEditsSize)

Anyway,  the solution is easy as the patch.", ,,,,,,,
HBASE-9811,"Hi there, we are trying to migrate a app from MySQL to HBase. One kind of the queries is pagination with large offset and small limit. We don't have too many such queries and so both MySQL and HBase should survive. (MySQL has no index for offset either.)

When comparing the performance on both systems, we found something interest: write ~1M values in a single row, and query with offset = 1M. So all values should be scanned on RS side.

When running the query on MySQL, the first query is pretty slow (more than 1 second) and then repeat the same query, it will become very low latency.

HBase on the other hand, repeating the query does not help much (~1s forever). I can confirm that all data are in block cache and all the time is spent on in-memory data processing. (We have flushed data to disk.)

I found ""reseek"" is the hot spot. It is caused by ColumnPaginationFilter returning NEXT_COL. If I replace this line by returning SKIP (which causes to call next rather than reseek), the latency is reduced to ~100ms.

So I think there must be some room for optimization.", ,,1,,,,,
HBASE-9813,"Here is something weird happened to my cluster.  When the master recovered, it thought RS was dead by mistake and SSH processed it.  Four hlog files were split and 4 regions were reassigned.

Later on when the RS tried to report status, it got YouAreDeadException from master.  Master tried to recover it again.  Now SSH split one more hlog file, assigned 0 more extra region.

Now the question is how come is the extra log file?  I think it caused a data loss (one row) in my case. How can we prevent it?  I can add a checking in the master to make sure a RS is really dead before marking it dead during recovery since it may be slow in reporting to the new master. But this is not a proper fix.  A proper fix should be in the RS side, not creating the extra hlog file.  Is this doable?

Here is the related master log.  The RS log does not have much thing interested.

{noformat}
2013-10-20 17:40:33,025 INFO  [master:e1119:36000] master.MasterFileSystem: Log folder hdfs://e1119.halxg.cloudera.com:35802/hbase/WALs/e1521.halxg.cloudera.com,36020,1382314725312 doesn't belong to a known region server, splitting
2013-10-20 17:40:33,432 INFO  [MASTER_SERVER_OPERATIONS-e1119:36000-0] handler.ServerShutdownHandler: Splitting logs for e1521.halxg.cloudera.com,36020,1382314725312 before assignment.
2013-10-20 17:40:33,461 DEBUG [MASTER_SERVER_OPERATIONS-e1119:36000-0] master.MasterFileSystem: Renamed region directory: hdfs://e1119.halxg.cloudera.com:35802/hbase/WALs/e1521.halxg.cloudera.com,36020,1382314725312-splitting
2013-10-20 17:40:33,462 INFO  [MASTER_SERVER_OPERATIONS-e1119:36000-0] master.SplitLogManager: dead splitlog workers [e1521.halxg.cloudera.com,36020,1382314725312]
2013-10-20 17:40:33,467 INFO  [MASTER_SERVER_OPERATIONS-e1119:36000-0] master.SplitLogManager: started splitting 3 logs in [hdfs://e1119.halxg.cloudera.com:35802/hbase/WALs/e1521.halxg.cloudera.com,36020,1382314725312-splitting]
2013-10-20 17:40:36,157 DEBUG [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://e1119.halxg.cloudera.com:35802/hbase/WALs/e1521.halxg.cloudera.com,36020,1382314725312-splitting/e1521.halxg.cloudera.com%2C36020%2C1382314725312.1382315954499 to hdfs://e1119.halxg.cloudera.com:35802/hbase/oldWALs/e1521.halxg.cloudera.com%2C36020%2C1382314725312.1382315954499
2013-10-20 17:40:37,357 DEBUG [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://e1119.halxg.cloudera.com:35802/hbase/WALs/e1521.halxg.cloudera.com,36020,1382314725312-splitting/e1521.halxg.cloudera.com%2C36020%2C1382314725312.1382315934942 to hdfs://e1119.halxg.cloudera.com:35802/hbase/oldWALs/e1521.halxg.cloudera.com%2C36020%2C1382314725312.1382315934942
2013-10-20 17:40:38,889 DEBUG [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://e1119.halxg.cloudera.com:35802/hbase/WALs/e1521.halxg.cloudera.com,36020,1382314725312-splitting/e1521.halxg.cloudera.com%2C36020%2C1382314725312.1382315971393 to hdfs://e1119.halxg.cloudera.com:35802/hbase/oldWALs/e1521.halxg.cloudera.com%2C36020%2C1382314725312.1382315971393
2013-10-20 17:40:38,906 INFO  [MASTER_SERVER_OPERATIONS-e1119:36000-0] master.SplitLogManager: finished splitting (more than or equal to) 255322969 bytes in 3 log files in [hdfs://e1119.halxg.cloudera.com:35802/hbase/WALs/e1521.halxg.cloudera.com,36020,1382314725312-splitting] in 5439ms
2013-10-20 17:40:38,907 INFO  [MASTER_SERVER_OPERATIONS-e1119:36000-0] handler.ServerShutdownHandler: Reassigning 4 region(s) that e1521.halxg.cloudera.com,36020,1382314725312 was carrying (and 0 regions(s) that were opening on this server)
2013-10-20 17:40:39,269 DEBUG [MASTER_SERVER_OPERATIONS-e1119:36000-0] master.DeadServer: Finished processing e1521.halxg.cloudera.com,36020,1382314725312
2013-10-20 17:40:39,269 INFO  [MASTER_SERVER_OPERATIONS-e1119:36000-0] handler.ServerShutdownHandler: Finished processing of shutdown of e1521.halxg.cloudera.com,36020,1382314725312
2013-10-20 17:40:46,764 DEBUG [FifoRpcScheduler.handler1-thread-11] master.ServerManager: Server REPORT rejected; currently processing e1521.halxg.cloudera.com,36020,1382314725312 as dead server
2013-10-20 17:40:46,871 ERROR [FifoRpcScheduler.handler1-thread-13] master.HMaster: Region server e1521.halxg.cloudera.com,36020,1382314725312 reported a fatal error:
ABORTING region server e1521.halxg.cloudera.com,36020,1382314725312: org.apache.hadoop.hbase.YouAreDeadException: Server REPORT rejected; currently processing e1521.halxg.cloudera.com,36020,1382314725312 as dead server
org.apache.hadoop.hbase.YouAreDeadException: org.apache.hadoop.hbase.YouAreDeadException: Server REPORT rejected; currently processing e1521.halxg.cloudera.com,36020,1382314725312 as dead server
Caused by: org.apache.hadoop.hbase.ipc.RemoteWithExtrasException(org.apache.hadoop.hbase.YouAreDeadException): org.apache.hadoop.hbase.YouAreDeadException: Server REPORT rejected; currently processing e1521.halxg.cloudera.com,36020,1382314725312 as dead server
2013-10-20 17:40:46,872 ERROR [FifoRpcScheduler.handler1-thread-14] master.HMaster: Region server e1521.halxg.cloudera.com,36020,1382314725312 reported a fatal error:
ABORTING region server e1521.halxg.cloudera.com,36020,1382314725312: IOE in log roller
java.io.FileNotFoundException: File does not exist: hdfs://e1119.halxg.cloudera.com:35802/hbase/WALs/e1521.halxg.cloudera.com,36020,1382314725312/e1521.halxg.cloudera.com%2C36020%2C1382314725312.1382315971393
2013-10-20 17:40:48,075 ERROR [FifoRpcScheduler.handler1-thread-15] master.HMaster: Region server e1521.halxg.cloudera.com,36020,1382314725312 reported a fatal error:
ABORTING region server e1521.halxg.cloudera.com,36020,1382314725312: regionserver:36020-0x1419fea35edda89 regionserver:36020-0x1419fea35edda89 received expired from ZooKeeper, aborting
2013-10-20 17:53:15,272 INFO  [master:e1119:36000] master.MasterFileSystem: Log folder hdfs://e1119.halxg.cloudera.com:35802/hbase/WALs/e1521.halxg.cloudera.com,36020,1382314725312 doesn't belong to a known region server, splitting
2013-10-20 17:53:15,806 INFO  [MASTER_SERVER_OPERATIONS-e1119:36000-0] handler.ServerShutdownHandler: Splitting logs for e1521.halxg.cloudera.com,36020,1382314725312 before assignment.
2013-10-20 17:53:15,825 DEBUG [MASTER_SERVER_OPERATIONS-e1119:36000-0] master.MasterFileSystem: Renamed region directory: hdfs://e1119.halxg.cloudera.com:35802/hbase/WALs/e1521.halxg.cloudera.com,36020,1382314725312-splitting
2013-10-20 17:53:15,825 INFO  [MASTER_SERVER_OPERATIONS-e1119:36000-0] master.SplitLogManager: dead splitlog workers [e1521.halxg.cloudera.com,36020,1382314725312]
2013-10-20 17:53:15,831 INFO  [MASTER_SERVER_OPERATIONS-e1119:36000-0] master.SplitLogManager: started splitting 1 logs in [hdfs://e1119.halxg.cloudera.com:35802/hbase/WALs/e1521.halxg.cloudera.com,36020,1382314725312-splitting]
2013-10-20 17:53:15,998 DEBUG [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://e1119.halxg.cloudera.com:35802/hbase/WALs/e1521.halxg.cloudera.com,36020,1382314725312-splitting/e1521.halxg.cloudera.com%2C36020%2C1382314725312.1382316046805 to hdfs://e1119.halxg.cloudera.com:35802/hbase/oldWALs/e1521.halxg.cloudera.com%2C36020%2C1382314725312.1382316046805
2013-10-20 17:53:16,014 INFO  [MASTER_SERVER_OPERATIONS-e1119:36000-0] master.SplitLogManager: finished splitting (more than or equal to) 15 bytes in 1 log files in [hdfs://e1119.halxg.cloudera.com:35802/hbase/WALs/e1521.halxg.cloudera.com,36020,1382314725312-splitting] in 183ms
2013-10-20 17:53:16,015 INFO  [MASTER_SERVER_OPERATIONS-e1119:36000-0] handler.ServerShutdownHandler: Reassigning 0 region(s) that e1521.halxg.cloudera.com,36020,1382314725312 was carrying (and 0 regions(s) that were opening on this server)
{noformat}", ,,,,,,,
HBASE-9815,"Hi there, we are trying to migrate a app from MySQL to HBase. One kind of the queries is pagination with large offset and small limit. We don't have too many such queries and so both MySQL and HBase should survive. (MySQL has no index for offset either.)

When comparing the performance on both systems, we found something interest: write ~1M values in a single row, and query with offset = 1M. So all values should be scanned on RS side.

When running the query on MySQL, the first query is pretty slow (more than 1 second) and then repeat the same query, it will become very low latency.

HBase on the other hand, repeating the query does not help much (~1s forever). I can confirm that all data are in block cache and all the time is spent on in-memory data processing. (We have flushed data to disk.)

I found ""reseek"" is the hot spot. It is caused by ColumnPaginationFilter returning NEXT_COL. If I replace this line by returning SKIP (which causes to call next rather than reseek), the latency is reduced to ~100ms.

So I think there must be some room for optimization.", ,,1,,,,,
HBASE-9818,"HFileBlock#istream seems to be null.  I was wondering should we hide FSDataInputStreamWrapper#useHBaseChecksum.

By the way, this happened when online schema change is enabled (encoding)

{noformat}
2013-10-22 10:58:43,321 ERROR [RpcServer.handler=28,port=36020] regionserver.HRegionServer:
java.lang.NullPointerException
        at org.apache.hadoop.hbase.io.hfile.HFileBlock$AbstractFSReader.readAtOffset(HFileBlock.java:1200)
        at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderV2.readBlockDataInternal(HFileBlock.java:1436)
        at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderV2.readBlockData(HFileBlock.java:1318)
        at org.apache.hadoop.hbase.io.hfile.HFileReaderV2.readBlock(HFileReaderV2.java:359)
        at org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader.loadDataBlockWithScanInfo(HFileBlockIndex.java:254)
        at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.seekTo(HFileReaderV2.java:503)
        at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.reseekTo(HFileReaderV2.java:553)
        at org.apache.hadoop.hbase.regionserver.StoreFileScanner.reseekAtOrAfter(StoreFileScanner.java:245)
        at org.apache.hadoop.hbase.regionserver.StoreFileScanner.reseek(StoreFileScanner.java:166)
        at org.apache.hadoop.hbase.regionserver.StoreFileScanner.enforceSeek(StoreFileScanner.java:361)
        at org.apache.hadoop.hbase.regionserver.KeyValueHeap.pollRealKV(KeyValueHeap.java:336)
        at org.apache.hadoop.hbase.regionserver.KeyValueHeap.generalizedSeek(KeyValueHeap.java:293)
        at org.apache.hadoop.hbase.regionserver.KeyValueHeap.requestSeek(KeyValueHeap.java:258)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.reseek(StoreScanner.java:603)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:476)
        at org.apache.hadoop.hbase.regionserver.KeyValueHeap.next(KeyValueHeap.java:129)
        at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.populateResult(HRegion.java:3546)
        at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.nextInternal(HRegion.java:3616)
        at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.nextRaw(HRegion.java:3494)
        at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.nextRaw(HRegion.java:3485)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.scan(HRegionServer.java:3079)
        at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:27022)
        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:1979)
        at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:90)
        at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.consumerLoop(SimpleRpcScheduler.java:160)
        at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.access$000(SimpleRpcScheduler.java:38)
        at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler$1.run(SimpleRpcScheduler.java:110)
        at java.lang.Thread.run(Thread.java:724)
2013-10-22 10:58:43,665 ERROR [RpcServer.handler=23,port=36020] regionserver.HRegionServer:
org.apache.hadoop.hbase.exceptions.OutOfOrderScannerNextException: Expected nextCallSeq: 53438 But the nextCallSeq got from client: 53437; request=scanner_id: 1252577470624375060 number_of_rows: 100 close_scanner: false next_call_seq: 53437
        at org.apache.hadoop.hbase.regionserver.HRegionServer.scan(HRegionServer.java:3030)
        at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:27022)
        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:1979)
        at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:90)
        at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.consumerLoop(SimpleRpcScheduler.java:160)
        at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.access$000(SimpleRpcScheduler.java:38)
        at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler$1.run(SimpleRpcScheduler.java:110)
        at java.lang.Thread.run(Thread.java:724)
{noformat}", ,,,,,,,
HBASE-9820,"While upgrading HBase dependency on Pig via PIG-3529, I've noticed that method {{Replication.decorateMasterConfiguration()}} can throw {{NullPointerException}} ([code|https://github.com/apache/hbase/blob/0.96/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java]) in case when property HBASE_MASTER_LOGCLEANER_PLUGINS won't be defined.

The issue was more on a pig side where we weren't propagating default HBase configuration resources (such as {{hbase-default.xml}}), but I was curious if it's expected that this property will be always defined? We might want to tweak the code a bit if this property can be null.", ,1,,,,,,
HBASE-9821,"We use a random number as our scanner id.  In one server, we guarantee that the scanner id is unique.  However, it is not guaranteed among different servers.  When a region server restarts quickly, we could run into some scanner id collision issue.

In one of my run:

{noformat}
2013-10-21 22:43:09,071 INFO  [RpcServer.handler=2,port=36020] regionserver.HRegionServer: Client tried to access missing scanner 4305495321392639779
2013-10-21 22:43:09,056 INFO  [RpcServer.handler=0,port=36020] regionserver.HRegionServer: Client tried to access missing scanner 4871518173034616791
2013-10-21 22:43:09,054 INFO  [RpcServer.handler=29,port=36020] regionserver.HRegionServer: Client tried to access missing scanner 2494346173615963501
2013-10-21 22:43:09,046 INFO  [RpcServer.handler=28,port=36020] regionserver.HRegionServer: Client tried to access missing scanner 8522578499834310167
2013-10-21 22:43:09,037 INFO  [RpcServer.handler=27,port=36020] regionserver.HRegionServer: Client tried to access missing scanner 6621035169671703961
2013-10-21 22:43:09,011 ERROR [RpcServer.handler=20,port=36020] regionserver.HRegionServer:
org.apache.hadoop.hbase.exceptions.OutOfOrderScannerNextException: Expected nextCallSeq: 18 But the nextCallSeq got from client: 4470; request=scanner_id: 848804760654927372 number_of_rows: 100 close_scanner: false next_call_seq: 4470
        at org.apache.hadoop.hbase.regionserver.HRegionServer.scan(HRegionServer.java:3030)
        at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:27022)
        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:1979)
        at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:90)
        at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.consumerLoop(SimpleRpcScheduler.java:160)
        at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.access$000(SimpleRpcScheduler.java:38)
        at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler$1.run(SimpleRpcScheduler.java:110)
        at java.lang.Thread.run(Thread.java:724)
2013-10-21 22:43:09,000 INFO  [RpcServer.handler=25,port=36020] regionserver.HRegionServer: Client tried to access missing scanner 4162107982028594792
{noformat}

Normally, the nextCallSeq can not be that different between the expected (18) and the one sent over from client (4470).  It must be because the new server happens to use the same scanner id.
", ,,,,,,,
HBASE-9825,"Running on hadoop 2, LoadIncrementalHFiles gives the following exception when loading from a remote cluster.
{code}
org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles$3@455e455e, java.io.IOException: j ava.io.IOException: java.lang.UnsupportedOperationException: Immutable Configuration
        at org.apache.hadoop.hbase.regionserver.CompoundConfiguration.setClass(CompoundConfiguration.java:516)
        at org.apache.hadoop.ipc.RPC.setProtocolEngine(RPC.java:195)
        at org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol(NameNodeProxies.java:250)
        at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:169)
        at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:130)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:482)
        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:445)
        at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:136)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2429)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:88)
        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2463)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2445)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:363)
        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:283)
        at org.apache.hadoop.hbase.regionserver.Store.assertBulkLoadHFileOk(Store.java:571)
        at org.apache.hadoop.hbase.regionserver.HRegion.bulkLoadHFiles(HRegion.java:3689)
        at org.apache.hadoop.hbase.regionserver.HRegion.bulkLoadHFiles(HRegion.java:3637)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.bulkLoadHFiles(HRegionServer.java:2939)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:60)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:37)
        at java.lang.reflect.Method.invoke(Method.java:611)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:320)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1426)
        at org.apache.hadoop.hbase.client.ServerCallable.withRetries(ServerCallable.java:186)
        at org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.tryAtomicRegionLoad(LoadIncrementalHFiles.java:567)
        at org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles$1.call(LoadIncrementalHFiles.java:317)
        at org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles$1.call(LoadIncrementalHFiles.java:315)
{code}

This does not happen when loading from the same FileSystem.", ,1,,,,,,
HBASE-9843,"This mainly fixes issues when we had ""long"" errors, for example a multi blocked when trying to obtain a lock that was finally failing after 60s. Previously we were trying only for 5 minutes. We now do all the tries. I've fixed stuff around this area to make it work.

There is also more logs.

I've changed the back off array. With the default pause of 100ms, even after 20 tries we still retry every 10s.

I've also changed the max per RS to something minimal. If the cluster is not in a very good state it's less aggressive. It seems to be a better default.

I've done two tests:
 - on a small; homogeneous cluster, I had the same performances
 - on a bigger, but heterogeneous cluster it was twice as fast.
", ,1,1,,,,,
HBASE-9847,"1) Started hbase cluster with two masters 
2) started shell.
3) Master switch happened.
From now onward not able to perform list command without restarting the shell. Its always pointing to old master.
{code}
hbase(main):003:0> list
TABLE

ERROR: java.net.ConnectException: Connection refused

Here is some help for this command:
List all tables in hbase. Optional regular expression parameter could
be used to filter the output. Examples:

  hbase> list
  hbase> list 'abc.*'

{code}

 ", ,,,,,,,
HBASE-9849,"If ""hbase.rest.readonly"" was set, all write operations should be forbidden via REST, right? So table schema deletion should also be forbidden in readonly mode?",1,,,,,,,
HBASE-9863,"TestZooKeeper#testRegionAssignmentAfterMasterRecoveryDueToZKExpiry sometimes hung.
Here were two recent occurrences:
https://builds.apache.org/job/PreCommit-HBASE-Build/7676/console
https://builds.apache.org/job/PreCommit-HBASE-Build/7671/console

There were 9 occurrences of the following in both stack traces:
{code}
""FifoRpcScheduler.handler1-thread-5"" daemon prio=10 tid=0x09df8800 nid=0xc17 waiting for monitor entry [0x6fdf8000]
   java.lang.Thread.State: BLOCKED (on object monitor)
  at org.apache.hadoop.hbase.master.TableNamespaceManager.isTableAvailableAndInitialized(TableNamespaceManager.java:250)
  - waiting to lock <0x7f69b5f0> (a org.apache.hadoop.hbase.master.TableNamespaceManager)
  at org.apache.hadoop.hbase.master.HMaster.isTableNamespaceManagerReady(HMaster.java:3146)
  at org.apache.hadoop.hbase.master.HMaster.getNamespaceDescriptor(HMaster.java:3105)
  at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.java:1743)
  at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.java:1782)
  at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java:38221)
  at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:1983)
  at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:92)
{code}
The test hung here:
{code}
""pool-1-thread-1"" prio=10 tid=0x74f7b800 nid=0x5aa5 in Object.wait() [0x74efe000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
  at java.lang.Object.wait(Native Method)
  - waiting on <0xcc848348> (a org.apache.hadoop.hbase.ipc.RpcClient$Call)
  at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1436)
  - locked <0xcc848348> (a org.apache.hadoop.hbase.ipc.RpcClient$Call)
  at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1654)
  at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1712)
  at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$BlockingStub.createTable(MasterProtos.java:40372)
  at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$5.createTable(HConnectionManager.java:1931)
  at org.apache.hadoop.hbase.client.HBaseAdmin$2.call(HBaseAdmin.java:598)
  at org.apache.hadoop.hbase.client.HBaseAdmin$2.call(HBaseAdmin.java:594)
  at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:116)
  - locked <0x7faa26d0> (a org.apache.hadoop.hbase.client.RpcRetryingCaller)
  at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:94)
  - locked <0x7faa26d0> (a org.apache.hadoop.hbase.client.RpcRetryingCaller)
  at org.apache.hadoop.hbase.client.HBaseAdmin.executeCallable(HBaseAdmin.java:3124)
  at org.apache.hadoop.hbase.client.HBaseAdmin.createTableAsync(HBaseAdmin.java:594)
  at org.apache.hadoop.hbase.client.HBaseAdmin.createTable(HBaseAdmin.java:485)
  at org.apache.hadoop.hbase.TestZooKeeper.testRegionAssignmentAfterMasterRecoveryDueToZKExpiry(TestZooKeeper.java:486)
{code}", ,,,,,,,
HBASE-9865,"WALEdit.heapSize() is incorrect in certain replication scenarios which may cause RegionServers to go OOM.

A little background on this issue.  We noticed that our source replication regionservers would get into gc storms and sometimes even OOM. 
We noticed a case where it showed that there were around 25k WALEdits to replicate, each one with an ArrayList of KeyValues.  The array list had a capacity of around 90k (using 350KB of heap memory) but had around 6 non null entries.

When the ReplicationSource.readAllEntriesToReplicateOrNextFile() gets a WALEdit it removes all kv's that are scoped other than local.  

But in doing so we don't account for the capacity of the ArrayList when determining heapSize for a WALEdit.  The logic for shipping a batch is whether you have hit a size capacity or number of entries capacity.  

Therefore if have a WALEdit with 25k entries and suppose all are removed: 
The size of the arrayList is 0 (we don't even count the collection's heap size currently) but the capacity is ignored.
This will yield a heapSize() of 0 bytes while in the best case it would be at least 100000 bytes (provided you pass initialCapacity and you have 32 bit JVM) 

I have some ideas on how to address this problem and want to know everyone's thoughts:

1. We use a probabalistic counter such as HyperLogLog and create something like:
	* class CapacityEstimateArrayList implements ArrayList
		** this class overrides all additive methods to update the probabalistic counts
		** it includes one additional method called estimateCapacity (we would take estimateCapacity - size() and fill in sizes for all references)
	* Then we can do something like this in WALEdit.heapSize:
	
{code}
  public long heapSize() {
    long ret = ClassSize.ARRAYLIST;
    for (KeyValue kv : kvs) {
      ret += kv.heapSize();
    }
    long nullEntriesEstimate = kvs.getCapacityEstimate() - kvs.size();
    ret += ClassSize.align(nullEntriesEstimate * ClassSize.REFERENCE);
    if (scopes != null) {
      ret += ClassSize.TREEMAP;
      ret += ClassSize.align(scopes.size() * ClassSize.MAP_ENTRY);
      // TODO this isn't quite right, need help here
    }
    return ret;
  }	
{code}

2. In ReplicationSource.removeNonReplicableEdits() we know the size of the array originally, and we provide some percentage threshold.  When that threshold is met (50% of the entries have been removed) we can call kvs.trimToSize()

3. in the heapSize() method for WALEdit we could use reflection (Please don't shoot me for this) to grab the actual capacity of the list.  Doing something like this:

{code}
public int getArrayListCapacity()  {
    try {
      Field f = ArrayList.class.getDeclaredField(""elementData"");
      f.setAccessible(true);
      return ((Object[]) f.get(kvs)).length;
    } catch (Exception e) {
      log.warn(""Exception in trying to get capacity on ArrayList"", e);
      return kvs.size();
    }
{code}


I am partial to (1) using HyperLogLog and creating a CapacityEstimateArrayList, this is reusable throughout the code for other classes that implement HeapSize which contains ArrayLists.  The memory footprint is very small and it is very fast.  The issue is that this is an estimate, although we can configure the precision we most likely always be conservative.  The estimateCapacity will always be less than the actualCapacity, but it will be close. I think that putting the logic in removeNonReplicableEdits will work, but this only solves the heapSize problem in this particular scenario.  Solution 3 is slow and horrible but that gives us the exact answer.

I would love to hear if anyone else has any other ideas on how to remedy this problem?  I have code for trunk and 0.94 for all 3 ideas and can provide a patch if the community thinks any of these approaches is a viable one.

", ,,1,,,,,
HBASE-9866,"In one use case, someone was trying to authorize with the REST server as a proxy user. That mode is not supported today. 
The curl request would be something like (assuming SPNEGO auth) - 
curl -i --negotiate -u : http://<HOST>:<PO",1,,,,,,,
HBASE-9867,"WALEdit.heapSize() is incorrect in certain replication scenarios which may cause RegionServers to go OOM.

A little background on this issue.  We noticed that our source replication regionservers would get into gc storms and sometimes even OOM. 
We noticed a case where it showed that there were around 25k WALEdits to replicate, each one with an ArrayList of KeyValues.  The array list had a capacity of around 90k (using 350KB of heap memory) but had around 6 non null entries.

When the ReplicationSource.readAllEntriesToReplicateOrNextFile() gets a WALEdit it removes all kv's that are scoped other than local.  

But in doing so we don't account for the capacity of the ArrayList when determining heapSize for a WALEdit.  The logic for shipping a batch is whether you have hit a size capacity or number of entries capacity.  

Therefore if have a WALEdit with 25k entries and suppose all are removed: 
The size of the arrayList is 0 (we don't even count the collection's heap size currently) but the capacity is ignored.
This will yield a heapSize() of 0 bytes while in the best case it would be at least 100000 bytes (provided you pass initialCapacity and you have 32 bit JVM) 

I have some ideas on how to address this problem and want to know everyone's thoughts:

1. We use a probabalistic counter such as HyperLogLog and create something like:
	* class CapacityEstimateArrayList implements ArrayList
		** this class overrides all additive methods to update the probabalistic counts
		** it includes one additional method called estimateCapacity (we would take estimateCapacity - size() and fill in sizes for all references)
	* Then we can do something like this in WALEdit.heapSize:
	
{code}
  public long heapSize() {
    long ret = ClassSize.ARRAYLIST;
    for (KeyValue kv : kvs) {
      ret += kv.heapSize();
    }
    long nullEntriesEstimate = kvs.getCapacityEstimate() - kvs.size();
    ret += ClassSize.align(nullEntriesEstimate * ClassSize.REFERENCE);
    if (scopes != null) {
      ret += ClassSize.TREEMAP;
      ret += ClassSize.align(scopes.size() * ClassSize.MAP_ENTRY);
      // TODO this isn't quite right, need help here
    }
    return ret;
  }	
{code}

2. In ReplicationSource.removeNonReplicableEdits() we know the size of the array originally, and we provide some percentage threshold.  When that threshold is met (50% of the entries have been removed) we can call kvs.trimToSize()

3. in the heapSize() method for WALEdit we could use reflection (Please don't shoot me for this) to grab the actual capacity of the list.  Doing something like this:

{code}
public int getArrayListCapacity()  {
    try {
      Field f = ArrayList.class.getDeclaredField(""elementData"");
      f.setAccessible(true);
      return ((Object[]) f.get(kvs)).length;
    } catch (Exception e) {
      log.warn(""Exception in trying to get capacity on ArrayList"", e);
      return kvs.size();
    }
{code}


I am partial to (1) using HyperLogLog and creating a CapacityEstimateArrayList, this is reusable throughout the code for other classes that implement HeapSize which contains ArrayLists.  The memory footprint is very small and it is very fast.  The issue is that this is an estimate, although we can configure the precision we most likely always be conservative.  The estimateCapacity will always be less than the actualCapacity, but it will be close. I think that putting the logic in removeNonReplicableEdits will work, but this only solves the heapSize problem in this particular scenario.  Solution 3 is slow and horrible but that gives us the exact answer.

I would love to hear if anyone else has any other ideas on how to remedy this problem?  I have code for trunk and 0.94 for all 3 ideas and can provide a patch if the community thinks any of these approaches is a viable one.

", ,,1,,,,,
HBASE-9868,"Profiling the client shows that we're spending some time in array copy (10% of the code execution, 3% of the total time) in some array copy that we can avoid.", ,,1,,,,,
HBASE-9869,"It javadoc says: ""TODO: This method during writing consumes 15% of CPU doing lookup"". This is still true, says Yourkit. With 0.96, we also spend more time in these methods. We retry more, and the AsyncProcess calls it in parallel.

I don't have the patch for this yet, but I will spend some time on it.", ,,1,,,,,
HBASE-9870,"In this method, we have

{code}
    if (block.getBlockType() == BlockType.ENCODED_DATA) {
      if (block.getDataBlockEncodingId() == onDisk.getId()) {
        // The block is already in the desired in-cache encoding.
        return block;
      }
{code}

This assumes onDisk encoding is the same as that of inCache.  This is not true when we change the encoding of a CF.  This could be one of the reasons I got data loss with online encoding change?

If I make sure onDisk == inCache all the time, my ITBLL with online encoding change worked once for me.", ,1,,,,,,
HBASE-9873,We should consider tags in the existing cells as well as tags coming in the cells within Increment/Append, ,,,,,,,
HBASE-9874,We should consider tags in the existing cells as well as tags coming in the cells within Increment/Append, ,,,,,,,
HBASE-9890,"If Map-Reduce jobs are started with by a proxy user that has already the delegation tokens, we get an exception on ""obtain token"" since the proxy user doesn't have the kerberos auth.

For example:
 * If we use oozie to execute RowCounter - oozie will get the tokens required (HBASE_AUTH_TOKEN) and it will start the RowCounter. Once the RowCounter tries to obtain the token, it will get an exception.
 * If we use oozie to execute LoadIncrementalHFiles - oozie will get the tokens required (HDFS_DELEGATION_TOKEN) and it will start the LoadIncrementalHFiles. Once the LoadIncrementalHFiles tries to obtain the token, it will get an exception.

{code}
 org.apache.hadoop.hbase.security.AccessDeniedException: Token generation only allowed for Kerberos authenticated clients
    at org.apache.hadoop.hbase.security.token.TokenProvider.getAuthenticationToken(TokenProvider.java:87)
{code}

{code}
org.apache.hadoop.ipc.RemoteException(java.io.IOException): Delegation Token can be issued only with kerberos or web authentication
	at org.apache.hadoop.hdfs.DFSClient.getDelegationToken(DFSClient.java:783)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getDelegationToken(DistributedFileSystem.java:868)
	at org.apache.hadoop.fs.FileSystem.collectDelegationTokens(FileSystem.java:509)
	at org.apache.hadoop.fs.FileSystem.addDelegationTokens(FileSystem.java:487)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:130)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:111)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:85)
	at org.apache.hadoop.filecache.TrackerDistributedCacheManager.getDelegationTokens(TrackerDistributedCacheManager.java:949)
	at org.apache.hadoop.mapred.JobClient.copyAndConfigureFiles(JobClient.java:854)
	at org.apache.hadoop.mapred.JobClient.copyAndConfigureFiles(JobClient.java:743)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:945)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:566)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:596)
	at org.apache.hadoop.hbase.mapreduce.RowCounter.main(RowCounter.java:173)
{code}",1,,,,,,,
HBASE-9902,"When Region server's time is ahead of Master's time and the difference is more than hbase.master.maxclockskew value, region server startup is not failing with ClockOutOfSyncException.
This causes some abnormal behavior as detected by our Tests.

    ServerManager.java#checkClockSkew
          long skew = System.currentTimeMillis() - serverCurrentTime;
        if (skew > maxSkew) {
          String message = ""Server "" + serverName + "" has been "" +
            ""rejected; Reported time is too far out of sync with master.  "" +
            ""Time difference of "" + skew + ""ms > max allowed of "" + maxSkew + ""ms"";
          LOG.warn(message);
          throw new ClockOutOfSyncException(message);
        }

    Above line results in negative value when Master's time is lesser than region server time and  "" if (skew > maxSkew) "" check fails to find the skew in this case.


    Please Note: This was tested in hbase 0.94.11 version and the trunk also currently has the same logic.

The fix for the same would be to make the skew positive value first as below:

 long skew = System.currentTimeMillis() - serverCurrentTime;
    skew = (skew < 0 ? -skew : skew);
    if (skew > maxSkew) {.....", ,1,,,,,,
HBASE-9904,"The HTable client cannot retry a scan operation in the getRegionServerWithRetries code path.
This will result in the client missing data. This can be worked around using hbase.client.retries.number to 1.

The whole problem is that Callable knows nothing about retries and the protocol it dances to as well doesn't support retires.
This fix will keep Callable protocol (ugly thing worth merciless refactoring) intact but will change
ScannerCallable to anticipate retries. What we want is to make failed operations to be identities for outside world:
N1 , N2 , F3 , N3 , F4 , F4 , N4 ... = N1 , N2 , N3 , N4 ...
where Nk are successful operation and Fk are failed operations.", ,,,,,,,
HBASE-9906,"After snaphot restore, we see failures to find the table in meta:
{code}
> disable 'tablefour'
> restore_snapshot 'snapshot_tablefour'
> enable 'tablefour'
ERROR: Table tablefour does not exist.'
{code}

This is quite subtle. From the looks of it, we successfully restore the snapshot, do the meta updates, return to the client about the status. The client then tries to do an operation for the table (like enable table, or scan in the test outputs) which fails because the meta entry for the region seems to be gone (in case of single region, the table will be reported missing). Subsequent attempts for creating the table will also fail because the table directories will be there, but not the meta entries.
For restoring meta entries, we are doing a delete then a put to the same region:
{code}
2013-11-04 10:39:51,582 INFO org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper: region to restore: 76d0e2b7ec3291afcaa82e18a56ccc30
2013-11-04 10:39:51,582 INFO org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper: region to remove: fa41edf43fe3ee131db4a34b848ff432
...
2013-11-04 10:39:52,102 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted [{ENCODED => fa41edf43fe3ee131db4a34b848ff432, NAME => 'tablethree_mod,,1383559723345.fa41edf43fe3ee131db4a34b848ff432.', STARTKEY => '', ENDKEY => ''}, {ENCODED => 76d0e2b7ec3291afcaa82e18a56ccc30, NAME => 'tablethree_mod,,1383561123097.76d0e2b7ec3291afcaa82e18a56ccc30.', STARTKE
2013-11-04 10:39:52,111 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Added 1
{code}
The root cause for this sporadic failure is that, the delete and subsequent put will have the same timestamp if they execute in the same ms. The delete will override the put in the same ts, even though the put have a larger ts.

See: HBASE-9905, HBASE-8770
Credit goes to [~huned] for reporting this bug. ", ,,,,,,,
HBASE-9915,"While debugging why reseek is so slow I found that it is quite broken for encoded scanners.
The problem is this:
AbstractScannerV2.reseekTo(...) calls isSeeked() to check whether scanner was seeked or not. If it was it checks whether the KV we want to seek to is in the current block, if not it always consults the index blocks again.
isSeeked checks the blockBuffer member, which is not used by EncodedScannerV2 and thus always returns false, which in turns causes an index lookup for each reseek.", ,,1,,,,,
HBASE-9917,"While debugging why reseek is so slow I found that it is quite broken for encoded scanners.
The problem is this:
AbstractScannerV2.reseekTo(...) calls isSeeked() to check whether scanner was seeked or not. If it was it checks whether the KV we want to seek to is in the current block, if not it always consults the index blocks again.
isSeeked checks the blockBuffer member, which is not used by EncodedScannerV2 and thus always returns false, which in turns causes an index lookup for each reseek.", ,,1,,,,,
HBASE-9918,"TestZooKeeper#testRegionAssignmentAfterMasterRecoveryDueToZKExpiry always failed at the following verification for me in my dev env(you have to run the single test  not the whole TestZooKeeper suite to reproduce)
{code}
assertEquals(""Number of rows should be equal to number of puts."", numberOfPuts, numberOfRows);
{code}

We missed two ZK listeners after master recovery MasterAddressTracker & ZKNamespaceManager. 

My current patch is to fix the JIRA issue while I'm wondering if we should totally remove the master failover implementation when ZK session expired because this causes reinitialize HMaster partially which is error prone and not a clean state to start from. 

 


", ,,,,,,,
HBASE-9926,"Currently the scanner doesn't check if a region is closing/closed. If a region is closed, then reopened, an old scanner could still refer to the closed HRegion instance.  So the scanner will miss some store file changes due to compaction.", ,,,,,,,
HBASE-9931,"Under load when I disabled network interface, all HBase threads were locked out.  I was expecting these threads to be released based on client.operation.timeout and rpc,timeout.

Here is a link for  thread dump.
https://www.dropbox.com/s/y1ng3yoywq09x2u/HBaseClient_Threaddump.txt

", ,,,,,,,
HBASE-9936,"I made a mistake while after re-enabling a table on which I did an `alter' to add a coprocessor: the .jar I specified wasn't a self-contained jar, and thus some dependent classes couldn't be found.

{code}
2013-11-09 02:39:05,994 INFO  [AM.ZK.Worker-pool2-t17] master.RegionStates: Transitioned {8568640c1da6ce0d5e27b656d28fe9fd state=PENDING_OPEN, ts=1383993545988, server=192.168.42.108,59570,1383993435386} to {8568640c1da6ce0d5e27b656d28fe9fd state=OPENING, ts=1383993545994, server=192.168.42.108,59570,1383993435386}
2013-11-09 02:39:05,995 DEBUG [RS_OPEN_REGION-192.168.42.108:59570-2] coprocessor.CoprocessorHost: Loading coprocessor class com.example.foo.hbase.FooCoprocessor with path /Users/tsuna/src/foo/target/scala-2.10/foo_2.10-0.1.jar and priority 1000
2013-11-09 02:39:06,005 DEBUG [RS_OPEN_REGION-192.168.42.108:59570-2] util.CoprocessorClassLoader: Finding class: com.example.foo.hbase.FooCoprocessor
2013-11-09 02:39:06,006 DEBUG [RS_OPEN_REGION-192.168.42.108:59570-2] util.CoprocessorClassLoader: Skipping exempt class org.apache.hadoop.hbase.coprocessor.BaseRegionObserver - delegating directly to parent
2013-11-09 02:39:06,007 DEBUG [RS_OPEN_REGION-192.168.42.108:59570-2] util.CoprocessorClassLoader: Skipping exempt class java.lang.Object - delegating directly to parent
2013-11-09 02:39:06,007 DEBUG [RS_OPEN_REGION-192.168.42.108:59570-2] util.CoprocessorClassLoader: Finding class: org.slf4j.LoggerFactory
2013-11-09 02:39:06,007 DEBUG [RS_OPEN_REGION-192.168.42.108:59570-2] util.CoprocessorClassLoader: Class org.slf4j.LoggerFactory not found - delegating to parent
2013-11-09 02:39:06,008 DEBUG [RS_OPEN_REGION-192.168.42.108:59570-2] util.CoprocessorClassLoader: Finding class: scala.collection.mutable.StringBuilder
2013-11-09 02:39:06,008 DEBUG [RS_OPEN_REGION-192.168.42.108:59570-2] util.CoprocessorClassLoader: Class scala.collection.mutable.StringBuilder not found - delegating to parent
2013-11-09 02:39:06,008 DEBUG [RS_OPEN_REGION-192.168.42.108:59570-2] util.CoprocessorClassLoader: Class scala.collection.mutable.StringBuilder not found in parent loader
2013-11-09 02:39:06,008 ERROR [RS_OPEN_REGION-192.168.42.108:59570-2] handler.OpenRegionHandler: Failed open of region=foo,,1383899959121.8568640c1da6ce0d5e27b656d28fe9fd., starting to roll back the global memstore size.
java.lang.IllegalStateException: Could not instantiate a region instance.
        at org.apache.hadoop.hbase.regionserver.HRegion.newHRegion(HRegion.java:3820)
        at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:4078)
        at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:4030)
        at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:3981)
        at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.openRegion(OpenRegionHandler.java:475)
        at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:140)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
        at java.lang.Thread.run(Thread.java:680)
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at org.apache.hadoop.hbase.regionserver.HRegion.newHRegion(HRegion.java:3817)
        ... 9 more
Caused by: java.lang.NoClassDefFoundError: scala/collection/mutable/StringBuilder
        at com.example.foo.hbase.FooCoprocessor.start(FooCoprocessor.scala:18)
        at org.apache.hadoop.hbase.coprocessor.CoprocessorHost$Environment.startup(CoprocessorHost.java:636)
        at org.apache.hadoop.hbase.coprocessor.CoprocessorHost.loadInstance(CoprocessorHost.java:259)
        at org.apache.hadoop.hbase.coprocessor.CoprocessorHost.load(CoprocessorHost.java:212)
        at org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.loadTableCoprocessors(RegionCoprocessorHost.java:192)
        at org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.<init>(RegionCoprocessorHost.java:154)
        at org.apache.hadoop.hbase.regionserver.HRegion.<init>(HRegion.java:532)
        at org.apache.hadoop.hbase.regionserver.HRegion.<init>(HRegion.java:442)
        ... 14 more
Caused by: java.lang.ClassNotFoundException: scala.collection.mutable.StringBuilder
        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
        at org.apache.hadoop.hbase.util.CoprocessorClassLoader.loadClass(CoprocessorClassLoader.java:299)
        ... 22 more
2013-11-09 02:39:06,010 INFO  [RS_OPEN_REGION-192.168.42.108:59570-2] handler.OpenRegionHandler: Opening of region {ENCODED => 8568640c1da6ce0d5e27b656d28fe9fd, NAME => 'foo,,1383899959121.8568640c1da6ce0d5e27b656d28fe9fd.', STARTKEY => '', ENDKEY => ''} failed, transitioning from OPENING to FAILED_OPEN in ZK, expecting version 1
{code}

The master retried 10 times and then seems to have given up.  I'm trying to point it to a different jar, but I can't, and the table now seems stuck in a not-enabled-nor-disabled state:

{code}
hbase(main):002:0> disable 'foo'

ERROR: org.apache.hadoop.hbase.TableNotEnabledException: foo
	at org.apache.hadoop.hbase.master.handler.DisableTableHandler.prepare(DisableTableHandler.java:100)
	at org.apache.hadoop.hbase.master.HMaster.disableTable(HMaster.java:1979)
	at org.apache.hadoop.hbase.master.HMaster.disableTable(HMaster.java:1990)
	at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java:38217)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2146)
	at org.apache.hadoop.hbase.ipc.RpcServer$Handler.run(RpcServer.java:1851)

Here is some help for this command:
Start disable of named table: e.g. ""hbase> disable 't1'""


hbase(main):003:0> enable 'foo'

ERROR: org.apache.hadoop.hbase.TableNotDisabledException: foo
	at org.apache.hadoop.hbase.master.handler.EnableTableHandler.prepare(EnableTableHandler.java:109)
	at org.apache.hadoop.hbase.master.HMaster.enableTable(HMaster.java:1954)
	at org.apache.hadoop.hbase.master.HMaster.enableTable(HMaster.java:1965)
	at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java:38215)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2146)
	at org.apache.hadoop.hbase.ipc.RpcServer$Handler.run(RpcServer.java:1851)

Here is some help for this command:
Start enable of named table: e.g. ""hbase> enable 't1'""
{code}", ,1,,,,,,
HBASE-9939,"Under load when I disabled network interface, all HBase threads were locked out.  I was expecting these threads to be released based on client.operation.timeout and rpc,timeout.

Here is a link for  thread dump.
https://www.dropbox.com/s/y1ng3yoywq09x2u/HBaseClient_Threaddump.txt

", ,,,,,,,
HBASE-9949,The StoreScanner constructor has multiple stages and there can be a race betwwen an ongoing compaction and the StoreScanner constructor where we might get the list of scanners before a compaction and seek on those scanners after the compaction., ,,,,,,,
HBASE-9952,"In our QA run, certain snapshot restore request seemed to hang.
The following was found in master log:
{code}
2013-11-11 08:52:43,887 INFO org.apache.hadoop.hbase.util.FSVisitor: No logs under directory:hdfs://qeempress21:8020/apps/hbase/data/.hbase-snapshot/snapshot_tablethree_mod/WALs^M
2013-11-11 08:52:43,887 INFO org.apache.hadoop.hbase.master.RegionStates: Transitioned {e048f867bbb31702dc1dd0498db53b82 state=SPLIT, ts=1384159894259, server=QEEMPRESS23,60020,1384146414618} to {e048f867bbb31702dc1dd0498db53b82 state=OFFLINE, ts=1384159963887, server=null}^M
2013-11-11 08:52:43,896 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted [{ENCODED => 214a354a9e9f07d3c4e9420d2fdd8d28, NAME => 'tablethree_mod,,1384159891126.214a354a9e9f07d3c4e9420d2fdd8d28.', STARTKEY => '', ENDKEY => '\x5C011'}, {ENCODED => 4dad288061dac82bb420fb6dd526eea5, NAME => 'tablethree_mod,\x5C011,1384159900704.4dad288061dac82bb420fb6dd526eea5.', STARTKEY => '\x5C011', ENDKEY => '\x5C100'}, {ENCODED => e98b84924307f22d4318cfe235445ba7, NAME => 'tablethree_mod,\x5C100,1384159900704.e98b84924307f22d4318cfe235445ba7.', STARTKEY => '\x5C100', ENDKEY => ''}]^M
2013-11-11 08:52:43,900 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Added 1^M
2013-11-11 08:52:43,913 DEBUG org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessLockBase: Released /hbase/table-lock/tablethree_mod/write-master:600000000000006^M
2013-11-11 08:52:43,913 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event C_M_RESTORE_SNAPSHOT^M
java.lang.NullPointerException^M
  at org.apache.hadoop.hbase.catalog.MetaEditor.deleteRegions(MetaEditor.java:488)^M
  at org.apache.hadoop.hbase.catalog.MetaEditor.overwriteRegions(MetaEditor.java:534)^M
  at org.apache.hadoop.hbase.master.snapshot.RestoreSnapshotHandler.handleTableOperation(RestoreSnapshotHandler.java:160)^M
  at org.apache.hadoop.hbase.master.handler.TableEventHandler.process(TableEventHandler.java:129)^M
  at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)^M
  at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)^M
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)^M
{code}
Looks like null was passed in the following call MetaEditor#overwriteRegions():
{code}
    deleteRegions(catalogTracker, regionInfos);
{code}", ,,,,,,,
HBASE-9958,, ,,1,,,,,
HBASE-9961,"Binding to a multicast address (such as ""hbase.status.multicast.address.ip"") seems to be the preferred method on most unix systems and linux(2,3). At least in RedHat, binding to multicast address might not filter out other traffic coming to the same port, but for different multi cast groups (2)]. However, on windows, you cannot bind to a non local (class D) address (1), which seems to be correct according to the spec.
    # http://msdn.microsoft.com/en-us/library/ms737550%28v=vs.85%29.aspx
    # https://bugzilla.redhat.com/show_bug.cgi?id=231899
    # http://stackoverflow.com/questions/10692956/what-does-it-mean-to-bind-a-multicast-udp-socket
    # https://issues.jboss.org/browse/JGRP-515

The solution is to bind to mcast address on linux, but a local address on windows. 

TestHCM is also failing because of this. 
", ,1,,,,,,
HBASE-9963,"The bin/start-hbase.sh script returns an exit status indicating success, and emits the message : 'starting master, logging to ...' ,  even though the master failed to start:
    $ ./bin/start-hbase.sh
    starting master, logging to /home/jason/3P/hbase-0.94.13/logs/hbase-jason-master-jvds.out
    $ echo $?
    0
#  ^- this indicates successful exit status on Linux / UNIX-like systems
    $ egrep 'start master|RuntimeException' logs/hbase-jason-master-jvds.log 
2013-11-13 14:15:28,309 ERROR org.apache.hadoop.hbase.master.HMasterCommandLine: Failed to start master
java.lang.RuntimeException: Failed suppression of fs shutdown hook: Thread[Thread-27,5,main]

The start-base.sh script should IMHO in the above scenario emit a message like 
'Failed to start the hbase master:   java.lang.RuntimeException: Failed suppression of fs shutdown hook'
and return a non-zero exit status.", ,,,,,,,
HBASE-9964,"The bin/start-hbase.sh script returns an exit status indicating success, and emits the message : 'starting master, logging to ...' ,  even though the master failed to start:
    $ ./bin/start-hbase.sh
    starting master, logging to /home/jason/3P/hbase-0.94.13/logs/hbase-jason-master-jvds.out
    $ echo $?
    0
#  ^- this indicates successful exit status on Linux / UNIX-like systems
    $ egrep 'start master|RuntimeException' logs/hbase-jason-master-jvds.log 
2013-11-13 14:15:28,309 ERROR org.apache.hadoop.hbase.master.HMasterCommandLine: Failed to start master
java.lang.RuntimeException: Failed suppression of fs shutdown hook: Thread[Thread-27,5,main]

The start-base.sh script should IMHO in the above scenario emit a message like 
'Failed to start the hbase master:   java.lang.RuntimeException: Failed suppression of fs shutdown hook'
and return a non-zero exit status.", ,1,,,,,,
HBASE-9968,"When we check whether the dead region is carrying root or meta, first we will check any transition znode for the region is there or not. In this case it got deleted. So from zookeeper we cannot find the region location. 
{code}
    try {
      data = ZKAssign.getData(master.getZooKeeper(), hri.getEncodedName());
    } catch (KeeperException e) {
      master.abort(""Unexpected ZK exception reading unassigned node for region=""
        + hri.getEncodedName(), e);
    }
{code}
Now we will check from the AssignmentManager whether its in online regions or not
{code}
    ServerName addressFromAM = getRegionServerOfRegion(hri);
    boolean matchAM = (addressFromAM != null &&
      addressFromAM.equals(serverName));
    LOG.debug(""based on AM, current region="" + hri.getRegionNameAsString() +
      "" is on server="" + (addressFromAM != null ? addressFromAM : ""null"") +
      "" server being checked: "" + serverName);
{code}
From AM we will get null because  while adding region to online regions we will check whether the RS is in onlineservers or not and if not we will not add the region to online regions.
{code}
      if (isServerOnline(sn)) {
        this.regions.put(regionInfo, sn);
        addToServers(sn, regionInfo);
        this.regions.notifyAll();
      } else {
        LOG.info(""The server is not in online servers, ServerName="" + 
          sn.getServerName() + "", region="" + regionInfo.getEncodedName());
      }
{code}


Even though the dead regionserver carrying ROOT region, its returning false. After that ROOT region never assigned.

Here are the logs
{code}
2013-11-11 18:04:14,730 INFO org.apache.hadoop.hbase.catalog.RootLocationEditor: Unsetting ROOT region location in ZooKeeper
2013-11-11 18:04:14,775 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: No previous transition plan was found (or we are ignoring an existing plan) for -ROOT-,,0.70236052 so generated a random one; hri=-ROOT-,,0.70236052, src=, dest=HOST-10-18-40-69,60020,1384173244404; 1 (online=1, available=1) available servers
2013-11-11 18:04:14,809 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region -ROOT-,,0.70236052 to HOST-10-18-40-69,60020,1384173244404
2013-11-11 18:04:18,375 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation: Looked up root region location, connection=org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@12133926; serverName=HOST-10-18-40-69,60020,1384173244404
2013-11-11 18:04:26,213 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENED, server=HOST-10-18-40-69,60020,1384173244404, region=70236052/-ROOT-
2013-11-11 18:04:26,213 INFO org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Handling OPENED event for -ROOT-,,0.70236052 from HOST-10-18-40-69,60020,1384173244404; deleting unassigned node
2013-11-11 18:04:31,553 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: based on AM, current region=-ROOT-,,0.70236052 is on server=null server being checked: HOST-10-18-40-69,60020,1384173244404
2013-11-11 18:04:31,561 DEBUG org.apache.hadoop.hbase.master.ServerManager: Added=HOST-10-18-40-69,60020,1384173244404 to dead servers, submitted shutdown handler to be executed, root=false, meta=false
{code}
{code}
2013-11-11 18:04:32,323 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: The znode of region -ROOT-,,0.70236052 has been deleted.
2013-11-11 18:04:32,323 INFO org.apache.hadoop.hbase.master.AssignmentManager: The server is not in online servers, ServerName=HOST-10-18-40-69,60020,1384173244404, region=70236052
2013-11-11 18:04:32,323 INFO org.apache.hadoop.hbase.master.AssignmentManager: The master has opened the region -ROOT-,,0.70236052 that was online on HOST-10-18-40-69,60020,1384173244404
{code}", ,,,,,,,
HBASE-9969,, ,,1,,,,,
HBASE-9973,"In our testing, we have uncovered that the ACL permissions for users with the 'A' credential do not hold after the upgrade to 0.96.x.

This is because in the ACL table, the entry for the admin user is a permission on the '_acl_' table with permission 'A'. However, because of the namespace transition, there is no longer an '_acl_' table. Therefore, that entry in the hbase:acl table is no longer valid.

Example:

{code}hbase(main):002:0> scan 'hbase:acl'
ROW                   COLUMN+CELL                                               
 TestTable            column=l:hdfs, timestamp=1384454830701, value=RW          
 TestTable            column=l:root, timestamp=1384455875586, value=RWCA        
 _acl_                column=l:root, timestamp=1384454767568, value=C           
 _acl_                column=l:tableAdmin, timestamp=1384454788035, value=A     
 hbase:acl            column=l:root, timestamp=1384455875786, value=C           
{code}

In this case, the following entry becomes meaningless:
{code} _acl_                column=l:tableAdmin, timestamp=1384454788035, value=A     {code}

As a result, 

Proposed fix:
I see the fix being relatively straightforward. As part of the migration, change any entries in the '_acl_' table with key '_acl_' into a new row with key 'hbase:acl', all else being the same. And the old entry would be deleted.

This can go into the standard migration script that we expect users to run.",1,,,,,,,
HBASE-9976,"A profiling show that the table name is reponsible for 25% of the memory needed to keep the region locations. As well, comparisons will be faster if two identical table names are a single java object.", ,,1,,,,,
HBASE-9978,"If the RpcServer is not able to find the method on the server throws an UnsupportedOperationException, but since is not wrapped in a DoNotRetry the client keeps retrying even if the operation doesn't exists.", ,,,,,,,
HBASE-9987,"Change a Map to a concurrentMap
Removed the ""cachedServer (introduced in HBASE-4785). I suspect that this function is not needed anymore as we also have a list of dead servers, and accessing the list is not blocking. I will dig into this more however.

The patch gives a 10% improvement with the NoClusterClient.", ,,1,,,,,
HBASE-9988,This functions does a lazy initialisation. It cost memory and it creates a synchronisation point., ,,1,,,,,
HBASE-9990,"You can construct a RpcRetryingCallerFactory, but actually the conf is read for each caller creation. Reading the conf is obviously expensive, and a profiling session shows it. If we want to sent hundreds of thousands of queries per second, we should not do that.

RpcRetryingCallerFactory.newCaller is called for each get, for example.

This is not a regression, we have something similar in 0.94.

On the 0.96, we see the creation of: java.util.regex.Matcher: 15739712b after a few thousand calls to ""get"".", ,,1,,,,,
HBASE-10000,"You can construct a RpcRetryingCallerFactory, but actually the conf is read for each caller creation. Reading the conf is obviously expensive, and a profiling session shows it. If we want to sent hundreds of thousands of queries per second, we should not do that.

RpcRetryingCallerFactory.newCaller is called for each get, for example.

This is not a regression, we have something similar in 0.94.

On the 0.96, we see the creation of: java.util.regex.Matcher: 15739712b after a few thousand calls to ""get"".", ,,,,,,,
HBASE-10010,"You can construct a RpcRetryingCallerFactory, but actually the conf is read for each caller creation. Reading the conf is obviously expensive, and a profiling session shows it. If we want to sent hundreds of thousands of queries per second, we should not do that.

RpcRetryingCallerFactory.newCaller is called for each get, for example.

This is not a regression, we have something similar in 0.94.

On the 0.96, we see the creation of: java.util.regex.Matcher: 15739712b after a few thousand calls to ""get"".", ,,1,,,,,
HBASE-10014,See HBASE-10001, ,,,,,,,
HBASE-10015,"Did some more profiling (this time with a sampling profiler) and StoreScanner.peek() showed up a lot in the samples. At first that was surprising, but peek is synchronized, so it seems a lot of the sync'ing cost is eaten there.
It seems the only reason we have to synchronize all these methods is because a concurrent flush or compaction can change the scanner stack, other than that only a single thread should access a StoreScanner at any given time.
So replaced updateReaders() with some code that just indicates to the scanner that the readers should be updated and then make it the using thread's responsibility to do the work.
The perf improvement from this is staggering. I am seeing somewhere around 3x scan performance improvement across all scenarios.

Now, the hard part is to reason about whether this is 100% correct. I ran TestAtomicOperation and TestAcidGuarantees a few times in a loop, all still pass.

Will attach a sample patch.", ,,1,,,,,
HBASE-10017,"Inside HRegionPartitioner class there is getPartition() method which should map first numPartitions regions to appropriate partitions 1:1. But based on condition last region is hashed which could lead to last reducer not having any data. This is considered serious issue.

I reproduced this only starting from 16 regions per table. Original defect was found in 0.94.6 but at least today's trunk and 0.91 branch head have the same HRegionPartitioner code in this part which means the same issue.
", ,,,,,,,
HBASE-10029,"Proxy to HA namenode with QJM created from org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider should close properly.

[Mail Archive|https://drive.google.com/file/d/0B22pkxoqCdvWSGFIaEpfR3lnT2M/edit?usp=sharing]

{code}
13/11/26 09:55:55 ERROR ipc.RPC: RPC.stopProxy called on non proxy.
java.lang.IllegalArgumentException: object is not an instance of declaring class
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:266)
        at $Proxy16.close(Unknown Source)
        at org.apache.hadoop.ipc.RPC.stopProxy(RPC.java:621)
        at org.apache.hadoop.hdfs.DFSClient.closeConnectionToNamenode(DFSClient.java:738)
        at org.apache.hadoop.hdfs.DFSClient.close(DFSClient.java:794)
        at org.apache.hadoop.hdfs.DistributedFileSystem.close(DistributedFileSystem.java:847)
        at org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:2524)
        at org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer.run(FileSystem.java:2541)
        at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
13/11/26 09:55:55 WARN util.ShutdownHookManager: ShutdownHook 'ClientFinalizer' failed, org.apache.hadoop.HadoopIllegalArgumentException: Cannot close proxy - is not Closeable or does not provide closeable invocation handler class $Proxy16
org.apache.hadoop.HadoopIllegalArgumentException: Cannot close proxy - is not Closeable or does not provide closeable invocation handler class $Proxy16
        at org.apache.hadoop.ipc.RPC.stopProxy(RPC.java:639)
        at org.apache.hadoop.hdfs.DFSClient.closeConnectionToNamenode(DFSClient.java:738)
        at org.apache.hadoop.hdfs.DFSClient.close(DFSClient.java:794)
        at org.apache.hadoop.hdfs.DistributedFileSystem.close(DistributedFileSystem.java:847)
        at org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:2524)
        at org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer.run(FileSystem.java:2541)
        at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
{code}", ,,,,,,,
HBASE-10045,HMaster's clear command still could bind the the master's jmx ports because it was using the wrong HBASE_OPTS., ,1,,,,,,
HBASE-10046,"This was observed in a cluster where HBase Master UI was not monitored for quite a while. During this period, a distributed log spitting task failed in an in-completable way and Master kept recreating the task over and over again.

And each such task would create a monitored status...
{code:title=SplitLogManager.java}
.......
  public long splitLogDistributed(final List<Path> logDirs, PathFilter filter) 
      throws IOException {
    MonitoredTask status = TaskMonitor.get().createStatus(
          ""Doing distributed log split in "" + logDirs);
.......
{code}
...which kept accumulating on heap.

Now these monitored tasks get cleaned only when someone looks at the service UI otherwise they keep growing boundless.

The postmortem of the heap dump showed that these task object occupied 99% of HBase master heap accumulated over a week.", ,,1,,,,,
HBASE-10047,"Continuing my profiling quest, I find that in scanning tall table (and filtering everything on the server) a quarter of the time is now spent in the postScannerFilterRow coprocessor hook.
", ,,1,,,,,
HBASE-10051,"While running rolling-restart.sh without arguments or with --master-only argument script throws error when trying to check did all regions are cleared from transition state.  Here is line causing error:
{code}
if [ ""$zunassigned"" == ""null"" ]; then zunassigned=""unassigned""; fi 
{code}

AFAIK default value of ""zookeeper.znode.unassiged"" is changed to ""region-in-transition"". So we should change this line to:
{code}
if [ ""$zunassigned"" == ""null"" ]; then zunassigned=""region-in-transition""; fi 
{code}

I will attach patch today.", ,1,,,,,,
HBASE-10060,"HBASE-10015 has some lengthy discussion. The solution there ended up replacing synchronized with ReentrantLock, which - somewhat surprisingly - yielded a non-trivial improvement for tall tables.
The goal should be to avoid locking in StoreScanner at all. StoreScanner is only accessed by a single thread *except* when we have a concurrent flush or a compaction, which is rare (we'd acquire and release the lock millions of times per second, and compact/flush a few time an hour at the most).
", ,,1,,,,,
HBASE-10062,"After HBASE-7544, if an HFile belongs to an encrypted family, it is encrypted on a per block basis. The encrypted blocks include the following header:
      // +--------------------------+
      // | vint plaintext length    |
      // +--------------------------+
      // | vint iv length           |
      // +--------------------------+
      // | iv data ...              |
      // +--------------------------+
      // | encrypted block data ... |
      // +--------------------------+
The reason for storing the plaintext length is so we can create an decryption stream over the encrypted block data and, no matter the internal details of the crypto algorithm (whether it adds padding, etc.) after reading the expected plaintext bytes we know the reader is finished. However my colleague Jerry Chen pointed out today this construction mandates the block be processed exactly that way. Storing and using the encrypted data length instead could provide more implementation flexibility down the road.",1,,,,,,,
HBASE-10070,"HBASE-10015 has some lengthy discussion. The solution there ended up replacing synchronized with ReentrantLock, which - somewhat surprisingly - yielded a non-trivial improvement for tall tables.
The goal should be to avoid locking in StoreScanner at all. StoreScanner is only accessed by a single thread *except* when we have a concurrent flush or a compaction, which is rare (we'd acquire and release the lock millions of times per second, and compact/flush a few time an hour at the most).
", ,,1,,,,,
HBASE-10077,HBASE-7544 introduces WAL encryption to prevent the leakage of protected data to disk by way of WAL files. However it is currently enabled globally for the regionserver. Encryption of WAL entries should depend on whether or not an entry in the WAL is to be stored within an encrypted column family.,1,,,,,,,
HBASE-10079,"Testing 0.96.1rc1.

With one process incrementing a row in a table, we increment single col.  We flush or do kills/kill-9 and data is lost.  flush and kill are likely the same problem (kill would flush), kill -9 may or may not have the same root cause.

5 nodes
hadoop 2.1.0 (a pre cdh5b1 hdfs).
hbase 0.96.1 rc1 

Test: 250000 increments on a single row an single col with various number of client threads (IncrementBlaster).  Verify we have a count of 250000 after the run (IncrementVerifier).

Run 1: No fault injection.  5 runs.  count = 250000. on multiple runs.  Correctness verified.  1638 inc/s throughput.
Run 2: flushes table with incrementing row.  count = 246875 !=250000.  correctness failed.  1517 inc/s throughput.  
Run 3: kill of rs hosting incremented row.  count = 243750 != 250000. Correctness failed.   1451 inc/s throughput.
Run 4: one kill -9 of rs hosting incremented row.  246878.!= 250000.  Correctness failed. 1395 inc/s (including recovery)
", ,,,,,,,
HBASE-10085,"We see this issue happened in a cluster restart:

1) when shutdown a cluster, some regions are in offline state because no Region servers are available(stop RS and then Master)
2) When the cluster restarts, the offlined regions are forced to be offline again and SSH skip re-assigning them by function AM.processServerShutdown as shown below.

{code}
2013-12-03 10:41:56,686 INFO  [master:h2-ubuntu12-sec-1386048659-hbase-8:60000] master.AssignmentManager: Processing 873dbd8c269f44d0aefb0f66c5b53537 in state: M_ZK_REGION_OFFLINE
2013-12-03 10:41:56,686 DEBUG [master:h2-ubuntu12-sec-1386048659-hbase-8:60000] master.AssignmentManager: RIT 873dbd8c269f44d0aefb0f66c5b53537 in state=M_ZK_REGION_OFFLINE was on deadserver; forcing offline
...
2013-12-03 10:41:56,739 DEBUG [AM.-pool1-t8] master.AssignmentManager: Force region state offline {873dbd8c269f44d0aefb0f66c5b53537 state=OFFLINE, ts=1386067316737, server=h2-ubuntu12-sec-1386048659-hbase-6.cs1cloud.internal,60020,1386066968696}
...
2013-12-03 10:41:57,223 WARN  [MASTER_SERVER_OPERATIONS-h2-ubuntu12-sec-1386048659-hbase-8:60000-3] master.RegionStates: THIS SHOULD NOT HAPPEN: unexpected {873dbd8c269f44d0aefb0f66c5b53537 state=OFFLINE, ts=1386067316737, server=h2-ubuntu12-sec-1386048659-hbase-6.cs1cloud.internal,60020,1386066968696} 
{code}", ,,,,,,,
HBASE-10087,"regression from HBASE-9963, found while looking at HBASE-10079.", ,,,,,,,
HBASE-10088,"When I misuse a secure hbase client to access a secure-disabled hbase server, I found the client will hang. The reason is that client will firstly invoke rpc method ""getProtocolVersion"", and the response from a secure-disabled server won't contain necessary fields processed by SecureClient. SecureClient will process the response as follows : (from SecureClient.receiveResponse()):
if (state == Status.SUCCESS.state) {
          Writable value = ReflectionUtils.newInstance(valueClass, conf);
          value.readFields(in);                 // read value
          if (LOG.isDebugEnabled()) {
            LOG.debug(""call #""+id+"", response is:\n""+value.toString());
          }
          // it's possible that this call may have been cleaned up due to a RPC
          // timeout, so check if it still exists before setting the value.
          if (call != null) {
            call.setValue(value);
          }
        } else if (state == Status.ERROR.state) {
          if (call != null) {
            call.setException(new RemoteException(WritableUtils.readString(in), WritableUtils
                .readString(in)));
          }
        } else if (state == Status.FATAL.state) {
          RemoteException exception = new RemoteException(WritableUtils.readString(in),
              WritableUtils.readString(in));
          // the call will be removed from call map, we must set Exception here to notify
          // the thread waited on the call
          if (call != null) {
            call.setException(exception);
          }
          // Close the connection
          markClosed(exception);
        }
        calls.remove(id);
As the above code, SecureClient need to read 'state' field from response. If the response is from a secure-disabled server, there will no 'state' field in response and SecureClient will get an illegal 'state', then the call will be removed from cached calls without notifying waiting thread. This will make the invoker waiting all the time. Although we should not use secure client to access secure-disabled server, users might encounter this situation because of misusing or error configuration. If the client will hang in this situation, users might not know the error quickly. Maybe, it is better to report an error in this situation so that users will know what happens quickly.",1,1,,,,,,
HBASE-10090,"Under very rare scenario, master could hang waiting for meta to be assigned while the meta server is dead.", ,,,,,,,
HBASE-10100,"We were trying to replicate hbase data over to a new datacenter recently.  After we turned on replication and then did our copy tables.  We noticed that verify replication had discrepancies.  

We ran a list_peers and it returned back both peers, the original datacenter we were replicating to and the new datacenter (this was correct).  

When grepping through the logs for a few regionservers we noticed that a few regionservers had the following entry in their logs:

2013-09-26 10:55:46,907 ERROR org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager: Error while adding a new peer java.net.UnknownHostException: xxx.xxx.flurry.com (this was due to a transient dns issue)

Thus a very small subet of our regionservers were not replicating to this new cluster while most were. 

We probably don't want to abort if this type of issue comes up, it could potentially be fatal if someone does an ""add_peer"" operation with a typo.  This could potentially shut down the cluster. 

One solution I can think of is keeping some flag in ReplicationSourceManager which is a boolean that keeps track of whether there was an errorAddingPeer.  Then in the logPositionAndCleanOldLogs we can do something like:

{code}
if (errorAddingPeer) {
      LOG.error(""There was an error adding a peer, logs will not be marked for deletion"");
      return;
    }
{code}

thus we are not deleting these logs from the queue.  You will notice your replicating queue rising on certain machines and you can still replay the logs, thus avoiding a lengthy copy table. 

I have a patch (with unit test) for the above proposal, if everyone thinks that is an okay solution.

An additional idea would be to add some retry logic inside the PeersWatcher class for the nodeChildrenChanged method.  Thus if there happens to be some issue we could sort it out without having to bounce that particular regionserver.  

Would love to hear everyones thoughts.





", ,,,,,,,
HBASE-10101,"Sometimes, I got this test timed out. The log is attached. It could be because the new cluster takes a while to process the dead server, or assign meta.", ,,,,,,,
HBASE-10111,"To avoid assigning/opening regions with missing files, verify that the snapshot is not corrupted before restoring/cloning it.

In 96 a corrupted file in a region is ""not a problem"" since the assignment will give up after awhile.
In 94 having a corrupted file in a region means looping forever, on ""enable"",  until manual intervention. (Easy way to test this is create a table, disable it, add a corrupted reference file and enable the table to start looping: you can use echo ""foo"" > aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa.aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa)", ,,,,,,,
HBASE-10117,"A while ago I introduced HRegoinScannerImpl.nextRaw() to allow coprocessors and scanners with caching > 1 to avoid repeated synchronization during scanning (which puts up memory fences, which in turn slows things down on multi core machines).

Looking at the code again I see that isFilterDone() is called from nextRaw() and isFilterDone() is synchronized.
The caller of nextRaw is required to ensure single threaded access to nextRaw() anyway, we can call an unsynchronized internal version of isFilterDone().
", ,,1,,,,,
HBASE-10119,"A while ago I introduced HRegoinScannerImpl.nextRaw() to allow coprocessors and scanners with caching > 1 to avoid repeated synchronization during scanning (which puts up memory fences, which in turn slows things down on multi core machines).

Looking at the code again I see that isFilterDone() is called from nextRaw() and isFilterDone() is synchronized.
The caller of nextRaw is required to ensure single threaded access to nextRaw() anyway, we can call an unsynchronized internal version of isFilterDone().
", ,,,,,,,
HBASE-10123,"Our defaults clash w/ the range linux assigns itself for creating come-and-go ephemeral ports; likely in our history we've clashed w/ a random, short-lived process.  While easy to change the defaults, we should just ship w/ defaults that make sense.  We could host ourselves up into the 7 or 8k range.

See http://www.ncftp.com/ncftpd/doc/misc/ephemeral_ports.html", ,1,,,,,,
HBASE-10124,"A while ago I introduced HRegoinScannerImpl.nextRaw() to allow coprocessors and scanners with caching > 1 to avoid repeated synchronization during scanning (which puts up memory fences, which in turn slows things down on multi core machines).

Looking at the code again I see that isFilterDone() is called from nextRaw() and isFilterDone() is synchronized.
The caller of nextRaw is required to ensure single threaded access to nextRaw() anyway, we can call an unsynchronized internal version of isFilterDone().
", ,,1,,,,,
HBASE-10126,"Expected behavior:
With the introduction of the table-lock, a user can issue a request for a snapshot of a table while that table is undergoing an online schema change and expect that snapshot request to complete correctly. Also, the same is true if a user issues a online schema change request while a snapshot attempt is ongoing.

Observed behavior:
Snapshot attempts time out when there is an ongoing online schema change because the table lock is not acquired by anyone else and the regions are closed and opened during the snapshot. 

TableEventHandler trace
{code}
// 1. client.addColumn() call from client...

// 2. The operation is now on the master
2013-12-12 12:09:57,613 DEBUG [MASTER] lock.ZKInterProcessLockBase: Acquired a lock for /hbase/table-lock/TestTable/write-master:452010000000001
2013-12-12 12:09:57,640 INFO  [MASTER] handler.TableEventHandler: Handling table operation C_M_ADD_FAMILY on table TestTable
2013-12-12 12:09:57,685 INFO  [MASTER] master.MasterFileSystem: AddColumn. Table = TestTable HCD = {NAME => 'x-1386850197327', DATA_BLOCK_ENCODING => 'NONE',$
2013-12-12 12:09:57,693 INFO  [MASTER] handler.TableEventHandler: Bucketing regions by region server...
...
2013-12-12 12:09:57,771 INFO  [MASTER] handler.TableEventHandler: Completed table operation C_M_ADD_FAMILY on table TestTable
2013-12-12 12:09:57,771 DEBUG [MASTER] master.AssignmentManager: Starting unassign of TestTable,,1386849056038.854b280$
2013-12-12 12:09:57,772 DEBUG [MASTER] lock.ZKInterProcessLockBase: Released /hbase/table-lock/TestTable/write-master:452010000000001

// 3. The Table*Handler operation is now completed, and the client notified with ""I'm done!""

// 4. Now the BulkReopen is starting doing the reopen
2013-12-12 12:09:57,772 INFO  [MASTER] master.RegionStates: Transitioned {854b280006aec464083778a5cb5f5456 state=OPEN,$
{code}", ,,,,,,,
HBASE-10136,"Expected behavior:
With the introduction of the table-lock, a user can issue a request for a snapshot of a table while that table is undergoing an online schema change and expect that snapshot request to complete correctly. Also, the same is true if a user issues a online schema change request while a snapshot attempt is ongoing.

Observed behavior:
Snapshot attempts time out when there is an ongoing online schema change because the table lock is not acquired by anyone else and the regions are closed and opened during the snapshot. 

TableEventHandler trace
{code}
// 1. client.addColumn() call from client...

// 2. The operation is now on the master
2013-12-12 12:09:57,613 DEBUG [MASTER] lock.ZKInterProcessLockBase: Acquired a lock for /hbase/table-lock/TestTable/write-master:452010000000001
2013-12-12 12:09:57,640 INFO  [MASTER] handler.TableEventHandler: Handling table operation C_M_ADD_FAMILY on table TestTable
2013-12-12 12:09:57,685 INFO  [MASTER] master.MasterFileSystem: AddColumn. Table = TestTable HCD = {NAME => 'x-1386850197327', DATA_BLOCK_ENCODING => 'NONE',$
2013-12-12 12:09:57,693 INFO  [MASTER] handler.TableEventHandler: Bucketing regions by region server...
...
2013-12-12 12:09:57,771 INFO  [MASTER] handler.TableEventHandler: Completed table operation C_M_ADD_FAMILY on table TestTable
2013-12-12 12:09:57,771 DEBUG [MASTER] master.AssignmentManager: Starting unassign of TestTable,,1386849056038.854b280$
2013-12-12 12:09:57,772 DEBUG [MASTER] lock.ZKInterProcessLockBase: Released /hbase/table-lock/TestTable/write-master:452010000000001

// 3. The Table*Handler operation is now completed, and the client notified with ""I'm done!""

// 4. Now the BulkReopen is starting doing the reopen
2013-12-12 12:09:57,772 INFO  [MASTER] master.RegionStates: Transitioned {854b280006aec464083778a5cb5f5456 state=OPEN,$
{code}", ,,,,,,,
HBASE-10137,"Current in BulkEnabler we are assigning one region at a time, instead we can use GeneralBulkAssigner to bulk assign multiple regions at a time.
{code}
      for (HRegionInfo region : regions) {
        if (assignmentManager.getRegionStates()
            .isRegionInTransition(region)) {
          continue;
        }
        final HRegionInfo hri = region;
        pool.execute(Trace.wrap(""BulkEnabler.populatePool"",new Runnable() {
          public void run() {
            assignmentManager.assign(hri, true);
          }
        }));
      }

{code}", ,,,,,,,
HBASE-10148,"Ted Yu reports that enabling distributed log replay by default, like:

{noformat}
Index: hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java
===================================================================
--- hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java	(revision 1550575)
+++ hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java	(working copy)
@@ -794,7 +794,7 @@

   /** Conf key that enables unflushed WAL edits directly being replayed to region servers */
   public static final String DISTRIBUTED_LOG_REPLAY_KEY = ""hbase.master.distributed.log.replay"";
-  public static final boolean DEFAULT_DISTRIBUTED_LOG_REPLAY_CONFIG = false;
+  public static final boolean DEFAULT_DISTRIBUTED_LOG_REPLAY_CONFIG = true;
   public static final String DISALLOW_WRITES_IN_RECOVERING =
       ""hbase.regionserver.disallow.writes.when.recovering"";
   public static final boolean DEFAULT_DISALLOW_WRITES_IN_RECOVERING_CONFIG = false;
{noformat}

causes TestVisibilityController#testAddVisibilityLabelsOnRSRestart to fail. It reveals an issue with label operations if the label table is recovering:

{noformat}
2013-12-12 14:53:53,133 DEBUG [RpcServer.handler=2,port=58108] visibility.VisibilityController(1046): Adding the label XYZ2013-12-12 14:53:53,137 ERROR [RpcServer.handler=2,port=58108] visibility.VisibilityController(1074): org.apache.hadoop.hbase.exceptions.RegionInRecoveryException: hbase:labels,,1386888826648.f14a399ba85cbb42c2c3b7547bf17c65. is recovering
2013-12-12 14:53:53,151 DEBUG [main] visibility.TestVisibilityLabels(405): response from addLabels: result {
  exception {
    name: ""org.apache.hadoop.hbase.exceptions.RegionInRecoveryException""
    value: ""org.apache.hadoop.hbase.exceptions.RegionInRecoveryException: hbase:labels,,1386888826648.f14a399ba85cbb42c2c3b7547bf17c65. is recovering at org.apache.hadoop.hbase.regionserver.HRegion.startRegionOperation(HRegion.java:5555) at org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:1763) at org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:1749) at org.apache.hadoop.hbase.security.visibility.VisibilityController.getExistingLabelsWithAuths(VisibilityController.java:1096) at org.apache.hadoop.hbase.security.visibility.VisibilityController.postBatchMutate(VisibilityController.java:672)""
{noformat}

Should we try to ride over this?",1,,,,,,,
HBASE-10155,in the postOpen() hook also we should say correctly the isRecovering status of the region. Now it always comes as false., ,,,,,,,
HBASE-10161,AccessController fixes for the issue also affecting VisibilityController described on HBASE-10148. Coprocessors that initialize in postOpen upcalls must check if the region is still in recovery and defer initialization until recovery is complete. We need to add a new CP hook for post recovery upcalls and modify existing CPs to defer initialization until this new hook as needed.,1,,,,,,,
HBASE-10169,"A while ago I introduced HRegoinScannerImpl.nextRaw() to allow coprocessors and scanners with caching > 1 to avoid repeated synchronization during scanning (which puts up memory fences, which in turn slows things down on multi core machines).

Looking at the code again I see that isFilterDone() is called from nextRaw() and isFilterDone() is synchronized.
The caller of nextRaw is required to ensure single threaded access to nextRaw() anyway, we can call an unsynchronized internal version of isFilterDone().
", ,,1,,,,,
HBASE-10173,"Cell level visibility labels are stored as cell tags. So HFile V3 is the minimum version which can support this feature. Better to have a version check in VisibilityController. Some one using this CP but with any HFile version as V2, we can better throw error.",1,,,,,,,
HBASE-10181,"Exception handling inside HbaseObjectWritable needs to be reworked, IMHO. 

For example:
At several places inside HbaseObjectWritable.readObject, exceptions are caught and rethrown as I/O Exception (including ClassNotFoundException!). 

So, if an implementation of readFields method throws a DoNotRetryIOException, HBase still ends up retrying. 

This problem exists at least in 0.94.12 version of HBase. ", ,,,,,,,
HBASE-10185,"Throwing a DoNotRetryIOException inside  Writable.write(Dataoutput) method doesn't prevent HBase from retrying. Debugging the code locally, I figured that the bug lies in the way HBaseClient simply throws an IOException when it sees that a connection has been closed unexpectedly.  

Method:
public Writable call(Writable param, InetSocketAddress addr,
                       Class<? extends VersionedProtocol> protocol,
                       User ticket, int rpcTimeout)

Excerpt of code where the bug is present:
while (!call.done) {
        if (connection.shouldCloseConnection.get()) {
          throw new IOException(""Unexpected closed connection"");
        }

Throwing this IOException causes the ServerCallable.translateException(t) to be a no-op resulting in HBase retrying. 

From my limited view and understanding of the code, one way I could think of handling this is by looking at the closeConnection member variable of a connection to determine what kind of exception should be thrown. 

Specifically, when a connection is closed, the current code does this: 

    protected synchronized void markClosed(IOException e) {
      if (shouldCloseConnection.compareAndSet(false, true)) {
        closeException = e;
        notifyAll();
      }
    }

Within HBaseClient's call method, the code could possibly be modified to:

while (!call.done) {
        if (connection.shouldCloseConnection.get() ) {
                 if(connection.closeException instanceof                   DoNotRetryIOException) {
throw closeException;
}
          throw new IOException(""Unexpected closed connection"");
        }
", ,,,,,,,
HBASE-10193,"While investigating a different issue, I realized that the fix for HBASE-9737 is not sufficient to prevent resource leak if a region fails to open for some reason, say a corrupt HFile.

The region may have, by then, opened other good HFiles in that store or other stores if it has more than one column family and their streams may leak if not closed.", ,,,,,,,
HBASE-10196,"HBCK will report an error now after the online merge,
because the files of merging regions still remain on HDFS which will be cleaned by CatalogJanitor later.


ERROR: Region { meta => null, hdfs =>
hdfs://hbasetest1:9000/hbase/data/default/dns/c6569a72cc3c2750d14976ab85f02315,
deployed =>  } on HDFS, but not listed in hbase:meta or deployed on any region server", ,,,,,,,
HBASE-10199,"A while ago I introduced HRegoinScannerImpl.nextRaw() to allow coprocessors and scanners with caching > 1 to avoid repeated synchronization during scanning (which puts up memory fences, which in turn slows things down on multi core machines).

Looking at the code again I see that isFilterDone() is called from nextRaw() and isFilterDone() is synchronized.
The caller of nextRaw is required to ensure single threaded access to nextRaw() anyway, we can call an unsynchronized internal version of isFilterDone().
", ,,1,,,,,
HBASE-10205,"The BucketCache WriterThread calls BucketCache.freeSpace() upon draining the RAM queue containing entries to be cached. freeSpace() in turn calls BucketSizeInfo.statistics() through BucketAllocator.getIndexStatistics(), which iterates over 'bucketList'. At the same time another WriterThread might call BucketAllocator.allocateBlock(), which may call BucketSizeInfo.allocateBlock(), add a bucket to 'bucketList' and consequently cause a ConcurrentModificationException. Calls to BucketAllocator.allocateBlock() are synchronized, but calls to BucketAllocator.getIndexStatistics() are not, which allows this race to occur.", ,,,,,,,
HBASE-10209,"A while ago I introduced HRegoinScannerImpl.nextRaw() to allow coprocessors and scanners with caching > 1 to avoid repeated synchronization during scanning (which puts up memory fences, which in turn slows things down on multi core machines).

Looking at the code again I see that isFilterDone() is called from nextRaw() and isFilterDone() is synchronized.
The caller of nextRaw is required to ensure single threaded access to nextRaw() anyway, we can call an unsynchronized internal version of isFilterDone().
", ,,1,,,,,
HBASE-10210,"Not sure of the root cause yet, I am at ""how did this ever work"" stage.
We see this problem in 0.96.1, but didn't in 0.96.0 + some patches.

It looks like RS information arriving from 2 sources - ZK and server itself, can conflict. Master doesn't handle such cases (timestamp match), and anyway technically timestamps can collide for two separate servers.

So, master YouAreDead-s the already-recorded reporting RS, and adds it too. Then it discovers that the new server has died with fatal error!

Note the threads.
Addition is called from master initialization and from RPC.
{noformat}
2013-12-19 11:16:45,290 INFO  [master:h2-ubuntu12-sec-1387431063-hbase-10:60000] master.ServerManager: Finished waiting for region servers count to settle; checked in 2, slept for 18262 ms, expecting minimum of 1, maximum of 2147483647, master is running.
2013-12-19 11:16:45,290 INFO  [master:h2-ubuntu12-sec-1387431063-hbase-10:60000] master.ServerManager: Registering server=h2-ubuntu12-sec-1387431063-hbase-8.cs1cloud.internal,60020,1387451803800
2013-12-19 11:16:45,290 INFO  [master:h2-ubuntu12-sec-1387431063-hbase-10:60000] master.HMaster: Registered server found up in zk but who has not yet reported in: h2-ubuntu12-sec-1387431063-hbase-8.cs1cloud.internal,60020,1387451803800
2013-12-19 11:16:45,380 INFO  [RpcServer.handler=4,port=60000] master.ServerManager: Triggering server recovery; existingServer h2-ubuntu12-sec-1387431063-hbase-8.cs1cloud.internal,60020,1387451803800 looks stale, new server:h2-ubuntu12-sec-1387431063-hbase-8.cs1cloud.internal,60020,1387451803800
2013-12-19 11:16:45,380 INFO  [RpcServer.handler=4,port=60000] master.ServerManager: Master doesn't enable ServerShutdownHandler during initialization, delay expiring server h2-ubuntu12-sec-1387431063-hbase-8.cs1cloud.internal,60020,1387451803800
...
2013-12-19 11:16:46,925 ERROR [RpcServer.handler=7,port=60000] master.HMaster: Region server h2-ubuntu12-sec-1387431063-hbase-8.cs1cloud.internal,60020,1387451803800 reported a fatal error:
ABORTING region server h2-ubuntu12-sec-1387431063-hbase-8.cs1cloud.internal,60020,1387451803800: org.apache.hadoop.hbase.YouAreDeadException: Server REPORT rejected; currently processing h2-ubuntu12-sec-1387431063-hbase-8.cs1cloud.internal,60020,1387451803800 as dead server

{noformat}

Presumably some of the recent ZK listener related changes b", ,,,,,,,
HBASE-10214,"RegionServer log
{code}
2013-12-18 15:17:45,771 DEBUG org.apache.hadoop.hbase.regionserver.HRegionServer: Waiting on 51b27391410efdca841db264df46085f
2013-12-18 15:17:45,776 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Connected to master at null

2013-12-18 15:17:48,776 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: STOPPED: Exiting; cluster shutdown set and not carrying any regions
2013-12-18 15:17:48,776 FATAL org.apache.hadoop.hbase.regionserver.HRegionServer: ABORTING region server node,60020,1384410974572: Unhandled exception: null
java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.HRegionServer.tryRegionServerReport(HRegionServer.java:880)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:753)
        at java.lang.Thread.run(Thread.java:662)
{code}", ,,,,,,,
HBASE-10215,"Lets suppose master went down while creating table then znode will be left in ENABLING state. Master to recover them on restart. 
If there are no meta entries for the table.
While recovering the table we are checking whether table exists in meta or not, if not we are removing the znode. After removing znode we need to throw TableNotFoundException. Presently not throwing the exception so the znode will be recrated. It will be stale forever. Even on master restart we cannot delete. We cannot create the table with same name also.

{code}
      // Check if table exists
      if (!MetaReader.tableExists(catalogTracker, tableName)) {
        // retainAssignment is true only during recovery.  In normal case it is false
        if (!this.skipTableStateCheck) {
          throw new TableNotFoundException(tableName);
        } 
        try {
          this.assignmentManager.getZKTable().removeEnablingTable(tableName, true);
        } catch (KeeperException e) {
          // TODO : Use HBCK to clear such nodes
          LOG.warn(""Failed to delete the ENABLING node for the table "" + tableName
              + "".  The table will remain unusable. Run HBCK to manually fix the problem."");
        }
      }
{code}
", ,,,,,,,
HBASE-10221,"In a visibility controller unit test, we fail a VisibilityClient.getAuths call with an AccessDeniedException, leading to a NPE in the client:

{noformat}
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hbase.util.Bytes.compareTo(Bytes.java:1017)
        at org.apache.hadoop.hbase.util.Bytes$ByteArrayComparator.compare(Bytes.java:151)
        at org.apache.hadoop.hbase.util.Bytes$ByteArrayComparator.compare(Bytes.java:140)
        at java.util.TreeMap.compare(TreeMap.java:1188)
        at java.util.TreeMap.put(TreeMap.java:531)
        at java.util.Collections$SynchronizedMap.put(Collections.java:2041)
        at org.apache.hadoop.hbase.client.HTable$15.update(HTable.java:1486)
        at org.apache.hadoop.hbase.client.HTable$16.call(HTable.java:1515)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
{noformat}", ,,,,,,,
HBASE-10222,"A while ago I introduced HRegoinScannerImpl.nextRaw() to allow coprocessors and scanners with caching > 1 to avoid repeated synchronization during scanning (which puts up memory fences, which in turn slows things down on multi core machines).

Looking at the code again I see that isFilterDone() is called from nextRaw() and isFilterDone() is synchronized.
The caller of nextRaw is required to ensure single threaded access to nextRaw() anyway, we can call an unsynchronized internal version of isFilterDone().
", ,,1,,,,,
HBASE-10225,"Just noticed that while looking at HBASE-10047.
In 0.94 (and presumably in trunk, will check later) we have this:
{code}
    protected boolean nextRow(byte [] currentRow, int offset, short length) throws IOException {
     ...
      if (this.region.getCoprocessorHost() != null) {
        return this.region.getCoprocessorHost().postScannerFilterRow(this, currentRow);
      }
      return true;
    }
{code}
Notice how we only pass currentRow into the coprocessor, but not offset and length. Anything using this hook currently is 100% broken. The hook was added in 0.94.5 (HBASE-5664), it never worked correctly.

[~anoopsamjohn], you had added the hook. Do you still need it?

We can either remove it (I'd prefer that in the light of the performance issued observed in HBASE-10047, we can leave the stub in BaseRegionObserver in 0.94, but document that it is no-op), or we'd have to have change its signature to be able to pass offset and length as well. 

Since nobody noticed nobody is using this hook currently, so both should be valid options.

(Making a new standalone copy of the rowkey just to pass into this method absolutely out of the question for performance reasons).
", ,,,,,,,
HBASE-10226,"Namespace grants for a user are supposed to supercede table level permissions, a middle tier between table grants and global grants. We are not always checking.",1,,,,,,,
HBASE-10227,"When opening a region, all stores are examined to get the max MemstoreTS and it's used as the initial mvcc for the region, and then split hlogs are replayed. In fact the edits in split hlogs have kvs with greater mvcc than all MemstoreTS in all store files, but replaying them don't increment the mvcc according at all. From an overall perspective this mvcc recovering is 'logically' incorrect/incomplete.

Why currently it doesn't incur problem is because no active scanners exists and no new scanners can be created before the region opening completes, so the mvcc of all kvs in the resulted hfiles from hlog replaying can be safely set to zero. They are just treated as kvs put 'earlier' than the ones in HFiles with mvcc greater than zero(say 'earlier' since they have mvcc less than the ones with non-zero mvcc, but in fact they are put 'later'), and without any incorrect impact just because during region opening there are no active scanners existing / created.

This bug is just in 'logic' sense for the time being, but if later on we need to survive mvcc in the region's whole logic lifecycle(across regionservers) and never set them to zero, this bug needs to be fixed first.", ,,,,,,,
HBASE-10228,Currently the shell scripts supports attributes but not the new apis in the Query class. So this JIRA is to support them thro shell.,1,,,,,,,
HBASE-10237,"The following logs record such  process:

// *once allocate root and meta to node209*
2013-12-23 10:45:34,130 INFO  \[MASTER_OPEN_REGION-node201.vipcloud,60000,1386903739776-3\] handler.OpenedRegionHandler (OpenedRegionHandler.ja
va:debugLog(145)) - Handling OPENED event for .META.,,1.1028785192 from node209.vipcloud,60020,1387272038024; deleting unassigned node
2013-12-23 14:53:36,268 INFO  \[MASTER_OPEN_REGION-node201.vipcloud,60000,1386903739776-4\] handler.OpenedRegionHandler (OpenedRegionHandler.java:debugLog(145)) - Handling OPENED event for \-ROOT\-,,0.70236052 from node209.vipcloud,60020,1387272038024; deleting unassigned node

// *master restart*
2013-12-23 16:30:19 CST Starting master on node201.vipcloud

// *209 comming*
2013-12-23 16:30:33,698 INFO  \[master-node201.vipcloud,60000,1387787422616\] master.ServerManager (ServerManager.java:recordNewServer(280)) -
Registering server=node209.vipcloud,60020,1387272038024

// *209 out*
2013-12-23 16:30:37,106 INFO  \[main-EventThread\] zookeeper.RegionServerTracker (RegionServerTracker.java:nodeDeleted(93)) - RegionServer ephe
meral node deleted, processing expiration \[node209.vipcloud,60020,1387272038024\]

// *delay processing 209 for initialization*
2013-12-23 16:30:37,107 INFO  \[main-EventThread\] master.ServerManager (ServerManager.java:expireServer(384)) - Master doesn't enable ServerSh
utdownHandler during initialization, delay expiring server node209.vipcloud,60020,1387272038024

// *assign root to node209 for data in zk node*
2013-12-23 16:30:42,120 INFO \[master-node201.vipcloud,60000,1387787422616\] master.HMaster (HMaster.java:assignRoot(756)) - \-ROOT\- assigned=0
, rit=false, location=node209.vipcloud,60020,1387272038024

// *problem happened when assign META to node209, validation passed first but when check server available failed. Therefore not update the regions struct for META*
s1?013-12-23 16:30:42,322 INFO  \[master-node201.vipcloud,60000,1387787422616\] master.AssignmentManager (AssignmentManager.java:regionOnline(126
4)) - The server is not in online servers, ServerName=node209.vipcloud,60020,1387272038024, region=1028785192
s2?013-12-23 16:30:42,323 INFO  \[master-node201.vipcloud,60000,1387787422616\] master.HMaster (HMaster.java:assignMeta(814)) - .META. assigned=0
, rit=false, location=node209.vipcloud,60020,1387272038024

// *handle node209 shutdown, only do reassign for \-ROOT\- but not for .META. because it is NOT updated in memory*
2013-12-23 16:31:35,978 INFO  \[MASTER_META_SERVER_OPERATIONS-node201.vipcloud,60000,1387787422616-0\] handler.MetaServerShutdownHandler (MetaS
erverShutdownHandler.java:process(78)) - Server node209.vipcloud,60020,1387272038024 was carrying ROOT. Trying to assign.

// *verifaction for META failed (data in \-ROOT\- still stored as node209), but no reassign. for META. META won't be online forever*
2013-12-23 16:31:40,048 INFO  \[MASTER_SERVER_OPERATIONS-node201.vipcloud,60000,1387787422616-0\] catalog.CatalogTracker (CatalogTracker.java:
verifyRegionLocation(582)) - Failed verification of .META.,,1 at address=node209.vipcloud,60020,1387272038024; org.apache.hadoop.hbase.ipc.HBa
seClient$FailedServerException: This server is in the failed servers list: node209.vipcloud/192.168.30.132:60020

2013-12-23 16:35:46,764 INFO  \[MASTER_SERVER_OPERATIONS-node201.vipcloud,60000,1387787422616-0\] catalog.CatalogTracker (CatalogTracker.java:v
erifyRegionLocation(582)) - Failed verification of .META.,,1 at address=node209.vipcloud,60020,1387272038024; org.apache.hadoop.hbase.ipc.HBa
seClient$FailedServerException: This server is in the failed servers list: node209.vipcloud/192.168.30.132:60020", ,,,,,,,
HBASE-10241,"A while ago I introduced HRegoinScannerImpl.nextRaw() to allow coprocessors and scanners with caching > 1 to avoid repeated synchronization during scanning (which puts up memory fences, which in turn slows things down on multi core machines).

Looking at the code again I see that isFilterDone() is called from nextRaw() and isFilterDone() is synchronized.
The caller of nextRaw is required to ensure single threaded access to nextRaw() anyway, we can call an unsynchronized internal version of isFilterDone().
", ,,1,,,,,
HBASE-10242,"Just noticed that while looking at HBASE-10047.
In 0.94 (and presumably in trunk, will check later) we have this:
{code}
    protected boolean nextRow(byte [] currentRow, int offset, short length) throws IOException {
     ...
      if (this.region.getCoprocessorHost() != null) {
        return this.region.getCoprocessorHost().postScannerFilterRow(this, currentRow);
      }
      return true;
    }
{code}
Notice how we only pass currentRow into the coprocessor, but not offset and length. Anything using this hook currently is 100% broken. The hook was added in 0.94.5 (HBASE-5664), it never worked correctly.

[~anoopsamjohn], you had added the hook. Do you still need it?

We can either remove it (I'd prefer that in the light of the performance issued observed in HBASE-10047, we can leave the stub in BaseRegionObserver in 0.94, but document that it is no-op), or we'd have to have change its signature to be able to pass offset and length as well. 

Since nobody noticed nobody is using this hook currently, so both should be valid options.

(Making a new standalone copy of the rowkey just to pass into this method absolutely out of the question for performance reasons).
", ,,,,,,,
HBASE-10247,"A while ago I introduced HRegoinScannerImpl.nextRaw() to allow coprocessors and scanners with caching > 1 to avoid repeated synchronization during scanning (which puts up memory fences, which in turn slows things down on multi core machines).

Looking at the code again I see that isFilterDone() is called from nextRaw() and isFilterDone() is synchronized.
The caller of nextRaw is required to ensure single threaded access to nextRaw() anyway, we can call an unsynchronized internal version of isFilterDone().
", ,,1,,,,,
HBASE-10249,New issue to keep track of this., ,,,,,,,
HBASE-10252,"A while ago I introduced HRegoinScannerImpl.nextRaw() to allow coprocessors and scanners with caching > 1 to avoid repeated synchronization during scanning (which puts up memory fences, which in turn slows things down on multi core machines).

Looking at the code again I see that isFilterDone() is called from nextRaw() and isFilterDone() is synchronized.
The caller of nextRaw is required to ensure single threaded access to nextRaw() anyway, we can call an unsynchronized internal version of isFilterDone().
", ,,1,,,,,
HBASE-10257,Master aborts due to assignment race, ,,,,,,,
HBASE-10263,make LruBlockCache single/multi/in-memory ratio user-configurable and provide preemptive mode for in-memory type block], ,1,1,,,,,
