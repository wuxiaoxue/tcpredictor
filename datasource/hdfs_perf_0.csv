"When the first datanode's write to second datanode fails or times out DFSClient ends up marking first datanode as the bad one and removes it from the pipeline. Similar problem exists on DataNode as well and it is fixed in HADOOP-3339. From HADOOP-3339 : 

""The main issue is that BlockReceiver thread (and DataStreamer in the case of DFSClient) interrupt() the 'responder' thread. But interrupting is a pretty coarse control. We don't know what state the responder is in and interrupting has different effects depending on responder state. To fix this properly we need to redesign how we handle these interactions.""

When the first datanode closes its socket from DFSClient, DFSClient should properly read all the data left in the socket.. Also, DataNode's closing of the socket should not result in a TCP reset, otherwise I think DFSClient will not be able to read from the socket.",0
"Before 0.18, when Datanode restarts, it deletes files under data-dir/tmp  directory since these files are not valid anymore. But in 0.18 it moves these files to normal directory incorrectly making them valid blocks. One of the following would work :

- remove the tmp files during upgrade, or
- if the files under /tmp are in pre-18 format (i.e. no generation), delete them.

Currently effect of this bug is that, these files end up failing block verification and eventually get deleted. But cause incorrect over-replication at the namenode before that.

Also it looks like our policy regd treating files under tmp needs to be defined better. Right now there are probably one or two more bugs with it. Dhruba, please file them if you rememeber.
",0
"When a client lease expires the lease monitor removes it from the list of leases, but does not release pendingCreates files. When the name-node sees a file in pendingCreates which does not have a lease for the original client it throws AlreadyBeingCreatedException.This was reported in several issues like HADOOP-1411, but has never been resolved.I see two approaches: clean pendingCreates when leases expire, or reuse entries in pending creates for which there are no lease.The first is preferable imo since it cleans memory at the right time.",0
"The shutdown methods in Namenode arent properly synchronized; the stop() method should be marked as sync or the stopRequested field marked as volatile.
",0
"Currently NameNode treats either the new replica or existing replicas as corrupt if the new replica's length is inconsistent with NN recorded block length. The correct behavior should be
1. For a block that is not under construction, the new replica should be marked as corrupt if its length is inconsistent (no matter shorter or longer) with the NN recorded block length;
2. For an under construction block, if the new replica's length is shorter than the NN recorded block length, the new replica could be marked as corrupt; if the new replica's length is longer, NN should update its recorded block length. But it should not mark existing replicas as corrupt. This is because NN recorded length for an under construction block does not accurately match the block length on datanode disk. NN should not judge an under construction replica to be corrupt by looking at the inaccurate information:  its recorded block length.",0
"Currently disk space quota exceed exception spits out stack trace . It's better to show error message instead of stack trace .

{code}
somehost:Hadoop guesti$ bin/hdfs dfsadmin -setSpaceQuota 2 2344
somehost:Hadoop guest$ bin/hadoop fs -put conf 2344
09/06/19 16:44:30 WARN hdfs.DFSClient: DataStreamer Exception: org.apache.hadoop.hdfs.protocol.QuotaExceededException: org.apache.hadoop.hdfs.protocol.QuotaExceededException: The quota of /user/guest/2344 is exceeded: namespace quota=-1 file count=4, diskspace quota=2 diskspace=67108864
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        ..................
{code}",0
"In Hypertable, there is a process called the RangeServer which is responsible for handling updates and queries on portions (ranges) of a database table.  When an update arrives at the RangeServer, it first gets written to a commit log and then it goes into an in-memory table structure.  If at any time, the RangeServer dies, when it comes back up, it replays the commit log to reconstruct the state of the in-memory table structure.  As it stands now, when the RangeServer comes up, it cannot read the commit log file until the soft lease limit has expired.  This causes the RangeServer to hang with all of the ranges that it was managing unaccessible.

We would like to be able to configure the soft lease limit per-file so that we can set the limit to 0 for our commit log files.  That would allow Hypertable to recover immediately after a crash.
",0
It appears possible to raise an NPE in DataNode.handshake() if the startup protocol gets interrupted or fails in some manner,0
Currently invalideBlock does not allow to delete a replica only if at least two valid replicas exist before deletion is scheduled. This is too restrictive if the replica to delete is a corrupt one. NameNode could delete a corrupt replica as long as at least one copy (no matter valid or corrupt) will be left.,0
"I encountered a bug when trying to upload data using the Hadoop DFS Client.  
After receiving a NotReplicatedYetException, the DFSClient will normally retry its upload up to some limited number of times.  In this case, I found that this retry loop continued indefinitely, to the point that the number of tries remaining was negative:
2009-03-25 16:20:02 [INFO] 
2009-03-25 16:20:02 [INFO] 09/03/25 16:20:02 INFO hdfs.DFSClient: Waiting for replication for 21 seconds
2009-03-25 16:20:03 [INFO] 09/03/25 16:20:02 WARN hdfs.DFSClient: NotReplicatedYetException sleeping /apollo/env/SummaryMySQL/var/logstore/fiorello_logs_2009
0325_us/logs_20090325_us_13 retries left -1


The stack trace for the failure that's retrying is:
2009-03-25 16:20:02 [INFO] 09/03/25 16:20:02 INFO hdfs.DFSClient: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.NotReplicated
YetException: Not replicated yet:<filename>
2009-03-25 16:20:02 [INFO]      at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1266)
2009-03-25 16:20:02 [INFO]      at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:351)
2009-03-25 16:20:02 [INFO]      at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
2009-03-25 16:20:02 [INFO]      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
2009-03-25 16:20:02 [INFO]      at java.lang.reflect.Method.invoke(Method.java:597)
2009-03-25 16:20:02 [INFO]      at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:481)
2009-03-25 16:20:02 [INFO]      at org.apache.hadoop.ipc.Server$Handler.run(Server.java:894)
2009-03-25 16:20:02 [INFO] 
2009-03-25 16:20:02 [INFO]      at org.apache.hadoop.ipc.Client.call(Client.java:697)
2009-03-25 16:20:02 [INFO]      at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:216)
2009-03-25 16:20:02 [INFO]      at $Proxy0.addBlock(Unknown Source)
2009-03-25 16:20:02 [INFO]      at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
2009-03-25 16:20:02 [INFO]      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
2009-03-25 16:20:02 [INFO]      at java.lang.reflect.Method.invoke(Method.java:597)
2009-03-25 16:20:02 [INFO]      at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
2009-03-25 16:20:02 [INFO]      at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
2009-03-25 16:20:02 [INFO]      at $Proxy0.addBlock(Unknown Source)
2009-03-25 16:20:02 [INFO]      at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.locateFollowingBlock(DFSClient.java:2814)
2009-03-25 16:20:02 [INFO]      at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2696)
2009-03-25 16:20:02 [INFO]      at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:1996)
2009-03-25 16:20:02 [INFO]      at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2183)

Fixes logical error in DFSClient::DFSOutputStream::DataStreamer::locateFollowingBlock that caused infinite retries on write. Modified DFSClient constructor to allow unit testing of locateFollowingBlock and added unit tests. 
",0
"If a reported block has a different generation stamp then the one stored in the NameNode, the reported block will be considered as invalid.  This is incorrect since blocks with larger generation stamp are valid.",0
"Empty file of size 0 is created when QuotaExceed exception occurs while copying a file. This file is created with the same name of which file copy is tried .
I.E if operation 
Hadoop fs -copyFromLocal testFile1 /testDir   
Fails due to quota exceed exception then testFile1 of size 0 is created in testDir on HDFS.


Steps to verify 

1) Create testDir and apply space quota of 16kb
2) Copy file say testFile of size greater than 16kb from local file system
3) You should see QuotaException error 
4) testFile of size 0 is created in testDir which is not expected .",0
"When I ran a test on a one-node dfs cluster, the test failed because of the exception listed below. First of all, I wonder why chooseTarget returns 0 target when # of datanodes is 1. Secondly I think it would be nicer if startFile retries chooseTarget before throws the exception back to client.java.io.IOException: Failed to create file /user/hairong/xxx on client 127.0.0.1 because there were not enough datanodes available. Found 0 datanodes but MIN_REPLICATION for the cluster is configured to be 1. at rg.apache.hadoop.dfs.FSNamesystem.startFile(FSNamesystem.java:813)at org.apache.hadoop.dfs.NameNode.create(NameNode.java:294)at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:339)at org.apache.hadoop.ipc.Server$Handler.run(Server.java:573)",0
"Chown , chgrp , chmod operations allowed when namenode is in safemode .",0
"TestBackupNode may fail with different reasons:
- Unable to open edit log file .\build\test\data\dfs\name-backup1\current\edits (FSEditLog.java:open(371))
- NullPointerException at org.apache.hadoop.hdfs.server.namenode.EditLogBackupOutputStream.flushAndSync(EditLogBackupOutputStream.java:163)
- Fatal Error : All storage directories are inaccessible.
Will provide more information in the comments.",0
Original access token may have expired when re-establishing the pipeline within processDatanodeError().,0
"If we can set replication factor for directory. we can modify the DFSClent.create() method, pass 0 for the default block replication. Namenode check create request, if blockreplication is 0, it will give its parent dir replication factor to the file blockreplication factor. This will simplify the administration work. You know we can set /Test or /Tmp dir's replication factor 2 or 1, then all their children files and dirs replication factor is 2 or 1 defaultly.",0
"I was able to compile 0.18.2 in eclipse into a new OSGi bundle using eclipse PDE. Using Spring to control the HDFS nodes, however, seems out of the question for the time being because of inter-dependencies between packages that should be separate OSGi bundles (for example, SecondaryNameNode includes direct references to StatusHttpServer, which should be in a bundle with a ""web"" personality that is separate from Hadoop Core). Looking through the code that starts the daemons, it would seem code changes are necessary to allow for components to be dependency-injected. Rather than instantiating a StatusHttpServer inside the SecondaryNameNode, that reference should (at the very least) be able to be dependency-injected (for example from an OSGi service from another bundle). Adding setters for infoServer would allow that reference to be injected by Spring. This is just an example of the changes that would need to be made to get Hadoop to live happily inside an OSGi container.As a starting point, it would be nice if Hadoop core was able to be split into a client bundle that could be deployed into OSGi containers that would provide client-only access to HDFS clusters.",0
"When namenode becomes unresponsive by HADOOP-4693 (large filelist calls), metrics has been helpful in finding out the cause.
When gc time hikes, ""FileListed"" metrics also hiked.
In 0.18, after we fixed ""FileListed"" metrics so that it shows number of operations instead of number of files listed (HADOOP-3683), I stopped seeing this relationship graph.
Can we bring back ""NumbverOfFilesListed"" metrics?",0
"Java implementation of tmpreaper utility for HDFS. Helps when you expect processes to die before they can clean up. I have perl unit tests that can be ported over to java or groovy if the hadoop team is interested in this utility. One issue is that the unit tests set the modification time of test files, which is unsupported in HDFS (as far as I can tell).",0
"As now the fsck can do pretty well,but when the developer happened to the log such Block blk_28622148 is not valid.etc
We wish to know which file and the datanodes the block belongs to.It can be solved by running ""bin/hadoop fsck -files -blocks -locations / | grep <blockid>"" ,but as mentioned early in the HADOOP-4945 ,it's not an effective way in a big product cluster.
so maybe we could do something to let the fsck more convenience .",0
"This proposal is to extend the DataNode Transfer protocol (described in DataNode.java lines 916 - 995) to support ""options"", in the spirit of IP or TCP options. This should make this protocol more extensible, allowing the client to include metadata along with commands. This would support efforts to include end-to-end and causal tracing into Hadoop, and hopefully other efforts as well.
Options should have a type, and be of variable length. It should be possible to include multiple options along with each datanode command. The option should apply to both the command and any data that is part of the command. If the datanode does not understand a given option, it should ignore it. Options should be sent end-to-end through intermediate datanodes, if necessary. For example, if an OP_WRITE_BLOCK command is pipelined through several machines, the options should be sent along the pipeline. Nodes along the pipeline may modify the options.
BTW, If HADOOP-4005 (concrete datanode protocol) is implemented, then it should solve this problem by simply letting the user add state to the concrete protocol class.",0
"Occasionally a file is corrupted because a block goes missing. This may occur if all the Datanodes containing a given block shutdown or crash, or from defects in the Datanode or Namenode code-base. A simple diagnostic tool is needed that can trace and analyze the historical movement of the blocks in a given file.",0
"migrate server logs to HDFS for space and durability.
This add another option for improving performance with a multi-tiered storage solution. In addition, durability and availability of the logs for analysis could be improved.",0
"Currently, there is no way to verify that a copy of the fsImage is not corrupt. I propose that we should have an offline tool that loads the fsImage into memory to see if it is usable. This will allow us to automate backup testing to some extent.
One can start a namenode process on the fsImage to see if it can be loaded, but this is not easy to automate.
To use HDFS in production, it is greatly desired to have both checkpoints - and have some idea that the checkpoints are valid! No one wants to see the day where they reload from backup only to find that the fsImage in the backup wasn't usable.",0
"An FTP server that sits on top of a distributed filesystem like HDFS has many benefits. It allows the storage and management of data via clients that do not know HDFS but understand other more popular transport mechanism like FTP. The data is thus managed via a standard and more popular protocol, support for which is widely available.
The idea is to leverage what is already available in Apache http://mina.apache.org/ftpserver.html and build on top of it. This FTP server can be embedded easily in hadoop and can easily be programmed to talk to HDFS via an Ftplet which is run by the FTP server.
Ideally there should be options to configure FTP server settings (in hadoop-default.xml) which allows FTP server to be started when HDFS is booted.",0
"There should be a facility to migrate old files away from a production cluster. Access to those files from applications should continue to work transparently, without changing application code, but maybe with reduced performance. The policy engine that does this could be layered on HDFS rather than being built into HDFS itself.",0
Provide a mechanism to trigger block scans in a datanode upon request. Support interfaces for commands sent by the namenode and through the HTTP interface.,0
An API to concatenate files of same size and replication factor on HDFS into a single larger file.,0
"In some hadoop map/reduce and dfs use cases, including a specific case that arises in my own work, users would like to populate dfs with a family of hundreds or thousands of directory trees, each of which consists of thousands of files. In our case, the trees each have perhaps 20 gigabytes; two or three 3-10-gigabyte files, a thousand small ones, and a large number of files of intermediate size. I am writing this JIRA to encourage discussion of a new facility I want to create and contribute to the dfs core.
– The problem
You can't store such families of trees in dfs in the obvious manner. The problem is that the name nodes can't handle the millions or ten million files that result from such a family, especially if there are a couple of families. I understand that dfs will not be able to accommodate tens of millions of files in one instance for quite a while.
– Exposed API of my proposed solution
I would therefore like to produce, and contribute to the dfs core, a new tool that implements an abstraction called a Hadoop Archive [or harchive]. Conceptually, a harchive is a unit, but it manages a space that looks like a directory tree. The tool exposes an interface that allows a user to do the following:
directory-level operations
create a harchive [either empty, or initially populated form a locally-stored directory tree] . The namespace for harchives is the same as the space of possible dfs directory locators, and a harchive would in fact be implemented as a dfs directory with specialized contents.
Add a directory tree to an existing harchive in a specific place within the harchive
retrieve a directory tree or subtree at or beneath the root of the harchive directory structure, into a local directory tree
file-level operations
add a local file to a specific place in the harchive
modify a file image in a specific place in the harchive to match a local file
delete a file image in the harchive.
move a file image within the harchive
open a file image in the harchive for reading or writing.
stream operations
open a harchive file image for reading or writing as a stream, in a manner similar to dfs files, and read or write it [ie., hdfsRead(...) ]. This would include random access operators for reading.
management operations
commit a group of changes [which would be made atomically -- there would be no way half of a change could be made to a harchive if a client crashes].
clean up a harchive, if it's gotten less performant because of extensive editing
delete a harchive
We would also implement a command line interface.
– Brief sketch of internals
A harchive would be represented as a small collection of files, called segments, in a dfs directory at the harchive's location. Each segment would contain some of the files of the harchive's file images in a format to be determined, plus a harchive index. We may group files by size, or some other criteria. It is likely that harchives would contain only one segment in common cases.
Changes would be made by adding the text of the new files, either by rewriting an existing segment that contains not much more data than the size of the changes or by creating a new segment, complete with a new index. When dfs comes to be enhanced to allow appends to dfs files, as requested by HADOOP-1700 , we would be able to take advantage of that.
Often, when a harchive is initially populated, it could be a single segment, and a file it contains could be accessed with two random accesses into the segment. The first access retrieves the index, and the second access retrieves the beginning of the file. We could choose to put smaller files closer to the index to allow lower average amortized costs per byte.
We might instead choose to represent a harchive as one file or a few files for the large represented files, and smaller files for the represented smaller files. That lets us make modifications by copying at lower cost.
The segment containing the index is found by a naming convention. Atomicity is obtained by creating indices and renaming the files containing them according to the convention, when a change is committed.",0
"WebDAV stands for Distributed Authoring and Versioning. It is a set of extensions to the HTTP protocol that lets users collaboratively edit and manage files on a remote web server. It is often considered as a replacement for NFS or SAMBA
HDFS (Hadoop Distributed File System) needs a friendly file system interface. DFSShell commands are unfamiliar. Instead it is more convenient for Hadoop users to use a mountable network drive. A friendly interface to HDFS will be used both for casual browsing of data and for bulk import/export.
The FUSE provider for HDFS is already available ( http://issues.apache.org/jira/browse/HADOOP-17 ) but it had scalability problems. WebDAV is a popular alternative.
The typical licensing terms for WebDAV tools are also attractive: 
GPL for Linux client tools that Hadoop would not redistribute anyway. 
More importantly, Apache Project/Apache license for Java tools and for server components. 
This allows for a tighter integration with the HDFS code base.
There are some interesting Apache projects that support WebDAV.
But these are probably too heavyweight for the needs of Hadoop:
Tomcat servlet: http://tomcat.apache.org/tomcat-4.1-doc/catalina/docs/api/org/apache/catalina/servlets/WebdavServlet.html
Slide: http://jakarta.apache.org/slide/
Being HTTP-based and ""backwards-compatible"" with Web Browser clients, the WebDAV server protocol could even be piggy-backed on the existing Web UI ports of the Hadoop name node / data nodes. WebDAV can be hosted as (Jetty) servlets. This minimizes server code bloat and this avoids additional network traffic between HDFS and the WebDAV server.
General Clients (read-only):
Any web browser
Linux Clients: 
Mountable GPL davfs2 http://dav.sourceforge.net/
FTP-like GPL Cadaver http://www.webdav.org/cadaver/
Server Protocol compliance tests:
http://www.webdav.org/neon/litmus/ 
A goal is for Hadoop HDFS to pass this test (minus support for Properties)
Pure Java clients:
DAV Explorer Apache lic. http://www.ics.uci.edu/~webdav/
WebDAV also makes it convenient to add advanced features in an incremental fashion:
file locking, access control lists, hard links, symbolic links.
New WebDAV standards get accepted and more or less featured WebDAV clients exist.
core http://www.webdav.org/specs/rfc2518.html
ACLs http://www.webdav.org/specs/rfc3744.html
redirects ""soft links"" http://greenbytes.de/tech/webdav/rfc4437.html
BIND ""hard links"" http://www.webdav.org/bind/
quota http://tools.ietf.org/html/rfc4331",0
"When user support gets added to HDFS, administrators are going to need to be able to set the namenode such that it only allows connections/interactions from the administrative user. This is particularly important after upgrades and for other administrative work that may require the changing of user/group ownership, permissions, location of files within the HDFS, etc.",0
"Much of Hadoop's behavior is client-driven, with clients responsible for contacting individual datanodes to read and write data, as well as dividing up work for map and reduce tasks. In a large deployment with many concurrent users, identifying the effects of individual clients on the infrastructure is a challenge. The use of data pipelining in HDFS and Map/Reduce make it hard to follow the effects of a given client request through the system.
This proposal is to instrument the HDFS, IPC, and Map/Reduce layers of Hadoop with X-Trace. X-Trace is an open-source framework for capturing causality of events in a distributed system. It can correlate operations making up a single user request, even if those operations span multiple machines. As an example, you could use X-Trace to follow an HDFS write operation as it is pipelined through intermediate nodes. Additionally, you could trace a single Map/Reduce job and see how it is decomposed into lower-layer HDFS operations.
Matei Zaharia and Andy Konwinski initially integrated X-Trace with a local copy of the 0.14 release, and I've brought that code up to release 0.17. Performing the integration involves modifying the IPC protocol, inter-datanode protocol, and some data structures in the map/reduce layer to include 20-byte long tracing metadata. With release 0.18, the generated traces could be collected with Chukwa.
I've attached some example traces of HDFS and IPC layers from the 0.17 patch to this JIRA issue.
More information about X-Trace is available from http://www.x-trace.net/ as well as in a paper that appeared at NSDI 2007, available online at http://www.usenix.org/events/nsdi07/tech/fonseca.html",0
Support HDFS snapshots. It should support creating snapshots without shutting down the file system. Snapshot creation should be lightweight and a typical system should be able to support a few thousands concurrent snapshots. There should be a way to surface (i.e. mount) a few of these snapshots simultaneously.,0
BookKeeper is a system to reliably log streams of records (https://issues.apache.org/jira/browse/ZOOKEEPER-276). The NameNode is a natural target for such a system for being the metadata repository of the entire file system for HDFS.,0
"The code currently works as follows.
HftpFileSystem::open(path, bufferSize) issues a GET request to, e.g., http://namenode/data/path
On the namenode, /data/path is handled by FileDataServlet. FileDataServlet chooses a datanode (using JspHelper.bestNode) and issues an http redirect response to the datanode (e.g., http://datanode/streamFile?filename=path&... )
/streamFile?filename=path is called on the data node, which is handled by org.apache.hadoop.hdfs.server.namenode.StreamFile. StreamFIle creates a DFSClient and serves the appropriate file.
To handle range requests, the following can be done:
Modify /streamFile to handle range requests
Modify the way FileDataServlet chooses a datanode (it should use the block locations in the byte-range being requested, not the block locations for the entire file)
Add a method to HftpFileSystem that takes one or more byte range arguments (depending on the answer to the question below)
Confirm that when HttpURLConnection follows redirects, it maintains headers. Specifically, the Range header will need to be sent to the datanode after the redirect response comes back from the namenode.",0
"It would be helpful if there were a way to query the namenode to verify that it is basically healthy. In particular, that all the expected threads are running, data structures appear sane, etc. Administrators could use this interface to verify that the namenode is both up and essentially functional, attaching cron jobs, notification, etc. as required.",0
"Is there a plan to provide an option to enable metrics (both system and application level) collection on the JobTracker and NameNode from TaskTrackers and DataNodes respectively? Visualizing this data can help us understand what is happening on the various nodes in the cluster in terms of CPU, Memory, Disk Utilization, Network IO etc and possibly spot issues when cluster performance degrades.",0
"HDFS should support symbolic links. A symbolic link is a special type of file that contains a reference to another file or directory in the form of an absolute or relative path and that affects pathname resolution. Programs which read or write to files named by a symbolic link will behave as if operating directly on the target file. However, archiving utilities can handle symbolic links specially and manipulate them directly.",0
"It would be very useful to have a command that we could give a hdfs directory to, that would use fsck to find the block locations of the data files in that directory and group them by host and display the distribution graphically. We did this by hand and it was very for finding a skewed distribution that was causing performance problems. The tool should also be able to group by rack id and generate a similar plot.",0
"Currently HDFS has a socket level protocol for serving HDFS data to clients. Clients do not use RPCs to read or write data. Fundamentally there is no reason why this data transfer can not use RPCs.
This jira is place holder for any porting Datanode transfers to RPC. This topic has been discussed in varying detail many times, the latest being in the context of HADOOP-3856. There are quite a few issues to be resolved both at API level and at implementation level.
We should probably copy some of the comments from HADOOP-3856 to here.",0
"his issue is for adding timeout and shutdown of idle DFSClient <-> DataNode connections.
Applications can have DFS usage patterns than deviate from that of MR 'norm' where files are generally opened, sucked down as fast as is possible, and then closed. For example, at the other extreme, hbase wants to support fast random reading of key values over a sometimes relatively large set of MapFiles or MapFile equivalents. To avoid paying startup costs on every random read – opening the file and reading in the index each time – hbase just keeps all of its MapFiles open all the time.
In an hbase cluster of any significant size, this can add up to lots of file handles per process: See HADOOP-2577, "" [hbase] Scaling: Too many open file handles to datanodes"" for an accounting.
Given how DFSClient and DataXceiveServer interact when random reading, and given past observations that have the client-side file handles mostly stuck in CLOSE_WAIT (See HADOOP-2341, 'Datanode active connections never returns to 0'), a suggestion made up on the list today, that idle connections should be timedout and closed, would help applications that have hbase-like access patterns conserve file handles and allow them scale.
Below is context that comes of the mailing list under the subject: 'Re: Multiplexing sockets in DFSClient/datanodes?'stack wrote:
> Doug Cutting wrote:
>> RPC also tears down idle connections, which HDFS does not.  I wonder how much doing that alone might help your case?  That would probably be much simpler to implement.  Both client and server must already handle connection failures, so it shouldn't be too great of a change to have one or both sides actively close things down if they're idle for more than a few seconds.
>
> If we added tear down of idle sockets, that'd work for us and, as you suggest, should be easier to do than rewriting the client to use async i/o.   Currently, random reading, its probably rare that the currently opened HDFS block has the wanted offset and so a tear down of the current socket and an open of a new one is being done anyways.

HADOOP-2346 helps with the Datanode side of the problem. We still need DFSClient to clean up idle connections (otherwise these sockets will stay in CLOSE_WAIT state on the client). This would require an extra thread on client to clean up these connections. You could file a jira for it.",0
"When a datanode ran out of disk space, it throws DiskOutOfSpaceException internally. However, it does not notify the writing client about the exception. As a result, the client will wait until socket timeout.",0
"related issues have put a lot of efforts to provide the first implementation of append. However, append is such a complex feature. It turns out that there are issues that were initially seemed trivial but needs a careful design. This jira revisits append, aiming for a design and implementation supporting a semantics that are acceptable to its users.",0
"Currently DFSOutputStream.close() waits for ever if Namenode keeps throwing NotYetReplicated exception, for whatever reason. Its pretty annoying for a user. Shoud the loop inside close have a timeout? If so how much? It could probably something like 10 minutes.",0
"There are a number of edge cases that the two file system implementations handle differently. In particular:
When trying to make a directory under an existing file, HDFS throws an IOException while LocalFileSystem doesn't.
The FileSytem#listStatus(Path) method returns null for a non-existent file on HDFS, while LocalFileSytem returns an empty FileStatus array.
When trying to rename a non-existent path, LocalFileSystem throws an IOException, while HDFS returns false.
When renaming a file or directory to a non-existent directory (e.g. /a/b to /c/d, where /c doesn't exist) LocalFileSystem succeeds (returns true) while HDFS fails (false).
When renaming a file (or directory) as an existing file (or directory) LocalFileSystem succeeds (returns true) while HDFS fails (false).
We should document the expected behaviour for these cases in FileSystem's javadoc, and make sure all implementations conform to it.",0
"The Hadoop Namenode couldn't execute as Windows service,
because UnixUserGroupInformation#getUnixUserName() throws IOException.
The 'whoami' command output is 'nt authority\system' in default Windows service,
so I suggest to change the method like follows:
static String getUnixUserName() throws IOException {
String[] result = executeShellCommand(
new String[]
{Shell.USER_NAME_COMMAND}
);
if (Shell.WINDOWS)
{ return toString(result); }
else if (result.length!=1)
{ throw new IOException(""Expect one token as the result of "" + Shell.USER_NAME_COMMAND + "": "" + toString(result)); }
return result[0];",0
"Configuration parameters should be fully validated before name nodes or data nodes begin service.
Required parameters must be present.
Required and optional parameters must have values of proper type and range.
Undefined parameters must not be present.
(I was recently observing some confusion whose root cause was a mis-spelled parameter.)",0
"If there is an uncaught exception, some threads in a server may die silently. The corresponding error message does not show up in the log.",0
"I'd like to propose we have a standard interface for hadoop components, the things that get started or stopped when you bring up a namenode. currently, some of these classes have a stop() or shutdown() method, with no standard name/interface, but no way of seeing if they are live, checking their health of shutting them down reliably. Indeed, there is a tendency for the spawned threads to not want to die; to require the entire process to be killed to stop the workers.
Having a standard interface would make it easier for
management tools to manage the different things
monitoring the state of things
subclassing
The latter is interesting as right now TaskTracker and JobTracker start up threads in their constructor; that's very dangerous as subclasses may have their methods called before they are full initialised. Adding this interface would be the right time to clean up the startup process so that subclassing is less risky.",0
"both dfs client buffering (and i imagine map-reduce intermediate data) and datanode try to honor the same space reservation (dfs.du.reserved). But this is problematic because once hdfs/data-node fill up a node - there's no space left for map-reduce computations.
ideally - hdfs should be allowed to consume upto some watermark (say 60%) and then dfs buffering/intermediate storage should be allowed to consume space upto some higher watermark (say 90%). this way the node will always remain usable.
we are hitting this problem in a cluster where a few nodes have lower amount of space. while the cluster overall has space left, these nodes are hitting their space limits. but now tasks scheduled on these nodes fail because dfs client does not find space to buffer to. there's no workaround really i can think of.
another option would be to globally allocate hdfs blocks based on space availability (keep all nodes at the same space utilization % approx.).",0
"hadoop fs -put may fail due to different reasons, such as the source file does not exist, the destination file already exists, permission denied, or exceptions during writing.
However, it returns the same code (-1), making it impossible to tell what is the actual cause of the failure.",0
"During writing a block to data nodes, if the dfs client detects a bad data node in the write pipeline, it will re-construct a new data pipeline, 
excluding the detected bad data node. This implies that when the client finishes writing the block, the number of the replicas for the block 
may be lower than the intended replication factor. If the ratio of the number of replicas to the intended replication factor is lower than
certain threshold (say 0.68), then the client should send a request to the name node to replicate that block immediately.",0
DataBlockScanner should also check for truncated blocks and report them as corrupt block to the NN,0
"It would be helpful if, in addition to the block, datanode, and client, the cause of the error were included in ClientProtocol/DatanodeProtocol::reportBadBlocks logs.",0
"Name node performance suffers if either the replication queue is to big, or the avail space at data nodes is too small. In either case, the administrator should be notified.
If the situation is really desperate, the name node perhaps should enter safe mode.",0
"Currently, DFSClient maintains one socket per open file. For most map/reduce operations, this is not a problem because there just aren't many open files.
However, HBase has a very different usage model in which a single region region server could have thousands (10*3 but less than 10*4) open files. This can cause both datanodes and region servers to run out of file handles.
What I would like to see is one connection for each dfsClient, datanode pair. This would reduce the number of connections to hundreds or tens of sockets.
The intent is not to process requests totally asychronously (overlapping block reads and forcing the client to reassemble a whole message out of a bunch of fragments), but rather to queue requests from the client to the datanode and process them serially, differing from the current implementation in that rather than use an exclusive socket for each file, only one socket is in use between the client and a particular datanode.",0
"As mentioned in previous bug report:
We're running a smallish cluster with very different machines, some with only 60 gb harddrives
This creates a problem when inserting files into the dfs, these machines run out of space quickly while some have plenty of space free.
So instead of just shuffling the nodes, I've created a quick patch that first sorts the target nodes by (freespace / blocks).
It then randomizes the position of the first third of the nodes (so we don't put all the blocks in the file on the same machine)
I'll let you guys figure out how to improve this.
/Johan",0
"On one of our clusters we ended up with 11 singly-replicated corrupted blocks (checksum errors) such that jobs were failing because of no live blocks available.
fsck reports the system as healthy, although it is not.
I argue that fsck should have an option to check whether under-replicated blocks are okay.
Even better, the namenode should automatically check under-replicated blocks with repeated replication failures for corruption and list them somewhere on the GUI. And for checksum errors, there should be an option to undo the corruption and recompute the checksums.
Question: Is it at all probable that two or more replications of a block have checksum errors? If not, then we could reduce the checking to singly-replicated blocks.",0
"One is able to inject all sorts of faults into Hadoop's classes using new fault injection framework (HADOOP-6003). 
I've been injecting unchecked exception (RuntimeException) into BlockReceiver.receivePacket() method before any
  of write() operations (e.g. line 401, 449, 463, 529) and running some of the existing HDFS tests. The injection of unchecked exceptions causes DataXceiver to die silently and without any traces.

From a debugger run it seems like some threads are being left alive or not notified about the exception.",0
"Fuse-dfs should cache fs handles on a per-user basis. This significantly increases performance, and has the side effect of fixing the current code which leaks fs handles.
The original bug description follows:
I run the following test:
1. Run hadoop DFS in single node mode
2. start up fuse_dfs
3. copy my source tree, about 250 megs, into the DFS
cp -av * /mnt/hdfs/
in /var/log/messages I keep seeing:
Dec 22 09:02:08 bodum fuse_dfs: ERROR: hdfs trying to utime /bar/backend-trunk2/src/machinery/hadoop/output/2008/11/19 to 1229385138/1229963739
and then eventually
Dec 22 09:03:49 bodum fuse_dfs: ERROR: could not connect to dfs fuse_dfs.c:1333
Dec 22 09:03:49 bodum fuse_dfs: ERROR: could not connect to dfs fuse_dfs.c:1333
Dec 22 09:03:49 bodum fuse_dfs: ERROR: could not connect to dfs fuse_dfs.c:1037
Dec 22 09:03:49 bodum fuse_dfs: ERROR: could not connect to dfs fuse_dfs.c:1333
Dec 22 09:03:49 bodum fuse_dfs: ERROR: could not connect to dfs fuse_dfs.c:1037
Dec 22 09:03:49 bodum fuse_dfs: ERROR: could not connect to dfs fuse_dfs.c:1333
Dec 22 09:03:49 bodum fuse_dfs: ERROR: could not connect to dfs fuse_dfs.c:1209
Dec 22 09:03:49 bodum fuse_dfs: ERROR: could not connect to dfs fuse_dfs.c:1037
Dec 22 09:03:49 bodum fuse_dfs: ERROR: could not connect to dfs fuse_dfs.c:1037
Dec 22 09:03:49 bodum fuse_dfs: ERROR: could not connect to dfs fuse_dfs.c:1037
Dec 22 09:03:49 bodum fuse_dfs: ERROR: could not connect to dfs fuse_dfs.c:1037
Dec 22 09:03:49 bodum fuse_dfs: ERROR: could not connect to dfs fuse_dfs.c:1037
Dec 22 09:03:49 bodum fuse_dfs: ERROR: could not connect to dfs fuse_dfs.c:1037
Dec 22 09:03:49 bodum fuse_dfs: ERROR: could not connect to dfs fuse_dfs.c:1037
Dec 22 09:03:49 bodum fuse_dfs: ERROR: could not connect to dfs fuse_dfs.c:1037
Dec 22 09:03:49 bodum fuse_dfs: ERROR: could not connect to dfs fuse_dfs.c:1037
Dec 22 09:03:49 bodum fuse_dfs: ERROR: could not connect to dfs fuse_dfs.c:1209
Dec 22 09:03:49 bodum fuse_dfs: ERROR: could not connect to dfs fuse_dfs.c:1037
Dec 22 09:03:49 bodum fuse_dfs: ERROR: could not connect to dfs fuse_dfs.c:1037
Dec 22 09:03:49 bodum fuse_dfs: ERROR: could not connect to dfs fuse_dfs.c:1037
Dec 22 09:03:49 bodum fuse_dfs: ERROR: could not connect to dfs fuse_dfs.c:1333
Dec 22 09:03:49 bodum fuse_dfs: ERROR: could not connect to dfs fuse_dfs.c:1209
Dec 22 09:03:49 bodum fuse_dfs: ERROR: could not connect to dfs fuse_dfs.c:1037
and the file system hangs. hadoop is still running and I don't see any errors in it's logs. I have to unmount the dfs and restart fuse_dfs and then everything is fine again. At some point I see the following messages in the /var/log/messages:
ERROR: dfs problem - could not close file_handle(139677114350528) for /bar/backend-trunk2/src/machinery/hadoop/input/2008/12/14/actionrecordlog-8339-93825052368848-1229278807.log fuse_dfs.c:1464
Dec 22 09:04:49 bodum fuse_dfs: ERROR: dfs problem - could not close file_handle(139676770220176) for /bar/backend-trunk2/src/machinery/hadoop/input/2008/12/14/actionrecordlog-8140-93825025883216-1229278759.log fuse_dfs.c:1464
Dec 22 09:05:13 bodum fuse_dfs: ERROR: dfs problem - could not close file_handle(139677114812832) for /bar/backend-trunk2/src/machinery/hadoop/input/2008/12/14/actionrecordlog-8138-93825070138960-1229251587.log fuse_dfs.c:1464
Is this a known issue? Am I just flooding the system too much. All of this is being performed on a single, dual core, machine.",0
"when cached block locations are no longer valid (e.g., datanodes restart on different ports), pread() will fail, whereas normal read() still succeeds through re-fetching of block locations from namenode (up to a max number of times). ",0
"It is easy to manage user accounts using LDAP. by adding support for LDAP, proxy can do IP authorization in a headless fashion.
when a user send a request, proxy extract IP address and request PathInfo from the request. then it searches the LDAP server to get the allowed HDFS root paths given the IP address. Proxy will match the user request PathInfo with the allowed HDFS root path, return 403 if it could not find a match.",0
"In HADOOP-4559, a general REST API for reporting metrics was proposed but work seems to have stalled. In the interim, we have a simple XML translation of the existing NameNode status page which provides the same metrics as the human-readable page. This is a relatively lightweight addition to provide some machine-understandable metrics reporting.",0
"Current implementation shuts DataNode down completely when one of the configured volumes of the storage fails.
This is rather wasteful behavior because it decreases utilization (good storage becomes unavailable) and imposes extra load on the system (replication of the blocks from the good volumes). These problems will become even more prominent when we move to mixed (heterogeneous) clusters with many more volumes per Data Node.",0
"Job History Log Analyzer parses and analyzes history logs of map-reduce jobs. History logs contain information about execution of jobs, tasks, and attempts. The tool focuses on submission, launch, start, and finish times, as well as the success or failure of jobs, tasks and attempt.
The analyzer calculates per hour slot utilization and pending times on clusters running map-reduce jobs.",0
This is a tool for analyzing file size distribution in HDFS using a modified Offline Image Viewer tool.,0
"hdfsExists does not call destroyLocalReference for jPath anytime,
hdfsDelete does not call it when it fails, and
hdfsRename does not call it for jOldPath and jNewPath when it fails",0
"errno can be affected by other lib calls, so should always be set right before return stmt and never before making other library calls.",0
"1. hdfs_write  does not check hdfsWrite return code so -1 return code is ignored.
2. hdfs_write uses int for overall file length
",0
"Data transfer was tested with simulated exceptions as below:
# create files with dfs
# write 1 byte
# close file
# open the same file
# read the 1 byte and compare results

The file was closed successfully but we got an IOException(Could not get block locations...) when the file was reopened for reading.",0
"Unit tests are failing on windows due to a problem with rename. 
The failing code is around line 520 in FSImage.java:
      assert curDir.exists() : ""Current directory must exist."";
      assert !prevDir.exists() : ""prvious directory must not exist."";
      assert !tmpDir.exists() : ""prvious.tmp directory must not exist."";
      // rename current to tmp
      rename(curDir, tmpDir);
      // save new image
      if (!curDir.mkdir())
        throw new IOException(""Cannot create directory "" + curDir);",0
"The goal of this JIRA is to discuss how the cost of raw storage for a HDFS file system can be reduced. Keeping three copies of the same data is very costly, especially when the size of storage is huge. One idea is to reduce the replication factor and do erasure coding of a set of blocks so that the over probability of failure of a block remains the same as before.
Many forms of error-correcting codes are available, see http://en.wikipedia.org/wiki/Erasure_code. Also, recent research from CMU has described DiskReduce https://opencirrus.org/system/files/Gibson-OpenCirrus-June9-09.ppt.
My opinion is to discuss implementation strategies that are not part of base HDFS, but is a layer on top of HDFS.",0
SimpleDateFormat is not thread safe. Multiple threads accessing the servlet can cause threading issues,0
"Here is the failed stack trace:
{noformat}
Testcase: testOutputStreamClosedTwice took 1.026 sec
        Caused an ERROR
null
java.nio.channels.ClosedByInterruptException
        at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:184)
        at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:341)
        at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:55)
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:146)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:107)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
        at java.io.DataOutputStream.flush(DataOutputStream.java:106)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2438)
{noformat}",0
"Current trunk first calls detach to unlinks a finalized replica before appending to this block. Unlink is done by temporally copying the block file in the ""current"" subtree to a directory called ""detach"" under the volume's daa directory and then copies it back when unlink succeeds. On datanode restarts, datanodes recover faied unlink by copying replicas under ""detach"" to ""current"".
There are two bugs with this implementation:
1. The ""detach"" directory does not include in a snapshot. so rollback will cause the ""detaching"" replicas to be lost.
2. After a replica is copied to the ""detach"" directory, the information of its original location is lost. The current implementation erroneously assumes that the replica to be unlinked is under ""current"". This will make two instances of replicas with the same block id to coexist in a datanode. Also if a replica under ""detach"" is corrupt, the corrupt replica is moved to ""current"" without being detected, polluting datanode data.",0
BlockSender sets a wrong position in ChecksumException that indicates the offset where crc mismatch occurs.,0
"When writing a file, the pipeline status read timeouts for datanodes are not set up properly.",0
"Includes a java program to query the namenode for corrupt replica information at some interval. If a corrupt replica is found, a map reduce job is launched that will search (supplied) log files for one or more block ids. The mapred job can be used independently of the java client program and can also be used for arbitrary text searches.",0
"This bugs affects fuse-dfs severely. In my test, about 1GB memory were exhausted and the fuse-dfs mount directory was disconnected after writing 14000 files. This bug is related to the memory leak problem of this issue: http://issues.apache.org/jira/browse/HDFS-420. 

The bug can be fixed very easily. In function hdfsFreeFileInfo() in file hdfs.c (under c++/libhdfs/) change code block:

    //Free the mName
    int i;
    for (i=0; i < numEntries; ++i) {
        if (hdfsFileInfo[i].mName) {
            free(hdfsFileInfo[i].mName);
        }
    }

into:

    // free mName, mOwner and mGroup
    int i;
    for (i=0; i < numEntries; ++i) {
        if (hdfsFileInfo[i].mName) {
            free(hdfsFileInfo[i].mName);
        }
        if (hdfsFileInfo[i].mOwner){
            free(hdfsFileInfo[i].mOwner);
        }
        if (hdfsFileInfo[i].mGroup){
            free(hdfsFileInfo[i].mGroup);
        }
    }

I am new to Jira and haven't figured out a way to generate .patch file yet. Could anyone help me do that so that others can commit the changes into the code base. Thanks!
",0
"{{BlockManager.invalidateCorruptReplicas()}} iterates over DatanodeDescriptor-s while removing corrupt replicas from the descriptors. This causes {{ConcurrentModificationException}} if there is more than one replicas of the block. I ran into this exception debugging different scenarios in append, but it should be fixed in the trunk too.",0
ListPathsServlet throws NullPointerException when listing on a path which is not found in the namesystem.,0
"If during a write, the dfsclient sees that a block replica location for a newly allocated block is not-connectable, it re-requests the NN to get a fresh set of replica locations of the block. It tries this dfs.client.block.write.retries times (default 3), sleeping 6 seconds between each retry ( see DFSClient.nextBlockOutputStream).
This setting works well when you have a reasonable size cluster; if u have few datanodes in the cluster, every retry maybe pick the dead-datanode and the above logic bails out.
Our solution: when getting block location from namenode, we give nn the excluded datanodes. The list of dead datanodes is only for one block allocation.",0
"When I work on HDFS-624, I saw TestFileAppend3#TC7 occasionally fails. After lots of debug, I saw that the client unexpected received a response of ""-2 SUCCESS SUCCESS"" in which -2 is the packet sequence number. This happened in a pipeline of 2 datanodes and one of them failed. It turned out when block receiver fails, it shuts down itself and interrupts the packet responder but responder tries to handle interruption with the condition ""Thread.isInterrupted()"" but unfortunately a thread's interrupt status is not set in some cases as explained in the Thread#interrupt javadoc:

 If this thread is blocked in an invocation of the wait(), wait(long), or wait(long, int) methods of the Object  class, or of the join(), join(long), join(long, int), sleep(long), or sleep(long, int), methods of this class, then its interrupt status will be cleared and it will receive an InterruptedException. 

So datanode does not detect the interruption and continues as if no error occurs.",0
DFSClient cannot read the bytes in the last block in the case that the last block is being written.,0
Starting a data-node with an empty data directory fails in append branch.,0
FileAppend2 sometimes hangs on not able to complete a file. It turns out that sometimes DataNode reports a wrong replica length to NameNode thus NameNode marks it as corrupt. The file is not able to be closed because there is no good replica.,0
"estFileAppend3 hangs because it fails on close the file. The following is the snippet of logs that shows the cause of the problem:
[junit] 2009-10-01 07:00:00,719 WARN hdfs.DFSClient (DFSClient.java:setupPipelineForAppendOrRecovery(3004)) - Error Recovery for block blk_-4098350497078465335_1007 in pipeline 127.0.0.1:58375, 127.0.0.1:36982: bad datanode 127.0.0.1:36982
[junit] 2009-10-01 07:00:00,721 INFO datanode.DataNode (DataXceiver.java:opWriteBlock(224)) - Receiving block blk_-4098350497078465335_1007 src: /127.0.0.1:40252 dest: /127.0.0.1:58375
[junit] 2009-10-01 07:00:00,721 INFO datanode.DataNode (FSDataset.java:recoverClose(1248)) - Recover failed close blk_-4098350497078465335_1007
[junit] 2009-10-01 07:00:00,723 INFO datanode.DataNode (DataXceiver.java:opWriteBlock(369)) - Received block blk_-4098350497078465335_1008 src: /127.0.0.1:40252 dest: /127.0.0.1:58375 of size 65536
[junit] 2009-10-01 07:00:00,724 INFO hdfs.StateChange (BlockManager.java:addStoredBlock(1006)) - BLOCK* NameSystem.addStoredBlock: addStoredBlock request received for blk_-4098350497078465335_1008 on 127.0.0.1:58375 size 65536 But it does not belong to any file.
[junit] 2009-10-01 07:00:00,724 INFO namenode.FSNamesystem (FSNamesystem.java:updatePipeline(3946)) - updatePipeline(block=blk_-4098350497078465335_1007, newGenerationStamp=1008, newLength=65536, newNodes=[127.0.0.1:58375], clientName=DFSClient_995688145)",0
"After a BlockReceiver finish receiving the last packet of a block, it waits until the ack queue becomes empty. It them assumes that all acks have been sent and shunts down network connections. The current code removes a packet from the ack queue before its ack is sent. So there is a chance that the connection gets closed before an ack is sent.",0
"java.io.IOException: java.lang.NullPointerException
 at org.apache.hadoop.hdfs.server.datanode.FSDataset.updateReplicaUnderRecovery(FSDataset.java:2089)
 at org.apache.hadoop.hdfs.server.datanode.DataNode.updateReplicaUnderRecovery(DataNode.java:1598)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
 at java.lang.reflect.Method.invoke(Method.java:597)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:516)
 ...",0
"Renaming src to destination where src has exceeded the quota to a dst without sufficent quota fails. During this failure, src is deleted. ",0
"When I run TestClientProtocolForPipelineRecovery, I always see that the block receiver throws IOException complaining about mismatched checksum when receiving the last data packet. It turns out the checksum of last packet was unexpectedly set to be zero.",0
"Our secondary name node is not able to start on NullPointerException:
ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedSetTimes(FSDirectory.java:1232)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedSetTimes(FSDirectory.java:1221)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.loadFSEdits(FSEditLog.java:776)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSEdits(FSImage.java:992)
        at
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.doMerge(SecondaryNameNode.java:590)
        at
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.access$000(SecondaryNameNode.java:473)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doMerge(SecondaryNameNode.java:350)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:314)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:225)
        at java.lang.Thread.run(Thread.java:619)

This was caused by setting access time on a non-existent file.",0
"the append write failed on ""Too many open files"":
Some bytes were failed to append to a file on the following error:
java.io.IOException: Cannot run program ""stat"": java.io.IOException: error=24, Too many open files
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:459)
	at java.lang.Runtime.exec(Runtime.java:593)
	at java.lang.Runtime.exec(Runtime.java:466)
	at org.apache.hadoop.fs.FileUtil$HardLink.getLinkCount(FileUtil.java:644)
	at org.apache.hadoop.hdfs.server.datanode.ReplicaInfo.unlinkBlock(ReplicaInfo.java:205)
	at org.apache.hadoop.hdfs.server.datanode.FSDataset.append(FSDataset.java:1075)
	at org.apache.hadoop.hdfs.server.datanode.FSDataset.append(FSDataset.java:1058)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.<init>(BlockReceiver.java:110)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.opWriteBlock(DataXceiver.java:258)
	at org.apache.hadoop.hdfs.protocol.DataTransferProtocol$Receiver.opWriteBlock(DataTransferProtocol.java:382)
	at org.apache.hadoop.hdfs.protocol.DataTransferProtocol$Receiver.processOp(DataTransferProtocol.java:323)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:111)",0
java.io.InputStream.available() returns an int which has the max value 2^31-1 = 2GB - 1B.  It won't work if the number of available bytes is >= 2GB.,0
"According to the design, the primary datanode should compare replicas' on disk length but it is current using Block.numBytes.",0
"Running some loadings on hdfs I had one of these on the DN XX.XX.XX.139:51010:

{code}
2009-10-21 04:57:02,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving block blk_6345892463926159834_1029 src: /XX,XX,XX.140:37890 dest: /XX.XX.XX.139:51010
2009-10-21 04:57:02,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder blk_6345892463926159834_1029 1 Exception java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:923)
        at java.lang.Thread.run(Thread.java:619)
{code}

On XX,XX,XX.140 side, it looks like this:

{code}
10-21 04:57:01,866 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving block blk_6345892463926159834_1029 src: /XX.XX.XX.140:37385 dest: /XX.XX.XX140:51010
2009-10-21 04:57:02,836 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder 2 for block blk_6345892463926159834_1029 terminating
2009-10-21 04:57:02,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(XX.XX.XX.140:51010, storageID=DS-1292310101-208.76.44.140-51010-1256100924816, infoPort=51075, ipcPort=51020):Exception writing block blk_6345892463926159834_1029 to mirror XX.XX.XX.139:51010
java.io.IOException: Connection reset by peer
    at sun.nio.ch.FileDispatcher.write0(Native Method)
    at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:29)
    at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:104)
    at sun.nio.ch.IOUtil.write(IOUtil.java:75)
    at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:334)
    at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:55)
    at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
    at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:146)
    at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:107)
    at java.io.BufferedOutputStream.write(BufferedOutputStream.java:105)
    at java.io.DataOutputStream.write(DataOutputStream.java:90)
    at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:466)
    at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:434)
    at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:573)
    at org.apache.hadoop.hdfs.server.datanode.DataXceiver.opWriteBlock(DataXceiver.java:352)
    at org.apache.hadoop.hdfs.protocol.DataTransferProtocol$Receiver.opWriteBlock(DataTransferProtocol.java:382)
    at org.apache.hadoop.hdfs.protocol.DataTransferProtocol$Receiver.processOp(DataTransferProtocol.java:323)
    at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:111)
    at java.lang.Thread.run(Thread.java:619)
{code}

Here is the bit of code inside the run method:
{code}
 922                   pkt = ackQueue.getFirst();
 923                   expected = pkt.seqno;
{code}

So 'pkt' is null?  But LinkedList API says that it throws NoSuchElementException if list is empty so you'd think we wouldn't get a NPE here.  What am I missing?",0
"In hdfsOpenFile in libhdfs invokeMethod needs to cast the block size argument to a jlong so a full 8 bytes are passed (rather than 4 plus some garbage which causes writes to fail due to a bogus block size). 

",0
"We recently started to use hadoop-0.20.1 in our production environment (less than 2 weeks ago) and already had 3 instances of truncated files, more than we had for months using hadoop-0.18.3.
Writing is done using libhdfs, although it rather seems to be a problem on the server side.

I will post some relevant logs (they are too large to be put into the description)


 ",0
"We had a balancer that had not made any progress for a long time. It turned out it was repeatingly asking Namenode for a partial block list of one datanode, which was done while the balancer was running.

NameNode should notify Balancer that the datanode is not available and Balancer should stop asking for the datanode's block list.
",0
"implements hsync by default as hflush. As descriibed in HADOOP-6313, the real expected semantics should be ""flushes out to all replicas and all replicas have done posix fsync equivalent - ie the OS has flushed it to the disk device (but the disk may have it in its cache)."" This jira aims to implement the expected behaviour.",0
"When processing edits log, quota verification is not done and the used quota for directories is not updated. The update is done at the end of processing edits log. This rule is broken by change introduced in HDFS-677. This causes namenode from handling rename operation from edits log due to quota verification failure. Once this happens, namenode does not proceed edits log any further. This results in check point failure on backup node or secondary namenode. This also prevents namenode from coming up.",0
"When trying to run the balancer, I get a NullPointerException:

2009-11-10 11:08:14,235 ERROR org.apache.hadoop.hdfs.server.balancer.Balancer: java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.namenode.BlockPlacementPolicy.getInstance(BlockPlacementPolicy.java:161)
        at org.apache.hadoop.hdfs.server.balancer.Balancer.checkReplicationPolicyCompatibility(Balancer.java:784)
        at org.apache.hadoop.hdfs.server.balancer.Balancer.<init>(Balancer.java:792)
        at org.apache.hadoop.hdfs.server.balancer.Balancer.main(Balancer.java:814)

This happens when trying to use bin/start-balancer or bin/hdfs balancer -threshold 10

The config files (hdfs-site and core-site) have as fs.default.name hdfs://namenode:9000.",0
"If a block is request by too many mappers/reducers (say, 3000) at the same time, a BlockMissingException is thrown because it exceeds the upper limit (I think 256 by default) of number of threads accessing the same block at the same time. The DFSClient wil catch that exception and retry 3 times after waiting for 3 seconds. Since the wait time is a fixed value, a lot of clients will retry at about the same time and a large portion of them get another failure. After 3 retries, there are about 256*4 = 1024 clients got the block. If the number of clients are more than that, the job will fail.",0
"The Balancer's AccessKeyUpdater handles exceptions badly. In particular:

1. Catching Exception too low. The wrapper around setKeys should only catch IOException.
2. InterruptedException is ignored. It should be caught at the top level and exit run.
3. Throwable is not caught. It should be caught at the top level and kill the Balancer server process.

{code}
  class AccessKeyUpdater implements Runnable {

    public void run() {
      while (shouldRun) {
        try {
          accessTokenHandler.setKeys(namenode.getAccessKeys());
        } catch (Exception e) {
          LOG.error(StringUtils.stringifyException(e));
        }
        try {
          Thread.sleep(keyUpdaterInterval);
        } catch (InterruptedException ie) {
        }
      }
    }
  }
{code}",0
"As part of looking at using Kerberos, we want to avoid the case where both the primary (and optional secondary) KDC go offline causing a replication storm as the DataNodes' service tickets time out and they lose the ability to connect to the NameNode. However, this is a specific case of a more general problem of loosing too many nodes too quickly. I think we should have an option to go into safe mode if the cluster size goes down more than N% in terms of DataNodes.",0
"DistributedFileSystem.getFileBlockLocations() may occasionally return numeric ips as hostnames. This seems to be a breach of the FileSystem.getFileBlockLocation() contract:
{noformat}
  /**
   * Return an array containing hostnames, offset and size of 
   * portions of the given file.  For a nonexistent 
   * file or regions, null will be returned.
   *
   * This call is most helpful with DFS, where it returns 
   * hostnames of machines that contain the given file.
   *
   * The FileSystem will simply return an elt containing 'localhost'.
   */
  public BlockLocation[] getFileBlockLocations(FileStatus file, 
      long start, long len) throws IOException
{noformat}

One (maybe minor) consequence of this issue is: When a job includes such numeric ips in in its splits' locations, JobTracker would not be able to assign the job's map tasks local to the file blocks.

We should either fix the implementation or change the contract. In the latter case, JobTracker needs to be fixed to maintain both the hostnames and ips of the TaskTrackers.",0
"FSDataset.getNextVolume() uses FSVolume.getAvailable() to determine whether to allocate a block on a volume. This doesn't factor in other in-flight blocks that have been ""promised"" space on the volume. The resulting issue is that, if a volume is nearly full but not full, multiple blocks will be allocated on that volume, and then they will all hit ""Out of space"" errors during the write.",0
"Currently BlockReceiver#PacketResponder interleaves receiving ack message and sending ack message for the same packet. It reads a portion of the message, sends a portion of its ack, and continues like this until it hits the end of the message. The problem is that if it gets an error receiving the ack, it is not able to send an ack that reflects the source of the error.

The PacketResponder should receives the whole packet ack message first and then constuct and sends out its ack.",0
"{{FSNamesystem.internalReleaseLease()}} uses the result of {{iFile#numBlocks();}} call to get a number of an under construction file's blocks. {{numBlock()}} can return 0 if the file doesn't have any blocks yet. This will cause {{internalReleaseLease()}} to throw ArrayIndexOutOfBoundException.

In case of a single block file, the same method will throw NullPointerException because the penultimate block is going to be null according to the logic of INodeFile#getPenultimateBlock().",0
"I've seen in practice (and it's been reported on the list) cases where the datanode's tmp dir can become quite full with abandoned blocks. There's an ancient comment from April 07:
  // REMIND - mjc - eventually we should have a timeout system
  // in place to clean up block files left by abandoned clients.
  // We should have some timer in place, so that if a blockfile
  // is created but non-valid, and has been idle for >48 hours,
  // we can GC it safely.",0
"The first libhdfs operation that is performed is not thread-safe; this is because the creation of a JVM is not protected by a mutex.

We have been able to trigger this by doing the following:
1) Start a few GNOME sessions on the box.  Make sure you are running the gnome volume manager.  The volume manager will perform a GETATTR operation on any newly mounted file system.
2) Start fuse-dfs in debug mode.  As soon as it starts, you will see two or more GETATTR calls almost instantly.
3) fuse-dfs segfaults; if you ran this with GDB, you'll see a stack trace coming from libhdfs starting up a new JVM.

I imagine you could replicate this more simply by having two threads that call libhdfs simultaneously.

I have a patch for fuse-dfs which avoids the problem, but we probably need to fix it in libhdfs itself.",0
"The BackupNode (via HADOOP-4539) receives a stream of transactions from NameNode. However, the BackupNode does not have block locations of blocks. It would be nice if the NameNode can forward all block reports (that it receives from DataNodes) to the BackupNode.",0
"Haven't seen this in practice, but the lock order is inconsistent. processReport locks FSNamesystem, then calls UpgradeManager.startUpgrade, getUpgradeState, and getUpgradeStatus (each of which locks the UpgradeManager). FSNameSystem.processDistributedUpgradeCommand calls upgradeManager.processUpgradeCommand which is synchronized on UpgradeManager, which can call FSNameSystem.leaveSafeMode which synchronizes on FSNamesystem.",0
"FSEditLog.rollEditLog locks the log, then the FSImage. FSImage.close locks the image, then the log.",0
"FileSystem$Cache.get (synchronized) calls Configuration.getProps which is synchronized. Configuration.getProps can call FileSystem$Cache.get, also.",0
Power outages are a common reason for a DataNode to become unavailable. Having a data structure to represent to the power topology of your data center can be used to implement a power-aware replica placement policy.,0
"In FSNamesystem.registerDatanode, it checks if a registering node is a reregistration of an old one based on storage ID. If so, it simply updates the old one with the new registration info. However, the new ipcPort is lost when this happens.

I produced manually this by setting up a DN with IPC port set to 0 (so it picks an ephemeral port) and then restarting the DN. At this point, the NN's view of the ipcPort is stale, and clients will not be able to achieve pipeline recovery.

This should be easy to fix and unit test, but not sure when I'll get to it, so anyone else should feel free to grab it if they get to it first.",0
"This jira tracks implementation of delegation token and corresponding changes in Namenode and DFS Api to issue, renew and cancel delegation tokens.",0
"This one is tough to describe, but essentially the following order of events is seen to occur:

# A client marks one replica of a block to be corrupt by telling the NN about it
# Replication is then scheduled to make a new replica of this node
# The replication completes, such that there are now 3 good replicas and 1 corrupt replica
# The DN holding the corrupt replica sends a block report. Rather than telling this DN to delete the node, the NN instead marks this as a new *good* replica of the block, and schedules deletion on one of the good replicas.

I don't know if this is a dataloss bug in the case of 1 corrupt replica with dfs.replication=2, but it seems feasible. I will attach a debug log with some commentary marked by '============>', plus a unit test patch which I can get to reproduce this behavior reliably. (it's not a proper unit test, just some edits to an existing one to show it)",0
"I was playing around with corrupting fsimage and edits logs when there are multiple dfs.name.dirs specified. I noticed that:
 * As long as your corruption does not make the image invalid, eg changes an opcode so it's an invalid opcode HDFS doesn't notice and happily uses a corrupt image or applies the corrupt edit.
* If the first image in dfs.name.dir is ""valid"" it replaces the other copies in the other name.dirs, even if they are different, with this first image, ie if the first image is actually invalid/old/corrupt metadata than you've lost your valid metadata, which can result in data loss if the namenode garbage collects blocks that it thinks are no longer used.

How about we maintain a checksum as part of the image and edit log and check those on startup and refuse to startup if they are different. Or at least provide a configuration option to do so if people are worried about the overhead of maintaining checksums of these files. Even if we assume dfs.name.dir is reliable storage this guards against operator errors.",0
"closing the edits log file can race with write to edits log file operation resulting in OP_INVALID end-of-file marker being initially overwritten by the concurrent (in setReadyToFlush) threads and then removed twice from the buffer, losing a good byte from edits log.

Example:
{code}
FSNameSystem.rollEditLog() -> FSEditLog.divertFileStreams() -> FSEditLog.closeStream() -> EditLogOutputStream.setReadyToFlush()
FSNameSystem.rollEditLog() -> FSEditLog.divertFileStreams() -> FSEditLog.closeStream() -> EditLogOutputStream.flush() -> EditLogFileOutputStream.flushAndSync()
OR
FSNameSystem.rollFSImage() -> FSIMage.rollFSImage() -> FSEditLog.purgeEditLog() -> FSEditLog.revertFileStreams() -> FSEditLog.closeStream() ->EditLogOutputStream.setReadyToFlush() 
FSNameSystem.rollFSImage() -> FSIMage.rollFSImage() -> FSEditLog.purgeEditLog() -> FSEditLog.revertFileStreams() -> FSEditLog.closeStream() ->EditLogOutputStream.flush() -> EditLogFileOutputStream.flushAndSync()

VERSUS

FSNameSystem.completeFile -> FSEditLog.logSync() -> EditLogOutputStream.setReadyToFlush()
FSNameSystem.completeFile -> FSEditLog.logSync() -> EditLogOutputStream.flush() -> EditLogFileOutputStream.flushAndSync()
OR 
Any FSEditLog.write
{code}

Access on the edits flush operations is synchronized only in the FSEdits.logSync() method level. However at a lower level access to EditsLogOutputStream setReadyToFlush(), flush() or flushAndSync() is NOT synchronized. These can be called from concurrent threads like in the example above

So if a rollEditLog or rollFSIMage is happening at the same time with a write operation it can race for EditLogFileOutputStream.setReadyToFlush that will overwrite the the last byte (normally the FSEditsLog.OP_INVALID which is the ""end-of-file marker"") and then remove it twice (from each thread) in flushAndSync()! Hence there will be a valid byte missing from the edits log that leads to a SecondaryNameNode silent failure and a full HDFS failure upon cluster restart. 

We got to this point after investigating a corrupted edits file that made HDFS unable to start with 

{code:title=namenode.log}
java.io.IOException: Incorrect data format. logVersion is -20 but writables.length is 768. 
        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.loadEditRecords(FSEditLog.java:450
{code}

EDIT: moved the logs to a comment to make this readable
",0
"After running kill -STOP on the datanode in the middle of a write pipeline, the client takes far longer to recover than it should. The ResponseProcessor times out in the correct interval, but doesn't interrupt the DataStreamer, which appears to not be subject to the same timeout. The client only recovers once the OS actually declares the TCP stream dead, which can take a very long time.

I've experienced this on 0.20.1, haven't tried it yet on trunk or 0.21.",0
"he DFS write pipeline code has some really hairy multi-threaded synchronization. There have been a lot of bugs produced by this (HDFS-101, HDFS-793, HDFS-915, tens of others) since it's very hard to understand the message passing, lock sharing, and interruption properties. The reason for the multiple threads is to be able to simultaneously send and receive. If instead of using multiple threads, it used nonblocking IO, I think the whole thing would be a lot less error prone.
I think we could do this in two halves: one half is the DFSOutputStream. The other half is BlockReceiver. I opened this JIRA first as I think it's simpler (only one TCP connection to deal with, rather than an up and downstream)
Opinions? Am I crazy? I would like to see some agreement on the idea before I spend time writing code.",0
"I think this is a regression caused by HDFS-127 -- DFSInputStream is supposed to only go back to the NN max.block.acquires times, but in trunk it goes back twice as many - the default is 3, but I am counting 7 calls to getBlockLocations before an exception is thrown.",0
"I've written a fault injection test that simply throws exceptions on every ack.readFields call in ResponseProcessor. This ought to raise an exception to the client trying to use the stream, but instead it ignores it silently.",0
Delegation Token should also contain the real user which got it issues in behalf of an effective user.,0
"HADOOP-6526 details why UGI.getUserName() will not work to identify users. Until the proposed UGI.getLocalName() is implemented, calls to getUserName() should be replaced with the short name. ",0
"We've seen defective applications cause havoc on the NameNode, for e.g. by doing 100k+ 'listStatus' on very large directories (60k files) etc.
I'd like to start a discussion around how we prevent such, and possibly malicious applications in the future, taking down the NameNode.
Thoughts?",0
This is a continuation of a discussion from HDFS-909. The FSImage.saveFSImage function (implementing dfsadmin -saveNamespace) can corrupt the NN storage such that all current edits are lost.,0
"fuse-dfs dfs_readdir assumes that DistributedFileSystem#listStatus returns Paths with the same scheme/authority as the dfs.name.dir used to connect. If NameNode.DEFAULT_PORT port is used listStatus returns Paths that have authorities without the port (see HDFS-960), which breaks the following code. 

{code}
// hack city: todo fix the below to something nicer and more maintainable but
// with good performance
// strip off the path but be careful if the path is solely '/'
// NOTE - this API started returning filenames as full dfs uris
const char *const str = info[i].mName + dfs->dfs_uri_len + path_len + ((path_len == 1 && *path == '/') ? 0 : 1);
{code}

Let's make the path parsing here more robust. listStatus returns normalized paths so we can find the start of the path by searching for the 3rd slash. A more long term solution is to have hdfsFileInfo maintain a path object or at least pointers to the relevant URI components.",0
The NameNode recovers a lease even when it is in safemode. ,0
"Without an fsync, it's common that filesystems will delay the writing of metadata to the journal until all of the data blocks have been flushed. If the system crashes while the dirty pages haven't been flushed, the file is left in an indeterminate state. In some FSs (eg ext4) this will result in a 0-length file. In others (eg XFS) it will result in the correct length but any number of data blocks getting zeroed. Calling FileChannel.force before closing the FSImage prevents this issue.",0
The Delegation tokens should be persisted in the FsImage and EditLogs so that they are valid to be used after namenode shutdown and restart.,0
"The adminstrator puts the namenode is safemode and then issues the savenamespace command. This can corrupt the edits log. The problem is that  when the NN enters safemode, there could still be pending logSycs occuring from other threads. Now, the saveNamespace command, when executed, would save a edits log with partial writes. I have seen this happen on 0.20.

https://issues.apache.org/jira/browse/HDFS-909?focusedCommentId=12828853&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#action_12828853",0
"Assuming the user authenticates to the NameNode in the browser, allow them to browse the file system by adding a delegation token the the url when it is redirected to a datanode.",0
"This makes it possible to use block access token as shared key for client-to-datanode authentication over RPC. However, access authorization is still based on block access token semantics.",0
This jira is intended to add security check in hdfs to allow issue and renewal of delegation tokens only for kerberos authenticated clients.,0
"Right now, if NameNode is configured to use Kerberos, SecondaryNameNode will fail to start.",0
authorization checks for inter-server protocol (based on HADOOP-6600),0
Namenode needs to be tweaked to use the new kerberized-back ssl connector. ,0
This jira tracks implementation of security for Fsck. Fsck should make an authenticated connection to the namenode.,0
HFTPFileSystem should be updated to use the delegation tokens so that it can talk to the secure namenodes.,0
 When delegation tokens are read from the edit logs...same object is used to read the identifier and is stored in the token cache. This is wrong because same object is getting updated.,0
The secondary namenode fails to retrieve the entire fsimage from the Namenode. It fetches a part of the fsimage but believes that it has fetched the entire fsimage file and proceeds ahead with the checkpointing. Stack traces will be attached below.,0
"If a directory has a quota less than blockSize * numReplicas then you can't add a file to it, even if the file size is less than the quota. This is because FSDirectory#addBlock updates the count assuming at least one block is written in full. We don't know how much of the block will be written when addBlock is called and supporting such small quotas is not important so perhaps we should document this and log an error message instead of making small (blockSize * numReplicas) quotas work.

{code}
// check quota limits and updated space consumed
updateCount(inodes, inodes.length-1, 0, fileINode.getPreferredBlockSize()*fileINode.getReplication(), true);
{code}

You can reproduce with the following commands:
{code}
$ dd if=/dev/zero of=temp bs=1000 count=64
$ hadoop fs -mkdir /user/eli/dir
$ hdfs dfsadmin -setSpaceQuota 191M /user/eli/dir
$ hadoop fs -put temp /user/eli/dir  # Causes DSQuotaExceededException
{code}",0
"Last week I recovered a corrupt namenode image that was completely sane except that the ""number of files"" in the header was set to 1, rather than the correct number (many million). The NN in question had been running for some time, so I believe the 2NN uploaded this broken image as a checkpoint. After this point, of course, no further checkpoints occurred, and the NN failed to load its image upon restart.

Not sure how this happens - my only thought is that we may need to add synchronization on the nsCount field in INodeDirectoryWithQuota, but that's a long shot.",0
Currently anyone can connect and download image/edits from Namenode. In a secure cluster we can verify the identity of the principal making the request; we should disallow requests from anyone except the NN and SNN principals (and their hosts due to the lousy KerbSSL limitation).,0
"nn_browsedfscontent.jsp  calls getDelegationToken even if security is disabled, which causes NPE. ",0
" The delegation token added to the UGI in getUGI method in the JspHelper does not have service set. Therefore, this token cannot be used to connect to the namenode.",0
"If connection to the first datanode fails, DFSClient does not retry in getFileChecksum(..).",0
Looks like it tries to get DelegationToken and fails because SecureManger on Server doesn't start in non-secure environment.,0
Ticket credentials expire and therefore clients opening https connections (only the NN and SNN doing image/edits exchange) should re-login before opening those connections.,0
HDFS currently uses a single namenode that limits scalability of the cluster. This jira proposes an architecture to scale the nameservice horizontally using multiple namenodes.,0
"In BlockReceiver.receivePacket, it calls replicaInfo.setBytesOnDisk before calling flush(). Therefore, if there is a concurrent reader, it's possible to race here - the reader will see the new length while those bytes are still in the buffers of BlockReceiver. Thus the client will potentially see checksum errors or EOFs. Additionally, the last checksum chunk of the file is made accessible to readers even though it is not stable.",0
"If a writer is appending to a block with replication factor 1, and that block has become corrupt, a reader will report the corruption to the NN. Then when the writer tries to complete the file, it will loop forever with an error like:
[junit] 2010-03-21 17:40:08,093 INFO namenode.FSNamesystem (FSNamesystem.java:checkFileProgress(1613)) - BLOCK* NameSystem.checkFileProgress: block blk_-4256412191814117589_1001
{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[127.0.0.1:56782|RBW]]}
has not reached minimal replication 1
[junit] 2010-03-21 17:40:08,495 INFO hdfs.DFSClient (DFSOutputStream.java:completeFile(1435)) - Could not complete file /TestReadWhileWriting/file1 retrying...
Should add tests that cover the case of a writer appending to a block that is corrupt while a reader accesses it.",0
"This is an umbrella jira for discussing availability of the HDFS NN and providing references to other Jiras that improve its availability. This includes, but is not limited to, automatic failover.",0
The naming and handling of NN's fsImage and edit logs can be significantly improved resulting simpler and more robust code.,0
"When performing a massive distcp through hftp, we saw many tasks fail with 

{quote}
2010-04-06 17:56:43,005 INFO org.apache.hadoop.tools.DistCp: FAIL 2010/0/part-00032 : java.io.IOException: File size not matched: copied 193855488 bytes (184.9m) to tmpfile (=hdfs://omehost.com:8020/somepath/part-00032)
but expected 1710327403 bytes (1.6g) from hftp://someotherhost/somepath/part-00032
        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.copy(DistCp.java:435)
        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.map(DistCp.java:543)
        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.map(DistCp.java:310)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)
        at org.apache.hadoop.mapred.Child.main(Child.java:159)
{quote}

This means that read itself didn't fail but the resulted file was somehow smaller.
",0
"The current HDFS implementation specifies that the first replica is local and the other two replicas are on any two random nodes on a random remote rack. This means that if any three datanodes die together, then there is a non-trivial probability of losing at least one block in the cluster. This JIRA is to discuss if there is a better algorithm that can lower probability of losing a block.",0
"hanged protocol name (may be used in hadoop-policy.xml)
from security.refresh.usertogroups.mappings.protocol.acl to security.refresh.user.mappings.protocol.acl",0
"The current HDFS design says that newly allocated blocks for a file are not persisted in the NN transaction log when the block is allocated. Instead, a hflush() or a close() on the file persists the blocks into the transaction log. It would be nice if we can immediately persist newly allocated blocks (as soon as they are allocated) for specific files.",0
"Here, input is a folder containing all .xml files from ./conf  
Then trying the command:
./bin/hadoop fs -copyFromLocal input input

The following message is displayed: 
{noformat}
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Operation not supported
INFO hdfs.DFSClient: Abandoning block blk_-1884214035513073759_1010
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Protocol not available
INFO hdfs.DFSClient: Abandoning block blk_5533397873275401028_1010
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Protocol not available
INFO hdfs.DFSClient: Abandoning block blk_-237603871573204731_1011
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Protocol not available
INFO hdfs.DFSClient: Abandoning block blk_-8668593183126057334_1011
WARN hdfs.DFSClient: DataStreamer Exception: java.io.IOException: Unable to create new block.
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2845)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:2102)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2288)

WARN hdfs.DFSClient: Error Recovery for block blk_-8668593183126057334_1011 bad datanode[0] nodes == null
WARN hdfs.DFSClient: Could not get block locations. Source file ""/user/max/input/core-site.xml"" - Aborting...
copyFromLocal: Protocol not available
ERROR hdfs.DFSClient: Exception closing file /user/max/input/core-site.xml : java.net.SocketException: Protocol not available
java.net.SocketException: Protocol not available
        at sun.nio.ch.Net.getIntOption0(Native Method)
        at sun.nio.ch.Net.getIntOption(Net.java:178)
        at sun.nio.ch.SocketChannelImpl$1.getInt(SocketChannelImpl.java:419)
        at sun.nio.ch.SocketOptsImpl.getInt(SocketOptsImpl.java:60)
        at sun.nio.ch.SocketOptsImpl.sendBufferSize(SocketOptsImpl.java:156)
        at sun.nio.ch.SocketOptsImpl$IP$TCP.sendBufferSize(SocketOptsImpl.java:286)
        at sun.nio.ch.OptionAdaptor.getSendBufferSize(OptionAdaptor.java:129)
        at sun.nio.ch.SocketAdaptor.getSendBufferSize(SocketAdaptor.java:328)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.createBlockOutputStream(DFSClient.java:2873)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2826)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:2102)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2288)
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Operation not supported
INFO hdfs.DFSClient: Abandoning block blk_-1884214035513073759_1010
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Protocol not available
INFO hdfs.DFSClient: Abandoning block blk_5533397873275401028_1010
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Protocol not available
INFO hdfs.DFSClient: Abandoning block blk_-237603871573204731_1011
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Protocol not available
INFO hdfs.DFSClient: Abandoning block blk_-8668593183126057334_1011
WARN hdfs.DFSClient: DataStreamer Exception: java.io.IOException: Unable to create new block.
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2845)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:2102)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2288)

WARN hdfs.DFSClient: Error Recovery for block blk_-8668593183126057334_1011 bad datanode[0] nodes == null
WARN hdfs.DFSClient: Could not get block locations. Source file ""/user/max/input/core-site.xml"" - Aborting...
copyFromLocal: Protocol not available
ERROR hdfs.DFSClient: Exception closing file /user/max/input/core-site.xml : java.net.SocketException: Protocol not available
java.net.SocketException: Protocol not available
        at sun.nio.ch.Net.getIntOption0(Native Method)
        at sun.nio.ch.Net.getIntOption(Net.java:178)
        at sun.nio.ch.SocketChannelImpl$1.getInt(SocketChannelImpl.java:419)
        at sun.nio.ch.SocketOptsImpl.getInt(SocketOptsImpl.java:60)
        at sun.nio.ch.SocketOptsImpl.sendBufferSize(SocketOptsImpl.java:156)
        at sun.nio.ch.SocketOptsImpl$IP$TCP.sendBufferSize(SocketOptsImpl.java:286)
        at sun.nio.ch.OptionAdaptor.getSendBufferSize(OptionAdaptor.java:129)
        at sun.nio.ch.SocketAdaptor.getSendBufferSize(SocketAdaptor.java:328)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.createBlockOutputStream(DFSClient.java:2873)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2826)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:2102)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2288)
{noformat}
However, only empty files are created on HDFS.",0
"Here, input is a folder containing all .xml files from ./conf  
Then trying the command:
./bin/hadoop fs -copyFromLocal input input

The following message is displayed: 
{noformat}
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Operation not supported
INFO hdfs.DFSClient: Abandoning block blk_-1884214035513073759_1010
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Protocol not available
INFO hdfs.DFSClient: Abandoning block blk_5533397873275401028_1010
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Protocol not available
INFO hdfs.DFSClient: Abandoning block blk_-237603871573204731_1011
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Protocol not available
INFO hdfs.DFSClient: Abandoning block blk_-8668593183126057334_1011
WARN hdfs.DFSClient: DataStreamer Exception: java.io.IOException: Unable to create new block.
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2845)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:2102)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2288)

WARN hdfs.DFSClient: Error Recovery for block blk_-8668593183126057334_1011 bad datanode[0] nodes == null
WARN hdfs.DFSClient: Could not get block locations. Source file ""/user/max/input/core-site.xml"" - Aborting...
copyFromLocal: Protocol not available
ERROR hdfs.DFSClient: Exception closing file /user/max/input/core-site.xml : java.net.SocketException: Protocol not available
java.net.SocketException: Protocol not available
        at sun.nio.ch.Net.getIntOption0(Native Method)
        at sun.nio.ch.Net.getIntOption(Net.java:178)
        at sun.nio.ch.SocketChannelImpl$1.getInt(SocketChannelImpl.java:419)
        at sun.nio.ch.SocketOptsImpl.getInt(SocketOptsImpl.java:60)
        at sun.nio.ch.SocketOptsImpl.sendBufferSize(SocketOptsImpl.java:156)
        at sun.nio.ch.SocketOptsImpl$IP$TCP.sendBufferSize(SocketOptsImpl.java:286)
        at sun.nio.ch.OptionAdaptor.getSendBufferSize(OptionAdaptor.java:129)
        at sun.nio.ch.SocketAdaptor.getSendBufferSize(SocketAdaptor.java:328)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.createBlockOutputStream(DFSClient.java:2873)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2826)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:2102)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2288)
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Operation not supported
INFO hdfs.DFSClient: Abandoning block blk_-1884214035513073759_1010
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Protocol not available
INFO hdfs.DFSClient: Abandoning block blk_5533397873275401028_1010
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Protocol not available
INFO hdfs.DFSClient: Abandoning block blk_-237603871573204731_1011
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Protocol not available
INFO hdfs.DFSClient: Abandoning block blk_-8668593183126057334_1011
WARN hdfs.DFSClient: DataStreamer Exception: java.io.IOException: Unable to create new block.
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2845)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:2102)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2288)

WARN hdfs.DFSClient: Error Recovery for block blk_-8668593183126057334_1011 bad datanode[0] nodes == null
WARN hdfs.DFSClient: Could not get block locations. Source file ""/user/max/input/core-site.xml"" - Aborting...
copyFromLocal: Protocol not available
ERROR hdfs.DFSClient: Exception closing file /user/max/input/core-site.xml : java.net.SocketException: Protocol not available
java.net.SocketException: Protocol not available
        at sun.nio.ch.Net.getIntOption0(Native Method)
        at sun.nio.ch.Net.getIntOption(Net.java:178)
        at sun.nio.ch.SocketChannelImpl$1.getInt(SocketChannelImpl.java:419)
        at sun.nio.ch.SocketOptsImpl.getInt(SocketOptsImpl.java:60)
        at sun.nio.ch.SocketOptsImpl.sendBufferSize(SocketOptsImpl.java:156)
        at sun.nio.ch.SocketOptsImpl$IP$TCP.sendBufferSize(SocketOptsImpl.java:286)
        at sun.nio.ch.OptionAdaptor.getSendBufferSize(OptionAdaptor.java:129)
        at sun.nio.ch.SocketAdaptor.getSendBufferSize(SocketAdaptor.java:328)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.createBlockOutputStream(DFSClient.java:2873)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2826)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:2102)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2288)
{noformat}
However, only empty files are created on HDFS.",0
"The offending code is in {{DFSOutputStream.nextBlockOutputStream}}

This function retries several times to call {{createBlockOutputStream}}. Each time when it fails, it leaves a {{Socket}} object in {{DFSOutputStream.s}}.
That object is never closed, but overwritten the next time {{createBlockOutputStream}} is called.
",0
"ound that when the DN uses client verification of a block that is open for writing, it will add it to the DataBlockScanner prematurely.",0
"Once HADOOP-6748 is done, HDFS should pass administrator acl when HTTPServer is constructed.",0
"A very easy way to spot the bug is to do a second restart in TestRestartDFS and check that the modification time on root is the same as it was before the second restart.

The problem is modifying time of the parent if the modification time of the child is greater than parent's in addToParent.

So if you have /DIR/File then on creation of a file modification time of the DIR will be set, but on cluster restart, or when secondary is checkpointing and reading the image it will add DIR to ""/"" and write the new modification time for ""/"" which is the modification time of DIR.

This is clearly a bug. I will attach a patch with one more parameter being passed from the loadFSImage that says to not propagate the time.",0
completeFile should check that the caller still owns the lease of the file that it's completing. This is for the 'testCompleteOtherLeaseHoldersFile' case in HDFS-1139.,0
"Suppose the NameNode is in safemode. Then we try to shuut it down by invoking NameNode.stop(). The stop() method interrupts all waiting threads, which in turn, causes the SafeMode monitor to exit and thus triggering replicating/deleting of blocks.",0
"During lease recovery, the lease gets reassigned to a special NN holder. This is not currently persisted to the edit log, which means that after an NN restart, the original leaseholder could end up allocating more blocks or completing a file that has already started recovery.",0
"Currently we use block access tokens to allow datanodes to verify clients' identities, however we don't have a way for clients to verify the authenticity of the datanodes themselves.",0
FSN.appendFile is made up of two synchronized sections. The second section assumes that the file has not gotten modified during the unsynchronized part in between. We should recheck the lease in the second block.,0
"In HDFS-630 we added an excludedNodes parameter when allocating a block. In the case of a cluster that uses transient IPC ports, this list can accumulate past incarnations of restarted datanodes. Then, in NetworkTopology.countNumOfAvailableNodes, we count each of these ""ghost"" nodes against the total number of available nodes, and decide that there are no spots to place replicas, even though plenty are alive.

To reproduce, write data into HDFS with a very small block size (say 4KB) and then repeatedly kill and restart the local DN configured to use a transient port. After you have done so N times, where N is the number of nodes in the cluster, the NN will fail to allocate any targets even though N other nodes are still alive.",0
"When block recovery starts (eg due to NN recovering lease) it needs to interrupt any writers currently writing to those blocks. Otherwise, an old writer (who hasn't realized he lost his lease) can continue to write+sync to the blocks, and thus recovery ends up truncating data that has been sync()ed.",0
"HDFS Quota counts will be missed between a clear quota operation and a set quota.

When setting quota for a dir, the INodeDirectory will be replaced by INodeDirectoryWithQuota and dir.isQuotaSet() becomes true. When INodeDirectoryWithQuota  is newly created, quota counting will be performed. However, when clearing quota, the quota conf is set to -1 and dir.isQuotaSet() becomes false while INodeDirectoryWithQuota will NOT be replaced back to INodeDirectory.

FSDirectory.updateCount just update the quota count for inodes that isQuotaSet() is true. So after clear quota for a dir, its quota counts will not be updated and it's reasonable. But when re seting quota for this dir, quota counting will not be performed and some counts will be missed.",0
"I saw this failure once on my internal Hudson job that runs the append tests 48 times a day:
junit.framework.AssertionFailedError: expected:<114688> but was:<98304>
	at org.apache.hadoop.hdfs.AppendTestUtil.check(AppendTestUtil.java:112)
	at org.apache.hadoop.hdfs.TestFileAppend3.testTC2(TestFileAppend3.java:116)
",0
Missing an isInitialized() check in updateScanStatusInternal,0
"- Summary: Block is lost if primary datanode crashes in the middle tryUpdateBlock.
 
- Setup:
# available datanode = 2
# replica = 2
# disks / datanode = 1
# failures = 1
# failure type = crash
When/where failure happens = (see below)
 
- Details:
 Suppos",0
"- Summary: when appending to partial block, if is possible that
retrial when facing an exception fails due to a checksum mismatch.
Append operation is not atomic (either complete or fail completely).
 
- Setup:
+ # available datanodes = 2
+# disks / datan",0
"Setup:
--------
+ # available datanodes = 2
+ # disks / datanode = 1
+ # failures = 1
+ failure type = crash
+ When/where failure happens = during primary's recoverBlock
 
Details:
----------
Say client is appending to block X1 in 2 datanodes: dn1 and dn2.
First it needs to make sure both dn1 and dn2  agree on the new GS of the block.
1) Client first creates DFSOutputStream by calling
 
>OutputStream result = new DFSOutputStream(src, buffersize, progress,
>                                            lastBlock, stat, conf.getInt(""io.bytes.per.checksum"", 512));
 
in DFSClient.append()
 
2) The above DFSOutputStream constructor in turn calls processDataNodeError(true, true)
(i.e, hasError = true, isAppend = true), and starts the DataStreammer
 
> processDatanodeError(true, true);  /* let's call this PDNE 1 */
> streamer.start();
 
Note that DataStreammer.run() also calls processDatanodeError()
> while (!closed && clientRunning) {
>  ...
>      boolean doSleep = processDatanodeError(hasError, false); /let's call this PDNE 2*/
 
3) Now in the PDNE 1, we have following code:
 
> blockStream = null;
> blockReplyStream = null;
> ...
> while (!success && clientRunning) {
> ...
>    try {
>         primary = createClientDatanodeProtocolProxy(primaryNode, conf);
>         newBlock = primary.recoverBlock(block, isAppend, newnodes); /*exception here*/
>         ...
>    catch (IOException e) {
>         ...
>         if (recoveryErrorCount > maxRecoveryErrorCount) { 
>         // this condition is false
>         }
>         ...
>         return true;
>    } // end catch
>    finally {...}
>    
>    this.hasError = false;
>    lastException = null;
>    errorIndex = 0;
>    success = createBlockOutputStream(nodes, clientName, true);
>    }
>    ...
 
Because dn1 crashes during client call to recoverBlock, we have an exception.
Hence, go to the catch block, in which processDatanodeError returns true
before setting hasError to false. Also, because createBlockOutputStream() is not called
(due to an early return), blockStream is still null.
 
4) Now PDNE 1 has finished, we come to streamer.start(), which calls PDNE 2.
Because hasError = false, PDNE 2 returns false immediately without doing anything

> if (!hasError) { return false; }
 
5) still in the DataStreamer.run(), after returning false from PDNE 2, we still have
blockStream = null, hence the following code is executed:

if (blockStream == null) {
   nodes = nextBlockOutputStream(src);
   this.setName(""DataStreamer for file "" + src + "" block "" + block);
   response = new ResponseProcessor(nodes);
   response.start();
}
 
nextBlockOutputStream which asks namenode to allocate new Block is called.
(This is not good, because we are appending, not writing).
Namenode gives it new Block ID and a set of datanodes, including crashed dn1.
this leads to createOutputStream() fails because it tries to contact the dn1 first.
(which has crashed). The client retries 5 times without any success,
because every time, it asks namenode for new block! Again we see
that the retry logic at client is weird!

*This bug was found by our Failure Testing Service framework:
http://www.eecs.berkeley.edu/Pubs/TechRpts/2010/EECS-2010-98.html
For questions, please email us: Thanh Do (thanhdo@cs.wisc.edu) and 
Haryadi Gunawi (haryadi@eecs.berkeley.edu)*",0
"- Summary: the recoverBlock is not atomic, leading retrial fails when 
facing a failure.
 
- Setup:
+ # available datanodes = 3
+ # disks / datanode = 1
+ # failures = 2
+ failure type = crash
+ When/where failure happens = (see below)
 
- Details:
Suppos",0
"- Summary: block is corrupted if a crash happens before writing to checksumOut but
after writing to dataOut. 
 
- Setup:
+ # available datanodes = 1
+ # disks / datanode = 1
+ # failures = 1
+ failure type = crash
+When/where failure happens = (see below)",0
"- Setup:
+  # datanodes = 2
+ replication factor = 2
+ failure type = transient (i.e. a java I/O call that throws I/O Exception or returns false)
+ # failures = 2
+ When/where failures happen: (This is a subtle bug) The first failure is a transient failur",0
When a datanode heartbeat times out namenode marks it dead. The subsequent heartbeat from the datanode is rejected with a command to datanode to re-register. However namenode accepts block report from the datanode although it is marked dead.,0
"Saw this issue on a cluster where some ops people were doing network changes without shutting down DNs first. So, recovery ended up getting started at multiple different DNs at the same time, and some race condition occurred that caused a block to get permanently stuck in recovery mode. What seems to have happened is the following:
- FSDataset.tryUpdateBlock called with old genstamp 7091, new genstamp 7094, while the block in the volumeMap (and on filesystem) was genstamp 7093
- we find the block file and meta file based on block ID only, without comparing gen stamp
- we rename the meta file to the new genstamp _7094
- in updateBlockMap, we do comparison in the volumeMap by oldblock *without* wildcard GS, so it does *not* update volumeMap
- validateBlockMetaData now fails with ""blk_7739687463244048122_7094 does not exist in blocks map""

After this point, all future recovery attempts to that node fail in getBlockMetaDataInfo, since it finds the _7094 gen stamp in getStoredBlock (since the meta file got renamed above) and then fails since _7094 isn't in volumeMap in validateBlockMetadata

Making a unit test for this is probably going to be difficult, but doable.",0
"Ryan Rawson came upon this nasty bug in HBase cluster testing. What happened was the following:
1) File's original writer died
2) Recovery client tried to open file for append - looped for a minute or so until soft lease expired, then append call initiated recovery
3) Recovery completed successfully
4) Recovery client calls append again, which succeeds on the NN
5) For some reason, the block recovery that happens at the start of append pipeline creation failed on all datanodes 6 times, causing the append() call to throw an exception back to HBase master. HBase assumed the file wasn't open and put it back on a queue to try later
6) Some time later, it tried append again, but the lease was still assigned to the same DFS client, so it wasn't able to recover.

The recovery failure in step 5 is a separate issue, but the problem for this JIRA is that the NN can think it failed to open a file for append when the NN thinks the writer holds a lease. Since the writer keeps renewing its lease, recovery never happens, and no one can open or recover the file until the DFS client shuts down.",0
"Ran into a bad issue in testing overnight. One of the writers experienced an OOME in the middle of writing a checksum chunk to the stream inside a sync() call. It then proceeded to retry recovery on each DN in the pipeline, but each recovery failed because its internal checksum buffer was borked in some way - on the DNs I see ""Unexpected checksum mismatch"" errors after each recovery attempt.

When another client tried to recover the file using appendFile, it got the ""Partial CRC 3766269197 does not match value computed the  last time file was closed"" error (plus there was only one replica left in targets). It thus failed to set up the append pipeline, and ran into HDFS-1262.

This was on 0.20-append, though it may happen on trunk as well.",0
"Datanodes in the process of being decomissioned should still be decomissioning after namenode restarts. Currently they are marked as dead after a restart.


Details:

Nodes can be safely removed from a cluster by marking them as decomissioned and waiting for their data to be replicated elsewhere. This is accomplished by adding a node to the filed referenced by dfs.hosts.excluded, then refreshing nodes.

Decomissioning means block reports from the decomissioned datanode are no longer accepted by the namenode, meaning for decomissioning to occur the NN must have an existing block report. That is, a datanode can transition from: live --> decomissioning --> dead. Nodes can NOT transition from: dead --> decomissioning --> dead.

Operationally this is problematic because intervention is required should the NN restart while nodes are decomissioning, meaning in-house administration tools must be more complex, or more likely admins have to babysit the decomissioning process.

Someone more familiar with the code might have a better idea, but perhaps the first block report for dfs.hosts.excluded hosts should be accepted?",0
The datanode receives data from a client write request or from a replication request. It is useful to configure the cluster to so that dedicated bandwidth is allocated for client writes and replication traffic. This requires that the client-writes and replication traffic be configured to operate on two different ports to the datanode.,0
"Over the past few weeks we've had several datanodes with bad disks that suffer ext3 corruption, and consequently start reporting impossible values for how full they are. This particular node, for example, has a configured capacity of 10.86TB but reports 1733.95TB used, for a total of 15973.57% utilization.

Node 	 Last Contact 	Admin State 	Configured Capacity (TB) 	Used (TB) 	Non DFS  Used (TB) 	Remaining  (TB) 	Used  (%) 	Used  (%) 	Remaining (%) 	Blocks 
hadoop2254	 44	In Service 	10.86	1733.95	0	5.24	15973.57   48.25	65602 

If we can avoid generating such bogus data on the datanode that would be great.  But if the namenode receives such an impossible block report, it should definitely consider that datanode to be not trustworthy, and in my opinion, make it dead.

The ""fix"" in our case was either to fsck or replace the bad disk.",0
"HDFS-520 introduced a new DataNode constructor, which tries to set up an RPC connection to the NN before a Kerberos login is done. This causes datanode to fail.",0
"Currently when you decommission a node each block is only inserted into neededReplications if it is not there yet. This causes a problem of a block sitting in a low priority queue when all replicas sit on the nodes being decommissioned.
The common usecase for decommissioning nodes for us is proactively exclude them before they went bad, so it would be great to get the blocks at risk onto the live datanodes as quickly as possible.",0
"iff --git src/java/org/apache/hadoop/hdfs/DFSInputStream.java src/java/org/apache/hadoop/hdfs/DFSInputStream.java
index c52b259..325d08f 100644
--- src/java/org/apache/hadoop/hdfs/DFSInputStream.java
+++ src/java/org/apache/hadoop/hdfs/DFSInputStream.java
@@ -39,6 +39,7 @@ import org.apache.hadoop.hdfs.security.token.block.InvalidBlockTokenException;
 import org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier;
 import org.apache.hadoop.hdfs.server.datanode.ReplicaNotFoundException;
 import org.apache.hadoop.io.IOUtils;
+import org.apache.hadoop.ipc.RPC;
 import org.apache.hadoop.ipc.RemoteException;
 import org.apache.hadoop.net.NetUtils;
 import org.apache.hadoop.security.token.Token;
@@ -146,10 +147,12 @@ public class DFSInputStream extends FSInputStream {
     int replicaNotFoundCount = locatedblock.getLocations().length;
     
     for(DatanodeInfo datanode : locatedblock.getLocations()) {
+      final ClientDatanodeProtocol cdp = DFSClient.createClientDatanodeProtocolProxy(
+        datanode, dfsClient.conf, dfsClient.socketTimeout, locatedblock);
+      
       try {
-        final ClientDatanodeProtocol cdp = DFSClient.createClientDatanodeProtocolProxy(
-            datanode, dfsClient.conf, dfsClient.socketTimeout, locatedblock);
         final long n = cdp.getReplicaVisibleLength(locatedblock.getBlock());
+        
         if (n >= 0) {
           return n;
         }
@@ -166,7 +169,9 @@ public class DFSInputStream extends FSInputStream {
           DFSClient.LOG.debug(""Failed to getReplicaVisibleLength from datanode ""
               + datanode + "" for block "" + locatedblock.getBlock(), ioe);
         }
-      }
+      } finally {
+        RPC.stopProxy(cdp);        
+      } 
     }
 
     // Namenode told us about these locations, but none know about the replica",0
"open method in HftpFileSystem uses ByteRangeInputStream for url connection. But it does not add the delegation tokens, even if security is enabled, to the url before passing it to the ByteRangeInputStream. Therefore request fails if security is enabled.",0
"- Component: data node
 
- Version: 0.20-append
 
- Summary: we found a case that when a block is truncated during updateBlock,
the length on the ongoingCreates is not updated, hence leading to failed append.

 
- Setup:
# disks / datanode = 3
# failures ",0
"- Component: data node
 
- Version: 0.20-append
 
- Setup:
1) # disks / datanode = 3
2) # failures = 2
3) failure type = crash
4) When/where failure happens = (see below)
 
- Details:
Client writes to dn1-dn2-dn3. Write succeeds. We have blk_X_1001 in all",0
"When running 0.20 patched with HDFS-101, we sometimes see an error as follow:
WARN hdfs.DFSClient: DFSOutputStream ResponseProcessor exception for block blk_-2871223654872350746_21421120java.io.IOException: Responseprocessor: Expecting seq
no for block blk_-2871223654872350746_21421120 10280 but received 10281
at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:2570)

This indicates that DFS client expects an ack for packet N, but receives an ack for packet N+1.",0
"we found that the Datanode doesn't do a graceful shutdown and a block can be corrupted (data + checksum amounts off)
we can make the DN do a graceful shutdown in case there are open files. if this presents a problem to a timely shutdown, we can make a it a parameter of how long to wait for the full graceful shutdown before just exiting",0
"The current management unit in Hadoop is a node, i.e. if a node failed, it will be kicked out and all the data on the node will be replicated.
As almost all SATA controller support hotplug, we add a new command line interface to datanode, thus it can list, add or remove a volume online, which means we can change a disk without node decommission. Moreover, if the failed disk still readable and the node has enouth space, it can migrate data on the disks to other disks in the same node.
A more detailed design document will be attached.
The original version in our lab is implemented against 0.20 datanode directly, and is it better to implemented it in contrib? Or any other suggestion?",0
"On our cluster, 12 files were reported as corrupt by fsck even though the replicas on the datanodes were healthy.
Turns out that all the replicas (12 files x 3 replicas per file) were reported corrupt from one node.

Surprisingly, these files were still readable/accessible from dfsclient (-get/-cat) without any problems.

",0
"There's a bug in the quota code that causes them not to be respected when a file is not an exact multiple of the block size. Here's an example:
$ hadoop fs -mkdir /test
$ hadoop dfsadmin -setSpaceQuota 384M /test
$ ls dir/ | wc -l   # dir contains 101 files
101
$ du -ms dir        # each is 3mb
304 dir
$ hadoop fs -put dir /test
$ hadoop fs -count -q /test
        none             inf       402653184      -550502400            2          101          317718528 hdfs://haus01.sf.cloudera.com:10020/test
$ hadoop fs -stat ""%o %r"" /test/dir/f30
134217728 3    # three 128mb blocks
INodeDirectoryWithQuota caches the number of bytes consumed by it's children in diskspace. The quota adjustment code has a bug that causes diskspace to get updated incorrectly when a file is not an exact multiple of the block size (the value ends up being negative).
This causes the quota checking code to think that the files in the directory consumes less space than they actually do, so the verifyQuota does not throw a QuotaExceededException even when the directory is over quota. However the bug isn't visible to users because fs count -q reports the numbers generated by INode#getContentSummary which adds up the sizes of the blocks rather than use the cached INodeDirectoryWithQuota#diskspace value.
In FSDirectory#addBlock the disk space consumed is set conservatively to the full block size * the number of replicas:
updateCount(inodes, inodes.length-1, 0,
    fileNode.getPreferredBlockSize()*fileNode.getReplication(), true);
In FSNameSystem#addStoredBlock we adjust for this conservative estimate by subtracting out the difference between the conservative estimate and what the number of bytes actually stored was:
//Updated space consumed if required.
INodeFile file = (storedBlock != null) ? storedBlock.getINode() : null;
long diff = (file == null) ? 0 :
    (file.getPreferredBlockSize() - storedBlock.getNumBytes());

if (diff > 0 && file.isUnderConstruction() &&
    cursize < storedBlock.getNumBytes()) {
...
    dir.updateSpaceConsumed(path, 0, -diff*file.getReplication());
We do the same in FSDirectory#replaceNode when completing the file, but at a file granularity (I believe the intent here is to correct for the cases when there's a failure replicating blocks and recovery). Since oldnode is under construction INodeFile#diskspaceConsumed will use the preferred block size (vs of Block#getNumBytes used by newnode) so we will again subtract out the difference between the full block size and what the number of bytes actually stored was:",0
"1. Say we have 2 racks: rack-0 and rack-1. 
Rack-0 has dn1, dn2, dn3. Rack-0 has dn4, dn5, dn6.
 
2. Suppose client is in rack-0, and the write pipeline is:
client --> localnode --> other rack --> other rack
In this example we have the pipeline client-dn1-dn4-dn6.
That is rack0-rack0-rack1-rack1. So far so good.
 
3. Now other client comes, and append to file.
This client is also in rack-0. Interestingly,
the append pipeline is client-dn6-dn4-dn1.
That is the new client (from rack0) sends packet 
to the first node in pipeline (dn6) which belongs to rack1.

This bug was found by our Failure Testing Service framework:
http://www.eecs.berkeley.edu/Pubs/TechRpts/2010/EECS-2010-98.html
For questions, please email us: Thanh Do (thanhdo@cs.wisc.edu) and 
Haryadi Gunawi (haryadi@eecs.berkeley.edu)",0
"We experienced a data loss situation that due to double failures.
One is transient disk failure with edits logs and the other is corrupted fstime.
 
Here is the detail:
 
1. NameNode has 2 edits directory (say edit0 and edit1)
 
2. During an update to edit0, there is a transient disk failure,
making NameNode bump the fstime and mark edit0 as stale
and continue working with edit1. 
 
3. NameNode is shut down. Now, and unluckily fstime in edit0
is corrupted. Hence during NameNode startup, the log in edit0
is replayed, hence data loss.

This bug was found by our Failure Testing Service framework:
http://www.eecs.berkeley.edu/Pubs/TechRpts/2010/EECS-2010-98.html
For questions, please email us: Thanh Do (thanhdo@cs.wisc.edu) and 
Haryadi Gunawi (haryadi@eecs.berkeley.edu)",0
"We saw a case that NN keeps giving client nodes from the same rack, hence an exception 
from client when try to setup the pipeline. Client retries 5 times and fails.
 
Here is more details. Support we have 2 rack
- Rack 0: from dn1 to dn7
- Rack 1: from dn8 to dn14

Client asks for 3 dns and NN replies with dn1, dn8 and dn9, for example.
Because there is network partition, so client doesn't see any node in Rack 0.
Hence, client add dn1 to excludedNodes list, and ask NN again.
Interestingly, NN picks a different node (from those in excludedNodes) in Rack 0, 
and gives back to client, and so on. Client keeps retrying and after 5 times of retrials, 
write fails.

This bug was found by our Failure Testing Service framework:
http://www.eecs.berkeley.edu/Pubs/TechRpts/2010/EECS-2010-98.html
For questions, please email us: Thanh Do (thanhdo@cs.wisc.edu) and 
Haryadi Gunawi (haryadi@eecs.berkeley.edu)
",0
HDFS portion of HADOOP-6951.,0
Block Pool ID needs to be introduced in to DataTransferProtocol,0
"Currently Balancer does not obtain information from BlockPlacementPolicy so it can transfer the blocks without checking with BlockPlacementPolicy.
This causes the policy break after balancing the cluster.
There are some new policies proposed in HDFS-1094 and MAPREDUCE-1831 in which the block placement follows some pattern.
The pattern can be broken by Balancer.
I propose that we add the following method in BlockPlacementPolicy:
  abstract public boolean canBeMoved(String fileName, Block block,
    DatanodeInfo source, DatanodeInfo destination);
And make Balancer use it in
  private boolean isGoodBlockCandidate(Source source,
      BalancerDatanode target, BalancerBlock block)",0
"There are many instances when the same piece of data resides on multiple HDFS clusters in different data centers. The primary reason being that the physical limitation of one data center is insufficient to host the entire data set. In that case, the administrator(s) typically partition that data into two (or more) HDFS clusters on two different data centers and then duplicates some subset of that data into both the HDFS clusters.
In such a situation, there will be six physical copies of data that is duplicated, three copies in one data center and another three copies in another data center. It would be nice if we can keep fewer than 3 replicas on each of the data centers and have the ability to fix a replica in the local data center by copying data from the remote copy in the remote data center.",0
"Create multi-format parser for edits logs file, support binary and XML formats initially.
Parsing should work from any supported format to any other supported format (e.g. from binary to XML and from XML to binary).
The binary format is the format used by FSEditLog class to read/write edits file.
Primary reason to develop this tool is to help with troubleshooting, the binary format is hard to read and edit (for human troubleshooters).
Longer term it could be used to clean up and minimize parsers for fsimage and edits files. Edits parser OfflineEditsViewer is written in a very similar fashion to OfflineImageViewer. Next step would be to merge OfflineImageViewer and OfflineEditsViewer and use the result in both FSImage and FSEditLog. This is subject to change, specifically depending on adoption of avro (which would completely change how objects are serialized as well as provide ways to convert files to different formats).",0
"If the fsimage is very big. The network is full in a short time when SeconaryNamenode do checkpoint, leading to Jobtracker access Namenode to get relevant file data to fail in job initialization phase. So we limit transmission speed and compress transmission to resolve the problem.",0
"TestPipelines appears to be failing on trunk:
Should be RBW replica after sequence of calls append()/write()/hflush() expected:<RBW> but was:<FINALIZED>
junit.framework.AssertionFailedError: Should be RBW replica after sequence of calls append()/write()/hflush() expected:<RBW> but was:<FINALIZED>
at org.apache.hadoop.hdfs.TestPipelines.pipeline_01(TestPipelines.java:1",0
"It appears that all replicas of a block can end up in the same rack. The likelihood of such replicas seems to be directly related to decommissioning of nodes. 

Post rolling OS upgrade (decommission 3-10% of nodes, re-install etc, add them back) of a running cluster, all replicas of about 0.16% of blocks ended up in the same rack.

Hadoop Namenode UI etc doesn't seem to know about such incorrectly replicated blocks. ""hadoop fsck .."" does report that the blocks must be replicated on additional racks.

Looking at ReplicationTargetChooser.java, following seem suspect:

snippet-01:
{code}
    int maxNodesPerRack =
      (totalNumOfReplicas-1)/clusterMap.getNumOfRacks()+2;
{code}

snippet-02:
{code}
      case 2:
        if (clusterMap.isOnSameRack(results.get(0), results.get(1))) {
          chooseRemoteRack(1, results.get(0), excludedNodes,
                           blocksize, maxNodesPerRack, results);
        } else if (newBlock){
          chooseLocalRack(results.get(1), excludedNodes, blocksize,
                          maxNodesPerRack, results);
        } else {
          chooseLocalRack(writer, excludedNodes, blocksize,
                          maxNodesPerRack, results);
        }
        if (--numOfReplicas == 0) {
          break;
        }
{code}

snippet-03:
{code}
    do {
      DatanodeDescriptor[] selectedNodes =
        chooseRandom(1, nodes, excludedNodes);
      if (selectedNodes.length == 0) {
        throw new NotEnoughReplicasException(
                                             ""Not able to place enough replicas"");
      }
      result = (DatanodeDescriptor)(selectedNodes[0]);
    } while(!isGoodTarget(result, blocksize, maxNodesPerRack, results));
{code}",0
"We had an incident that the fsimage at secondary NameNode was truncated but got uploaded to the primary NameNode. The primary NameNode simply rolled the image without checking its integrity, therefore causing the fsimage to corrupt. The primary NameNode should check the new image's integrity before rolling fsimage.",0
"When there are no uncorrupted replicas of a block, FSNamesystem.getBlockLocations returns LocatedBlocks corresponding to corrupt blocks. When DFSClient converts these to BlockLocations, the information that the corresponding block is corrupt is lost. We should add a field to BlockLocation to indicate whether the corresponding block is corrupt in order to warn the client that reading this block will fail. This would be especially useful for tools such as RAID FSCK, which could then easily inspect whether data or parity blocks are corrupted without having to make direct RPC calls.",0
"Sometimes when primary crashes during image transfer secondary namenode would hang trying to read the image from HTTP connection forever.
It would be great to set timeouts on the connection so if something like that happens there is no need to restart the secondary itself.
In our case restarting components is handled by the set of scripts and since the Secondary as the process is running it would just stay hung until we get an alarm saying the checkpointing doesn't happen.",0
"After HDFS-1071, saveNamespace now appears to ""succeed"" even if all of the individual directories failed to save.",0
"When I work on HDFS-1070, I come across a very strange bug. Occasionally when loading a compressed image, NameNode throws an exception complaining that the image file is corrupt. However, the result returned by the md5sum utility matches the checksum value stored in the VERSION file. 

It turns out the image loader leaves 4 bytes unread after loading all the real data of an image. Those unread bytes may be some compression-related meta-info. The image loader should make sure to read to the end of file in order for an correct checksum.",0
"Fsck shows one of the files in our dfs cluster is corrupt.

/bin/hadoop fsck aFile -files -blocks -locations
aFile: 4633 bytes, 2 block(s): 
aFile: CORRUPT block blk_-4597378336099313975
OK
0. blk_-4597378336099313975_2284630101 len=0 repl=3 [...]
1. blk_5024052590403223424_2284630107 len=4633 repl=3 [...]Status: CORRUPT

On disk, these two blocks are of the same size and the same content. It turns out the writer of the file is from a multiple threaded map task. Each thread may write to the same file. One possible interaction of two threads might make this to happen:
[T1: create aFile] [T2: delete aFile] [T2: create aFile][T1: addBlock 0 to aFile][T2: addBlock1 to aFile]...

Because T1 and T2 have the same client name, which is the map task id, the above interactions could be done without any lease exception, thus eventually leading to a corrupt file. To solve the problem, a mapreduce task's client name could be formed by its task id followed by a random number.",0
"On 32 bit JVM, SocketOutputStream.transferToFully() fails if the block size is >= 2GB. We should fall back to a normal transfer in this case. 


{code}
2010-12-02 19:04:23,490 ERROR datanode.DataNode (BlockSender.java:sendChunks(399)) - BlockSender.sendChunks() exception: java.io.IOException: Value too large
 for defined data type
        at sun.nio.ch.FileChannelImpl.transferTo0(Native Method)
        at sun.nio.ch.FileChannelImpl.transferToDirectly(FileChannelImpl.java:418)
        at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:519)
        at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:204)
        at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendChunks(BlockSender.java:386)
        at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:475)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.opReadBlock(DataXceiver.java:196)
        at org.apache.hadoop.hdfs.protocol.DataTransferProtocol$Receiver.opReadBlock(DataTransferProtocol.java:356)
        at org.apache.hadoop.hdfs.protocol.DataTransferProtocol$Receiver.processOp(DataTransferProtocol.java:328)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:130)
        at java.lang.Thread.run(Thread.java:619)
{code}",0
"In HDFS-895 the handling of interrupts during hflush/close was changed to preserve interrupt status. This ends up creating an infinite loop in waitForAckedSeqno if the waiting thread gets interrupted, since Object.wait() has a strange semantic that it doesn't give up the lock even momentarily if the thread is already in interrupted state at the beginning of the call.

We should decide what the correct behavior is here - if a thread is interrupted while it's calling hflush() or close() should we (a) throw an exception, perhaps InterruptedIOException (b) ignore, or (c) wait for the flush to finish but preserve interrupt status on exit?",0
"we have seen an instance where a external outage caused many datanodes to reboot at around the same time. This resulted in many corrupted blocks. These were recently written blocks; the current implementation of HDFS Datanodes do not sync the data of a block file when the block is closed.
1. Have a cluster-wide config setting that causes the datanode to sync a block file when a block is finalized.
2. Introduce a new parameter to the FileSystem.create() to trigger the new behaviour, i.e. cause the datanode to sync a block-file when it is finalized.
3. Implement the FSDataOutputStream.hsync() to cause all data written to the specified file to be written to stable storage.",0
"When a datanode receives a ""Connection reset by peer"" from the namenode.register(), it exits. This causes many datanodes to die.",0
"Configuration.writeXml holds a lock on itself and then writes the XML to an output stream, during which DFSOutputStream will try to get a lock on ackQueue/dataQueue. Meanwihle the DataStreamer thread will call functions like conf.getInt() and deadlock against the other thread, since it could be the same conf object.

This causes a deterministic deadlock whenever the serialized form is larger than block size.",0
"Lines listed below will caused a NullPointerException in DFSUtil.locatedBlocks2Locations (line 208) because EMPTY_BLOCK_LOCS  will return null when calling blocks.getLocatedBlocks()
{noformat}
   /** a default LocatedBlocks object, its content should not be changed */
   private final static LocatedBlocks EMPTY_BLOCK_LOCS = new LocatedBlocks();
{noformat}",0
"When NameNod decides to redirect a read request to a datanode, if it can not find a live node that contains a block of the file, NameNode should choose a random datanode instead of throwing an exception.

This is because a live node test is against its http port. A non-functional jetty servlet (may due to bug like JETTY-1264) does not mean that the replica on that DataNode is not readable. Redirecting the read request to a random datanode could make hftp function better when DataNodes hit bugs like JETTY-1264.",0
"Currently, there is no checking on TGT validity when calling getDelegationToken(). The TGT may expire and the call will fail.",0
"When the disk becomes full name node is shutting down and if we try to start after making the space available It is not starting and throwing the below exception.



{code:xml} 

2011-01-24 23:23:33,727 ERROR org.apache.hadoop.hdfs.server.namenode.FSNamesystem: FSNamesystem initialization failed.
java.io.EOFException
	at java.io.DataInputStream.readFully(DataInputStream.java:180)
	at org.apache.hadoop.io.UTF8.readFields(UTF8.java:117)
	at org.apache.hadoop.hdfs.server.namenode.FSImageSerialization.readString(FSImageSerialization.java:201)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:185)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:93)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:60)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSEdits(FSImage.java:1089)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:1041)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:487)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.loadFSImage(FSDirectory.java:149)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initialize(FSNamesystem.java:306)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:284)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:328)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:356)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:577)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:570)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1529)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1538)
2011-01-24 23:23:33,729 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.io.EOFException
	at java.io.DataInputStream.readFully(DataInputStream.java:180)
	at org.apache.hadoop.io.UTF8.readFields(UTF8.java:117)
	at org.apache.hadoop.hdfs.server.namenode.FSImageSerialization.readString(FSImageSerialization.java:201)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:185)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:93)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:60)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSEdits(FSImage.java:1089)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:1041)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:487)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.loadFSImage(FSDirectory.java:149)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initialize(FSNamesystem.java:306)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:284)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:328)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:356)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:577)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:570)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1529)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1538)

2011-01-24 23:23:33,730 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at linux124/10.18.52.124
************************************************************/


{code} 
",0
"Suppose a source datanode S is writing to a destination datanode D in a write pipeline.  We have an implicit assumption that _if S catches an exception when it is writing to D, then D is faulty and S is fine._  As a result, DFSClient will take out D from the pipeline, reconstruct the write pipeline with the remaining datanodes and then continue writing .

However, we find a case that the faulty machine F is indeed S but not D.  In the case we found, F has a faulty network interface (or a faulty switch port) in such a way that the faulty network interface works fine when transferring a small amount of data, say 1MB, but it often fails when transferring a large amount of data, say 100MB.

It is even worst if F is the first datanode in the pipeline.  Consider the following:
# DFSClient creates a pipeline with three datanodes.  The first datanode is F.
# F catches an IOException when writing to the second datanode. Then, F reports the second datanode has error.
# DFSClient removes the second datanode from the pipeline and continue writing with the remaining datanode(s).
# The pipeline now has two datanodes but (2) and (3) repeat.
# Now, only F remains in the pipeline.  DFSClient continues writing with one replica in F.
# The write succeeds and DFSClient is able to *close the file successfully*.
# The block is under replicated.  The NameNode schedules replication from F to some other datanode D.
# The replication fails for the same reason.  D reports to the NameNode that the replica in F is corrupted.
# The NameNode marks the replica in F is corrupted.
# The block is corrupted since no replica is available.

We were able to manually divide the replicas into small files and copy them out from F without fixing the hardware.  The replicas seems uncorrupted.  This is a *data availability problem*.",0
"The top of FSEditLog.logSync has the following assertion:
{code}
        assert editStreams.size() > 0 : ""no editlog streams"";
{code}
which should actually come after checking to see if the sync was already batched in by another thread.

This is related to a second bug in which the same case causes synctxid to be reset to 0",0
"In the current design, if there is a datanode/network failure in the write pipeline, DFSClient will try to remove the failed datanode from the pipeline and then continue writing with the remaining datanodes. As a result, the number of datanodes in the pipeline is decreased. Unfortunately, it is possible that DFSClient may incorrectly remove a healthy datanode but leave the failed datanode in the pipeline because failure detection may be inaccurate under erroneous conditions.
We propose to have a new mechanism for adding new datanodes to the pipeline in order to provide a stronger data guarantee.",0
"After closing an input stream on DFS, seeking slightly ahead of the last read will throw an NPE:

java.lang.NullPointerException
        at org.apache.hadoop.hdfs.DFSInputStream.seek(DFSInputStream.java:749)
        at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:42)",0
"Currently NameNode allows format command while it running. In this case the command is executed partially (Lock file is deleted) and an exception thrown. Because of this Name Node should be formatted after restart. This sort of cases can happen accidentally. To prevent such cases Name Node should not execute the format command partially while it is running. It can stright away throw exception/log saying, Name Node is running.
",0
"In secure mode, when disks fail more than volumes tolerated, datanode process doesn't exit properly and it just hangs even though shutdown method is called. 

",0
"In Append HDFS-200 issue,
If file has 10 blocks and after writing 5 blocks if client invokes sync method then NN will persist the blocks information in edits. 
After this if we restart the NN, All the DataNodes will reregister with NN. But DataNodes are not sending the blocks being written information to NN. DNs are sending the blocksBeingWritten information in DN startup. So, here NameNode can not find that the 5 persisted blocks belongs to which datanodes. This information can build based on block reports from DN. Otherwise we will loose this 5 blocks information even NN persisted that block information in edits. 
",0
"HDFS-903 added the MD5 checksum of the fsimage to the VERSION file in each image directory. This allows it to verify that the FSImage didn't get corrupted or accidentally replaced on disk.
With HDFS-1073, there may be multiple fsimage_N files in a storage directory corresponding to different checkpoints. So having a single MD5 in the VERSION file won't suffice. Instead we need to store an MD5 per image file.",0
" Several issues are causing this problem
1. The last LocatedBlock returned by getBlockLocations doesn't have BlockToken.
2. The blockPoolTokenSecretManager is not initialized before created rpc server in datanode.
3. The getReplicaVisibleLength API in datanode expects WRITE permission in the block token, but the block tokens are generated with read permission only in getBlockLocations at namenode.",0
"TEST SETUP
Using attached sample hdfs java program that illustrates the issue.
Using cluster with Kerberos enabled on cluster

# Compile instructions
$ javac Symlink.java -cp `hadoop classpath`
$ jar -cfe Symlink.jar Symlink Symlink.class

# create test file for symlink to use
1. hadoop fs -touchz /user/username/filetest

# Create symlink using file context
2. hadoop jar Symlink.jar ln /user/username/filetest /user/username/testsymlink

# Verify owner of test file
3. hadoop jar Symlink.jar ls /user/username/
-rw-r--r-- username hdfs /user/jeagles/filetest
-rwxrwxrwx username@XX.XXXX.XXXXX.XXX hdfs /user/username/testsymlink

RESULTS
1. Owner shows 'username@XX.XXXX.XXXXX.XXX' for symlink, expecting 'username'.
2. Symlink is corrupted and can't removed, since it was created with an invalid user


------------------------
Sample program to create Symlink

FileContext fc = FileContext.getFileContext(getConf());
fc.createSymlink(target, symlink, false);

---------------------------------------",0
"If a user installs 0.20.203 and then upgrades to 0.21 with an editslog present, 0.21 will corrupt the file system due to opcode re-usage.",0
"HADOOP-2330 added a feature to preallocate space in the local file system for the NN transaction log. That change seeks past the current end of the file and writes out some data, which on most systems results in the intervening data in the file being filled with zeros. Most underlying file systems have special handling for sparse files, and don't actually allocate blocks on disk for blocks of a file which consist completely of 0x00.
I've seen cases in the wild where the volume an edits dir is on fills up, resulting in a partial final transaction being written out to disk. If you examine the bytes of this (now corrupt) edits file, you'll see the partial final transaction followed by a lot of zeros, suggesting that the preallocation previously succeeded before the volume ran out of space. If we fill the preallocated space with something other than zeros, we'd likely see the failure at preallocation time, rather than transaction-writing time, and so cause the NN to crash earlier, without a partial transaction being written out.
I also hypothesize that filling the preallocated space in the edits log with something other than 0x00 will result in a performance improvement in NN throughput. I haven't tested this yet, but I intend to as part of this JIRA.",0
A DN should decommission itself rather than shutdown whenever the configured threshold of volume failures (dfs.datanode.failed.volumes.tolerated) has been reached. This allows the namenode to distinguish between a datanode that might not come back up soon (and will need replication) vs one that is just restarting.,0
"Current implementation is not using XML for that, so we will pass it as a part of response message.
",0
Mkdirs only uses the supplied FsPermission for the last directory of the path.  Paths 0..N-1 will all inherit the parent dir's permissions -even if- inheritPermission is false.  This is a regression from somewhere around 0.20.9 and does not follow posix semantics.,0
"After taking a snapshot in Federation (by starting up namenode with option -upgrade), it appears that the current directory of data node does not contain the block files.  We have also verified that upgrading from 20 to Federation does not have this problem.",0
"BlockMissingException is thrown under this test scenario:
Two different processes doing concurrent file r/w: one read and the other write on the same file
  - writer keep doing file write
  - reader doing position file read from beginning of the file to the visible end of file, repeatedly

The reader is basically doing:
  byteRead = in.read(currentPosition, buffer, 0, byteToReadThisRound);
where CurrentPostion=0, buffer is a byte array buffer, byteToReadThisRound = 1024*10000;

Usually it does not fail right away. I have to read, close file, re-open the same file a few times to create the problem. I'll pose a test program to repro this problem after I've cleaned up a bit my current test program.

",0
"I discovered this in the course of trying to implement a fix for HDFS-1505.

Per the comment for {{FSImage.saveNamespace(...)}}, the algorithm for save namespace proceeds in the following order:

# rename current to lastcheckpoint.tmp for all of them,
# save image and recreate edits for all of them,
# rename lastcheckpoint.tmp to previous.checkpoint.

The problem is that step 3 occurs regardless of whether or not an error occurs for all storage directories in step 2. Upon restart, the NN will see non-existent or corrupt {{current}} directories, and no {{lastcheckpoint.tmp}} directories, and so will conclude that the storage directories are not formatted.

This issue appears to be present on both 0.22 and 0.23. This should arguably be a 0.22/0.23 blocker.",0
"While testing Disk Fail Inplace, We encountered the NPE from this part of the code. 

File[] files = dir.listFiles();
for (File f : files) {
...
}

This is kinda of an API issue. When a disk is bad (or name is not a directory), this API (listFiles, list) return null rather than throwing an exception. This 'for loop' throws a NPE exception. And same applies to dir.list() API.

Fix all the places where null condition was not checked.
 ",0
"There is a situation where one datanode can have more than one copy of same block due to a disk fails and comes back after sometime in a datanode. And these duplicate blocks are not getting deleted even after datanode and namenode restart.

This situation can only happen in a corner case , when due to disk failure, the data block is replicated to other disk of the same datanode.


To simulate this scenario I copied a datablock and the associated .meta file from one disk to another disk of same datanode, so the datanode is having 2 copy of same replica. Now I restarted datanode and namenode. Still the extra data block and meta file is not deleted from the datanode

ls -l `find /grid/{0,1,2,3}/hadoop/var/hdfs/data/current -name blk_*`
-rw-r--r-- 1 hdfs users 7814 May 13 21:05 /grid/1/hadoop/var/hdfs/data/current/blk_1727421609840461376
-rw-r--r-- 1 hdfs users   71 May 13 21:05 /grid/1/hadoop/var/hdfs/data/current/blk_1727421609840461376_579992.meta
-rw-r--r-- 1 hdfs users 7814 May 13 21:14 /grid/3/hadoop/var/hdfs/data/current/blk_1727421609840461376
-rw-r--r-- 1 hdfs users   71 May 13 21:14 /grid/3/hadoop/var/hdfs/data/current/blk_1727421609840461376_579992.meta",0
"Before going to the root cause lets see the read behavior for a file having more than 10 blocks in append case.. 
Logic: 
==== 
There is prefetch size dfs.read.prefetch.size for the DFSInputStream which has default value of 10 
This prefetch size is the number of blocks that the client will fetch from the namenode for reading a file.. 
For example lets assume that a file X having 22 blocks is residing in HDFS 
The reader first fetches first 10 blocks from the namenode and start reading 
After the above step , the reader fetches the next 10 blocks from NN and continue reading 
Then the reader fetches the remaining 2 blocks from NN and complete the write 
Cause: 
======= 
Lets see the cause for this issue now... 
Scenario that will fail is ""Writer wrote 10+ blocks and a partial block and called sync. Reader trying to read the file will not get the last partial block"" . 

Client first gets the 10 block locations from the NN. Now it checks whether the file is under construction and if so it gets the size of the last partial block from datanode and reads the full file 
However when the number of blocks is more than 10, the last block will not be in the first fetch. It will be in the second or other blocks(last block will be in (num of blocks / 10)th fetch) 
The problem now is, in DFSClient there is no logic to get the size of the last partial block(as in case of point 1), for the rest of the fetches other than first fetch, the reader will not be able to read the complete data synced...........!! 

also the InputStream.available api uses the first fetched block size to iterate. Ideally this size has to be increased


",0
"Null pointer exception comes when Namenode recovery happens and there is no response from client to NN more than the hardlimit for NN recovery and the current block is more than the prev block size in NN 
1. Write using a client to 2 datanodes
2. Kill one data node and allow pipeline recovery.
3. write somemore data to the same block
4. Parallely allow the namenode recovery to happen
Null pointer exception will come in addStoreBlock api.


 

",0
"The following sequence leaves the namespace in an inconsistent/broken state:
- format NN using 0.20 (or any prior release, probably)
- run hdfs namenode -upgrade on 0.22. ^C the NN once it comes up.
- run hdfs namenode -rollback on 0.22  (this should fail but doesn't!)

This leaves the name directory in a state such that the version file claims it's an 0.20 namespace, but the fsimage is in 0.22 format. It then crashes when trying to start up.",0
"In high availability setup, with an active and standby namenode, there is a possibility of two namenodes sending commands to the datanode. The datanode must honor commands from only the active namenode and reject the commands from standby, to prevent corruption. This invariant must be complied with during fail over and other states such as split brain. This jira addresses issues related to this, design of the solution and implementation.",0
"In getJNIEnv, we go though LIBHDFS_OPTS with strok and count the number of args. Then create an array of options based on that information. But when we actually setup the options we only the first arg. I believe the fix is pasted inline.

{noformat}
Index: src/c++/libhdfs/hdfsJniHelper.c
===================================================================
--- src/c++/libhdfs/hdfsJniHelper.c	(revision 1124544)
+++ src/c++/libhdfs/hdfsJniHelper.c	(working copy)
@@ -442,6 +442,7 @@
             int argNum = 1;
             for (;argNum < noArgs ; argNum++) {
                 options[argNum].optionString = result; //optHadoopArg;
+                result = strtok( NULL, jvmArgDelims);
             }
         }
{noformat}",0
"This scenario is applicable in NN and BNN case.

When the namenode goes down after creating the edits.new, on subsequent restart the divertFileStreams will not happen to edits.new as the edits.new file is already present and the size is zero.

so on trying to saveCheckPoint an exception occurs 
2011-05-23 16:38:57,476 WARN org.mortbay.log: /getimage: java.io.IOException: GetImage failed. java.io.IOException: Namenode has an edit log with timestamp of 2011-05-23 16:38:56 but new checkpoint was created using editlog  with timestamp 2011-05-23 16:37:30. Checkpoint Aborted.

This is a bug or is that the behaviour.





",0
"Backup namenode initiates the checkpointing process. 
As a part of checkpointing based on the timestamp it tries to download the FSImage or use the existing one.

Then it tries to save the FSImage.

During this time it tries to close the editLog streams.

Parallely when a client tries to close a file just after the checkpointing process closes the editLog Stream then we get an exception saying
java.io.IOException: java.lang.IllegalStateException: !!! WARNING !!! File system changes are not persistent. No journal streams.

Here the saveNameSpace api closes all the editlog streams resulting in this issue.



 
",0
"added the ability to run two secondary namenodes at the same time. However, there were two races I found when stress testing this (by running two NNs each checkpointing in a tight loop with no sleep):
1) the writing of the seen_txid file was not atomic, so it was at some points reading an empty file
2) it was possible for two checkpointers to try to take a checkpoint at the same transaction ID, which would cause the two image downloads to collide and fail",0
"If the image transfer process receives a client-side error when transferring edit logs, it will throw an exception before it has completely read all of the input from the server-side servlet. Then, the finally clause will throw a new error, since the received length is less than the length given in the header. This masks the client-side exception and makes it look like a network error or a server-side problem.",0
{{SafeModeInfo.getTurnOffTip()}} under-reports the number of blocks needed to reach the safemode threshold.,0
Removal and restoration of storage directories on checkpointing failure doesn't work properly. Sometimes it throws a NullPointerException and sometimes it doesn't take off a failed storage directory,0
This has been failing on Hudson for the last two builds and fails on my local box as well.,0
"The junit test failed when iterates a number of times with larger chunk size on Linux. Once a while, the visible number of bytes seen by a reader is slightly less than what was supposed to be. 

When run with the following parameter, it failed more often on Linux ( as reported by John George) than my Mac:
  private static final int WR_NTIMES = 300;
  private static final int WR_CHUNK_SIZE = 10000;

Adding more debugging output to the source, this is a sample of the output:
Caused by: java.io.IOException: readData mismatch in byte read: expected=2770000 ; got 2765312
        at org.apache.hadoop.hdfs.TestWriteRead.readData(TestWriteRead.java:141)
",0
"Currently in the 1073 branch, the following steps ends up with a very confused 2NN:
format NN, run NN
start 2NN, perform some checkpoints
reformat NN, start NN on new namespace
restart same 2NN
The 2NN currently saves the new VERSION info into its local storage directory but doesn't clear out the old checkpoint or edits files. This is obviously wrong and might lead to a corrupt checkpoint getting uploaded.
If the 2NN has storage directories with VERSION info, and connects to an NN with different VERSION info, there are two alternatives:
a) refuse to perform any checkpoints until the operator issues a ""secondarynamenode -format"" command (this is similar to how the backupnode/checkpointnode works)
b) clear the current contents of the storage directory and save the new NN's VERSION info.",0
"The refactoring in HDFS-2003 allowed findbugs to expose two potential bugs:
- the atime field logged with OP_MKDIR is unused
- the timestamp field logged with OP_CONCAT_DELETE is unused

The concat issue is definitely real. The atime for MKDIR might always be identical to mtime in that case, in which case it could be ignored.",0
"As a part of datanode process hang, this part of code was introduced in 0.20.204 to clean up all the waiting threads.

-      try {
-          readPool.awaitTermination(10, TimeUnit.SECONDS);
-      } catch (InterruptedException e) {
-       LOG.info(""Exception occured in doStop:"" + e.getMessage());
-      }
-      readPool.shutdownNow();

This was clearly meant for production, but all the unit tests uses minidfscluster and minimrcluster for shutdown which waits on this part of the code. Due to this, we saw increase in unit test run times. So removing this code. 
",0
"This is the design for automatic hot HA for HDFS NameNode. It involves use of HA software and LoadReplicator - external to Hadoop components, which substantially simplify the architecture by separating HA- from Hadoop-specific problems. Without the external components it provides warm standby with manual failover.",0
"The following code can throw NPE if callGetBlockLocations returns null.

If server returns null 

{code}
    List<LocatedBlock> locatedblocks
        = callGetBlockLocations(namenode, src, 0, Long.MAX_VALUE).getLocatedBlocks();
{code}

The right fix for this is server should throw right exception.

",0
"This JIRA addresses the following case:
NN is running with 2 storage dirs
1 of the dirs fails
2NN makes a checkpoint
Currently, if GetImageServlet fails to open any of the local files to receive a checkpoint, it will fail the entire checkpoint upload process. Instead, it should continue to receive checkpoints in the non-failed directories.",0
"This JIRA is to address the following scenario/bug:
The NN is configured with an edits-only storage dir in /edits and an image-only storage dir in /image
The image dir fails while it is running. Since the edits dir is still valid it does not immediately shut itself down. 2NN continues to try to checkpoint, but fails because it can't upload an image anywhere
Operator fixes the disk on /image and instructs the NN to restore removed storage
The 2NN should now be able to download/upload a checkpoint successfully.
Currently this does not work since the NN clears the storage dir upon restoring it. With the 1073 design, out-of-date files aren't a problem, and in fact can be used to restore the namespace.",0
"2011-06-17 11:43:23,096 ERROR org.apache.hadoop.hdfs.server.namenode.Checkpointer: Throwable Exception in doCheckpoint: 
java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedSetTimes(FSDirectory.java:1765)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedSetTimes(FSDirectory.java:1753)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.loadEditRecords(FSEditLog.java:708)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.loadFSEdits(FSEditLog.java:411)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.loadFSEdits(FSEditLog.java:378)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSEdits(FSImage.java:1209)
        at org.apache.hadoop.hdfs.server.namenode.BackupStorage.loadCheckpoint(BackupStorage.java:158)
        at org.apache.hadoop.hdfs.server.namenode.Checkpointer.doCheckpoint(Checkpointer.java:243)
        at org.apache.hadoop.hdfs.server.namenode.Checkpointer.run(Checkpointer.java:141)
",0
"As the title describes the problem:  if the include host list contains host name, after restarting namenodes, the datanodes registrant is denied by namenodes.  This is because after namenode is restarted, the still alive data node will try to register itself with the namenode and it identifies itself with its *IP address*.  However, namenode only allows all the hosts in its hosts list to registrant and all of them are hostnames. So namenode would deny the datanode registration.
",0
"
I can see that if data node receives some IO error, this can cause checkDir storm.
What I mean:
1) any error produces DataNode.checkDiskError call
2) this call locks volume:
 java.lang.Thread.State: RUNNABLE
       at java.io.UnixFileSystem.getBooleanAttributes0(Native Method)
       at java.io.UnixFileSystem.getBooleanAttributes(UnixFileSystem.java:228)
       at java.io.File.exists(File.java:733)
       at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsCheck(DiskChecker.java:65)
       at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:86)
       at org.apache.hadoop.hdfs.server.datanode.FSDataset$FSDir.checkDirTree(FSDataset.java:228)
       at org.apache.hadoop.hdfs.server.datanode.FSDataset$FSDir.checkDirTree(FSDataset.java:232)
       at org.apache.hadoop.hdfs.server.datanode.FSDataset$FSDir.checkDirTree(FSDataset.java:232)
       at org.apache.hadoop.hdfs.server.datanode.FSDataset$FSDir.checkDirTree(FSDataset.java:232)
       at org.apache.hadoop.hdfs.server.datanode.FSDataset$FSVolume.checkDirs(FSDataset.java:414)
       at org.apache.hadoop.hdfs.server.datanode.FSDataset$FSVolumeSet.checkDirs(FSDataset.java:617)
       - locked <0x000000080a8faec0> (a org.apache.hadoop.hdfs.server.datanode.FSDataset$FSVolumeSet)
       at org.apache.hadoop.hdfs.server.datanode.FSDataset.checkDataDir(FSDataset.java:1681)
       at org.apache.hadoop.hdfs.server.datanode.DataNode.checkDiskError(DataNode.java:745)
       at org.apache.hadoop.hdfs.server.datanode.DataNode.checkDiskError(DataNode.java:735)
       at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.close(BlockReceiver.java:202)
       at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:151)
       at org.apache.hadoop.io.IOUtils.closeStream(IOUtils.java:167)
       at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:646)
       at org.apache.hadoop.hdfs.server.datanode.DataXceiver.opWriteBlock(DataXceiver.java:352)
       at org.apache.hadoop.hdfs.protocol.DataTransferProtocol$Receiver.opWriteBlock(DataTransferProtocol.java:390)
       at org.apache.hadoop.hdfs.protocol.DataTransferProtocol$Receiver.processOp(DataTransferProtocol.java:331)
       at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:111)
       at java.lang.Thread.run(Thread.java:619)

3) This produces timeouts on other calls, e.g.
2011-06-17 17:35:03,922 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: checkDiskError: exception:
java.io.InterruptedIOException
       at java.io.FileOutputStream.writeBytes(Native Method)
       at java.io.FileOutputStream.write(FileOutputStream.java:260)
       at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
       at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
       at java.io.DataOutputStream.flush(DataOutputStream.java:106)
       at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.close(BlockReceiver.java:183)
       at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:151)
       at org.apache.hadoop.io.IOUtils.closeStream(IOUtils.java:167)
       at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:646)
       at org.apache.hadoop.hdfs.server.datanode.DataXceiver.opWriteBlock(DataXceiver.java:352)
       at org.apache.hadoop.hdfs.protocol.DataTransferProtocol$Receiver.opWriteBlock(DataTransferProtocol.java:390)
       at org.apache.hadoop.hdfs.protocol.DataTransferProtocol$Receiver.processOp(DataTransferProtocol.java:331)
       at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:111)
       at java.lang.Thread.run(Thread.java:619)

4) This, in turn, produces more ""dir check calls"".

5) All the cluster works very slow because of half-working node.
",0
"If a decommissioned node is removed from the decommissioned list, namenode does not delete the excess replicas it created while the node was decommissioned.",0
This sub-task will be a place holder for all the disk fail inplace issues related to Datanode.,0
"DataXceiver#run currently swallows all exceptions, it should instead plumb them up to DataXceiverServer#run so it can decide whether the exception should be tolerated or the daemon should exit. An IOE should be tolerated (because it's likely just an issue with a particular thread, or an intermittent failure), as it is today, but eg j.l.Error should not. 

This came up in the following bug I'm seeing on a test cluster: if there's eg a NoClassDefFoundError thrown in DataXceiver#run (because the host jars were replaced out from underneath it, it ran out of descriptors, etc.) we'll end up with a datanode that is alive but always fails because it can't create any DataXceiver threads. In this case the datanode should shut itself down rather than continue to run.",0
"Volume failures detected on startup are not currently counted/reported as such. Eg if you have configured 4 volumes, 2 tolerated failures, and you start a DN with two failed volumes it will come up and report (to the NN) no failed volumes. The DN will still be able to tolerate 2 additional volume failures (ie it's OK with no valid volumes remaining). The intent of the volume failure toleration config value is that if more than this # of volumes of the total set of configured volumes have failed the DN should shutdown, therefore volume failures detected on startup should count against this quota. ",0
"On startup, the NN reads the fstime file of all the configured dfs.name.dirs to determine which one to load. However, if any of the searched directories contain an empty or malformed fstime file, the NN will fail to start. The NN should be able to just proceed with starting and ignore the directory containing the bad fstime file.",0
"Currently in order to change the value of the balancer bandwidth (dfs.datanode.balance.bandwidthPerSec), the datanode daemon must be restarted.
The optimal value of the bandwidthPerSec parameter is not always (almost never) known at the time of cluster startup, but only once a new node is placed in the cluster and balancing is begun. If the balancing is taking too long (bandwidthPerSec is too low) or the balancing is taking up too much bandwidth (bandwidthPerSec is too high), the cluster must go into a ""maintenance window"" where it is unusable while all of the datanodes are bounced. In large clusters of thousands of nodes, this can be a real maintenance problem because these ""mainenance windows"" can take a long time and there may have to be several of them while the bandwidthPerSec is experimented with and tuned.
A possible solution to this problem would be to add a -bandwidth parameter to the balancer tool. If bandwidth is supplied, pass the value to the datanodes via the OP_REPLACE_BLOCK and OP_COPY_BLOCK DataTransferProtocol requests. This would make it necessary, however, to change the DataTransferProtocol version.",0
I've seen a couple times where a unit test has timed out. jstacking shows the cluster is stuck trying to shut down one of the DataNode HTTP servers. The DataNodeBlockScanner thread also seems to be in a tight loop in its main loop.,0
"Either I am doing something incredibly stupid, or something about my environment is completely weird, or may be it really is a valid bug. I am running a NameNode deadlock consistently with 0.23 HDFS. I could never start NN successfully.",0
Lock cycle detected by jcarder,0
"Even If createEditsLogFile failes in startJournalSpool of BackUpStorage, BackUPNode will proceed with exceptions. 
",0
"At the end of checkpoint BackupNode tries to convergeJournalSpool() and calls revertFileStreams(). The latter closes each file stream, and tries to rename the corresponding file to its permanent location current/edits. If for any reason the rename fails processIOError() is called for failed streams. processIOError() will try to close the stream again and will get NPE in EditLogFileOutputStream.close() because bufCurrent was set to null by the previous close.",0
"I am trying to remove file from BackupNode using
{code}hadoop fs -fs hdfs://backup.node.com:50100 -rm /README.txt{code}
which is supposed to fail. But it seems to be hanging forever.
Needs some investigation. It used to throw SafeModeException if I remember correctly.",0
"A block has one replica marked as corrupt and two good ones. countNodes() correctly detects that there are only 2 live replicas, and fsck reports the block as under-replicated. But ReplicationMonitor never schedules replication of good replicas.",0
aims to enable client fail-over for HDFS RPCs. This JIRA is to enable HTTP client fail over where appropriate.,0
"We are seeing the following issue around recoverLease over in hbaselandia.  DFSClient calls recoverLease to assume ownership of a file.  The recoverLease returns to the client but it can take time for the new state to propagate.  Meantime, an incoming read fails though its using updated block info.  Thereafter all read retries fail because on exception we revert to stale block view and we never recover.  Laxman reports this issue in the below mailing thread:

See this thread for first report of this issue: http://search-hadoop.com/m/S1mOHFRmgk2/%2527FW%253A+Handling+read+failures+during+recovery%2527&subj=FW+Handling+read+failures+during+recovery

Chatting w/ Hairong offline, she suggests this a general issue around lease recovery no matter how it triggered (new recoverLease or not).

I marked this critical.  At least over in hbase it is since we get set stuck here recovering a crashed server.",0
"Here's the scenario:

* You run the NN and 2NN (2NN A) on the same machine.
* You don't have the address of the 2NN configured, so it's defaulting to 127.0.0.1.
* There's another 2NN (2NN B) running on a second machine.
* When a 2NN is done checkpointing, it says ""hey NN, I have an updated fsimage for you. You can download it from this URL, which includes my IP address, which is x""

And here's the steps that occur to cause this issue:

# Some edits happen.
# 2NN A (on the NN machine) does a checkpoint. All is dandy.
# Some more edits happen.
# 2NN B (on a different machine) does a checkpoint. It tells the NN ""grab the newly-merged fsimage file from 127.0.0.1""
# NN happily grabs the fsimage from 2NN A (the 2NN on the NN machine), which is stale.
# NN renames edits.new file to edits. At this point the in-memory FS state is fine, but the on-disk state is missing edits.
# The next time a 2NN (any 2NN) tries to do a checkpoint, it gets an up-to-date edits file, with an outdated fsimage, and tries to apply those edits to that fsimage.
# Kaboom.",0
TestRenameWhileOpen is failing in branch-0.20-security.,0
"If hftp cannot locate either a hdfs or hftp token in the ugi, it will call {{getDelegationToken}} to acquire one from the remote nn.  This method may return a null {{Token}} if security is disabled(*)  on the remote nn.  Hftp will internally call its {{setDelegationToken}} which will throw a NPE when the token is {{null}}.

(*) Actually, if any problem happens while acquiring the token it assumes security is disabled!  However, it's a pre-existing issue beyond the scope of the token renewal changes.",0
"In NNStorage.java:
There are many stream closures in finally block. 
There is a chance that they can mask the root exceptions.
So, better to follow the pattern like below:
  try{
     ............
     ............
     stream.close();
     stream =null;
   }
   finally{
     IOUtils.cleanup(LOG, stream);
   }",0
"A JournalManager may take hold of resources for the duration of their lifetime. This isn't the case at the moment for FileJournalManager, but BookKeeperJournalManager will, and it's conceivable that FileJournalManager could take a lock on a directory etc.
This JIRA is to add Closeable to JournalManager so that these resources can be cleaned up when FSEditLog is closed.",0
"When all volumes failed at back up node side, it is waiting for ever.",0
"

in core-site.xml if default filesystem's uri is misconfigured , NN does not come up and in NN shows NPE

java.lang.NullPointerException
        at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:142)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:195)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:225)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:259)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:458)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1226)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1235)


It is better to give meaningful error messege than NPE",0
"Scenario:
I have a cluster of 4 DN ,each of them have 12disks.

In hdfs-site.xml I have ""dfs.datanode.failed.volumes.tolerated=3"" 

During the execution of distcp (hdfs->hdfs), I am failing 3 disks in one Datanode, by making Data Directory permission 000, The distcp job is successful but , I am getting some NullPointerException in Datanode log

In one thread
$hadoop distcp  /user/$HADOOPQA_USER/data1 /user/$HADOOPQA_USER/data3

In another thread in a datanode
$ chmod 000 /xyz/{0,1,2}/hadoop/var/hdfs/data

where [ dfs.data.dir is set as /xyz/{0..11}/hadoop/var/hdfs/data ]

Log Snippet from the Datanode
=============

2011-09-19 12:43:40,314 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected error trying to delete block
blk_7065198814142552283_62557. BlockInfo not found in volumeMap.
2011-09-19 12:43:40,314 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected error trying to delete block
blk_7066946313092770579_39189. BlockInfo not found in volumeMap.
2011-09-19 12:43:40,314 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected error trying to delete block
blk_7070305189404753930_49359. BlockInfo not found in volumeMap.
2011-09-19 12:43:40,327 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Error processing datanode Command
java.io.IOException: Error in deleting blocks.
        at org.apache.hadoop.hdfs.server.datanode.FSDataset.invalidate(FSDataset.java:1820)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.processCommand(DataNode.java:1074)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.processCommand(DataNode.java:1036)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.offerService(DataNode.java:891)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.run(DataNode.java:1419)
        at java.lang.Thread.run(Thread.java:619)
2011-09-19 12:43:41,304 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode:
DatanodeRegistration(xx.xxx.xxx.xxx:xxxx, storageID=xx-xxxxxxxxxxxx-xx.xxx.xxx.xxx-xxxx-xxxxxxxxxxx, infoPort=1006,
ipcPort=8020):DataXceiver
java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner$LogFileHandler.appendLine(DataBlockScanner.java:788)
        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.updateScanStatusInternal(DataBlockScanner.java:365)
        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.verifiedByClient(DataBlockScanner.java:308)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:205)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:99)
        at java.lang.Thread.run(Thread.java:619)
2011-09-19 12:43:43,313 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected error trying to delete block
blk_7071818644980664768_40827. BlockInfo not found in volumeMap.
2011-09-19 12:43:43,313 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected error trying to delete block
blk_7073840977856837621_62108. BlockInfo not found in volumeMap.",0
"Distcp with hftp is failing.

{noformat}
$hadoop   distcp hftp://<NNhostname>:50070/user/hadoopqa/1316814737/newtemp 1316814737/as
11/09/23 21:52:33 INFO tools.DistCp: srcPaths=[hftp://<NNhostname>:50070/user/hadoopqa/1316814737/newtemp]
11/09/23 21:52:33 INFO tools.DistCp: destPath=1316814737/as
Retrieving token from: https://<NN IP>:50470/getDelegationToken
Retrieving token from: https://<NN IP>:50470/getDelegationToken?renewer=mapred
11/09/23 21:52:34 INFO security.TokenCache: Got dt for hftp://<NNhostname>:50070/user/hadoopqa/1316814737/newtemp;uri=<NN IP>:50470;t.service=<NN IP>:50470
org.apache.hadoop.ipc.RemoteException: name=hadoopqa != expected=null
	at org.apache.hadoop.ipc.RemoteException.valueOf(RemoteException.java:102)
	at org.apache.hadoop.hdfs.HftpFileSystem$LsParser.startElement(HftpFileSystem.java:385)
	...
{noformat}",0
"$ hadoop distcp hftp://<NN Hostname>:50070/user/hadoopqa/input1/part-00000 /user/hadoopqa/out3
11/09/30 18:57:59 INFO tools.DistCp: srcPaths=[hftp://<NN Hostname>:50070/user/hadoopqa/input1/part-00000]
11/09/30 18:57:59 INFO tools.DistCp: destPath=/user/hadoopqa/out3
11/09/30 18:58:00 INFO security.TokenCache: Got dt for
hftp://<NN Hostname>:50070/user/hadoopqa/input1/part-00000;uri=<NN IP>:50470;t.service=<NN IP>:50470
11/09/30 18:58:00 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 24 for hadoopqa on <NN IP>:8020
11/09/30 18:58:00 INFO security.TokenCache: Got dt for
/user/hadoopqa/out3;uri=<NN IP>:8020;t.service=<NN IP>:8020
11/09/30 18:58:00 INFO tools.DistCp: /user/hadoopqa/out3 does not exist.
11/09/30 18:58:00 INFO tools.DistCp: sourcePathsCount=1
11/09/30 18:58:00 INFO tools.DistCp: filesToCopyCount=1
11/09/30 18:58:00 INFO tools.DistCp: bytesToCopyCount=1.0g
11/09/30 18:58:01 INFO mapred.JobClient: Running job: job_201109300819_0007
11/09/30 18:58:02 INFO mapred.JobClient:  map 0% reduce 0%
11/09/30 18:58:25 INFO mapred.JobClient: Task Id : attempt_201109300819_0007_m_000000_0, Status : FAILED
java.io.IOException: Copied: 0 Skipped: 0 Failed: 1
        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.close(DistCp.java:582)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)
        at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)
        at org.apache.hadoop.mapred.Child.main(Child.java:249)

11/09/30 18:58:41 INFO mapred.JobClient: Task Id : attempt_201109300819_0007_m_000000_1, Status : FAILED
java.io.IOException: Copied: 0 Skipped: 0 Failed: 1
        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.close(DistCp.java:582)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)
        at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)
        at org.apache.hadoop.mapred.Child.main(Child.java:249)

11/09/30 18:58:56 INFO mapred.JobClient: Task Id : attempt_201109300819_0007_m_000000_2, Status : FAILED
java.io.IOException: Copied: 0 Skipped: 0 Failed: 1
        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.close(DistCp.java:582)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)
        at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)
        at org.apache.hadoop.mapred.Child.main(Child.java:249)

11/09/30 18:59:14 INFO mapred.JobClient: Job complete: job_201109300819_0007
11/09/30 18:59:14 INFO mapred.JobClient: Counters: 6
11/09/30 18:59:14 INFO mapred.JobClient:   Job Counters 
11/09/30 18:59:14 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=62380
11/09/30 18:59:14 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0
11/09/30 18:59:14 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0
11/09/30 18:59:14 INFO mapred.JobClient:     Launched map tasks=4
11/09/30 18:59:14 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=0
11/09/30 18:59:14 INFO mapred.JobClient:     Failed map tasks=1
11/09/30 18:59:14 INFO mapred.JobClient: Job Failed: # of failed Map Tasks exceeded allowed limit. FailedCount: 1.
LastFailedTask: task_201109300819_0007_m_000000
With failures, global counters are inaccurate; consider running with -i
Copy failed: java.io.IOException: Job failed!
        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1257)
        at org.apache.hadoop.tools.DistCp.copy(DistCp.java:667)
        at org.apache.hadoop.tools.DistCp.run(DistCp.java:881)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
        at org.apache.hadoop.tools.DistCp.main(DistCp.java:908)





",0
"Currently the json response from a webhdfs api is such
curl -i ""http://localhost:50070/webhdfs/foo/b.patch?op=GETFILESTATUS""
{
""accessTime"":1315969120753,
""blockSize"":33554432,
...... 
}
We should return some thing like
{
""FileStatus"":
{ ""accessTime"":1315969120753, ""blockSize"":33554432, .... }
}",0
hadoop dfs command with webhdfs fails on secure hadoop,0
This is HDFS part of HADOOP-7721. ,0
with webhdfs enabled in secure mode the auth to local mappings are not being respected.,0
"We encountered a situation where the namenode dropped into safe mode after a temporary outage of an NFS mount.

At 12:10 the NFS server goes offline

Oct  8 12:10:05 <namenode> kernel: nfs: server <nfs host> not responding, timed out

This caused the namenode to conclude resource issues:

2011-10-08 12:10:34,848 WARN org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker: Space available on volume '<nfs host>' is 0, which is below the configured reserved amount 104857600

Temporary loss of NFS mount shouldn't cause safemode.",0
"If my complete hostname is  host1.abc.xyz.com, only complete hostname must be used to access data via hdfs://

I am running following in .20.205 Client to get data from .20.205 NN (host1)
$hadoop dfs -copyFromLocal /etc/passwd  hdfs://host1/tmp
copyFromLocal: Wrong FS: hdfs://host1/tmp, expected: hdfs://host1.abc.xyz.com
Usage: java FsShell [-copyFromLocal <localsrc> ... <dst>]

$hadoop dfs -copyFromLocal /etc/passwd  hdfs://host1.abc/tmp/
copyFromLocal: Wrong FS: hdfs://host1.blue/tmp/1, expected: hdfs://host1.abc.xyz.com
Usage: java FsShell [-copyFromLocal <localsrc> ... <dst>]


$hadoop dfs -copyFromLocal /etc/passwd  hftp://host1.abc.xyz/tmp/
copyFromLocal: Wrong FS: hdfs://host1.blue/tmp/1, expected: hdfs://host1.abc.xyz.com
Usage: java FsShell [-copyFromLocal <localsrc> ... <dst>]


Only following is supported 
$hadoop dfs -copyFromLocal /etc/passwd  hdfs://host1.abc.xyz.com/tmp/
",0
"OutOfMemoryError brings down DataNode, when DataXceiverServer tries to spawn a new data transfer thread.",0
"When looking for hdfs delegation tokens, Hftp converts the service to a string and compares it to a text.",0
"Retrieving tokens from an insecure cluster throws an NPE.  This prevents copies from insecure to secure clusters.  The problem is in {{DFSClient.getDelegationToken()}}.  It attempts to set the service and log the token w/o a null check.

FetchDT will also throw exceptions when fetching tokens from an insecure cluster.",0
"When file is deleted during its creation {{FSNamesystem.checkLease(String src, String holder)}} throws {{LeaseExpiredException}}. It would be more informative if it thrown {{FileNotFoundException}}.",0
TestBackupNode fails due to NullPointerException,0
"I wrote a test which runs append() in a loop on a single file with a single replica, appending 0~100 bytes each time. If this races with the BlockPoolSliceScanner, I observe the BlockPoolSliceScanner getting FNFE, then reporting the block as bad to the NN. This causes the writer thread to loop forever on completeFile() since it doesn't see a valid replica.",0
"curl -L -u : --negotiate -i ""http://NN:50070/webhdfs/v1/tmp/webhdfs_data/file_small_data.txt?op=OPEN""
the following exception is thrown by the datanode when the redirect happens.
{""RemoteException"":{""exception"":""IOException"",""javaClassName"":""java.io.IOException"",""message"":""Call to failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]""}}
Interestingly when using ./bin/hadoop with a webhdfs path we are able to cat or tail a file successfully.",0
"Instantiation of the hftp filesystem is causing a token to be implicitly created and added to a custom token renewal thread.  With the new token renewal feature in the JT, this causes the mapreduce {{obtainTokensForNamenodes}} to fetch two tokens (an implicit and uncancelled token, and an explicit token) and leave a spurious renewal thread running.  This thread should not be running in the JT.

After speaking with Owen, the quick solution is to lazy fetch the token, and to lazy start the renewer thread.",0
"CHANGES.txt in hdfs-project has 2 sections titled ""Release 0.21.1 - Unreleased"". They are not identical, should be merged.",0
"Currently the content-type header is not being set and for some reason for append it is being set to the form encoded content type making jersey parameter parsing fail.

For this and to avoid any kind of proxy transcoding the content-type should be set to binary.

{code}
  conn.setRequestProperty(""Content-Type"", ""application/octet-stream"");
{code}
",0
The RPC timeout for block recovery does not take into account that it issues multiple RPCs itself. This can cause recovery to fail if the network is congested or DNs are busy.,0
"The client gets stuck in the following loop if an rpc its issued to recover a block timed out:

{noformat}
DataStreamer#run
1.  processDatanodeError
2.     DN#recoverBlock
3.        DN#syncBlock
4.           NN#nextGenerationStamp
5.  sleep 1s
6.  goto 1
{noformat}

Once we've timed out onece at step 2 and loop, step 2 throws an IOE because the block is already being recovered and step 4 throws an IOE because the block GS is now out of date (the previous, timed-out, request got a new GS and updated the block). Eventually the client reaches max retries, considers all DNs bad, and close throws an IOE.

The client should be able to succeed if one of its requests to recover the block succeeded. It should still fail if another client (eg HBase via recoverLease or the NN via releaseLease) succesfully recovered the block. One way to handle this would be to not timeout the request to recover the block. Another would be able to make a subsequent call to recoverBlock succeed eg by updating the block's sequence number to be the latest value that was updated by the same client in the previous request (ie it can recover over itself but not another client).",0
When testing MapReduce on top of an HA cluster we ran into the following bug: some uses of HDFS paths go through a canonicalization step which ensures that the authority component in the URI includes a port number. So our hdfs://logical-nn-uri/foo path turned into hdfs://logical-nn-uri:8020/foo. The code which looks up the failover proxy provider then failed to find the associated config. We should only compare the hostname portion of the URI when looking up proxy providers.,0
"There's a bug in FSEditLog#rollEditLog which results in the NN process exiting if a single name dir has failed. Here's the relevant code:

{code}
close()  // So editStreams.size() is 0 
foreach edits dir {
  ..
  eStream = new ...  // Might get an IOE here
  editStreams.add(eStream);
} catch (IOException ioe) {
  removeEditsForStorageDir(sd);  // exits if editStreams.size() <= 1  
}
{code}

If we get an IOException before we've added two edits streams to the list we'll exit, eg if there's an error processing the 1st name dir we'll exit even if there are 4 valid name dirs. The fix is to move the checking out of removeEditsForStorageDir (nee processIOError) or modify it so it can be disabled in some cases, eg here where we don't yet know how many streams are valid.",0
"The append/create requests should require 'application/octet-stream' as content-type when uploading data. This is to prevent the default content-type form-encoded (used as default by some HTTP libraries) to be used or text based content-types to be used.

If the form-encoded content type is used, then Jersey tries to process the upload stream as parameters
If a test base content-type is used, HTTP proxies/gateways could do attempt some transcoding on the stream thus corrupting the data.",0
"hadoop is into a recovery mode and save namespace to disk before the system starting service. however, there are many situation will cause hadoop enter recovery mode like missing VERSION file and ckpt file exists due to last failure of checkpoint.
in recovery mode, namespace is loaded from previous fsimage, and the default numFiles of namespace.rootDir is 1. the numFiles number is read from fsimage (readInt as version, readInt as namespaceId, readLong as numFiles).
the numFiles number is not updated in namespace when saving namespace.
save namespace just after load fsimage which actually write numFiles which is default value 1 to disk.
the next time to load the saved fsimage from disk when rebooting or secondarynamenode doing checkpoint, the system will crash (OOM) because this fsimage is incorrect.",0
"When testing the tail'ing of a local file with the read short circuit on, I get:

{noformat}
2012-01-06 00:17:31,598 WARN org.apache.hadoop.hdfs.DFSClient: BlockReaderLocal requested with incorrect offset:  Offset 0 and length 8230400 don't match block blk_-2842916025951313698_454072 ( blockLen 124 )
2012-01-06 00:17:31,598 WARN org.apache.hadoop.hdfs.DFSClient: BlockReaderLocal: Removing blk_-2842916025951313698_454072 from cache because local file /export4/jdcryans/dfs/data/blocksBeingWritten/blk_-2842916025951313698 could not be opened.
2012-01-06 00:17:31,599 INFO org.apache.hadoop.hdfs.DFSClient: Failed to read block blk_-2842916025951313698_454072 on local machine java.io.IOException:  Offset 0 and length 8230400 don't match block blk_-2842916025951313698_454072 ( blockLen 124 )
2012-01-06 00:17:31,599 INFO org.apache.hadoop.hdfs.DFSClient: Try reading via the datanode on /10.4.13.38:51010
java.io.EOFException: hdfs://sv4r11s38:9100/hbase-1/.logs/sv4r13s38,62023,1325808100311/sv4r13s38%2C62023%2C1325808100311.1325808100818, entryStart=7190409, pos=8230400, end=8230400, edit=5
{noformat}",0
"In HDFS-2709 it was discovered that there's a potential race wherein edits log files are pre-allocated before the version number is written into the header of the file. This can cause the NameNode to read an invalid HDFS layout version, and hence fail to read the edit log file. We should write the header, then pre-allocate the rest of the file after this point.",0
"Observe this from BackupNode tests:

java.lang.IllegalArgumentException: not a proxy instance
	at java.lang.reflect.Proxy.getInvocationHandler(Unknown Source)
	at org.apache.hadoop.ipc.RPC.stopProxy(RPC.java:557)
	at org.apache.hadoop.hdfs.server.namenode.BackupNode.stop(BackupNode.java:194)
	at org.apache.hadoop.hdfs.server.namenode.TestBackupNode.testCheckpoint(TestBackupNode.java:355)
	at org.apache.hadoop.hdfs.server.namenode.TestBackupNode.testBackupNode(TestBackupNode.java:241)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at junit.framework.TestCase.runTest(TestCase.java:168)
	at junit.framework.TestCase.runBare(TestCase.java:134)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
",0
"It seems like HDFS-900 may have regressed in trunk since it was committed without a regression test. In HDFS-2742 I saw the following sequence of events:
- A block at replication 2 had one of its replicas marked as corrupt on the NN
- NN scheduled deletion of that replica in {{invalidateWork}}, and removed it from the block map
- The DN hosting that block sent a block report, which caused the replica to get re-added to the block map as if it were good
- The deletion request was passed to the DN and it deleted the block
- Now we're in a bad state, where the NN temporarily thinks that it has two good replicas, but in fact one of them has been deleted. If we lower replication of this block at this time, the one good remaining replica may be deleted.",0
"The following sequence of events results in a replica mistakenly marked corrupt:
1. Pipeline is open with 2 replicas
2. DN1 generates a block report but is slow in sending to the NN (eg some flaky network). It gets ""stuck"" right before the block report RPC.
3. Client closes the file.
4. DN2 is fast and sends blockReceived to the NN. NN marks the block as COMPLETE
5. DN1's block report proceeds, and includes the block in an RBW state.
6. (x) NN incorrectly marks the replica as corrupt, since it is an RBW replica on a COMPLETE block.",0
"We've been testing HBase on clusters running trunk and seen an issue where they seem to lose their HDFS leases after a couple of hours of runtime. We don't quite have enough data to understand what's happening, but the NN is expiring them, claiming the hard lease period has elapsed. The clients report no error until their output stream gets killed underneath them.",0
"When tested the HA(internal) with continuous switch with some 5mins gap, found some *blocks missed* and namenode went into safemode after next switch.
   
   After the analysis, i found that this files already deleted by clients. But i don't see any delete commands logs namenode log files. But namenode added that blocks to invalidateSets and DNs deleted the blocks.
   When restart of the namenode, it went into safemode and expecting some more blocks to come out of safemode.

   Here the reason could be that, file has been deleted in memory and added into invalidates after this it is trying to sync the edits into editlog file. By that time NN asked DNs to delete that blocks. Now namenode shuts down before persisting to editlogs.( log behind)
   Due to this reason, we may not get the INFO logs about delete, and when we restart the Namenode (in my scenario it is again switch), Namenode expects this deleted blocks also, as delete request is not persisted into editlog before.

   I reproduced this scenario with bedug points. *I feel, We should not add the blocks to invalidates before persisting into Editlog*. 

    Note: for switch, we used kill -9 (force kill)

  I am currently in 0.20.2 version. Same verified in 0.23 as well in normal crash + restart  scenario.
 
",0
"When the NN processes mis-replicated blocks while exiting safemode, it considers under-construction blocks as under-replicated, inserting them into the neededReplicationsQueue. This makes them show up as corrupt in the metrics and UI momentarily, until they're pulled off the queue. At that point, it realizes that they aren't in fact under-replicated, correctly. This affects both the HA branch and trunk/23, best I can tell.",0
"When i execute the following operations and wait for checkpoint to complete.

fs.mkdirs(new Path(""/test1""));
FSDataOutputStream create = fs.create(new Path(""/test/abc.txt"")); //dont close
fs.rename(new Path(""/test/""), new Path(""/test1/""));

Check-pointing is failing with the following exception.

2012-01-23 15:03:14,204 ERROR namenode.FSImage (FSImage.java:run(795)) - Unable to save image for E:\HDFS-1623\hadoop-hdfs-project\hadoop-hdfs\build\test\data\dfs\name3
java.io.IOException: saveLeases found path /test1/est/abc.txt but no matching entry in namespace.[/test1/est/abc.txt]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveFilesUnderConstruction(FSNamesystem.java:4336)
	at org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Saver.save(FSImageFormat.java:588)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.saveFSImage(FSImage.java:761)
	at org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.run(FSImage.java:789)
	at java.lang.Thread.run(Unknown Source)",0
"Though HAServiceProtocol is indeed annotated with the @KerberosInfo annotation, it specifies the Common server principal, which is intended to be overridden at run-time with the value of the correct principal (e.g. as is done in MRAdmin and DFSAdmin.) We need to do something similar for HAAdmin.",0
The test is timing out after 45 seconds. It's also failed in the last two nightly builds.,0
"I met a situation, where DataStreamer#processDatanodeError throws OOME when creating ResponseProcessor thread. 
Due to this Datastreamer thread died. When clinet closing the stream, it keeps waiting.

Looks this is because, when clinet closes, it will enque one packet by marking that a lastpacket and wait for the ack. Here Datastreamer thread died and no one is there for processsing the packet from dataqueue. Obviously will not get any ack and it will keep wait in close.

This i have seen in 20.2 version. when i verified, this problem will not be there in trunk as processDatanodeError already guarded with try/catch. This problem can be there in branch-1 ",0
"Since we send the block tokens unencrypted to the datanode, we currently start the datanode as root using jsvc and get a secure (< 1024) port.
If we have the datanode generate a nonce and send it on the connection and the sends an hmac of the nonce back instead of the block token it won't reveal any secrets. Thus, we wouldn't require a secure port and would not require root or jsvc.",0
"In an HA setup I was running for the past week, I just noticed that checkpoints weren't getting properly uploaded, since the SBN was connecting to http://0.0.0.0:50070/ rather than the correct dfs.http.address. So, it was uploading checkpoints to itself instead of the peer. We should add sanity checks during startup to ensure that the configuration is correct.",0
"If same edits directory is configured in twice, both are treated independently. Edit log roll is called on the same directory twice causing exceptions.",0
"In {{Storage.tryLock()}}, we call {{lockF.deleteOnExit()}} regardless of whether we successfully lock the directory. So, if another NN has the directory locked, then we'll fail to lock it the first time we start another NN. But our failed start attempt will still remove the other NN's lockfile, and a second attempt will erroneously start.",0
"I started a DN on a machine that was completely out of space on one of its drives. I saw the following:

2012-02-02 09:56:50,499 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for block pool Block pool BP-448349972-172.29.5.192-1323816762969 (storage id DS-507718931-172.29.5.194-11072-12978
42002148) service to styx01.sf.cloudera.com/172.29.5.192:8021
java.io.IOException: Mkdirs failed to create /data/1/scratch/todd/styx-datadir/current/BP-448349972-172.29.5.192-1323816762969/tmp
        at org.apache.hadoop.hdfs.server.datanode.FSDataset$BlockPoolSlice.<init>(FSDataset.java:335)

but the DN continued to run, spewing NPEs when it tried to do block reports, etc. This was on the HDFS-1623 branch but may affect trunk as well.",0
"In one of my clusters, observed this situation.
This issue looks to be due to time out in ResponseProcesser at client side, it is marking first DataNode as bad.
This happens in 20.2 version. This can be there in branch-1 as well and will check for trunk.",0
"The SecondaryNameNode should log a message and refuse to start if HA is enabled since the StandbyNode checkpoints by default and IIRC we have not yet enabled the ability to have multiple checkpointers in the NN.
On the HA branch the 2NN does not currently start from start-dfs.sh because getconf -secondarynamenodes claims the http-address is not configured even though it is, though this seems like a bug, in branch 23 getconf will correctly return localhost:50090.
 <property>
   <name>dfs.namenode.secondary.http-address</name>
   <value>localhost:50090</value>
 </property>

</configuration>
hadoop-0.24.0-SNAPSHOT $ ./bin/hdfs getconf -secondarynamenodes
Incorrect configuration: secondary namenode address dfs.namenode.seconda",0
"When shared edits dir is bounced, standby NN is put into safemode by the NameNodeResourceMonitor(). However, there is no path for it to exit out of safe mode when shared edits dir reappears.",0
"If I call ""bin/hdfs fetchdt /tmp/mytoken"" without a ""--renewer foo"" argument, then it will throw a NullPointerException:

Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getDelegationToken(ClientNamenodeProtocolTranslatorPB.java:830)

this is because getDelegationToken is being called with a null renewer",0
"If there are multiple blocks to be recovered, it ends up translating to N copies of the first block instead of the N different blocks.",0
"When we rename the file with overwrite flag as true, it will delete the destination file blocks. After deleting the blocks, whenever it releases the fsNameSystem lock, NN can give the invalidation work to corresponding DNs to delete the blocks.
Parallaly it will sync the rename related edits to editlog file. At this step before NN sync the edits if NN crashes, NN can stuck into safemode on restart. This is because block already deleted from the DN as part of invalidations. But dst file still exist as rename edits not persisted in log file and no DN will report that blocks now.

This is similar to HDFS-2815
 ",0
"In doing scale testing of trunk at r1291606, I hit the following:

java.io.IOException: Error replaying edit log at offset 1354251
Recent opcode offsets: 1350014 1350176 1350312 1354251
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:418)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:93)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:79)
...
Caused by: java.lang.ClassCastException: org.apache.hadoop.hdfs.server.namenode.INodeFile cannot be cast to org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:213)
        ... 13 more
",0
"I saw the following logs on my test cluster:
{code}
2012-02-22 14:35:22,887 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: startFile: recover lease [Lease.  Holder: DFSClient_attempt_1329943893604_0007_m_000376_0_453973131_1, pendingcreates: 1], src=/benchmarks/TestDFSIO/io_data/test_io_6 from client DFSClient_attempt_1329943893604_0007_m_000376_0_453973131_1
2012-02-22 14:35:22,887 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Recovering lease=[Lease.  Holder: DFSClient_attempt_1329943893604_0007_m_000376_0_453973131_1, pendingcreates: 1], src=/benchmarks/TestDFSIO/io_data/test_io_6
2012-02-22 14:35:22,888 WARN org.apache.hadoop.hdfs.StateChange: BLOCK* internalReleaseLease: All existing blocks are COMPLETE, lease removed, file closed.
2012-02-22 14:35:22,888 WARN org.apache.hadoop.hdfs.StateChange: DIR* FSDirectory.replaceNode: failed to remove /benchmarks/TestDFSIO/io_data/test_io_6
2012-02-22 14:35:22,888 WARN org.apache.hadoop.hdfs.StateChange: DIR* NameSystem.startFile: FSDirectory.replaceNode: failed to remove /benchmarks/TestDFSIO/io_data/test_io_6
{code}
It seems like, if {{recoverLeaseInternal}} succeeds in {{startFileInternal}}, then the INode will be replaced with a new one, meaning the later {{replaceNode}} call can fail.",0
"With a valid hdfs kerberos ticket, the dfsadmin subcommand '-refreshServiceAcl' still fails on Kerb authentication. Please see the comment for more details.
",0
"Saw this in [build #1888|https://builds.apache.org/job/PreCommit-HDFS-Build/1888//testReport/org.apache.hadoop.hdfs.server.datanode/TestMulitipleNNDataBlockScanner/testBlockScannerAfterRestart/].
{noformat}
java.util.ConcurrentModificationException
	at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
	at java.util.HashMap$EntryIterator.next(HashMap.java:834)
	at java.util.HashMap$EntryIterator.next(HashMap.java:832)
	at org.apache.hadoop.hdfs.server.datanode.FSDataset$FSVolume.getDfsUsed(FSDataset.java:557)
	at org.apache.hadoop.hdfs.server.datanode.FSDataset$FSVolumeSet.getDfsUsed(FSDataset.java:809)
	at org.apache.hadoop.hdfs.server.datanode.FSDataset$FSVolumeSet.access$1400(FSDataset.java:774)
	at org.apache.hadoop.hdfs.server.datanode.FSDataset.getDfsUsed(FSDataset.java:1124)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.sendHeartBeat(BPOfferService.java:406)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.offerService(BPOfferService.java:490)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.run(BPOfferService.java:635)
	at java.lang.Thread.run(Thread.java:662)
{noformat}
",0
"The SETOWNER call returns an empty body. But the header has ""Content-Type: application/json"", which is a contradiction (empty string is not valid json). This appears to happen for SETTIMES and SETPERMISSION as well.",0
"HDFS-1112 added a feature whereby the edit log automatically calls logSync() if the buffered data crosses a threshold. However, the code checks {{bufReady.size()}} rather than {{bufCurrent.size()}} -- which is incorrect since the writes themselves go into {{bufCurrent}}.",0
This JIRA is to address a TODO in NameNode about handling the possibility of an incomplete HA state transition.,0
"I executed section 3.4 of Todd's HA test plan. https://issues.apache.org/jira/browse/HDFS-1623
1. A large file upload is started.
2. While the file is being uploaded, the administrator kills the first NN and performs a failover.
3. After the file finishes being uploaded, it is verified for correct length and contents.

For the test, I have a vm_template styx01:/home/schu/centos64-2-5.5.qcow2. styx01 hosted the active NN and styx02 hosted the standby NN.

In the log files I attached, you can see that on styx01 I began file upload.
hadoop fs -put centos64-2.5.5.qcow2

After waiting several seconds, I kill -9'd the active NN on styx01 and manually failed over to the NN on styx02. I ran into exception below. (rest of the stacktrace in the attached file styx01_uploadLargeFile)

12/02/29 14:12:52 WARN retry.RetryInvocationHandler: A failover has occurred since the start of this method invocation attempt.
put: Failed on local exception: java.io.EOFException; Host Details : local host is: ""styx01.sf.cloudera.com/172.29.5.192""; destination host is: """"styx01.sf.cloudera.com""\
:12020;
12/02/29 14:12:52 ERROR hdfs.DFSClient: Failed to close file /user/schu/centos64-2-5.5.qcow2._COPYING_
java.io.IOException: Failed on local exception: java.io.EOFException; Host Details : local host is: ""styx01.sf.cloudera.com/172.29.5.192""; destination host is: """"styx01.\
sf.cloudera.com"":12020;
        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
        at org.apache.hadoop.ipc.Client.call(Client.java:1145)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:188)
        at $Proxy9.addBlock(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:302)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:164)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:83)
        at $Proxy10.addBlock(Unknown Source)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1097)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:973)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:455)
Caused by: java.io.EOFException
        at java.io.DataInputStream.readInt(DataInputStream.java:375)
        at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:830)
        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:762)",0
"When LeaseRenewer gets an IOException while attempting to renew for a client, it retries after sleeping 500ms. If the exception is caused by a condition that will never change, it keeps talking to the name node until the DFSClient object is closed or aborted.  With the FileSystem cache, a DFSClient can stay alive for very long time. We've seen the cases in which node managers and long living jobs flooding name node with this type of calls.

The current proposal is to abort the client when RemoteException is caught during renewal. LeaseRenewer already does abort on all clients when it sees a SocketTimeoutException.",0
"TestHFlush.testHFlushInterrupted can fail occasionally due to a race: if a thread is interrupted while calling close(), then the {{finally}} clause of the {{close}} function sets {{closed = true}}. At this point it has enqueued the ""end of block"" packet to the DNs, but hasn't called {{completeFile}}. Then, if {{close}} is called again (as in the test case), it will be short-circuited since {{closed}} is already true. Thus {{completeFile}} never ends up getting called. This also means that the test can fail if the pipeline is running slowly, since the assertion that the file is the correct length won't see the last packet or two.",0
"was the umbrella task for implementation of NN HA capabilities. However, it only focused on manually-triggered failover.
Given that the HDFS-1623 branch will be merged shortly, I'm opening this JIRA to consolidate/track subtasks for automatic failover support and related improvements.",0
"When HDFS-2185 is implemented, ZooKeeper can be used to locate the active NameNode. We can use this from the DFS client in order to connect to the correct NN without having to configure a list of possibly-active NNs.",0
"NamenodeFsck#lostFoundMove, when it fails to create a file for a block continues on to the next block (There's a comment ""perhaps we should bail out here..."" but it doesn't). It should instead fail the move for that particular file (unwind the directory creation and not delete the original file). Otherwise a transient failure speaking to the NN means this block is lost forever.",0
"There's a small race in BlockManager#close, we close the BlocksMap before the replication monitor, which means the replication monitor can NPE if it tries to access the blocks map. We need to swap the order (close the blocks map after shutting down the repl monitor).",0
"During the NameNode startup process, we load an image, and then apply edit logs to it until we believe that we have all the latest changes. Unfortunately, if there is an I/O error while reading any of these files, in most cases, we simply abort the startup process. We should try harder to locate a readable edit log and/or image file.
There are three main use cases for this feature:
1. If the operating system does not honor fsync (usually due to a misconfiguration), a file may end up in an inconsistent state.
2. In certain older releases where we did not use fallocate() or similar to pre-reserve blocks, a disk full condition may cause a truncated log in one edit directory.
3. There may be a bug in HDFS which results in some of the data directories receiving corrupt data, but not all. This is the least likely use case.
Proposed changes to normal NN startup
We should try a different FSImage if we can't load the first one we try.
We should examine other FSEditLogs if we can't load the first one(s) we try.
We should fail if we can't find EditLogs that would bring us up to what we believe is the latest transaction ID.
Proposed changes to recovery mode NN startup:
we should list out all the available storage directories and allow the operator to select which one he wants to use.",0
"Using distcp with '-skipcrccheck' still seems to cause CRC checksums to happen. 

Ran into this while debugging an issue associated with source and destination having different blocksizes, and not using the preserve blocksize parameter (-pb). In both 23.1 and 23.2 builds, trying to bypass the checksum verification by using the '-skipcrcrcheck' parameter had no effect, the distcp still failed on checksum errors.

Test scenario to reproduce;
do not use '-pb' and try a distcp from 20.205 (default blksize=128M) to .23 (default blksize=256M), the distcp fails on checksum errors, which is expected due to checksum calculation (tiered aggregation of all blks). Trying the same distcp only providing '-skipcrccheck' still fails with the same checksum error, it is expected that checksum would now be bypassed and the distcp would proceed.
",0
"When testing the combination of NN HA + security + yarn, I found that the mapred job submission cannot pick up the logic URI of a nameservice. 

I have logic URI configured in core-site.xml
{code}
<property>
 <name>fs.defaultFS</name>
 <value>hdfs://ns1</value>
</property>
{code}

HDFS client can work with the HA deployment/configs:
{code}
[root@nn1 hadoop]# hdfs dfs -ls /
Found 6 items
drwxr-xr-x   - hbase  hadoop          0 2012-03-07 20:42 /hbase
drwxrwxrwx   - yarn   hadoop          0 2012-03-07 20:42 /logs
drwxr-xr-x   - mapred hadoop          0 2012-03-07 20:42 /mapred
drwxr-xr-x   - mapred hadoop          0 2012-03-07 20:42 /mr-history
drwxrwxrwt   - hdfs   hadoop          0 2012-03-07 21:57 /tmp
drwxr-xr-x   - hdfs   hadoop          0 2012-03-07 20:42 /user
{code}

but cannot submit a mapred job with security turned on
{code}
[root@nn1 hadoop]# /usr/lib/hadoop/bin/yarn --config ./conf jar share/hadoop/mapreduce/hadoop-mapreduce-examples-0.24.0-SNAPSHOT.jar randomwriter out
Running 0 maps.
Job started: Wed Mar 07 23:28:23 UTC 2012
java.lang.IllegalArgumentException: java.net.UnknownHostException: ns1
	at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:431)
	at org.apache.hadoop.security.SecurityUtil.buildDTServiceName(SecurityUtil.java:312)
	at org.apache.hadoop.fs.FileSystem.getCanonicalServiceName(FileSystem.java:217)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:119)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:97)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:80)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:137)
	at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:411)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:326)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1221)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1218)
....
{code}0.24",0
"With a singly-replicated block that's corrupted, issuing a read against it twice in succession (e.g. if ChecksumException is caught by the client) gives a NullPointerException.

Here's the body of a test that reproduces the problem:

{code}

    final short REPL_FACTOR = 1;
    final long FILE_LENGTH = 512L;
    cluster.waitActive();
    FileSystem fs = cluster.getFileSystem();

    Path path = new Path(""/corrupted"");

    DFSTestUtil.createFile(fs, path, FILE_LENGTH, REPL_FACTOR, 12345L);
    DFSTestUtil.waitReplication(fs, path, REPL_FACTOR);

    ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, path);
    int blockFilesCorrupted = cluster.corruptBlockOnDataNodes(block);
    assertEquals(""All replicas not corrupted"", REPL_FACTOR, blockFilesCorrupted);

    InetSocketAddress nnAddr =
        new InetSocketAddress(""localhost"", cluster.getNameNodePort());
    DFSClient client = new DFSClient(nnAddr, conf);
    DFSInputStream dis = client.open(path.toString());
    byte[] arr = new byte[(int)FILE_LENGTH];
    boolean sawException = false;
    try {
      dis.read(arr, 0, (int)FILE_LENGTH);
    } catch (ChecksumException ex) {     
      sawException = true;
    }
    
    assertTrue(sawException);
    sawException = false;
    try {
      dis.read(arr, 0, (int)FILE_LENGTH); // <-- NPE thrown here
    } catch (ChecksumException ex) {     
      sawException = true;
    } 
{code}

The stack:

{code}
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:492)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:545)
        [snip test stack]
{code}

and the problem is that currentNode is null. It's left at null after the first read, which fails, and then is never refreshed because the condition in read that protects blockSeekTo is only triggered if the current position is outside the block's range. 
",0
"In testing what happens in HA split brain scenarios, I ended up with an edits log that was named edits_47-47 but actually had two edits in it (#47 and #48). The edits loading process should detect this situation and barf. Otherwise, the problem shows up later during loading or even on the next restart, and is tough to fix.",0
"Currently, one of the weak points of the HA design is that it relies on shared storage such as an NFS filer for the shared edit log. One alternative that has been proposed is to depend on BookKeeper, a ZooKeeper subproject which provides a highly available replicated edit log on commodity hardware. This JIRA is to implement another alternative, based on a quorum commit protocol, integrated more tightly in HDFS and with the requirements driven only by HDFS's needs rather than more generic use cases. More details to follow.",0
"Steps to reproduce:
- turned on ha and security
- run a mapred job, and wait to finish
- failover to another namenode
- run the mapred job again, it fails. ",0
"If a data node is added to the exclude list and the name node is restarted, the decomissioning happens right away on the data node registration. At this point the initial block report has not been sent, so the name node thinks the node has zero blocks and the decomissioning completes very quick, without replicating the blocks on that node.",0
"STEP锛?1, deploy a single node hdfs  0.23.1 cluster and configure hdfs as:
A) enable webhdfs
B) enable append
C) disable permissions
2, start hdfs
3, run the test script as attached

RESULT:
expected: a file named testFile should be created and populated with 32K * 5000 zeros, HDFS should be OK.
I got: script cannot be finished, file has been created but not be populated as expected, actually append operation failed.

Datanode log shows that, blockscaner report a bad replica and nanenode decide to delete it. Since it is a single node cluster, append fail. It makes no sense that the script failed every time.

Datanode and Namenode logs are attached.",0
httpfs does not support calls to get/renew tokens nor delegation token authentication.,0
"*Block Report* can *race* with *Block Recovery* with closeFile flag true.

 Block report generated just before block recovery at DN side and due to N/W problems, block report got delayed to NN. 
After this, recovery success and generation stamp modifies to new one. 
And primary DN invokes the commitBlockSynchronization and block got updated in NN side. Also block got marked as complete, since the closeFile flag was true. Updated with new genstamp.

Now blockReport started processing at NN side. This particular block from RBW (when it generated the BR at DN), and file was completed at NN side.
Finally block will be marked as corrupt because of genstamp mismatch.

{code}
 case RWR:
      if (!storedBlock.isComplete()) {
        return null; // not corrupt
      } else if (storedBlock.getGenerationStamp() != iblk.getGenerationStamp()) {
        return new BlockToMarkCorrupt(storedBlock,
            ""reported "" + reportedState + "" replica with genstamp "" +
            iblk.getGenerationStamp() + "" does not match COMPLETE block's "" +
            ""genstamp in block map "" + storedBlock.getGenerationStamp());
      } else { // COMPLETE block, same genstamp
{code}




",0
"Scenario 1
==========
Start NN and BNN
stop NN and BNN
Format NN and start only BNN

Then BNN as getting Nullpointer and getting shutdown 
{noformat}
12/03/20 21:26:05 ERROR ipc.RPC: Tried to call RPC.stopProxy on an object that is not a proxy.
java.lang.IllegalArgumentException: not a proxy instance
	at java.lang.reflect.Proxy.getInvocationHandler(Proxy.java:637)
	at org.apache.hadoop.ipc.RPC.stopProxy(RPC.java:591)
	at org.apache.hadoop.hdfs.server.namenode.BackupNode.stop(BackupNode.java:194)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:547)
	at org.apache.hadoop.hdfs.server.namenode.BackupNode.<init>(BackupNode.java:86)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:847)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:908)
12/03/20 21:26:05 ERROR ipc.RPC: Could not get invocation handler null for proxy class class org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB, or invocation handler is not closeable.
12/03/20 21:26:05 ERROR namenode.NameNode: Exception in namenode join
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.namenode.NameNode.getFSImage(NameNode.java:609)
	at org.apache.hadoop.hdfs.server.namenode.BackupNode.stop(BackupNode.java:205)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:547)
	at org.apache.hadoop.hdfs.server.namenode.BackupNode.<init>(BackupNode.java:86)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:847)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:908)
12/03/20 21:26:05 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at HOST-10-18-40-233/10.18.40.233
************************************************************/
{noformat}",0
"When a restore fails, rollEditLog() also fails even if there are healthy directories. Any exceptions from recovering the removed directories should not fail checkpoint process.",0
"Currently, the edit log loader does not handle bad or malicious input sensibly.
We can often cause OutOfMemory exceptions, null pointer exceptions, or other unchecked exceptions to be thrown by feeding the edit log loader bad input. In some environments, an out of memory error can cause the JVM process to be terminated.
It's clear that we want these exceptions to be thrown as IOException instead of as unchecked exceptions. We also want to avoid out of memory situations.
The main task here is to put a sensible upper limit on the lengths of arrays and strings we allocate on command. The other task is to try to avoid creating unchecked exceptions (by dereferencing potentially-NULL pointers, for example). Instead, we should verify ahead of time and give a more sensible error message that reflects the problem with the input.",0
"For HA, a logical name is visible in URIs - add an explicit logical name
",0
"Cluster setup:

1NN,Three DN(DN1,DN2,DN3),replication factor-2,""dfs.blockreport.intervalMsec"" 300,""dfs.datanode.directoryscan.interval"" 1

step 1: write one file ""a.txt"" with sync(not closed)
step 2: Delete the blocks in one of the datanode say DN1(from rbw) to which replication happened.
step 3: close the file.

Since the replication factor is 2 the blocks are replicated to the other datanode.

Then at the NN side the following cmd is issued to DN from which the block is deleted
-------------------------------------------------------------------------------------
{noformat}
2012-03-19 13:41:36,905 INFO org.apache.hadoop.hdfs.StateChange: BLOCK NameSystem.addToCorruptReplicasMap: duplicate requested for blk_2903555284838653156 to add as corrupt on XX.XX.XX.XX by /XX.XX.XX.XX because reported RBW replica with genstamp 1002 does not match COMPLETE block's genstamp in block map 1003
2012-03-19 13:41:39,588 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* Removing block blk_2903555284838653156_1003 from neededReplications as it has enough replicas.
{noformat}

From the datanode side in which the block is deleted the following exception occured


{noformat}
2012-02-29 13:54:13,126 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected error trying to delete block blk_2903555284838653156_1003. BlockInfo not found in volumeMap.
2012-02-29 13:54:13,126 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Error processing datanode Command
java.io.IOException: Error in deleting blocks.
	at org.apache.hadoop.hdfs.server.datanode.FSDataset.invalidate(FSDataset.java:2061)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActive(BPOfferService.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActor(BPOfferService.java:545)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.processCommand(BPServiceActor.java:690)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:522)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:662)
	at java.lang.Thread.run(Thread.java:619)
{noformat}",0
"1) DN1->DN2->DN3 are in pipeline.
2) Client killed abruptly
3) one DN has restarted , say DN3
4) In DN3 info.wasRecoveredOnStartup() will be true
5) NN recovery triggered, DN3 skipped from recovery due to above check.
6) Now DN1, DN2 has blocks with generataion stamp 2 and DN3 has older generation stamp say 1 and also DN3 still has this block entry in ongoingCreates
7) as part of recovery file has closed and got only two live replicas ( from DN1 and DN2)
8) So, NN issued the command for replication. Now DN3 also has the replica with newer generation stamp.
9) Now DN3 contains 2 replicas on disk. and one entry in ongoing creates with referring to blocksBeingWritten directory.

When we call append/ leaseRecovery, it may again skip this node for that recovery as blockId entry still presents in ongoingCreates with startup recovery true.
It may keep continue this dance for evry recovery.
And this stale replica will not be cleaned untill we restart the cluster. Actual replica will be trasferred to this node only through replication process.

Also unnecessarily that replicated blocks will get invalidated after next recoveries....
",0
"Hftp connections do not have read timeouts.  This leads to indefinitely hung sockets when there is a network outage during which time the remote host closed the socket.

This may also affect WebHdfs, etc.",0
WebHDFS connections may indefinitely hang due to no timeouts on the connection.  WebHDFS should be adapted in a similar fashion to HDFS-3166 for hftp.,0
"org.apache.hadoop.hdfs.TestLeaseRecovery2.testHardLeaseRecoveryAfterNameNodeRestart seems to be failing intermittently on jenkins.

{code}
org.apache.hadoop.hdfs.TestLeaseRecovery2.testHardLeaseRecoveryAfterNameNodeRestart
Failing for the past 1 build (Since Failed#2163 )
Took 8.4 sec.
Error Message

Lease mismatch on /hardLeaseRecovery owned by HDFS_NameNode but is accessed by DFSClient_NONMAPREDUCE_1147689755_1  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2076)  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2051)  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalDatanode(FSNamesystem.java:1983)  at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getAdditionalDatanode(NameNodeRpcServer.java:492)  at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getAdditionalDatanode(ClientNamenodeProtocolServerSideTranslatorPB.java:311)  at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:42604)  at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:417)  at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:891)  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1661)  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1657)  at java.security.AccessController.doPrivileged(Native Method)  at javax.security.auth.Subject.doAs(Subject.java:396)  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1205)  at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1655) 

Stacktrace

org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: Lease mismatch on /hardLeaseRecovery owned by HDFS_NameNode but is accessed by DFSClient_NONMAPREDUCE_1147689755_1
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2076)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2051)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalDatanode(FSNamesystem.java:1983)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getAdditionalDatanode(NameNodeRpcServer.java:492)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getAdditionalDatanode(ClientNamenodeProtocolServerSideTranslatorPB.java:311)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:42604)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:417)
        ...
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:83)
	at $Proxy15.getAdditionalDatanode(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getAdditionalDatanode(ClientNamenodeProtocolTranslatorPB.java:317)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:828)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:930)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:741)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:416)
{code}",0
"If an administrator wants to ignore fencing (perhaps because he knows that the other namenode has been taken out of comission for sure, but the fencing is ssh-based), there should be a flag to the failover command to allow this.",0
"The initReplicaRecovery function may return null to indicate that the block doesn't exist on the local node. However, the translator doesn't handle this case, which results in NPEs.",0
"I have seen one situation with Hbase cluster.

Scenario is as follows:

1)1.5 blocks has been written and synced.

2)Suddenly cluster has been restarted.

Reader opened the file and trying to get the length., By this time partial block contained DNs are not reported to NN. So, locations for this partial block would be 0. In this case, DFSInputStream assumes that, 1 block size as final size.

But reader also assuming that, 1 block size is the final length and setting his end marker. Finally reader ending up reading only partial data. Due to this, HMaster could not replay the complete edits. 

Actually this happend with 20 version. Looking at the code, same should present in trunk as well.


{code}
    int replicaNotFoundCount = locatedblock.getLocations().length;
    
    for(DatanodeInfo datanode : locatedblock.getLocations()) {
..........
..........
 // Namenode told us about these locations, but none know about the replica
    // means that we hit the race between pipeline creation start and end.
    // we require all 3 because some other exception could have happened
    // on a DN that has it.  we want to report that error
    if (replicaNotFoundCount == 0) {
      return 0;
    }

{code}",0
"{{fs.getCanonicalService()}} must be equal to {{fs.getDelegationToken(renewer).getService()}}.  When HA is enabled, the DFS token's service is a logical uri, but {{dfs.getCanonicalService()}} is only returning the hostname of the logical uri.",0
"HDFS treats the mere presence of a topology script being configured as evidence that there are multiple racks. If there is in fact only a single rack, the NN will try to place the blocks on at least two racks, and thus blocks will be considered to be under-replicated.",0
"An unhealthy storage directory and its edit stream can be removed from editStreams list. There are multiple places to remove unhealthy directory and its stream, such as log sync, checkpointing.

Method removeEditsStreamsAndStorageDirs() could unneceissarily invoke fatalExit() if another thread removes one error editStream before current thread. This race could be between logSync requests, or between logSync and any place where a storage directory may be removed, e.g., during checkpointing. ",0
"The {{Hdfs AbstractFileSystem}} is overwriting the token service set by the {{DFSClient}}.  The service is not necessarily the correct one since {{DFSClient}} is responsible for the service.  Most importantly, this improper behavior is overwriting the HA logical service which indirectly renders {{FileContext}} incompatible with HA.",0
"To reproduce:
# Configure a NameNode with namedirs and a shared edits dir, all of which are empty.
# Run hdfs namenode -format. Namedirs and shared edits dir gets populated.
# Delete the contents of the namedirs. Leave the shared edits dir as is. Check the timestamps of the shared edits dir contents.
# Run format again. The namedirs as well as the shared edits dir get formatted. The shared edits dir's contents have been replaced without any prompting.",0
"Most users store multiple copies of the FSImage in order to prevent catastrophic data loss if a hard disk fails.  However, our image loading code is currently not set up to start reading another FSImage if loading the first one does not succeed.  We should add this capability.

We should also be sure to remove the FSImage directory that failed from the list of FSImage directories to write to, in the way we normally do when a write (as opopsed to read) fails.",0
"Token acquisition fails if a hftp or webhdfs filesystem is obtained with no port in the authority.  Building a token service requires a port, and the renewer needs the port.  The default port is not being used when there is no port in the uri.",0
"HttpFs supports only the permissions: [0-7][0-7][0-7]

In order to be compatible with webhdfs in needs to understand octal and sticky bit permissions (e.g. 0777, 01777...)

Example of error:
curl -L -X PUT ""http://localhost:14000/webhdfs/v1/user/romain/test?permission=01777&op=SETPERMISSION&user.name=romain"" 
{""RemoteException"":{""message"":""java.lang.IllegalArgumentException: Parameter [permission], invalid value [01777], value must be [default|(-[-r][-w][-x][-r][-w][-x][-r][-w][-x])|[0-7][0-7][0-7]]"",""exception"":""QueryParamException"",""javaClassName"":""com.sun.jersey.api.ParamException$QueryParamException""}}

Works with WebHdfs:
curl -L -X PUT ""http://localhost:50070/webhdfs/v1/user/romain/test?permission=01777&op=SETPERMISSION&user.name=romain"" 
echo $?
0



curl -L -X PUT ""http://localhost:14000/webhdfs/v1/user/romain/test?permission=999999&op=SETPERMISSION&user.name=romain"" 
{""RemoteException"":{""message"":""java.lang.IllegalArgumentException: Parameter [permission], invalid value [999999], value must be [default|(-[-r][-w][-x][-r][-w][-x][-r][-w][-x])|[0-7][0-7][0-7]]"",""exception"":""QueryParamException"",""javaClassName"":""com.sun.jersey.api.ParamException$QueryParamException""}}",0
We should make sure to abort when there are no edit log directories left to write to.  It seems that there is at least one case that is slipping through the cracks right now in branch-1.,0
"Hftp tries to select a token based on the non-secure port in the uri, instead of the secure-port.  This breaks hftp on a secure cluster and there is no workaround.",0
"Hftp transfers >2GB hang after the transfer is complete.  The problem appears to be caused by java internally using an int for the content length.  When it overflows 2GB, it won't check the bounds of the reads on the input stream.  The client continues reading after all data is received, and the client blocks until the server times out the connection -- _many_ minutes later.  In conjunction with hftp timeouts, all transfers >2G fail with a read timeout.",0
"Currently in GetImageServlet, we catch Exception but not other Errors or RTEs. So, if the code ends up throwing one of these exceptions, the ""response.sendError()"" code doesn't run, but the finally clause does run. This results in the servlet returning HTTP 200 OK and an empty response, which causes the client to think it got a successful image transfer.",0
"Even after encountering an OP_INVALID, we should check the end of the edit log to make sure that it contains no more edits.
This will catch things like rare race conditions or log corruptions that would otherwise remain undetected. They will got from being silent data loss scenarios to being cases that we can detect and fix.
Using recovery mode, we can choose to ignore the end of the log if necessary.",0
"NameNode#initializeGenericKeys exits early if neither a nameservice nor NN ID is passed. However, this method also serves to set fs.defaultFS in the configuration object stored by the NN to the NN RPC address after generic keys have been configured. This should be done in all cases.",0
"In DataXceiver, we currently use Socket.setSoTimeout to try to manage the read timeout when switching between reading the initial opCode, reading a keepalive opcode, and reading the status after a successfully sent block. However, since all of these reads use the same underlying DataInputStream, the change to the socket timeout isn't respected. Thus, they all occur with whatever timeout is set on the socket at the time of DataXceiver construction. In practice this turns out to be 0, which can cause infinitely hung xceivers.",0
"Configured ""dfs.namenode.checkpoint.dir"" and ""dfs.namenode.checkpoint.edits.dir"" to more than 6 comma seperated directories 
 Started NN,DN,SNN
 Secondary Namenode gets shutdown without throwing any exception

But the descriptions says that ""If this is a comma-delimited list of directories then the image is
replicated in all of the directories for redundancy.""

SNN logs
========
{noformat}2012-04-26 13:08:37,534 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = HOST-xx-xx-xx-xx/xx.xx.xx.xx
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.0.0-SNAPSHOT
STARTUP_MSG:   build =  -r ; compiled by 'isap' on Fri Apr 20 09:10:53 IST 2012
************************************************************/
2012-04-26 13:08:38,728 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2012-04-26 13:08:38,861 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2012-04-26 13:08:38,861 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2012-04-26 13:08:39,176 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at HOST-xx-xx-xx-xx/xx.xx.xx.xx
************************************************************/{noformat}

",0
"Something along the lines of
UserGroupInformation.loginUserFromKeytab(<blah blah>)
Filesystem fs = FileSystem.get(new URI(""webhdfs://blah""), conf)
doesn't work as webhdfs doesn't use the correct context and the user shows up to the spnego filter without kerberos credentials:
Exception in thread ""main"" java.io.IOException: Authentication failed, url=http://<NN>:50070/webhdfs/v1/?op=GETDELEGATIONTOKEN&user.name=<USER>
 at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.getHttpUrlConnection(WebHdfsFileSystem.java:337)
 at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.httpConnect(WebHdfsFileSystem.java:347)
 at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.run(WebHdfsFileSystem.java:403)
 at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.getDelegationToken(WebHdfsFileSystem.java:675)
 at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.initDelegationToken(WebHdfsFileSystem.java:176)
 at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.initialize(WebHdfsFileSystem.java:160)
 at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1386)
...
Caused by: org.apache.hadoop.security.authentication.client.AuthenticationException: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)
 at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.doSpnegoSequence(KerberosAuthenticator.java:232)
 at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.authenticate(KerberosAuthenticator.java:141)
 at org.apache.hadoop.security.authentication.client.AuthenticatedURL.openConnection(AuthenticatedURL.java:217)
 at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.getHttpUrlConnection(WebHdfsFileSystem.java:332)
 ... 16 more
Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)
 at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:130)
...
Explicitly getting the current user's context via a doAs block works, but this should be done by webhdfs. ",0
All replicas of a block can be removed if bad DataNodes come up and down during cluster restart resulting in data loss.,0
"The testcase is failing because the MiniDFSCluster is shutdown before the secret manager can change the key, which calls system.exit with no edit streams available.

{code}

    [junit] 2012-05-04 15:03:51,521 WARN  common.Storage (FSImage.java:updateRemovedDirs(224)) - Removing storage dir /home/horton/src/hadoop/build/test/data/dfs/name1
    [junit] 2012-05-04 15:03:51,522 FATAL namenode.FSNamesystem (FSEditLog.java:fatalExit(388)) - No edit streams are accessible
    [junit] java.lang.Exception: No edit streams are accessible
    [junit]     at org.apache.hadoop.hdfs.server.namenode.FSEditLog.fatalExit(FSEditLog.java:388)
    [junit]     at org.apache.hadoop.hdfs.server.namenode.FSEditLog.exitIfNoStreams(FSEditLog.java:407)
    [junit]     at org.apache.hadoop.hdfs.server.namenode.FSEditLog.removeEditsAndStorageDir(FSEditLog.java:432)
    [junit]     at org.apache.hadoop.hdfs.server.namenode.FSEditLog.removeEditsStreamsAndStorageDirs(FSEditLog.java:468)
    [junit]     at org.apache.hadoop.hdfs.server.namenode.FSEditLog.logSync(FSEditLog.java:1028)
    [junit]     at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.logUpdateMasterKey(FSNamesystem.java:5641)
    [junit]     at org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager.logUpdateMasterKey(DelegationTokenSecretManager.java:286)
    [junit]     at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.updateCurrentKey(AbstractDelegationTokenSecretManager.java:150)
    [junit]     at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.rollMasterKey(AbstractDelegationTokenSecretManager.java:174)
    [junit]     at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover.run(AbstractDelegationTokenSecretManager.java:385)
    [junit]     at java.lang.Thread.run(Thread.java:662)
    [junit] Running org.apache.hadoop.hdfs.security.TestDelegationToken
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] Test org.apache.hadoop.hdfs.security.TestDelegationToken FAILED (crashed)
{code}",0
"After fixing the datanode side of keepalive to properly disconnect stale clients, (HDFS-3357), the client side has the following issue: when it connects to a DN, it first tries to use cached sockets, and will try a configurable number of sockets from the cache. If there are more cached sockets than the configured number of retries, and all of them have been closed by the datanode side, then the client will throw an exception and mark the replica node as dead.",0
"DFSUtil#substituteForWildcardAddress subs in a default hostname if the given hostname is 0.0.0.0. However, this function throws an exception if the given hostname is set to 0.0.0.0 and security is enabled, regardless of whether the default hostname is also 0.0.0.0. This function shouldn't throw an exception unless both addresses are set to 0.0.0.0.",0
Here is the JIRA to BookKeeper support issues with NN HA. We can file all the BookKeeperJournalManager issues under this JIRA for more easy tracking.,0
"As Todd points out in this comment, the current scheme for a checkpointing daemon to upload a merged fsimage file to an NN is to issue an HTTP get request to tell the target NN to issue another GET request back to the checkpointing daemon to retrieve the merged fsimage file. There's no fundamental reason the checkpointing daemon can't just use an HTTP POST or PUT to send back the merged fsimage file, rather than the double-GET scheme.",0
"Scenario:
=========
start Namenode and datanode by configuring three storage dir's for namenode
write 10 files
edit version file of one of the storage dir and give layout version as 123 which different with default(-40).
Stop namenode
start Namenode.


Then I am getting follwong exception...


{noformat}
2012-05-13 19:01:41,483 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Exception in namenode join
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.namenode.NNStorage.getStorageFile(NNStorage.java:686)
	at org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getEditsInStorageDir(FSImagePreTransactionalStorageInspector.java:243)
	at org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getLatestEditsFiles(FSImagePreTransactionalStorageInspector.java:261)
	at org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getEditLogStreams(FSImagePreTransactionalStorageInspector.java:276)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:596)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:247)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:498)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:390)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:354)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:368)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:402)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:564)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:545)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1093)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1151)
2012-05-13 19:01:41,485 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 

{noformat}
",0
"Scenario: 

I am using the script Mapping Technique for Dynamic Addition of rack.

1. Namenode is running with three datanode(xx.xx.xx.45,xx.xx.xx.91,xx.xx.xx.157)

xx.xx.xx.45 : /datacenter1/rack1
xx.xx.xx.91 : /datacenter1/rack2
xx.xx.xx.157 :/datacenter1/rack3

2. Stop datanode xx.xx.xx.157.

3. change the rack of xx.xx.xx.157 to rack4 in Script file.

'xx.xx.xx.45' : '/datacenter1/rack1'
'xx.xx.xx.91' : '/datacenter1/rack2'
'xx.xx.xx.157' : '/datacenter1/rack4'

4. Start datanode xx.xx.xx.157.

Expected Result : Datanode Should be add in rack4

Actual Result :   Datanode Added in old rack only (rack3).
",0
"BKJM: NN startup is failing, when tries to recoverUnfinalizedSegments() a bad inProgress_ Znodes",0
"Scenario:
=========

1. Cluster with 4 DataNodes.
2. Written file to 3 DNs, DN1->DN2->DN3
3. Stopped DN3,
Now Append to file is failing due to addDatanode2ExistingPipeline is failed.

 *CLinet Trace* 
{noformat}
2012-04-24 22:06:09,947 INFO  hdfs.DFSClient (DFSOutputStream.java:createBlockOutputStream(1063)) - Exception in createBlockOutputStream
java.io.IOException: Bad connect ack with firstBadLink as *******:50010
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1053)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:943)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)
2012-04-24 22:06:09,947 WARN  hdfs.DFSClient (DFSOutputStream.java:setupPipelineForAppendOrRecovery(916)) - Error Recovery for block BP-1023239-10.18.40.233-1335275282109:blk_296651611851855249_1253 in pipeline *****:50010, ******:50010, *****:50010: bad datanode ******:50010
2012-04-24 22:06:10,072 WARN  hdfs.DFSClient (DFSOutputStream.java:run(549)) - DataStreamer Exception
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:866)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:843)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:934)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)
2012-04-24 22:06:10,072 WARN  hdfs.DFSClient (DFSOutputStream.java:hflush(1515)) - Error while syncing
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:866)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:843)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:934)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:866)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:843)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:934)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)
{noformat}

 *DataNode Trace*  

{noformat}

2012-05-17 15:39:12,261 ERROR datanode.DataNode (DataXceiver.java:run(193)) - host0.foo.com:49744:DataXceiver error processing TRANSFER_BLOCK operation  src: /127.0.0.1:49811 dest: /127.0.0.1:49744
java.io.IOException: BP-2001850558-xx.xx.xx.xx-1337249347060:blk_-8165642083860293107_1002 is neither a RBW nor a Finalized, r=ReplicaBeingWritten, blk_-8165642083860293107_1003, RBW
  getNumBytes()     = 1024
  getBytesOnDisk()  = 1024
  getVisibleLength()= 1024
  getVolume()       = E:\MyWorkSpace\branch-2\Test\build\test\data\dfs\data\data1\current
  getBlockFile()    = E:\MyWorkSpace\branch-2\Test\build\test\data\dfs\data\data1\current\BP-2001850558-xx.xx.xx.xx-1337249347060\current\rbw\blk_-8165642083860293107
  bytesAcked=1024
  bytesOnDisk=102
at org.apache.hadoop.hdfs.server.datanode.DataNode.transferReplicaForPipelineRecovery(DataNode.java:2038)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.transferBlock(DataXceiver.java:525)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opTransferBlock(Receiver.java:114)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:78)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:189)
	at java.lang.Thread.run(Unknown Source)
{noformat}",0
"The balancer determines the set of NN URIs to balance by looking at fs.defaultFS and all possible dfs.namenode.(service)rpc-address settings. If fs.defaultFS is, for example, set to ""hdfs://foo.example.com:8020/"" (note the trailing ""/"") and the rpc-address is set to ""hdfs://foo.example.com:8020"" (without a ""/""), then the balancer will conclude that there are two NNs and try to balance both. However, since both of these URIs refer to the same actual FS instance, the balancer will exit with ""java.io.IOException: Another balancer is running.  Exiting ...""",0
"Standby NN has got the ledgerlist with list of all files, including the inprogress file (with say inprogress_val1)
Active NN has done finalization and created new inprogress file.
Standby when proceeds further finds that the inprogress file which it had in the list is not present and NN gets shutdown
NN Logs
=========
2012-05-17 22:15:03,867 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Image file of size 201 saved in 0 seconds.
2012-05-17 22:15:03,874 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode /xx.xx.xx.102:8020
2012-05-17 22:15:03,923 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 111
2012-05-17 22:15:03,923 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/home/May8/hadoop-3.0.0-SNAPSHOT/hadoop-root/dfs/name/current/fsimage_0000000000000000109, cpktTxId=0000000000000000109)
2012-05-17 22:15:03,961 FATAL org.apache.hadoop.hdfs.server.namenode.FSEditLog: Error: purgeLogsOlderThan 0 failed for required journal (JournalAndStream(mgr=org.apache.hadoop.contrib.bkjournal.BookKeeperJournalManager@142e6767, stream=null))
java.io.IOException: Exception reading ledger list from zk
at org.apache.hadoop.contrib.bkjournal.BookKeeperJournalManager.getLedgerList(BookKeeperJournalManager.java:531)
at org.apache.hadoop.contrib.bkjournal.BookKeeperJournalManager.purgeLogsOlderThan(BookKeeperJournalManager.java:444)
at org.apache.hadoop.hdfs.server.namenode.JournalSet$5.apply(JournalSet.java:541)
at org.apache.hadoop.hdfs.server.namenode.JournalSet.mapJournalsAndReportErrors(JournalSet.java:322)
at org.apache.hadoop.hdfs.server.namenode.JournalSet.purgeLogsOlderThan(JournalSet.java:538)
at org.apache.hadoop.hdfs.server.namenode.FSEditLog.purge",0
"Start NN
Let NN standby services be started.
Before the editLogTailer is initialised start ZKFC and allow the activeservices start to proceed further.


Here editLogTailer.catchupDuringFailover() will throw NPE.

void startActiveServices() throws IOException {
    LOG.info(""Starting services required for active state"");
    writeLock();
    try {
      FSEditLog editLog = dir.fsImage.getEditLog();
      
      if (!editLog.isOpenForWrite()) {
        // During startup, we're already open for write during initialization.
        editLog.initJournalsForWrite();
        // May need to recover
        editLog.recoverUnclosedStreams();
        
        LOG.info(""Catching up to latest edits from old active before "" +
            ""taking over writer role in edits logs."");
        editLogTailer.catchupDuringFailover();


{noformat}
2012-05-18 16:51:27,585 WARN org.apache.hadoop.ipc.Server: IPC Server Responder, call org.apache.hadoop.ha.HAServiceProtocol.getServiceStatus from XX.XX.XX.55:58003: output error
2012-05-18 16:51:27,586 WARN org.apache.hadoop.ipc.Server: IPC Server handler 8 on 8020, call org.apache.hadoop.ha.HAServiceProtocol.transitionToActive from XX.XX.XX.55:58004: error: java.lang.NullPointerException
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:602)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1287)
	at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)
	at org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:63)
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1219)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:978)
	at org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive(HAServiceProtocolServerSideTranslatorPB.java:107)
	at org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceProtocolService$2.callBlockingMethod(HAServiceProtocolProtos.java:3633)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:427)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:916)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1692)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1688)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1232)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1686)
2012-05-18 16:51:27,586 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 8020 caught an exception
java.nio.channels.ClosedChannelException
	at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:133)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:324)
	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:2092)
	at org.apache.hadoop.ipc.Server.access$2000(Server.java:107)
	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:930)
	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:994)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1738)
{noformat}",0
"When one tries to run `hdfs groups' with security enabled, you'll get an error like the following:

{noformat}
java.io.IOException: Failed to specify server's Kerberos principal name;
{noformat}",0
"The HostsFileReader silently fails if the includes or excludes files do
not exist or are not readable. The current behavior is to overwrite the
existing set of hosts or excluded hosts, regardless of whether or not
the includes/excludes file exists.

This behavior was introduced in HADOOP-5643 to support updating the job
tracker's node lists. The HostsFileReader is no longer used by the job
tracker. If this behavior was intentional, it no longer seems necessary
for any reason. The HostsFileReader is still used by NodeListManager as
well as the DatanodeManager, and in both cases, throwing an exception
when the include/exclude files aren't found/readable is desirable.

We should validate the given includes and excludes files before using
them.",0
BKJM:Switch from standby to active fails and NN gets shut down due to delay in clearing of lock,0
"The HttpFSServer.getEffectiveUser() method uses the principal name for proxy user verification. If the Kerberos is ON and the proxy user is a service principal (NAME/HOST) then the verification fails, instead the short name (just NAME) should be used.",0
"Currently, hftp uses http to the Namenode's https port, which doesn't work.",0
"If NamenodeRpcAddressParam is not passed to the datanode webhdfs methods, an NPE will be generated trying to use it. We should check the parameter and throw a more appropriate error message when it is missing.",0
"HttpFs seems to have these problems:
# can't set permissions to 777 at file creation or 1777 with setpermission
# does not accept 01777 permissions (which is valid in WebHdfs)

WebHdfs
curl -X PUT ""http://localhost:50070/webhdfs/v1/tmp/test-perm-webhdfs?permission=1777&op=MKDIRS&user.name=hue&doas=hue""
{""boolean"":true}
curl  ""http://localhost:50070/webhdfs/v1/tmp/test-perm-webhdfs?op=GETFILESTATUS&user.name=hue&doas=hue""
{""FileStatus"":{""accessTime"":0,""blockSize"":0,""group"":""supergroup"",""length"":0,""modificationTime"":1338581075040,""owner"":""hue"",""pathSuffix"":"""",""permission"":""1777"",""replication"":0,""type"":""DIRECTORY""}}

curl -X PUT ""http://localhost:50070/webhdfs/v1/tmp/test-perm-webhdfs?permission=01777&op=MKDIRS&user.name=hue&doas=hue""
{""boolean"":true}


HttpFs
curl -X PUT ""http://localhost:14000/webhdfs/v1/tmp/test-perm-httpfs?permission=1777&op=MKDIRS&user.name=hue&doas=hue""
{""boolean"":true}
curl  ""http://localhost:14000/webhdfs/v1/tmp/test-perm-httpfs?op=GETFILESTATUS&user.name=hue&doas=hue""
{""FileStatus"":{""pathSuffix"":"""",""type"":""DIRECTORY"",""length"":0,""owner"":""hue"",""group"":""supergroup"",""permission"":""755"",""accessTime"":0,""modificationTime"":1338580912205,""blockSize"":0,""replication"":0}}

curl -X PUT  ""http://localhost:14000/webhdfs/v1/tmp/test-perm-httpfs?op=SETPERMISSION&PERMISSION=1777&user.name=hue&doas=hue""
curl  ""http://localhost:14000/webhdfs/v1/tmp/test-perm-httpfs?op=GETFILESTATUS&user.name=hue&doas=hue""
{""FileStatus"":{""pathSuffix"":"""",""type"":""DIRECTORY"",""length"":0,""owner"":""hue"",""group"":""supergroup"",""permission"":""777"",""accessTime"":0,""modificationTime"":1338581075040,""blockSize"":0,""replication"":0}}

curl -X PUT ""http://localhost:14000/webhdfs/v1/tmp/test-perm-httpfs?permission=01777&op=MKDIRS&user.name=hue&doas=hue""
{""RemoteException"":{""message"":""java.lang.IllegalArgumentException: Parameter [permission], invalid value [01777], value must be [default|[0-1]?[0-7][0-7][0-7]]"",""exception"":""QueryParamException"",""javaClassName"":""com.sun.jersey.api.ParamException$QueryParamException""}}
",0
"Since the Balancer is a Hadoop Tool, it was updated to be directly aware of four-layer hierarchy instead of creating an alternative Balancer implementation. To accommodate extensibility, a new protected method, doChooseNodesForCustomFaultDomain is now called from the existing chooseNodes method so that a subclass of the Balancer could customize the balancer algotirhm for other failure and locality topologies. An alternative option is to encapsulate the algorithm used for the four-layer hierarchy into a collaborating strategy class.
The key changes introduced to support a four-layer hierarchy were to override the algorithm of choosing <source, target> pairs for balancing. Unit tests were created to test the new algorithm.
The algorithm now makes sure to choose the target and source node on the same node group for balancing as the first priority. Then the overall balancing policy is: first doing balancing between nodes within the same nodegroup then the same rack and off rack at last. Also, we need to check no duplicated replicas live in the same node group after balancing",0
"The StandbyCheckpointer and 2NN currently do the right thing in renewing their krb5 creds before attempting to perform a checkpoint to the active NN, but the active NN makes no attempt to renew its own krb5 creds before connecting to the standby NN or 2NN to fetch the new merged fsimage file.",0
"Currently DFS#isInSafeMode is not Checking for the NN state. It can be executed on any of the NNs.

But HBase will use this API to check for the NN safemode before starting up its service.

If first NN configured is in standby then DFS#isInSafeMode will check standby NNs safemode but hbase want state of Active NN.",0
"TestStandbyCheckpoints failed in [precommit build 2620|https://builds.apache.org/job/PreCommit-HDFS-Build/2620//testReport/] due to the following issue:
- both nodes were in Standby state, and configured to checkpoint ""as fast as possible""
- NN1 starts to save its own namespace
- NN2 starts to upload a checkpoint for the same txid. So, both threads are writing to the same file fsimage.ckpt_12, but the actual file contents correspond to the uploading thread's data.
- NN1 finished its saveNamespace operation while NN2 was still uploading. So, it renamed the ckpt file. However, the contents of the file are still empty since NN2 hasn't sent any bytes
- NN2 finishes the upload, and the rename() call fails, which causes the directory to be marked failed, etc.

The result is that there is a file fsimage_12 which appears to be a finalized image but in fact is incompletely transferred. When the transfer completes, the problem ""heals itself"" so there wouldn't be persistent corruption unless the machine crashes at the same time. And even then, we'd still have the earlier checkpoint to restore from.

This same race could occur in a non-HA setup if a user puts the NN in safe mode and issues saveNamespace operations concurrent with a 2NN checkpointing, I believe.",0
"I'm seeing the following NPE:

java.lang.NullPointerException
        at
org.apache.hadoop.hdfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:1633)
        at
org.apache.hadoop.hdfs.DFSClient$DFSInputStream.<init>(DFSClient.java:1593)
        at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:428)
        at
org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:187)
        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:456)


Here, the file gets opened while it's being written.   ",0
"Block Recovery initiated while write in progress at Datanode side. Found a lock between recovery, xceiver and packet responder.
",0
"NamenodeFsck.copyBlock creates a Socket using {{new Socket()}}, and thus that socket doesn't have an associated Channel. Then, it fails to create a BlockReader since RemoteBlockReader2 needs a socket channel.
(thanks to Hiroshi Yokoi for reporting)",0
"Proxy tokens are broken for hftp.  The impact is systems using proxy tokens, such as oozie jobs, cannot use hftp.",0
fuse_dfs should have support for Kerberos authentication. This would allow FUSE to be used in a secure cluster.,0
"If reading a file large enough for which the httpserver running webhdfs/httpfs uses chunked transfer encoding (more than 24K in the case of webhdfs), then the WebHdfsFileSystem client fails with an IOException with message *Content-Length header is missing*.

It looks like WebHdfsFileSystem is delegating opening of the inputstream to *ByteRangeInputStream.URLOpener* class, which checks for the *Content-Length* header, but when using chunked transfer encoding the *Content-Length* header is not present and  the *URLOpener.openInputStream()* method thrown an exception.
",0
"libhdfs does not consistently handle exceptions.  Sometimes we don't free the memory associated with them (memory leak).  Sometimes we invoke JNI functions that are not supposed to be invoked when an exception is active.

Running a libhdfs test program with -Xcheck:jni shows the latter problem clearly:
{code}
WARNING in native method: JNI call made with exception pending
WARNING in native method: JNI call made with exception pending
WARNING in native method: JNI call made with exception pending
WARNING in native method: JNI call made with exception pending
WARNING in native method: JNI call made with exception pending
Exception in thread ""main"" java.io.IOException: ...
{code}",0
"Scenario:
========= 
1. There are 2 clients cli1 and cli2 cli1 write a file F1 and not closed
2. The cli2 will call append on unclosed file and triggers a leaserecovery
3. Cli1 is closed
4. Lease recovery is completed and with updated GS in DN and got BlockReport since there is a mismatch in GS the block got corrupted
5. Now we got a CommitBlockSync this will also fail since the File is already closed by cli1 and state in NN is Finalized
",0
"Scenario:
===========
Writing files without close
renaming file
deleting files with four DN's and replication factor=3",0
"Scenario:
=========
Started four DN's(Say DN1,DN2,DN3 and DN4)
writing files with RF=3..
formed pipeline with DN1->DN2->DN3.
Since DN3 network is very slow.it's not able to send acks.
Again pipeline is fromed with DN1->DN2->DN4.
Here DN4 network is also slow.
So finally commitblocksync happend tp DN1 and DN2 successfully.

block present in all the four DN's(finalized state in two DN's and rbw state in another DN's)..

Here NN is asking replicate to DN3 and DN4,but it's failing since replcia's are already present in RBW dir.

",0
"When upgrading from 1.x to 2.0.0, the SecondaryNameNode can fail to start up:
{code}
2012-06-16 09:52:33,812 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: Inconsistent checkpoint fields.
LV = -40 namespaceID = 64415959 cTime = 1339813974990 ; clusterId = CID-07a82b97-8d04-4fdd-b3a1-f40650163245 ; blockpoolId = BP-1792677198-172.29.121.67-1339813967723.
Expecting respectively: -19; 64415959; 0; ; .
at org.apache.hadoop.hdfs.server.namenode.CheckpointSignature.validateStorageInfo(CheckpointSignature.java:120)
at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:454)
at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:334)
at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$2.run(SecondaryNameNode.java:301)
at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:438)
at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:297)
at java.lang.Thread.run(Thread.java:662)
{code}
The error check we're hitting came from HDFS-1073, and it's intended to verify that we're connecting to the correct NN.  But the check is too strict and considers ""different metadata version"" to be the same as ""different clusterID"".

I believe the check in {{doCheckpoint}} simply needs to explicitly check for and handle the update case.",0
"Open file for append
Write data and sync.
After next log roll and editlog tailing in standbyNN close the append stream.
Call append multiple times on the same file, before next editlog roll.
Now abruptly kill the current active namenode.

Here block is missed..

this may be because of All latest blocks were queued in StandBy Namenode. 
During failover, first OP_CLOSE was processing the pending queue and adding the block to corrupted block. ",0
"I have seen this in precommit build #2743

{noformat}
java.util.ConcurrentModificationException
	at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
	at java.util.HashMap$EntryIterator.next(HashMap.java:834)
	at java.util.HashMap$EntryIterator.next(HashMap.java:832)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.shutdown(FsVolumeImpl.java:209)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.shutdown(FsVolumeList.java:168)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.shutdown(FsDatasetImpl.java:1214)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:1105)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdownDataNodes(MiniDFSCluster.java:1324)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:1304)
	at org.apache.hadoop.hdfs.web.TestWebHdfsWithMultipleNameNodes.shutdownCluster(TestWebHdfsWithMultipleNameNodes.java:100)
{noformat}

",0
"Started NN's and zkfc's in Suse11.
Suse11 will have netcat installation and netcat -z will work(but nc -z wn't work)..
While executing following command, got command not found hence rc will be other than zero and assuming that server was down..Here we are ending up without checking whether service is down or not..
{code}
LOG.info(
            ""Indeterminate response from trying to kill service. "" +
            ""Verifying whether it is running using nc..."");
        rc = execCommand(session, ""nc -z "" + serviceAddr.getHostName() +
            "" "" + serviceAddr.getPort());
        if (rc == 0) {
          // the service is still listening - we are unable to fence
          LOG.warn(""Unable to fence - it is running but we cannot kill it"");
          return false;
        } else {
          LOG.info(""Verified that the service is down."");
          return true;          
        }

{code}


",0
"Let's assume:
1. replica factor = 4
2. source node in rack 1 has 1st replica, 2nd and 3rd replica are in rack 2, 4th replica in rack3 and target node is in rack3. 
So, It should be good for balancer to move replica from source node to target node but will return ""false"" in isGoodBlockCandidate(). I think we can fix it by simply making judgement that at least one replica node (other than source) is on the different rack of target node.",0
"TestBackupNode#testCheckpointNode fails because the following code in FSN#startActiveServices NPEs (resulting in a System.exit) because editLogTailer is set when starting standby services and if ha is not enabled we go directly to the active state. Looks like it should be wrapped with an haEnabled check.

{code}
LOG.info(""Catching up to latest edits from old active before "" +
   ""taking over writer role in edits logs."");
editLogTailer.catchupDuringFailover();
{code}",0
"Joris Bontje reports the following:

The following command results in a corrupt NN editlog (note the double slash and reading from stdin):
$ cat /usr/share/dict/words | hadoop fs -put - hdfs://localhost:8020//path/file

After this, restarting the namenode will result into the following fatal exception:
{code}
2012-07-10 06:29:19,910 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /var/lib/hadoop-hdfs/cache/hdfs/dfs/name/current/edits_0000000000000000173-0000000000000000188 expecting start txid #173
2012-07-10 06:29:19,912 ERROR org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: Encountered exception on operation MkdirOp [length=0, path=/, timestamp=1341915658216, permissions=cloudera:supergroup:rwxr-xr-x, opCode=OP_MKDIR, txid=182]
java.lang.ArrayIndexOutOfBoundsException: -1
{code}",0
"The changes from HDFS-2202 for 0.20.x/1.x failed to add in a checkSuperuserPrivilege();, and hence any user (not admins alone) can reset the balancer bandwidth across the cluster if they wished to.",0
"Currently all HDFS RPCs performed by NNs/DNs/clients can be optionally encrypted. However, actual data read or written between DNs and clients (or DNs to DNs) is sent in the clear. When processing sensitive data on a shared cluster, confidentiality of the data read/written from/to HDFS may be desired.",0
"{code}
    str = methSignature;
    while (*str != ')') str++;
    str++;
    returnType = *str;
{code}
This loop needs to check for {{'\0'}}. Also the following {{if/else if/else if}} cascade doesn't handle unexpected values.",0
"In {{FSEditLog.removeEditsForStorageDir}}, we iterate over the edits streams trying to find the stream corresponding to a given dir. To check equality, we currently use the following condition:
{code}
      File parentDir = getStorageDirForStream(idx);
      if (parentDir.getName().equals(sd.getRoot().getName())) {
{code}
... which is horribly incorrect. If two or more storage dirs happen to have the same terminal path component (eg /data/1/nn and /data/2/nn) then it will pick the wrong stream(s) to remove.",0
If the 2NN eg fails to write to one of its local checkpoint dirs while downloading an image (eg due to the dir failing because it's out of space etc) the entire checkpoint will fail even though there are other valid checkpoint directories. We should add local directory failure toleration and restoration (similar to the NN).,0
"Looks like there's a race hit by the test where we try to do block management while shutting down (eg allocate a block or compute replication work after BlocksMap#close, resulting in an NPE).

",0
"Client.java has this code:

{code}
    private synchronized void handleSaslConnectionFailure(
        final int currRetries, final int maxRetries, final Exception ex,
        final Random rand, final UserGroupInformation ugi) throws IOException,
        InterruptedException {
      ugi.doAs(new PrivilegedExceptionAction<Object>() {
        public Object run() throws IOException, InterruptedException {
          final short MAX_BACKOFF = 5000;
          closeConnection();
          disposeSasl();
          if (shouldAuthenticateOverKrb()) {
            if (currRetries < maxRetries) {
              if(LOG.isDebugEnabled()) {
                LOG.debug(""Exception encountered while connecting to ""
                    + ""the server : "" + ex);
              }
              // try re-login
              if (UserGroupInformation.isLoginKeytabBased()) {
                UserGroupInformation.getLoginUser().reloginFromKeytab();
              } else {
                UserGroupInformation.getLoginUser().reloginFromTicketCache();
              }
{code}

It's not renewing the UGI that had the problem, it's renewing the loginUser.",0
"Seeing the following NPE in the SecondaryNameNode when it tries to purge storage after a checkpoint:

12/07/17 19:05:22 WARN namenode.FSImage: Unable to purge old storage
java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.purgeLogsOlderThan(FSEditLog.java:1013)
        at org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager.purgeOldStorage(NNStorageRetentionManager.java:98)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.purgeOldStorage(FSImage.java:925)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:479)
",0
"Rebooting a DN with a corrupt block results in the blockscanner getting an exception while scanning:
{code}
2012-07-19 11:52:09,915 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Reporting bad block BP-1507505631-172.29.97.196-1337120439433:blk_137178859864142853_112083120
2012-07-19 11:52:09,927 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to report bad block BP-1507505631-172.29.97.196-1337120439433:blk_137178859864142853_112083120 to namenode :  Exception
org.apache.hadoop.ipc.StandbyException: Operation category WRITE is not supported in state standby
        at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
        at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1375)
{code}

In this case the block got deleted anyways possibly due to being reported to the NN by a client, but this might not happen if the file was not recently read.

I'll attach a larger chunk of the DN and NN logs.",0
"If a datanode ever re-registers with the namenode (e.g.: namenode restart, temporary network cut, etc.) then the datanode ends up registering with an IP address as the datanode name rather than the hostname.
",0
"When doing ""fs -put"" to a WebHdfsFileSystem (webhdfs://), the FsShell goes OOM if the file size is large. When I tested, 20MB files were fine, but 200MB didn't work.  

I also tried reading a large file by issuing ""-cat"" and piping to a slow sink in order to force buffering. The read path didn't have this problem. The memory consumption stayed the same regardless of progress.
",0
"When the file is opened for writing, the DFSClient calls one of the datanode owning the last block to get its size. If this datanode is dead, the socket exception is shallowed and the size of this last block is equals to zero. This seems to be fixed on trunk, but I didn't find a related Jira. On 1.0.3, it's not fixed. It's on the same area as HDFS-1950 or HDFS-3222.
",0
"Datanode sometimes does not shutdown because the block pool scanner thread keeps running. It prints out ""Starting a new period"" every five seconds, even after {{shutdown()}} is called.  Somehow the interrupt is missed.

{{DataBlockScanner}} will also terminate if {{datanode.shouldRun}} is false, but in {{DataNode#shutdown}}, {{DataBlockScanner#shutdown()}} is invoked before it is being set to false.

Is there any reason why {{datanode.shouldRun}} is set to false later? ",0
"Scenario:
=========

Start ANN and SNN with three DN's

Exclude DN1 from cluster by using decommission feature 

(./hdfs dfsadmin -fs hdfs://ANNIP:8020 -refreshNodes)

After decommission successful,do switch such that SNN will become Active.

Here exclude node(DN1) is included in cluster.Able to write files to excluded node since it's not excluded.

Checked SNN(Which Active before switch) UI decommissioned=1 and ANN UI 
decommissioned=0

One more Observation:
====================

All dfsadmin commands will create proxy only on nn1 irrespective of Active or standby.I think this also we need to re-look once..


I am not getting , why we are not given HA for dfsadmin commands..?

Please correct me,,If I am wrong.",0
"If a file is already open for write by one client, and another client calls {{fs.create()}} with {{overwrite=true}}, the file should be deleted and the new file successfully created. Instead, it is currently throwing AlreadyBeingCreatedException.

This is a regression since branch-1.",0
"When doing the forwardport of HDFS-2617 to trunk I've missed deleting the original connection creation when converting it to use the SecurityUtil class.

{code}
       connection = (HttpURLConnection) SecurityUtil.openSecureHttpConnection(url);
      connection = (HttpURLConnection)URLUtils.openConnection(url);
{code}
",0
"Our 0.23.3 nightly HDFS regression suite encountered a particularly nasty issue recently, which resulted in the cluster's default Namenode being unable to restart, this was on a 20 node Federated cluster with security. The cause appears to be that the NN was just starting to roll its edit log when a shutdown occurred, the shutdown was intentional to restart the cluster as part of an automated test.

The tests that were running do not appear to be the issue in themselves, the cluster was just wrapping up an adminReport subset and this failure case has not reproduce so far, nor was it failing previously. It looks like a chance occurrence of sending the shutdown just as the edit log roll was begun.

From the NN log, the following sequence is noted:

1. an InvalidateBlocks operation had completed
2. FSNamesystem: Roll Edit Log from [Secondary Namenode IPaddr]
3. FSEditLog: Ending log segment 23963
4. FSEditLog: Starting log segment at 23967
4. NameNode: SHUTDOWN_MSG
=> the NN shuts down and then is restarted...
5. FSImageTransactionalStorageInspector: Logs beginning at txid 23967 were are all in-progress
6. FSImageTransactionalStorageInspector: Marking log at /grid/[PATH]/edits_inprogress_0000000000000023967 as corrupt since it has no transactions in it.
7. NameNode: Exception in namenode join [main]java.lang.IllegalStateException: No non-corrupt logs for txid 23967
=> NN start attempts continue to cycle trying to restart but can't, failing on the same exception due to lack of non-corrupt edit logs

If observations are correct and issue is from shutdown happening as edit logs are rolling, does the NN have an equivalent to the conventional fs 'sync' blocking action that should be called, or perhaps has a timing hole?",0
"Calling ""yarn logs"" on a *running* application results in a NPE:

$ yarn logs -applicationId app_1344439655593_0002
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:606)
	at java.io.DataInputStream.readFully(DataInputStream.java:178)
	at java.io.DataInputStream.readLong(DataInputStream.java:399)
	at org.apache.hadoop.io.file.tfile.BCFile$Reader.<init>(BCFile.java:623)
	at org.apache.hadoop.io.file.tfile.TFile$Reader.<init>(TFile.java:803)
	at org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogReader.<init>(AggregatedLogFormat.java:266)
	at org.apache.hadoop.yarn.logaggregation.LogDumper.dumpAllContainersLogs(LogDumper.java:209)
	at org.apache.hadoop.yarn.logaggregation.LogDumper.run(LogDumper.java:109)
	at org.apache.hadoop.yarn.logaggregation.LogDumper.main(LogDumper.java:242)

Line numbers in the stack trace differ slightly from 2.0.0-alpha, due to the Cloudera patches, but the bug seems to have its origin here (patch for HDFS-1371: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@1125145).

At first sight this seems to be a two layer problem:
- yarn logs -- designed for finished apps only :( -- is looking for non-existing files here (?)
- the used HDFS-implementation, which should report block replica corruptions here, but results in NPE instead

",0
"The following command fails when data1 contains a 3gb file. It passes when using hftp or when the directory just contains smaller (<2gb) files, so looks like a webhdfs issue with large files.

{{hadoop distcp webhdfs://eli-thinkpad:50070/user/eli/data1 hdfs://localhost:8020/user/eli/data2}}
",0
"When an offset is specified, the HTTP header Content Length still contains the original file size. e.g. if the original file is 100 bytes, and the offset specified it 10, then HTTP Content Length ought to be 90. Currently it is still returned as 100.
This causes curl to give error 18, and JAVA to throw ConnectionClosedException",0
"When the 2NN wants to perform a checkpoint, it figures out the highest transaction ID of the fsimage files on the NN, and if the 2NN has a copy of that fsimage file (because it created that merged fsimage file the last time it did a checkpoint) then the 2NN won't download the fsimage file from the NN, and instead only gets the new edits files from the NN. In this case, the 2NN also doesn't even bother reloading the fsimage file it has from disk, since it has all of the namespace state in-memory. This all works just fine.

When the 2NN _doesn't_ have a copy of the relevant fsimage file (for example, if the NN had restarted since the last checkpoint) then the 2NN blows away its in-memory namespace state, downloads the fsimage file from the NN, and loads the newly-downloaded fsimage file from disk. The bug is that when the 2NN clears its in-memory state, it only resets the namespace, but not the delegation token map.

The fix is pretty simple - just make the delegation token map get cleared as well as the namespace state when a running 2NN needs to load a new fsimage from disk.

Credit to Stephen Chu for identifying this issue.",0
"Jitendra found out the following problem:
1. Handler : Acquires namesystem lock waits on SafemodeInfo lock at SafeModeInfo.isOn()
2. SafemodeMonitor : Calls SafeModeInfo.canLeave() which is synchronized so SafemodeInfo lock is acquired, but this method also causes following call sequence needEnter() -> getNumLiveDataNodes() -> getNumberOfDatanodes() -> getDatanodeListForReport() -> getDatanodeListForReport() . The getDatanodeListForReport is synchronized with FSNamesystem lock.",0
"When re-loading the FSImage, we should clear the existing genStamp and leases.

This is an issue in the 2NN, because it sometimes clears the existing FSImage and reloads a new one in order to get back in sync with the NN.",0
"In HeartbeatManager#heartbeatCheck, if some dead datanode is found, the monitor thread will acquire the write lock of namesystem, and recheck the safemode. If it is in safemode, the monitor thread will return from the heartbeatCheck function without release the write lock. This may cause the monitor thread wrongly holding the write lock forever.

The attached test case tries to simulate this bad scenario.",0
"The deadlock is between DFSOutputStream#close() and DFSClient#close().

",0
"When logging an OP_CLOSE to the edit log, the NN writes out an updated file mtime and atime. However, when reading in an OP_CLOSE from the edit log, the NN does not apply these values to the in-memory FS data structure. Because of this, a file's mtime or atime may appear to go back in time after an NN restart, or an HA failover.

Most of the time this will be harmless and folks won't notice, but in the event one of these files is being used in the distributed cache of an MR job when an HA failover occurs, the job might notice that the mtime of a cache file has changed, which in MR2 will cause the job to fail with an exception like the following:

{noformat}
java.io.IOException: Resource hdfs://ha-nn-uri/user/jenkins/.staging/job_1341364439849_0513/libjars/snappy-java-1.0.3.2.jar changed on src filesystem (expected 1342137814599, was 1342137814473
	at org.apache.hadoop.yarn.util.FSDownload.copy(FSDownload.java:90)
	at org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:49)
	at org.apache.hadoop.yarn.util.FSDownload$1.run(FSDownload.java:157)
	at org.apache.hadoop.yarn.util.FSDownload$1.run(FSDownload.java:155)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1232)
	at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:153)
	at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:49)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{noformat}

Credit to Sujay Rau for discovering this issue.",0
"Hftp ignores all exceptions generated while trying to get a token, based on the assumption that it means security is disabled.  Debugging problems is excruciatingly difficult when security is enabled but something goes wrong.  Job submissions succeed, but tasks fail because the NN rejects the user as unauthenticated.",0
"We see the following exception in our logs on a cluster:

{code}
2012-08-27 16:34:30,400 INFO org.apache.hadoop.hdfs.StateChange: *DIR* NameNode.reportBadBlocks
2012-08-27 16:34:30,400 ERROR org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:hdfs (auth:SIMPLE) cause:java.io.IOException: Cannot mark blk_8285012733733669474_140475196{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[172.29.97.219:50010|RBW]]}(same as stored) as corrupt because datanode :0 does not exist
2012-08-27 16:34:30,400 INFO org.apache.hadoop.ipc.Server: IPC Server handler 46 on 8020, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.reportBadBlocks from 172.29.97.219:43805: error: java.io.IOException: Cannot mark blk_8285012733733669474_140475196{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[172.29.97.219:50010|RBW]]}(same as stored) as corrupt because datanode :0 does not exist
java.io.IOException: Cannot mark blk_8285012733733669474_140475196{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[172.29.97.219:50010|RBW]]}(same as stored) as corrupt because datanode :0 does not exist
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.markBlockAsCorrupt(BlockManager.java:1001)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.findAndMarkBlockAsCorrupt(BlockManager.java:994)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.reportBadBlocks(FSNamesystem.java:4736)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.reportBadBlocks(NameNodeRpcServer.java:537)
        at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.reportBadBlocks(DatanodeProtocolServerSideTranslatorPB.java:242)
        at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:20032)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:453)
{code}",0
"We saw this issue with one block in a large test cluster. The client is storing the data with replication level 2, and we saw the following:
- the second node in the pipeline detects a checksum error on the data it received from the first node. We don't know if the client sent a bad checksum, or if it got corrupted between node 1 and node 2 in the pipeline.
- this caused the second node to get kicked out of the pipeline, since it threw an exception. The pipeline started up again with only one replica (the first node in the pipeline)
- this replica was later determined to be corrupt by the block scanner, and unrecoverable since it is the only replica",0
"I was attempting to set the umask of my fileContext and then do a mkdirs, but the umask wasn't applied as expected. 

doneDirFc = FileContext.getFileContext(doneDirPrefixPath.toUri(), conf);
doneDirFc.setUMask(JobHistoryUtils.HISTORY_DONE_DIR_UMASK);
doneDirFc.mkdir(path, fsp, true);

It appears to be using the default umask set in the conf (fs.permissions.umask-mode) and overrode the umask I set in fileContext. I had the default umask set to 077 and set the filecontext umask to 007.  The permissions on the directories it created were all rwx------.

",0
"HDFS-3873 fixed the case where all exceptions acquiring tokens for hftp were ignored.  Jobs would be submitted sans tokens, and then the tasks would eventually all fail trying to get the missing token.  HDFS-3873 made jobs fail to submit if the remote cluster is secure.

Unfortunately it regressed the ability for a secure cluster to access an insecure cluster over hftp.  The issue is unique to 23 due to KSSL.",0
"I shut down all the HDFS daemons in an Highly Available (automatic failover) cluster.

Then I started one NN and it transitioned it to active. No DNs were started, and I saw the red warning link on the NN web UI:
WARNING : There are 36 missing blocks. Please check the logs or run fsck in order to identify the missing blocks.

I clicked this to go to the corrupt_files.jsp page, which ran into the following error:

{noformat}
HTTP ERROR 500

Problem accessing /corrupt_files.jsp. Reason:

    Cannot run listCorruptFileBlocks because replication queues have not been initialized.
Caused by:

java.io.IOException: Cannot run listCorruptFileBlocks because replication queues have not been initialized.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.listCorruptFileBlocks(FSNamesystem.java:5035)
	at org.apache.hadoop.hdfs.server.namenode.corrupt_005ffiles_jsp._jspService(corrupt_005ffiles_jsp.java:78)
	at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:98)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
	at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:1039)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
{noformat}
",0
"When reading edits on startup, namenode may throw away blocks under construction. This is because the file inode is turned into a ""under construction"" one, but nothing is done to the last block. 

With append/hsync, this is not acceptable because it may drop sync'ed partial blocks.  In branch 2 and trunk, HDFS-1623 (HA) fixed this issue.",0
"If {{dfs.namenode.rpc-address}} is set to the wildcard some of links in the dfsnodelist.jsp and browseDirectory.jsp pages are broken because the nnaddr field is passed verbatim (eg nnaddr=0.0.0.0:8021).
",0
"When a datanode is configured to work in secure mode, the HttpServer connector listener is created by the SecureDataNodeStarter class in a privileged port and given to the HttpServer constructor to use.

When enabling SSL for the web ui the connector listener is created by the HttpServer if no connector is given in the constructor.

This means that in secure mode the DataNode HttpServer starts always in HTTP ",0
"In a secure HA cluster, we're seeing the following issue on the NN when the trash emptier tries to run:

WARN org.apache.hadoop.fs.TrashPolicyDefault: Trash can't list homes: java.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]; Host Details : local host \
is: ""xxxxx""; destination host is: ""xxxx"":8020;  Sleeping.

The issue seems to be that the trash emptier thread sends RPCs back to itself, but isn't wrapped in a doAs. Credit goes to Stephen Chu for discovering this.",0
"See discussion in HDFS-744. The actual sync/flush operation in BlockReceiver is not on a synchronous path from the DFSClient, hence it is possible that a DN loses data that it has already acknowledged as persisted to a client.

Edit: Spelling.
",0
"Scenario:
========

I started secure cluster by going thru following..

https://ccp.cloudera.com/display/CDHDOC/CDH3+Security+Guide..


Here SecondaryNamenode is getting shutdown by throwing NPE..

Please correct me If I am wrong...

Will attach conf and logs..",0
"Incorrect condition in {{FSNamesystem.getBlockLocatoins()}} can lead to updating times without write lock. In most cases this condition will force {{FSNamesystem.getBlockLocatoins()}} to hold write lock, even if times do not need to be updated.",0
"From HDFS-3931:
{quote}
# The test corrupts 2/3 replicas.
# client reports a bad block.
# NN asks a DN to re-replicate, and randomly picks the other corrupt replica.
# DN notices the incoming replica is corrupt and reports it as a bad block, but does not inform the NN that re-replication failed.
# NN keeps the block on pendingReplications.
# BP scanner wakes up on both DNs with corrupt blocks, both report corruption. NN reports both as duplicates, one from the client and one from the DN report above.
since block is on pendingReplications, NN does not schedule another replication.

Todd wrote:
I can think of a few ways to fix this:
...
 2) Add a field to the DN heartbeat which reports back a failed replication for a given block. The NN would use this to decrement the pendingReplication count, which would cause a new replication attempt to be made if it was still under-replicated.

This jira tracks implementing the DN heartbeat replication failure report.",0
"This is a follow up of HDFS-3983.
We should have a new filesystem client impl/binding for encrypted WebHDFS, i.e. swebhdfs://
On the server side, webhdfs and httpfs we should only need to start the service on a secured (HTTPS) endpoint.",0
"Block written and finalized
Later append called. Block GenTS got changed.

DN side log 
""Can't send invalid block BP-407900822-192.xx.xx.xx-1348830837061:blk_-9185630731157263852_108738"" logged continously

NN side log
""INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Error report from DatanodeRegistration(192.xx.xx.xx, storageID=DS-2040532042-192.xx.xx.xx-50010-1348830863443, infoPort=50075, ipcPort=50020, storageInfo=lv=-40;cid=123456;nsid=116596173;c=0): Can't send invalid block BP-407900822-192.xx.xx.xx-1348830837061:blk_-9185630731157263852_108738"" also logged continuosly.

The block checked for tansfer is the one with old genTS whereas the new block with updated genTS exist in the data dir.
",0
"The Namenode uses the loginAsNameNodeUser method in NameNode.java to login using the hdfs principal. This method in turn invokes SecurityUtil.login with a hostname (last parameter) obtained via a call to InetAddress.getHostName. This call does not always return the fully qualified host name, and thus causes the namenode to login to fail due to kerberos's inability to find a matching hdfs principal in the hdfs.keytab file. Instead it should use InetAddress.getCanonicalHostName. This is consistent with what is used internally by SecurityUtil.java to login in other services, such as the DataNode. ",0
"When datanodes are recommissioned, {BlockManager#processOverReplicatedBlocksOnReCommission()} is called for each rejoined node and excess blocks are added to the invalidate list. The problem is this is done while the namesystem write lock is held.
",0
"When getFileChecksum() is called against a zero-byte file, the branch-1 client returns MD5MD5CRC32FileChecksum with crcPerBlock=0, bytePerCrc=0 and md5=70bc8f4b72a86921468bf8e8441dce51, whereas a null is returned in trunk.

The null makes sense since there is no actual block checksums, but this breaks the compatibility when doing distCp and calling getFileChecksum() via webhdfs or hftp.

This JIRA is to make the client to return the same 'magic' value that the branch-1 and earlier clients return.",0
"We saw the following issue in a cluster:
- The 2NN downloads an edit log segment:
{code}
2012-10-29 12:30:57,433 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /xxxxxxx/current/edits_0000000000049136809-0000000000049176162 expecting start txid #49136809
{code}
- It fails in the middle of replay due to an OOME:
{code}
2012-10-29 12:31:21,021 ERROR org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: Encountered exception on operation AddOp [length=0, path=/xxxxxxxx
java.lang.OutOfMemoryError: Java heap space
{code}
- Future checkpoints then fail because the prior edit log replay only got halfway through the stream:
{code}
2012-10-29 12:32:21,214 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /xxxxx/current/edits_0000000000049176163-0000000000049177224 expecting start txid #49144432
2012-10-29 12:32:21,216 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.io.IOException: There appears to be a gap in the edit log.  We expected txid 49144432, but got txid 49176163.
{code}",0
"When libwebhdfs is not enabled, nativeMiniDfsClient frees uninitialized memory.

Details: jconfStr is declared uninitialized...
{code}
struct NativeMiniDfsCluster* nmdCreate(struct NativeMiniDfsConf *conf)
{
    struct NativeMiniDfsCluster* cl = NULL;
    jobject bld = NULL, bld2 = NULL, cobj = NULL;
    jvalue  val;
    JNIEnv *env = getJNIEnv();
    jthrowable jthr;
    jstring jconfStr;
{code}

and only initialized later if conf->webhdfsEnabled:
{code}
    ...
    if (conf->webhdfsEnabled) {
        jthr = newJavaStr(env, DFS_WEBHDFS_ENABLED_KEY, &jconfStr);
        if (jthr) {
            printExceptionAndFree(env, jthr, PRINT_EXC_ALL,
    ...
{code}

Then we try to free this uninitialized memory at the end, usually resulting in a crash.
{code}
    (*env)->DeleteLocalRef(env, jconfStr);
    return cl;
{code}",0
"It was notices by TestBackupNode.testCheckpointNode failure. When a backup node is getting started, it tries to enter active state and start common services. But when it fails to start services and exits, which is caught by the exit util.",0
"Mounting a fuse-dfs in readonly mode with ""-oro"" still allows the user to truncate files.
{noformat}
$ grep fuse /etc/fstab
hadoop-fuse-dfs#dfs://ubu-cdh-0.local /export/hdfs fuse noauto,ro 0 0
$ sudo mount /export/hdfs
$ hdfs dfs -ls /tmp
...
-rw-r--r--   3 ubuntu hadoop          4 2012-11-01 14:18 /tmp/blah.txt
$ echo foo > /export/hdfs/tmp/blah.txt
-bash: /export/hdfs/tmp/blah.txt: Permission denied
$  hdfs dfs -ls /tmp
...
-rw-r--r--   3 ubuntu hadoop          0 2012-11-01 14:28 /tmp/blah.txt
$ ps ax | grep dfs
...
13639 ?        Ssl    0:02 /usr/lib/hadoop/bin/fuse_dfs dfs://ubu-cdh-0.local /export/hdfs -o ro,dev,suid
{noformat}",0
"Credit to [~vanzin] for finding this. If you seek() to a position before the start of the file, the seek() call succeeds, and the following read() call throws an NPE. A more friendly approach would be to fail the seek() call.

{noformat}
Exception in thread ""main"" java.lang.NullPointerException
at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:670)
at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:685)
at java.io.DataInputStream.read(DataInputStream.java:83)
at fstest.main(fstest.java:12)
{noformat}",0
"When the EditLogTailer thread calls rollEdits() on the active NN via RPC, it currently does so without a timeout. So, if the active NN has frozen (but not actually crashed), this call can hang forever. This can then potentially prevent the standby from becoming active.

This may actually considered a side effect of HADOOP-6762 -- if the RPC were interruptible, that would also fix the issue.",0
"We recently saw an issue where a 2NN ran out of memory, even though it had a relatively small fsimage. When we looked at the heap dump, we saw that all of the memory had gone to entries in the NameCache.

It appears that the NameCache is staying in ""initializing"" mode forever, and therefore a long running 2NN leaks entries.",0
"Saw the following NPE in a log.

Think this is likely due to {{dn}} or {{dn.getFSDataset()}} being null, (not {{bpRegistration}}) due to a configuration or local directory failure.

{code}
2012-09-25 04:33:20,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode svsrs00127/11.164.162.226:8020 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec Initial delay: 0msec; heartBeatInterval=3000
2012-09-25 04:33:20,782 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in BPOfferService for Block pool BP-1678908700-11.164.162.226-1342785481826 (storage id DS-1031100678-11.164.162.251-5010-1341933415989) service to svsrs00127/11.164.162.226:8020
java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:434)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:520)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:673)
        at java.lang.Thread.run(Thread.java:722)
{code}",0
"I created a symlink using
{code}
...
    FileContext fc = FileContext.getFileContext(dst.fs.getUri());
    for (PathData src : srcs) {
      fc.createSymlink(src.path, dst.path, false);
    }
{code}
After doing this to create a symlink {{/foo/too.txt -> /foo/hello.txt}}, I tried to {{hdfs fsck}} and got the following:
{code}
[adi@host01 ~]$ hdfs fsck /
Connecting to namenode via http://host01:21070
FSCK started by adi (auth:SIMPLE) from /172.29.122.91 for path / at Fri Nov 16 15:59:18 PST 2012
FSCK ended at Fri Nov 16 15:59:18 PST 2012 in 3 milliseconds
hdfs://host01:21020/foo/hello.txt


Fsck on path '/' FAILED
{code}

It's very surprising that an unprivileged user can run code which so easily causes a fundamental administration tool to fail.",0
"In one test case, NameNode allocated a block and then was killed before the client got the addBlock response. After NameNode restarted, it couldn't get out of SafeMode waiting for the block which was never created. In trunk, NameNode can get out of SafeMode since it only counts complete blocks. However branch-1 doesn't have the clear notion of under-constructioned-block in Namenode. 

JIRA HDFS-4212 is to track the never-created-block issue and this JIRA is to fix NameNode in branch-1 so it can get out of SafeMode when never-created-block exists.

The proposed idea is for SafeMode not to count the zero-sized last block in an under-construction file as part of total blcok count.",0
FSDirectory.unprotectedSymlink(..) catches IOException and ignores it.  As a consequence it ignores QuotaExceededException.,0
"For Hadoop clusters configured to access directory information by LDAP, the FSNamesystem calls on behave of DFS clients might hang due to LDAP issues (including LDAP access issues caused by networking issues) while holding the single lock of FSNamesystem. That will result in the NN unresponsive and loss of the heartbeats from DNs.

The places LDAP got accessed by FSNamesystem calls are the instantiation of FSPermissionChecker, which could be moved out of the lock scope since the instantiation does not need the FSNamesystem lock. After the move, a DFS client hang will not affect other threads by hogging the single lock. This is especially helpful when we use separate RPC servers for ClientProtocol and DatanodeProtocol since the calls for DatanodeProtocol do not need to access LDAP. So even if DFS clients hang due to LDAP issues, the NN will still be able to process the requests (including heartbeats) from DNs.",0
"The reading of a fsimage will ignore leases for non-existent files, but the writing of an image will fail if there are leases for non-existent files.  If the image contains leases that reference a non-existent file, then the NN will fail to start, and the 2NN will start but fail to ever write an image.",0
"We've seen namenode keeps serving even after rollEditLog() failure. Instead of taking a corrective action or regard this condition as FATAL, it keeps on serving and modifying its file system state. No logs are written from this point, so if the namenode is restarted, there will be data loss.
",0
"This happened in our cluster,

>> Standby NN was keep doing checkpoint every one hour and uploading to Active NN was continuously failing due to some kerberos issue and nobody noticed this, since Active was servicing properly.

>> Active NN was up for long time with fsimage having very least transaction.

>> Standby NN has saved the checkpoint in its name dir and purged the txns > 1000000 from shared storage ( includes edits which are not present in Active NN's fsimage)

>> After some time Active NN is restarted and StandBy NN switched to Active.

Now current Standby not able to load any edits from shared storage, as expected edits are not present in shared storage. Its keep running idle.


So {{editLog.purgeLogsOlderThan(purgeLogsFrom);}} always should be called from Active NameNode.
",0
"In previous implementation for HADOOP-8468, 3rd replica is avoid to place on the same nodegroup of 2nd replica. But it didn't provide check on nodegroup of 1st replica, so if 2nd replica's rack is not efficient to place replica, then it is possible to place 3rd and 1st replica within the same node group. We need a change to remove all nodes from available nodes for placing replica if there already replica on the same nodegroup.",0
"When a being written file or it's ancestor directories is renamed, the path in the file lease is also renamed.  Then the writer of the file usually will fail since the file path in the writer is not updated.

Moreover, I think there is a bug as follow:

# Client writes 0's to F_0=""/foo/file"" and writes 1's to F_1=""/bar/file"" at the same time.
# Rename /bar to /baz
# Rename /foo to /bar

Then, writing to F_0 will fail since /foo/file does not exist anymore but writing to F_1 may succeed since /bar/file exits as a different file.  In such case, the content of /bar/file could be partly 0's and partly 1's.",0
"HDFS-3990 is a change that optimized some redundant DNS lookups.  As part of that change, {{DatanodeManager#registerDatanode}} now rejects attempts to register a datanode for which the name has not been resolved.  Unfortunately, this broke single-node developer setups on Windows, because Windows does not resolve 127.0.0.1 to ""localhost"".",0
"Blocks that have been identified as under-replicated are placed on one of several priority queues.  The highest priority queue is essentially reserved for situations in which only one replica of the block exists, meaning it should be replicated ASAP.

The ReplicationMonitor periodically computes replication work, and a call to BlockManager#chooseUnderReplicatedBlocks selects a given number of under-replicated blocks, choosing blocks from the highest-priority queue first and working down to the lowest priority queue.

In the subsequent call to BlockManager#computeReplicationWorkForBlocks, a source for the replication is chosen from among datanodes that have an available copy of the block needed.  This is done in BlockManager#chooseSourceDatanode.


chooseSourceDatanode's job is to choose the datanode for replication.  It chooses a random datanode from the available datanodes that has not reached its replication limit (preferring datanodes that are currently decommissioning).

However, the priority queue of the block does not inform the logic.  If a datanode holds the last remaining replica of a block and has already reached its replication limit, the node is dismissed outright and the replication is not scheduled.

In some situations, this could lead to data loss, as the last remaining replica could disappear if an opportunity is not taken to schedule a replication.  It would be better to waive the max replication limit in cases of highest-priority block replication.
",0
"Following issues in DFSInputStream are addressed in this jira:
1. read may not retry enough in some cases cause early failure
Assume the following call logic
{noformat} 
readWithStrategy()
  -> blockSeekTo()
  -> readBuffer()
     -> reader.doRead()
     -> seekToNewSource() add currentNode to deadnode, wish to get a different datanode
        -> blockSeekTo()
           -> chooseDataNode()
              -> block missing, clear deadNodes and pick the currentNode again
        seekToNewSource() return false
     readBuffer() re-throw the exception quit loop
readWithStrategy() got the exception,  and may fail the read call before tried MaxBlockAcquireFailures.
{noformat} 

2. In multi-threaded scenario(like hbase), DFSInputStream.failures has race condition, it is cleared to 0 when it is still used by other thread. So it is possible that  some read thread may never quit. Change failures to local variable solve this issue.

3. If local datanode is added to deadNodes, it will not be removed from deadNodes if DN is back alive. We need a way to remove local datanode from deadNodes when the local datanode is become live.",0
"When a DN marks a disk as bad, it stops using replicas on that disk.

However a long-running {{BlockReaderLocal}} instance will continue to access replicas on the failing disk.

Somehow we should let the in-client BlockReaderLocal know that a disk has been marked as bad so that it can stop reading from the bad disk.

From HDFS-4239:
bq. To rephrase that, a long running BlockReaderLocal will ride over local DN restarts and disk ""ejections"". We had to drain the RS of all its regions in order to stop it from using the bad disk.

",0
"If a DN is ready to send an incremental BR and the NN goes down, the DN will repeatedly try to reconnect.  The NN will then process the DN's incremental BR as an initial BR.  The NN now thinks the DN has only a few blocks, and will ignore all subsequent BRs from that DN until out of safemode -- which it may never do because of all the ""missing"" blocks on the affected DNs.",0
"Currently, in TransferFsImage.downloadEditsToStorage, we download the edits file directly to its finalized path. So, if the transfer fails in the middle, a half-written file is left and cannot be distinguished from a correct file. So, future checkpoints by the 2NN will fail, since the file is truncated in the middle -- but it won't ever download a good copy because it thinks it already has the proper file.",0
"When bringing up a namenode in standby mode, where DEBUG is enabled for namenode, the namenode will hit the following code in {{hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java}}:

{code}
 if (LOG.isDebugEnabled()) {
      LOG.debug(""edit log length: "" + in.length() + "", start txid: ""
          + expectedStartingTxId + "", last txid: "" + lastTxId);
    }
{code}.

However, if {{in}} has an {{EditLogFileInputStream}} as its {{streams[0]}}, this code is hit before the {{EditLogFileInputStream}}'s {{advertizedSize}} is initialized (before the HTTP client connects to the remote edit log server (i.e. the journal node)). This causes the following precondition to fail in {{EditLogFileInputStream:length()}}:

{code}
      Preconditions.checkState(advertisedSize != -1,
          ""must get input stream before length is available"");
{code}

which shuts down the namenode with the following log messages and stack trace:

{code}
2012-12-11 10:45:33,319 DEBUG ipc.ProtobufRpcEngine (ProtobufRpcEngine.java:invoke(217)) - Call: getEditLogManifest took 88ms
2012-12-11 10:45:33,336 DEBUG client.QuorumJournalManager (QuorumJournalManager.java:selectInputStreams(459)) - selectInputStream manifests:
172.16.175.1:8485: [[1,3]]
2012-12-11 10:45:33,351 DEBUG namenode.FSImage (FSImage.java:loadFSImage(605)) - Planning to load image :
FSImageFile(file=/tmp/hadoop-data/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2012-12-11 10:45:33,351 DEBUG namenode.FSImage (FSImage.java:loadFSImage(607)) - Planning to load edit log stream: org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@465098f9
2012-12-11 10:45:33,355 INFO  namenode.FSImage (FSImageFormat.java:load(168)) - Loading image file /tmp/hadoop-data/dfs/name/current/fsimage_0000000000000000000 using no compression
2012-12-11 10:45:33,355 INFO  namenode.FSImage (FSImageFormat.java:load(171)) - Number of files = 1
2012-12-11 10:45:33,356 INFO  namenode.FSImage (FSImageFormat.java:loadFilesUnderConstruction(383)) - Number of files under construction = 0
2012-12-11 10:45:33,357 INFO  namenode.FSImage (FSImageFormat.java:load(193)) - Image file of size 119 loaded in 0 seconds.
2012-12-11 10:45:33,357 INFO  namenode.FSImage (FSImage.java:loadFSImage(753)) - Loaded image for txid 0 from /tmp/hadoop-data/dfs/name/current/fsimage_0000000000000000000
2012-12-11 10:45:33,357 DEBUG namenode.FSImage (FSImage.java:loadEdits(686)) - About to load edits:
  org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@465098f9
2012-12-11 10:45:33,359 INFO  namenode.FSImage (FSImage.java:loadEdits(694)) - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@465098f9 expecting start txid #1
2012-12-11 10:45:33,361 DEBUG ipc.Client (Client.java:stop(1060)) - Stopping client
2012-12-11 10:45:33,363 DEBUG ipc.Client (Client.java:close(1016)) - IPC Client (1462017562) connection to Eugenes-MacBook-Pro.local/172.16.175.1:8485 from hdfs/eugenes-macbook-pro.local@EXAMPLE.COM: closed
2012-12-11 10:45:33,363 DEBUG ipc.Client (Client.java:run(848)) - IPC Client (1462017562) connection to Eugenes-MacBook-Pro.local/172.16.175.1:8485 from hdfs/eugenes-macbook-pro.local@EXAMPLE.COM: stopped, remaining connections 0
2012-12-11 10:45:33,464 FATAL namenode.NameNode (NameNode.java:main(1224)) - Exception in namenode join
java.lang.IllegalStateException: must get input stream before length is available
        at com.google.common.base.Preconditions.checkState(Preconditions.java:145)
        at org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream$URLLog.length(EditLogFileInputStream.java:405)
        at org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream.length(EditLogFileInputStream.java:258)
        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.length(RedundantEditLogInputStream.java:256)
        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.length(RedundantEditLogInputStream.java:256)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:125)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:88)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:697)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:642)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:259)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:604)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:447)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:409)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:400)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:434)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:606)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:591)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1153)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1219)
2012-12-11 10:45:33,470 INFO  util.ExitUtil (ExitUtil.java:terminate(84)) - Exiting with status 1
2012-12-11 10:45:33,471 INFO  namenode.NameNode (StringUtils.java:run(620)) - SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at Eugenes-MacBook-Pro.local/172.16.175.1
************************************************************/
{code}


",0
"In some nightly test runs we've seen pretty frequent failures of TestWebHdfsWithMultipleNameNodes. I've traced the root cause to an unsynchronized map access in the DataStorage class.

More details in the first comment.",0
"Edits dir must be listed in both dfs.namenode.edits.dir and dfs.namenode.edits.dir.required for the later to take effect. 
We should throw an invalid configuration error when a dir is in .required but not in dfs.namenode.edits.dir",0
"There's a bug in {{BlockPlacementPolicyDefault#chooseTarget}} with stale node avoidance enabled (HDFS-3912). If a NotEnoughReplicasException is thrown in the call to {{chooseRandom()}}, {{numOfReplicas}} is not updated together with the partial result in {{result}} since it is pass by value. The retry call to {{chooseTarget}} then uses this incorrect value.

This can be seen if you enable stale node detection for {{TestReplicationPolicy#testChooseTargetWithMoreThanAvaiableNodes()}}.",0
"In certain cases, higher-priority under-replicated blocks can be skipped by the replication policy implementation.  The current implementation maintains, for each priority level, an index into a list of blocks that are under-replicated.  Together, the lists compose a priority queue (see note later about branch-0.23).  In some cases when blocks are removed from a list, the caller (BlockManager) properly handles the index into the list from which it removed a block.  In some other cases, the index remains stationary while the list changes.  Whenever this happens, and the removed block happened to be at or before the index, the implementation will skip over a block when selecting blocks for replication work.

In situations when entire racks are decommissioned, leading to many under-replicated blocks, loss of blocks can occur.


Background: HDFS-1765

This patch to trunk greatly improved the state of the replication policy implementation.  Prior to the patch, the following details were true:
	* The block ""priority queue"" was no such thing: It was really set of trees that held blocks in natural ordering, that being by the blocks ID, which resulted in iterator walks over the blocks in pseudo-random order.
	* There was only a single index into an iteration over all of the blocks...
	* ... meaning the implementation was only successful in respecting priority levels on the first pass.  Overall, the behavior was a round-robin-type scheduling of blocks.

After the patch
	* A proper priority queue is implemented, preserving log n operations while iterating over blocks in the order added.
	* A separate index for each priority is key is kept...
	* ... allowing for processing of the highest priority blocks first regardless of which priority had last been processed.

The change was suggested for branch-0.23 as well as trunk, but it does not appear to have been pulled in.


The problem:

Although the indices are now tracked in a better way, there is a synchronization issue since the indices are managed outside of methods to modify the contents of the queue.

Removal of a block from a priority level without adjusting the index can mean that the index then points to the block after the block it originally pointed to.  In the next round of scheduling for that priority level, the block originally pointed to by the index is skipped.
",0
GetDataEncryptionKeyResponseProto member dataEncryptionKey should be optional to handle null response.,0
GetBlockKeysResponseProto#keys should be optional to handle null response,0
"JD Cryans found this issue: it seems like, if you open a file for read immediately after it's been created by the writer, after a block has been allocated, but before the block is created on the DNs, then you can end up with the following NPE:

java.lang.NullPointerException
       at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.updateBlockInfo(DFSClient.java:1885)
       at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:1858)
       at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.<init>(DFSClient.java:1834)
       at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:578)
       at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:154)

This seems to be because {{getBlockInfo}} returns a null block when the DN doesn't yet have the replica. The client should probably either fall back to a different replica or treat it as zero-length.",0
"The HA retry policy implementation appears to have broken non-HA {{DFSClient}} connect retries.  The ipc {{Client.Connection#handleConnectionFailure}} used to perform 45 connection attempts, but now it consults a retry policy.  For non-HA proxies, the policy does not handle {{ConnectException}}.",0
"test Environment: NN1,NN2,DN1,DN2,DN3
machine1:NN1,DN1
machine2:NN2,DN2
machine3:DN3

mathine1 is down.

2013-01-12 09:51:21,248 DEBUG ipc.Client (Client.java:setupIOstreams(562)) - Connecting to /160.161.0.155:8020
2013-01-12 09:51:38,442 DEBUG ipc.Client (Client.java:close(932)) - closing ipc connection to vm2/160.161.0.155:8020: 10000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/160.161.0.155:8020]
java.net.SocketTimeoutException: 10000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/160.161.0.155:8020]
 at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:213)
 at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:524)
 at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:489)
 at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:474)
 at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:568)
 at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:217)
 at org.apache.hadoop.ipc.Client.getConnection(Client.java:1286)
 at org.apache.hadoop.ipc.Client.call(Client.java:1156)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:184)
 at $Proxy9.create(Unknown Source)
 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:187)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
 at java.lang.reflect.Method.invoke(Method.java:597)
 at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:165)
 at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:84)
 at $Proxy10.create(Unknown Source)
 at org.apache.hadoop.hdfs.DFSOutputStream.<init>(DFSOutputStream.java:1261)
 at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1280)
 at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1128)
 at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1086)
 at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:232)
 at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:75)
 at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:806)
 at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)
 at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:715)
 at test.TestLease.main(TestLease.java:45)
2013-01-12 09:51:38,443 DEBUG ipc.Client (Client.java:close(940)) - IPC Client (31594013) connection to /160.161.0.155:8020 from hdfs/hadoop@HADOOP.COM: closed
2013-01-12 09:52:47,834 WARN  retry.RetryInvocationHandler (RetryInvocationHandler.java:invoke(95)) - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create. Not retrying because the invoked method is not idempotent, and unable to determine whether it was invoked
java.net.SocketTimeoutException: Call From szxy1x001833091/172.0.0.13 to vm2:8020 failed on socket timeout exception: java.net.SocketTimeoutException: 10000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/160.161.0.155:8020]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
 at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:743)
 at org.apache.hadoop.ipc.Client.call(Client.java:1180)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:184)
 at $Proxy9.create(Unknown Source)
 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:187)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
 at java.lang.reflect.Method.invoke(Method.java:597)
 at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:165)
 at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:84)
 at $Proxy10.create(Unknown Source)
 at org.apache.hadoop.hdfs.DFSOutputStream.<init>(DFSOutputStream.java:1261)
 at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1280)
 at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1128)
 at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1086)
 at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:232)
 at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:75)
 at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:806)
 at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)
 at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:715)
 at test.TestLease.main(TestLease.java:45)
Caused by: java.net.SocketTimeoutException: 10000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/160.161.0.155:8020]
 at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:213)
 at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:524)
 at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:489)
 at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:474)
 at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:568)
 at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:217)
 at org.apache.hadoop.ipc.Client.getConnection(Client.java:1286)
 at org.apache.hadoop.ipc.Client.call(Client.java:1156)
 ... 20 more
java.net.SocketTimeoutException: Call From szxy1x001833091/172.0.0.13 to vm2:8020 failed on socket timeout exception: java.net.SocketTimeoutException: 10000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/160.161.0.155:8020]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
 at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:743)
 at org.apache.hadoop.ipc.Client.call(Client.java:1180)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:184)
 at $Proxy9.create(Unknown Source)
 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:187)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
 at java.lang.reflect.Method.invoke(Method.java:597)
 at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:165)
 at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:84)
 at $Proxy10.create(Unknown Source)
 at org.apache.hadoop.hdfs.DFSOutputStream.<init>(DFSOutputStream.java:1261)
 at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1280)
 at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1128)
 at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1086)
 at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:232)
 at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:75)
 at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:806)
 at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)
 at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:715)
 at test.TestLease.main(TestLease.java:45)
Caused by: java.net.SocketTimeoutException: 10000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/160.161.0.155:8020]
 at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:213)
 at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:524)
 at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:489)
 at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:474)
 at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:568)
 at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:217)
 at org.apache.hadoop.ipc.Client.getConnection(Client.java:1286)
 at org.apache.hadoop.ipc.Client.call(Client.java:1156)
 ... 20 more
2013-01-12 09:54:52,269 DEBUG ipc.Client (Client.java:stop(1021)) - Stopping client
",0
"If HDFS is not the default file system (fs.default.name is something other than hdfs://...), then secondary namenode throws early on in its initialization. This is a needless check as far as I can tell, and blocks scenarios where HDFS services are up but HDFS is not the default file system.",0
"In {{HostnameFilter}}, if the remote address is NULL or the inet resolution fails the request will never make it to the httpfs server.
We should instead set the hostname to something like ""???"" and let the processing continue.",0
"The impact of class is org.apache.hadoop.hdfs.server.namenode.FSImage.java
{code}
boolean loadFSImage(MetaRecoveryContext recovery) throws IOException {
...
latestNameSD.read();
    needToSave |= loadFSImage(getImageFile(latestNameSD, NameNodeFile.IMAGE));
    LOG.info(""Image file of size "" + imageSize + "" loaded in "" 
        + (FSNamesystem.now() - startTime)/1000 + "" seconds."");
    
    // Load latest edits
    if (latestNameCheckpointTime > latestEditsCheckpointTime)
      // the image is already current, discard edits
      needToSave |= true;
    else // latestNameCheckpointTime == latestEditsCheckpointTime
      needToSave |= (loadFSEdits(latestEditsSD, recovery) > 0);
    
    return needToSave;
  }
{code}
If it is the normal flow of the checkpoint,the value of latestNameCheckpointTime  is equal to the value of latestEditsCheckpointTime锛宎nd it will exec 鈥渆lse鈥?
The problem is that锛宭atestNameCheckpointTime > latestEditsCheckpointTime锛?SecondNameNode starts checkpoint锛?...
NameNode锛歳ollFSImage锛孨ameNode shutdown after write latestNameCheckpointTime and before write latestEditsCheckpointTime.
Start NameNode锛歜ecause latestNameCheckpointTime > latestEditsCheckpointTime锛宻o the value of needToSave is true锛?and it will not update 鈥渞ootDir鈥?s nsCount that is the cluster's file number锛坲pdate exec at loadFSEdits 鈥淔SNamesystem.getFSNamesystem().dir.updateCountForINodeWithQuota()鈥濓級锛宎nd then 鈥渟aveNamespace鈥?will write file number to fsimage whit default value 鈥?鈥濄?The next time锛宭oadFSImage will fail.

Maybe锛宨t will work:
{code}
boolean loadFSImage(MetaRecoveryContext recovery) throws IOException {
...
latestNameSD.read();
    needToSave |= loadFSImage(getImageFile(latestNameSD, NameNodeFile.IMAGE));
    LOG.info(""Image file of size "" + imageSize + "" loaded in "" 
        + (FSNamesystem.now() - startTime)/1000 + "" seconds."");
    
    // Load latest edits
    if (latestNameCheckpointTime > latestEditsCheckpointTime){
      // the image is already current, discard edits
      needToSave |= true;
      FSNamesystem.getFSNamesystem().dir.updateCountForINodeWithQuota();
    }
    else // latestNameCheckpointTime == latestEditsCheckpointTime
      needToSave |= (loadFSEdits(latestEditsSD, recovery) > 0);
    
    return needToSave;
  }
{code}
",0
"File: /hadoop-1.0.1/hdfs/org/apache/hadoop/hdfs/server/datanode/FSDataset.java

from line 205:
{code}
      if (children == null || children.length == 0) {
        children = new FSDir[maxBlocksPerDir];
        for (int idx = 0; idx < maxBlocksPerDir; idx++) {
          children[idx] = new FSDir(new File(dir, DataStorage.BLOCK_SUBDIR_PREFIX+idx));
        }
      }
{code}
in FSDir constructer method if faild (  space full,so mkdir fails    ), but  the children still in use !


the the write comes(after I run balancer ) , when choose FSDir 

line 192:
    File file = children[idx].addBlock(b, src, false, resetIdx);

cause exceptions like this
{code}
java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.datanode.FSDataset$FSDir.addBlock(FSDataset.java:192)
        at org.apache.hadoop.hdfs.server.datanode.FSDataset$FSDir.addBlock(FSDataset.java:192)
        at org.apache.hadoop.hdfs.server.datanode.FSDataset$FSDir.addBlock(FSDataset.java:158)
        at org.apache.hadoop.hdfs.server.datanode.FSDataset$FSVolume.addBlock(FSDataset.java:495)
{code}



------------------------------------------------
should it like this 

{code}
      if (children == null || children.length == 0) {
          List childrenList = new ArrayList();
        
        for (int idx = 0; idx < maxBlocksPerDir; idx++) {
          try{
           childrenList .add( new FSDir(new File(dir, DataStorage.BLOCK_SUBDIR_PREFIX+idx)));
          }catch(Exception e){
          }
          children = childrenList.toArray();
        }
      }
{code}



----------------------------
bad consequence , in my cluster ,this datanode's num blocks became 0 .












",0
"After HADOOP-9181 went in, the secondary namenode immediately shuts down after it is started.  From the startup logs:

{noformat}
2013-01-22 19:54:28,826 INFO  namenode.SecondaryNameNode (SecondaryNameNode.java:initialize(299)) - Checkpoint Period   :3600 secs (60 min)
2013-01-22 19:54:28,826 INFO  namenode.SecondaryNameNode (SecondaryNameNode.java:initialize(301)) - Log Size Trigger    :40000 txns
2013-01-22 19:54:28,845 INFO  namenode.SecondaryNameNode (StringUtils.java:run(616)) - SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at xx
************************************************************/
{noformat}

I looked into the issue, and it's shutting down because SecondaryNameNode.main starts a bunch of daemon threads then returns.  With nothing but daemon threads remaining, the JVM sees no reason to keep going and proceeds to shutdown.  Apparently we were implicitly relying on the fact that the HttpServer QueuedThreadPool threads were not daemon threads to keep the secondary namenode process up.",0
"Moving open files and/or parent directories of open files will rewrite the leases to the new path.  The client is not notified so future stream operations will fail.  However, as long as the client keeps its lease active it will have a ""lock"" on the file in its new location.  This is not good for a daemon.

Leases should be released after the file is moved.",0
"Currently if one tries to configure HA NNs use the wildcard HTTP address when security is enabled, the NN will fail to start with an error like the following:
{code}
java.lang.IllegalArgumentException: java.io.IOException: Cannot use a wildcard address with security. Must explicitly set bind address for Kerberos
{code}
This is the case even if one configures an actual address for the other NN's HTTP address. There's no good reason for this, since we now check for the local address being set to 0.0.0.0 and determine the canonical hostname for Kerberos purposes using {{InetAddress.getLocalHost().getCanonicalHostName()}}, so we should remove the restriction.",0
"HDFS client tries to addBlock() to a file. If NameNode is busy the client can timeout and will reissue the same request again. The two requests will race with each other in {{FSNamesystem.getAdditionalBlock()}}, which can result in creating two new blocks on the NameNode while the client will know of only one of them. This eventually results in {{NotReplicatedYetException}} because the extra block is never reported by any DataNode, which stalls file creation and puts it in invalid state with an empty block in the middle.",0
"Today we got ourselves into a situation where we hard killed the cluster (kill -9 across the board on all processes) and upon restarting all DNs would permanently give up on of the NNs in our two NN HA setup (using QJM).

The HA setup is correct (prior to this we failed over the NNs many times for testing). Bouncing the DNs resolved the problem.

In the logs I see this exception:
{code}
2013-01-29 23:32:49,461 FATAL datanode.DataNode - Initialization failed for block pool Block pool BP-1852726028-<ip>-1358813649047 (storage id DS-60505003-<ip>-50010-1353106051747) service to <host>/<ip>:8020
java.io.IOException: Failed on local exception: java.io.IOException: Response is null.; Host Details : local host is: ""<host>/<ip>""; destination host is: ""<host>"":8020; 
        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:759)
        at org.apache.hadoop.ipc.Client.call(Client.java:1164)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:202)
        at $Proxy10.registerDatanode(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:164)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:83)
        at $Proxy10.registerDatanode(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.registerDatanode(DatanodeProtocolClientSideTranslatorPB.java:149)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.register(BPServiceActor.java:619)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:221)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:661)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Response is null.
        at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:885)
        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:813)
2013-01-29 23:32:49,463 WARN  datanode.DataNode - Ending block pool service for: Block pool BP-1852726028-<ip>-1358813649047 (storage id DS-60505003-<ip>-50010-1353106051747) service to <host>/<ip>:8020
{code}

So somehow in BPServiceActor.connectToNNAndHandshake() we made it all the way to register(). Then failed in bpNamenode.registerDatanode(bpRegistration) with an IOException, which is not caught and has the block pool service fail as a whole.

No doubt that was caused by one of the NNs being a weird state. While that happened the active NN claimed that the FS was corrupted and stayed in safe mode, and DNs only registered with the standby DN. Failing over to the 2nd NN and then restarting the first NN and failing did not change that.

No amount bouncing/failing over the HA NNs would have the DNs reconnect to one of the NNs.

In BPServiceActor.register(), should we catch IOException instead of SocketTimeoutException? That way it would continue to retry and eventually connect to the NN.
",0
"start balancer failed with ""Failed to create file [/system/balancer.id]""  if configure IP on fs.defaultFS
Note:
configure fs.defaultFS with hostname can resolve this issue

See details in the comment - https://issues.apache.org/jira/browse/HDFS-4458?focusedCommentId=13567802&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13567802

",0
"The 2NN currently has logic to detect when its on-disk FS metadata needs an upgrade with respect to the NN's metadata (i.e. the layout versions are different) and in this case it will proceed with the checkpoint despite storage signatures not matching precisely if the BP ID and Cluster ID do match exactly. However, in situations where we're upgrading from versions of HDFS prior to federation, which had no BP IDs or Cluster IDs, checkpoints will always fail with an error like the following:
{noformat}
13/01/31 17:02:25 ERROR namenode.SecondaryNameNode: checkpoint: Inconsistent checkpoint fields.
LV = -40 namespaceID = 403832480 cTime = 1359680537192 ; clusterId = CID-0df6ff22-1165-4c7d-9630-429972a7737c ; blockpoolId = BP-1520616013-172.21.3.106-1359680537136.
Expecting respectively: -19; 403832480; 0; ; .
{noformat}",0
"Connecting to HDFS using the libhdfs compiled library gives a segmentation vault and memory leaks; easily verifiable by valgrind.

Even a simple application program given below has memory leaks:


#include ""hdfs.h""
#include <iostream>

int main(int argc, char **argv) {

    hdfsFS fs = hdfsConnect(""localhost"", 9000);
    const char* writePath = ""/tmp/testfile.txt"";
    hdfsFile writeFile = hdfsOpenFile(fs, writePath, O_WRONLY|O_CREAT, 0, 0, 0);
    if(!writeFile) {
          fprintf(stderr, ""Failed to open %s for writing!\n"", writePath);
          exit(-1);
    }
    char* buffer = ""Hello, World!"";
    tSize num_written_bytes = hdfsWrite(fs, writeFile, (void*)buffer, strlen(buffer)+1);
    if (hdfsFlush(fs, writeFile)) {
           fprintf(stderr, ""Failed to 'flush' %s\n"", writePath);
          exit(-1);
    }
   hdfsCloseFile(fs, writeFile);
}


shell>valgrind  --leak-check=full ./sample

==12773== LEAK SUMMARY:
==12773==    definitely lost: 7,893 bytes in 21 blocks
==12773==    indirectly lost: 4,460 bytes in 23 blocks
==12773==      possibly lost: 119,833 bytes in 121 blocks
==12773==    still reachable: 1,349,514 bytes in 8,953 blocks

",0
"Trace:

{noformat}
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.getFullPathName(FSDirectory.java:1442)
	at org.apache.hadoop.hdfs.server.namenode.INode.getFullPathName(INode.java:269)
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.getName(INodeFile.java:163)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy.chooseTarget(BlockPlacementPolicy.java:131)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1157)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1063)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3085)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3047)
	at java.lang.Thread.run(Thread.java:619)

{noformat}

What I am seeing here is:

1) create a file and write with 2 DNS
2) Close the file.
3) Kill one DN
4) Let replication start.
  Info:
    {code}
 // choose replication targets: NOT HOLDING THE GLOBAL LOCK
      // It is costly to extract the filename for which chooseTargets is called,
      // so for now we pass in the block collection itself.
      rw.targets = blockplacement.chooseTarget(rw.bc,
          rw.additionalReplRequired, rw.srcNode, rw.liveReplicaNodes,
          excludedNodes, rw.block.getNumBytes());{code}

Here we are choosing target outside the global lock. Inside we will try to get the src path from blockCollection(nothing but INodeFile here).

see the code for FSDirectory#getFullPathName
 Here it is incrementing the depth until it has parent. and Later it will iterate and access parent again in next loop.

5) before going to secnd loop in FSDirectory#getFullPathName, if file is deleted by client then that parent would have been set as null. So, here accessing the parent can cause NPE because it is not under lock.

[~brahmareddy] reported this issue.",0
"The current lease renewal code in DFSClient gives up after several retries, if the soft limit exceeds. This causes the client to abort.  Without this self destruction behavior, lease renewal can be retried infinitely due to unrelated low-level issue and prevent DFSClient and associated Connection from getting garbage collected.  

However, giving up at passage of the soft limit has been shown to be too fragile; A long GC on namenode or transient network outage can make clients fail. We need the self-destruct behavior, but clients should be allowed to retry for longer period of time.",0
"{{DFSOutputStream#close}} can throw an {{IOException}} in some cases.  One example is if there is a pipeline error and then pipeline recovery fails.  Unfortunately, in this case, some of the resources used by the {{DFSOutputStream}} are leaked.  One particularly important resource is file leases.

So it's possible for a long-lived HDFS client, such as Flume, to write many blocks to a file, but then fail to close it.  Unfortunately, the {{LeaseRenewerThread}} inside the client will continue to renew the lease for the ""undead"" file.  Future attempts to close the file will just rethrow the previous exception, and no progress can be made by the client.",0
"This set of properties ...

<property><name>dfs.namenode.https-address.NameNode1</name><value>192.168.0.10:50470</value></property>
<property><name>dfs.namenode.http-address.NameNode1</name><value>192.168.0.10:50070</value></property>
<property><name>dfs.namenode.rpc-address.NameNode1</name><value>192.168.0.10:9000</value></property>
<property><name>dfs.nameservice.id</name><value>NameNode1</value></property>
<property><name>dfs.nameservices</name><value>NameNode1</value></property>

gives following issue while running balancer ...

2012-12-27 15:42:36,193 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: namenodes = [hdfs://queryio10.local:9000, hdfs://192.168.0.10:9000]
2012-12-27 15:42:36,194 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: p         = Balancer.Parameters[BalancingPolicy.Node, threshold=10.0]
2012-12-27 15:42:37,433 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/192.168.0.10:50010
2012-12-27 15:42:37,433 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: 0 over-utilized: []
2012-12-27 15:42:37,433 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: 0 underutilized: []
2012-12-27 15:42:37,436 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/192.168.0.10:50010
2012-12-27 15:42:37,436 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: 0 over-utilized: []
2012-12-27 15:42:37,436 INFO org.apache.hadoop.hdfs.server.balancer.Balancer: 0 underutilized: []
2012-12-27 15:42:37,570 WARN org.apache.hadoop.hdfs.DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /system/balancer.id File does not exist. Holder DFSClient_NONMAPREDUCE_1926739478_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2315)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2306)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2102)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:469)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:294)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:43138)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:454)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:910)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1694)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1690)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1367)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1688)

	at org.apache.hadoop.ipc.Client.call(Client.java:1164)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:202)
	at $Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:164)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:83)
	at $Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:285)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1150)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1003)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:463)
2012-12-27 15:42:37,579 ERROR org.apache.hadoop.hdfs.DFSClient: Failed to close file /system/balancer.id
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /system/balancer.id File does not exist. Holder DFSClient_NONMAPREDUCE_1926739478_1 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2315)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2306)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2102)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:469)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:294)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:43138)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:454)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:910)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1694)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1690)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1367)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1688)

	at org.apache.hadoop.ipc.Client.call(Client.java:1164)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:202)
	at $Proxy10.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:164)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:83)
	at $Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:285)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1150)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1003)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:463)
",0
"We found a case, where if the short circuit user name is configured correctly, but the user does not have enough permissions in unix, DFS operations fails with IOException, rather than silently failing over through datanode. ",0
"If client crashes just after allocating block( blocks not yet created in DNs) and NN also switched after this, then new Namenode will not know about locs.

Further details will be in comment.",0
This is similar to HDFS-4105. in the NameNodeHttpServer.start() initSpnego should look for dfs.web.authentication.kerberos.keytab before using dfs.namenode.keytab.file.,0
Webhdfs will NPE in {{validateResponse}} if an expected response code is received w/o a json payload containing the exception.,0
"Clients know to fail over to talk to the Active NN when they perform an RPC to the Standby NN and it throws a StandbyException. However, most places in the code that check if the NN is in the standby state do so inside the FSNS fsLock. Since this lock is held for the duration of the saveNamespace during a checkpoint, StandbyExceptions will not be thrown during this time.",0
"In 1.0, when short circuit is disallowed for a user, the DFSClient disables short circuit and falls back to the regular socket based reads over DataTransferProtocol. In release 2.0, this fallback functionality is not working.",0
"This is a really rare error that can hit if a NN shutdown happens during the checkpointing process.

Checkpointing and restarting nominally looks like this:

# FSImage is written to a tmp file and then renamed
# MD5 file is written to a tmp file and then renamed
# NN is killed and restarted
# NN scans storage directories and picks up the renamed image file
# NN validates that the image file matches its md5 file

If the NN is killed before step 2 completes, this is what happens:

# FSImage is written to a tmp file and then renamed
# NN is killed and restarted (no MD5 file!)
# NN scans storage directories and picks up the renamed image file
# Since there's no matching MD5 file, NN errors out with a checksum error

I think we can fix this by inverting the order of writing the image then md5, or inverting the order of reading the image then md5.",0
"The Client RPC I/O timeout mechanism appears to be configured by two core-site.xml paramters:

1. A boolean ipc.client.ping
2. A numeric value ipc.ping.interval

If ipc.client.ping is true, then we send a RPC ping every ipc.ping.interval milliseconds
If ipc.client.ping is false, then ipc.ping.interval turns into the socket timeout value.

The bug here is that while creating a Non HA proxy, the configured timeout value is ignored, and 0 is passed in. 0 is taken to mean 'wait forever' and the client RPC socket never times out.

Note that this bug is reproducible only in the case where the NN machine dies, i.e. the TCP stack with the NN IP address stops responding completely. The code does not take this path when you do a 'kill -9' of the NN process, since there is a TCP stack that is alive and sends out a TCP RST to the client, and that results in a socket error (not a timeout).

The fix is to pass in the correct configured value for timeout by calling Client.getTimeout(conf) instead of passing in 0.",0
"Webhdfs returns malformed json for directories that exceed the conf {{dfs.ls.limit}} value.  The streaming object returned by  {{NamenodeWebhdfsMethods#getListingStream}} will repeatedly call {{getListing}} for each segment of the directory listing.  {{getListingStream}} runs within the remote user's ugi and acquires the first segment of the directory, then returns a streaming object.  The streaming object is later executed _outside of the user's ugi_.  Luckily it runs as the host service principal (ie. {{host/namenode@REALM}}) so the result is permission denied for the ""host"" user:
{noformat}
org.apache.hadoop.security.AccessControlException: Permission denied: user=host, access=EXECUTE, inode=""/path"":someuser:group:drwx------
{noformat}

The exception causes the streamer to prematurely abort the json output leaving it malformed.  Meanwhile, the client sees the cryptic:
{noformat}
java.lang.IllegalStateException: unexpected end of array
        at org.mortbay.util.ajax.JSON.parseArray(JSON.java:902)
        [...]
        at
org.apache.hadoop.hdfs.web.WebHdfsFileSystem.jsonParse(WebHdfsFileSystem.java:242)
        at
org.apache.hadoop.hdfs.web.WebHdfsFileSystem.run(WebHdfsFileSystem.java:441)
        at
org.apache.hadoop.hdfs.web.WebHdfsFileSystem.listStatus(WebHdfsFileSystem.java:717)
        [...]
{noformat}",0
"pipeline DN1  DN2  DN3
stop DN2

pipeline added node DN4 located at 2nd position
DN1  DN4  DN3

recover RBW
DN4 after recover rbw
2013-04-01 21:02:31,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Recover RBW replica BP-325305253-10.2.201.14-1364820083462:blk_-9076133543772600337_1004
2013-04-01 21:02:31,570 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Recovering ReplicaBeingWritten, blk_-9076133543772600337_1004, RBW
  getNumBytes() = 134144
  getBytesOnDisk() = 134144
  getVisibleLength()= 134144
end at chunk (134144/512=262)

DN3 after recover rbw
2013-04-01 21:02:31,575 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Recover RBW replica BP-325305253-10.2.201.14-1364820083462:blk_-9076133543772600337_10042013-04-01 21:02:31,575 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Recovering ReplicaBeingWritten, blk_-9076133543772600337_1004, RBW
  getNumBytes() = 134028 
  getBytesOnDisk() = 134028
  getVisibleLength()= 134028

client send packet after recover pipeline
offset=133632  len=1008

DN4 after flush 
2013-04-01 21:02:31,779 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: FlushOrsync, file offset:134640; meta offset:1063
// meta end position should be floor(134640/512)*4 + 7 == 1059, but now it is 1063.

DN3 after flush
2013-04-01 21:02:31,782 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-325305253-10.2.201.14-1364820083462:blk_-9076133543772600337_1005, type=LAST_IN_PIPELINE, downstreams=0:[]: enqueue Packet(seqno=219, lastPacketInBlock=false, offsetInBlock=134640, ackEnqueueNanoTime=8817026136871545)
2013-04-01 21:02:31,782 DEBUG org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Changing meta file offset of block BP-325305253-10.2.201.14-1364820083462:blk_-9076133543772600337_1005 from 1055 to 1051
2013-04-01 21:02:31,782 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: FlushOrsync, file offset:134640; meta offset:1059

After checking meta on DN4, I found checksum of chunk 262 is duplicated, but data not.
Later after block was finalized, DN4's scanner detected bad block, and then reported it to NM. NM send a command to delete this block, and replicate this block from other DN in pipeline to satisfy duplication num.

I think this is because in BlockReceiver it skips data bytes already written, but not skips checksum bytes already written. And function adjustCrcFilePosition is only used for last non-completed chunk, but
not for this situation.

",0
"When security is not enabled, non-superusers can fetch the fsimage. This is problematic because the non-superusers can then process the fsimage for contents the user should not have access to.

For example, schu is not a superuser and does not have access to hdfs://user/hdfs/. However, schu can still fetch the fsimage and run the OfflineImageViewer on the fsimage to examine the contents of hdfs://user/hdfs/.

{code}
[schu@hdfs-vanilla-1 images]$ hadoop fs -ls /user/hdfs
ls: Permission denied: user=schu, access=READ_EXECUTE, inode=""/user/hdfs"":hdfs:supergroup:drwx------
[schu@hdfs-vanilla-1 images]$ hdfs dfsadmin -fetchImage ~/images/
13/04/08 12:45:20 INFO namenode.TransferFsImage: Opening connection to http://hdfs-vanilla-1.ent.cloudera.com:50070/getimage?getimage=1&txid=latest
13/04/08 12:45:21 INFO namenode.TransferFsImage: Transfer took 0.91s at 91.61 KB/s
[schu@hdfs-vanilla-1 images]$ hdfs oiv -i ~/images/fsimage_0000000000000947148 -o ~/images/oiv.out
{code}

When kerberos authentication is enabled, superuser privilege is enforced:
{code}
[testuser@hdfs-secure-1 ~]$ hdfs dfsadmin -fetchImage ~/images/
13/04/08 12:48:23 INFO namenode.TransferFsImage: Opening connection to http://hdfs-secure-1.ent.cloudera.com:50070/getimage?getimage=1&txid=latest
13/04/08 12:48:23 ERROR security.UserGroupInformation: PriviledgedActionException as:testuser@ENT.CLOUDERA.COM (auth:KERBEROS) cause:org.apache.hadoop.hdfs.server.namenode.TransferFsImage$HttpGetFailedException: Image transfer servlet at http://hdfs-secure-1.ent.cloudera.com:50070/getimage?getimage=1&txid=latest failed with status code 403
Response message:
Only Namenode, Secondary Namenode, and administrators may access this servlet
fetchImage: Image transfer servlet at http://hdfs-secure-1.ent.cloudera.com:50070/getimage?getimage=1&txid=latest failed with status code 403
Response message:
Only Namenode, Secondary Namenode, and administrators may access this servlet
[testuser@hdfs-secure-1 ~]$ 
{code}

We should still enforce checking privileges when kerberos authentication is disabled.",0
Currenly hdfs doesn't support Extended file ACL. In unix extended ACL can be achieved using getfacl and setfacl utilities. Is there anybody working on this feature ?,0
"Credit to Harsh for tracing down most of this.

If a DFSClient does create with overwrite multiple times on the same file, we can get into bad states. The exact failure mode depends on the state of the file, but at the least one DFSOutputStream will ""win"" over the others, leading to data loss in the sense that data written to the other DFSOutputStreams will be lost. While this is perhaps okay because of overwrite semantics, we've also seen other cases where the DFSClient loops indefinitely on close and blocks get marked as corrupt. This is not okay.

One fix for this is adding some locking to DFSClient which prevents a user from opening multiple concurrent output streams to the same path.",0
"Going into safemode involves stopping the secret manager's thread.  An interrupt is sent to this thread.  If this occurs while the thread is logging an edit, the resulting exception in the thread will cause the NN to exit.

The fix is to back-port a subset of HDFS-2579 which went into branch 2 & trunk.",0
"I have seen {{TestPipelinesFailover#testPipelineRecoveryStress}} fail sporadically due to timeout during {{loopRecoverLease}}, which waits for up to 30 seconds before timing out.",0
"The code in NNStorageRetentionManager#purgeOldStorage is intended to place a cap on the number of _extra_ edit log segments retained beyond what is strictly required to replay the FS history since the last fsimage. In fact this code currently places a limit on the _total_ number of extra edit log segments. If the number of required segments is greater than the configured cap, there will be no data loss, but an ugly error will be thrown and the NN will fail to start.

The fix is simple, and in the meantime a work-around is just to raise the value of dfs.namenode.max.extra.edits.segments.retained and start the NN.",0
"In some cases the last block replica of a file can be reported after the file was closed. In this case file inode is of type INodeFile. BlockManager.addStoredBlock() though expects it to be INodeFileUnderConstruction, and therefore class cast to MutableBlockCollection fails.",0
"With permissions enabled, the permission check in {{FSNamesystem#delete}} will incorrectly throw an UnresolvedLinkException if the path contains a symlink. This leads to FileContext resolving the symlink and instead deleting the link target.

The correct check is to see if the user has write permissions on the parent directory of the symlink, e.g.

{noformat}
-> % ls -ld symtest
drwxr-xr-x 2 root root 4096 Apr 26 14:12 symtest
-> % ls -l symtest
total 12
lrwxrwxrwx 1 root root 6 Apr 26 14:12 link -> target
-rw-r--r-- 1 root root 0 Apr 26 14:11 target
-> % rm -f symtest/link
rm: cannot remove `symtest/link': Permission denied
-> % sudo chown andrew symtest
-> % rm -f symtest/link       
-> % 
{noformat}",0
"Browsing filesystem via webui throws kerberos exception when NN service RPC is enabled in a secure cluster
To reproduce this error, 

Enable security 
Enable serviceRPC by setting dfs.namenode.servicerpc-address and use a different port than the rpc port.


Click on ""Browse the filesystem"" on NameNode web.

The following error will be shown :
Call to NN001/12.123.123.01:8030 failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
",0
"We saw the following sequence of events in a cluster result in losing the most recent genstamp of a block:
- client is writing to a pipeline of 3
- the pipeline had nodes fail over some period of time, such that it left 3 old-genstamp replicas on the original three nodes, having recruited 3 new replicas with a later genstamp.
-- so, we have 6 total replicas in the cluster, three with old genstamps on downed nodes, and 3 with the latest genstamp
- cluster reboots, and the nodes with old genstamps blockReport first. The replicas are correctly added to the corrupt replicas map since they have a too-old genstamp
- the nodes with the new genstamp block report. When the latest one block reports, chooseExcessReplicates is called and incorrectly decides to remove the three good replicas, leaving only the old-genstamp replicas.",0
"Webhdfs internally acquires a token that will be used for DN-based operations.  The token renewer in common will try to renew that token.  If a renewal fails for any reason, it will try to get another token.  If that fails, it gives up and the token webhdfs holds will soon expire.

A transient network outage or a restart of the NN may cause webhdfs to be left holding an expired token, effectively rendering webhdfs useless.  This is fatal for daemons.",0
"The NN and SBN do this dance during checkpoint image transfer with nested HTTP GETs via {{HttpURLConnection}}. When an admin does a {{-transitionToActive}} during this transfer, part of that is interrupting an ongoing checkpoint so we can transition immediately.

However, the {{thread.interrupt()}} in {{StandbyCheckpointer#stop}} gets swallowed by {{connection.getResponseCode()}} in {{TransferFsImage#doGetUrl}}. None of the methods in HttpURLConnection throw InterruptedException, so we need to do something else (perhaps HttpClient [1]):

[1]: http://hc.apache.org/httpclient-3.x/",0
"Courtesy Karri VRK Reddy!
{quote}
1. Namenode lost datanodes causing missing blocks
2. Namenode was put in safe mode
3. Datanode restarted on dead nodes 
4. Waited for lots of time for the NN UI to reflect the recovered blocks.
5. Forced NN out of safe mode and suddenly,  no more missing blocks anymore.
{quote}

I was able to replicate this on 0.23 and trunk. I set dfs.namenode.heartbeat.recheck-interval to 1 and killed the DN to simulate ""lost"" datanode. The opposite case also has problems (i.e. Datanode failing when NN is in safemode, doesn't lead to a missing blocks message)

Without the NN updating this list of missing blocks, the grid admins will not know when to take the cluster out of safemode.",0
"TestBlocksWithNotEnoughRacks occasionally fails during test tear down because RaplicationMonitor gets NPE.

Seen at https://builds.apache.org/job/Hadoop-Hdfs-trunk/1406/.",0
TestEditLogRace fails occasionally because it gets NPE from manipulating INodeMap while loading edits.,0
"Snapshot.Root, though is a subclass of INodeDirectory, is only used to indicate the root of a snapshot. In the meanwhile, AbstractINodeDiff#snapshotINode is used as copies recording the original state of an INode. Thus we should not put them into INodeMap. 

Currently when loading FSImage we did not check the type of inode and wrongly put these two types of nodes into INodeMap. This may replace the nodes that should stay in INodeMap.",0
"The DataNode is configured with ipc.client.ping false and ipc.ping.interval 14000. This configuration means that the IPC Client (DataNode, in this case) should timeout in 14000 seconds if the Standby NameNode does not respond to a sendHeartbeat.

What we observe is this: If the Standby NameNode happens to reboot for any reason, the DataNodes that are heartbeating to this Standby get stuck forever while trying to sendHeartbeat. See Stack trace included below. When the Standby NameNode comes back up, we find that the DataNode never re-registers with the Standby NameNode. Thereafter failover completely fails.

The desired behavior is that the DataNode's sendHeartbeat should timeout in 14 seconds, and keep retrying till the Standby NameNode comes back up. When it does, the DataNode should reconnect, re-register, and offer service.

Specifically, in the class DatanodeProtocolClientSideTranslatorPB.java, the method createNamenode should use RPC.getProtocolProxy and not RPC.getProxy to create the DatanodeProtocolPB object.

Stack trace of thread stuck in the DataNode after the Standby NN has rebooted:

Thread 25 (DataNode: [file:///opt/hadoop/data]  heartbeating to vmhost6-vm1/10.10.10.151:8020):
  State: WAITING
  Blocked count: 23843
  Waited count: 45676
  Waiting on org.apache.hadoop.ipc.Client$Call@305ab6c5
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.Object.wait(Object.java:485)
    org.apache.hadoop.ipc.Client.call(Client.java:1220)
    org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:202)
    sun.proxy.$Proxy10.sendHeartbeat(Unknown Source)
    sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    java.lang.reflect.Method.invoke(Method.java:597)
    org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:164)
    org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:83)
    sun.proxy.$Proxy10.sendHeartbeat(Unknown Source)
    org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:167)
    org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:445)
    org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:525)
    org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:676)
    java.lang.Thread.run(Thread.java:662)

DataNode RPC to Standby NameNode never times out. ",0
"Due to absence of explicit timeout in FileJournalManager, error conditions that incur long delay (usually until driver timeout) can make namenode unresponsive for long time. This directly affects NN's failure detection latency, which is critical in HA.
",0
"getMaxNodesPerRack() calculates the max replicas/rack like this:

{code}
int maxNodesPerRack = (totalNumOfReplicas-1)/clusterMap.getNumOfRacks()+2;
{code}

Since this does not consider the racks that are being decommissioned and the decommissioning state is only checked later in isGoodTarget(), certain blocks are not replicated even when there are many racks and nodes.",0
"When the root directory is set as snapshottable, its snapshot quota is changed from 0 to a positive number. While loading fsimage we should check the root's snapshot quota and add it to snapshottable directory list in SnapshotManager if necessary.",0
"Since metaSave cannot get the inode holding a orphaned/invalid block, it NPEs and stops generating further report. Normally ReplicationMonitor removes them quickly, but if the queue is huge, it takes very long time. Also in safe mode, they stay.",0
callGetBlockLocations() returns all the blocks of a file even when they are not present in the snap version,0
"Suppose we have the following operations:
1. rename /dir1/foo/bar to /dir2/bar2
2. rename /dir1/foo to /dir2/bar2/foo2

I.e., we rename a directory (foo) to a child of its prior descendant (bar). If we have taken snapshots on root before the above 2 rename operations, a circle can be generated consisting of nodes with the types INodeReference.WithName and INodeReference.DstReference:

WithName (foo) --> WithCount (for foo) --> foo2 --> WithName (bar) --> WithCount (for bar) --> bar2 --> DstReference (foo) --> WithCount (for foo)

When deleting a snapshot before the rename operations, the current code may hit an infinite loop when cleaning the snapshot data or updating the quota usage for the nodes in the above circle. This jira will fix the issue.

",0
"Scenario:
1. cluster with 4 DNs
2. the size of the file to be written is a little more than one block
3. write the first block to 3 DNs, DN1->DN2->DN3
4. all the data packets of first block is successfully acked and the client sets the pipeline stage to PIPELINE_CLOSE, but the last packet isn't sent out
5. DN2 and DN3 are down
6. client recovers the pipeline, but no new DN is added to the pipeline because of the current pipeline stage is PIPELINE_CLOSE
7. client continuously writes the last block, and try to close the file after written all the data
8. NN finds that the penultimate block doesn't has enough replica(our dfs.namenode.replication.min=2), and the client's close runs into indefinite loop(HDFS-2936), and at the same time, NN makes the last block's state to COMPLETE
9. shutdown the client
10. the file's lease exceeds hard limit
11. LeaseManager realizes that and begin to do lease recovery by call fsnamesystem.internalReleaseLease()
12. but the last block's state is COMPLETE, and this triggers lease manager's infinite loop and prints massive logs like this:
{noformat}
2013-06-05,17:42:25,695 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Lease [Lease.  Holder: DFSClient_NONMAPREDUCE_-1252656407_1, pendingcreates: 1] has expired hard
 limit
2013-06-05,17:42:25,695 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Recovering lease=[Lease.  Holder: DFSClient_NONMAPREDUCE_-1252656407_1, pendingcreates: 1], src=
/user/h_wuzesheng/test.dat
2013-06-05,17:42:25,695 WARN org.apache.hadoop.hdfs.StateChange: DIR* NameSystem.internalReleaseLease: File = /user/h_wuzesheng/test.dat, block blk_-7028017402720175688_1202597,
lastBLockState=COMPLETE
2013-06-05,17:42:25,695 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Started block recovery for file /user/h_wuzesheng/test.dat lease [Lease.  Holder: DFSClient_NONM
APREDUCE_-1252656407_1, pendingcreates: 1]
{noformat}
(the 3rd line log is a debug log added by us)",0
"If NN comes up in standby and stays that way, repl queues are empty so no work is generated even though ReplicationMonitor is running. But if an ANN transitions to standby, more work can be generated and sent. Also any remaining work will be sent to datanodes. 

Current code drains existing work and stops generating new work in safe mode. HDFS-4832 will make it immediately stop in safe mode. Same can be done for standby.

This change was also suggested in HDFS-3744.",0
"[~atm] and I found that when a directory is copied, its quotas settings aren't included.

{code}
[04:21:33] atm@simon:~/src/apache/hadoop.git$ hadoop fs -ls /user
Found 2 items
drwxr-xr-x   - atm  atm                 0 2013-06-07 16:17 /user/atm
drwx------   - hdfs supergroup          0 2013-06-07 16:21 /user/hdfs
[04:21:44] atm@simon:~/src/apache/hadoop.git$ hadoop fs -count -q /user/atm
         100              91            none             inf            7            2               3338 /user/atm
[04:21:51] atm@simon:~/src/apache/hadoop.git$ sudo -u hdfs -E `which hadoop` fs -cp /user/atm /user/atm-copy
[04:22:00] atm@simon:~/src/apache/hadoop.git$ hadoop fs -count -q /user/atm-copy
        none             inf            none             inf            6            1               3338 /user/atm-copy
{code}

This also means that a user will not retain quotas settings when the user takes snapshots and restores a subtree using snapshots because we use copy (not allowed to move snapshots).

",0
"As currently implemented, {{BlockPlacementPolicyWithNodeGroup}} does not properly fallback to local rack when no nodes are available in remote racks, resulting in an improper {{NotEnoughReplicasException}}.

{code:title=BlockPlacementPolicyWithNodeGroup.java}
  @Override
  protected void chooseRemoteRack(int numOfReplicas,
      DatanodeDescriptor localMachine, HashMap<Node, Node> excludedNodes,
      long blocksize, int maxReplicasPerRack, List<DatanodeDescriptor> results,
      boolean avoidStaleNodes) throws NotEnoughReplicasException {
    int oldNumOfReplicas = results.size();
    // randomly choose one node from remote racks
    try {
      chooseRandom(
          numOfReplicas,
          ""~"" + NetworkTopology.getFirstHalf(localMachine.getNetworkLocation()),
          excludedNodes, blocksize, maxReplicasPerRack, results,
          avoidStaleNodes);
    } catch (NotEnoughReplicasException e) {
      chooseRandom(numOfReplicas - (results.size() - oldNumOfReplicas),
          localMachine.getNetworkLocation(), excludedNodes, blocksize,
          maxReplicasPerRack, results, avoidStaleNodes);
    }
  }
{code}

As currently coded the {{chooseRandom()}} call in the {{catch}} block will never succeed as the set of nodes within the passed in node path (e.g. {{/rack1/nodegroup1}}) is entirely contained within the set of excluded nodes (both are the set of nodes within the same nodegroup as the node chosen first replica).

The bug is that the fallback {{chooseRandom()}} call in the catch block should be passing in the _complement_ of the node path used in the initial {{chooseRandom()}} call in the try block (e.g. {{/rack1}})  - namely:
{code}
NetworkTopology.getFirstHalf(localMachine.getNetworkLocation())
{code}

This will yield the proper fallback behavior of choosing a random node from _within the same rack_, but still excluding those nodes _in the same nodegroup_",0
"It is possible to steal or manipulate customer session and cookies, which might be used to impersonate a legitimate user,
allowing the hacker to view or alter user records, and to perform transactions as that user.
e.g.
GET /browseDirectory.jsp? dir=%2Fhadoop'""/><script>alert(759)</script> &namenodeInfoPort=50070

Also;

Phishing Through Frames

Try:
GET /browseDirectory.jsp? dir=%2Fhadoop%27%22%3E%3Ciframe+src%3Dhttp%3A%2F%2Fdemo.testfire.net%2Fphishing.html%3E
&namenodeInfoPort=50070 HTTP/1.1
Cookie: JSESSIONID=qd9i8tuccuam1cme71swr9nfi
Accept-Language: en-US
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;


",0
"Currently if one closes an OutputStream obtained from FileSystem#create and then calls write(...) on that closed stream, the write will appear to succeed without error though no data will be written to HDFS. A subsequent call to close will also silently appear to succeed. We should make it so that attempts to write to closed streams fails fast.",0
"As _root_, I mounted HDFS with fuse-dfs using the -ousetrash option.

As _testuser_, I cd into the mount and touch a test file at _/user/testuser/testFile1_. As the same user, I try to rm the file and run into an error:

{code}
[testuser@hdfs-vanilla-1 ~]$ cd /hdfs_mnt/user/testuser
[testuser@hdfs-vanilla-1 testuser]$ touch testFile1
[testuser@hdfs-vanilla-1 testuser]$ rm testFile1
rm: cannot remove `testFile1': Unknown error 255
{code}

I check the fuse-dfs debug output, and it shows that we attempt to mkdir /user/root/.Trash, which testuser doesn't have permissions to.

Ideally, we'd be able to remove testFile1 and have testFile1 be put into /user/testuser/.Trash instead of /user/root/.Trash.

Error in debug:
{code}
unlink /user/testuser/testFile1
hdfsCreateDirectory(/user/root/.Trash/Current/user/testuser): FileSystem#mkdirs error:
org.apache.hadoop.security.AccessControlException: Permission denied: user=testuser, access=WRITE, inode=""/user/root"":root:supergroup:drwxr-xr-x
						   at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:224)
						   at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:204)
						   at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:149)
						   at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:4716)
						   at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:4698)
						   at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:4672)
						   at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:3035)
						   at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInt(FSNamesystem.java:2999)
						   at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:2980)
						   at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:648)
						   at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:419)
						   at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:44970)
						   at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:453)
						   at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1002)
						   at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1701)
						   at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1697)
						   at java.security.AccessController.doPrivileged(Native Method)
						   at javax.security.auth.Subject.doAs(Subject.java:396)
						   at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
						   at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1695)

						   at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
						   at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
						   at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
						   at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
						   at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:90)
						   at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:57)
						   at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2153)
						   at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2122)
						   at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:545)
						   at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1913)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=testuser, access=WRITE, inode=""/user/root"":root:supergroup:drwxr-xr-x
       at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:224)
       at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:204)
       at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:149)
       at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:4716)
       at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:4698)
       at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:4672)
       at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:3035)
       at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInt(FSNamesystem.java:2999)
       at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:2980)
       at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:648)
       at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:419)
       at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:44970)
       at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:453)
       at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1002)
       at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1701)
       at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1697)
       at java.security.AccessController.doPrivileged(Native Method)
       at javax.security.auth.Subject.doAs(Subject.java:396)
       at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
       at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1695)

       at org.apache.hadoop.ipc.Client.call(Client.java:1225)
       at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:202)
       at $Proxy9.mkdirs(Unknown Source)
       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
       at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
       at java.lang.reflect.Method.invoke(Method.java:597)
       at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:164)
       at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:83)
       at $Proxy9.mkdirs(Unknown Source)
       at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:426)
       at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2151)
       ... 3 more
{code}",0
"When a new datanode is added to the pipeline, the client will trigger the block transfer process. In the current implementation, the src datanode calls the run() method of the DataTransfer to transfer the block, this method will mask the IOExceptions during the transfering, and will make the client not realize the failure during the transferring, as a result the client will mistake the failing transferring as successful one. ",0
"When a large number of nodes are removed by refreshing node lists, the network topology is updated. If the refresh happens at the right moment, the replication monitor thread may stuck in the while loop of {{chooseRandom()}}. This is because the cached cluster size is used in the terminal condition check of the loop. This usually happens when a block with a high replication factor is being processed. Since replicas/rack is also calculated beforehand, no node choice may satisfy the goodness criteria if refreshing removed racks. 

All nodes will end up in the excluded list, but the size will still be less than the cached cluster size, so it will loop infinitely. This was observed in a production environment.",0
"When a StandbyNameNode is becoming active, we should not bail out because a DNS entry for a quorum node cannot be resolved.  Currently it does fail in this scenario, with a message like this:

{code}
2013-07-03 21:28:40,576 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2013-07-03 21:28:40,579 FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Error encountered requiring NN shutdown. Shutting down immediately.
java.lang.IllegalArgumentException: Unable to construct journal, qjournal://hadoop-mm:8485;hadoop-nn-0:8485;hadoop-nn-1:8485/hadoop
at org.apache.hadoop.hdfs.server.namenode.FSEditLog.createJournal(FSEditLog.java:1254)
at org.apache.hadoop.hdfs.server.namenode.FSEditLog.initJournals(FSEditLog.java:226)
at org.apache.hadoop.hdfs.server.namenode.FSEditLog.initJournalsForWrite(FSEditLog.java:193)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:722)
<etc>
{code}

reported by Matt Bookman",0
"There is ""dfs.hosts.exclude"" configured before NN start. Active/Standby works well. 

1)Add two datanodes IPAddr to the exclude file on the both Active and Standby NN. 
2)run: hdfs dfsadmin -refreshNodes on the Active NN, but there isn't any logs in the ActiveNN log. but decommission logs showed in the StandbyNN log.

There is no decommission datanodes showed on the Active NN webUI. but there does have decommission datanodes showed on the Standby NN webUI.

But the decommission process is very very slow, it indicates it cannot finish forever.


I do think this is a bug.",0
"We've seen an issue in a secure cluster where, after a failover, the new NN isn't able to properly coordinate QJM recovery. The JNs fail to fetch logs from each other due to apparently not having a Kerberos TGT. It seems that we need to add the {{checkTGTAndReloginFromKeytab}} call prior to making the HTTP connection, since the java HTTP stuff doesn't do an automatic relogin",0
"In NamenodeFsck#check(), the getListing() and getBlockLocations() are not synchronized, so the file deletions or renames at the right moment can cause FileNotFoundException and failure of fsck.

Instead of failing, fsck should continue. Optionally it can record file system modifications it encountered, but since most modifications during fsck are not detected, there might be little value in recording these specifically.",0
"libhdfs has some code to translate Java exceptions into C error codes.  Unfortunately, the exceptions are returned to us in ""dotted"" format, but the code is expecting them to be in ""slash-separated"" format.  This results in most exceptions just leading to a generic error code.

We should fix this and add a unit test to ensure this continues to work.",0
"In one of our cluster, following has happened which failed HDFS write.

1. Standby NN was unstable and continously restarting due to some errors. But Active NN was stable.
2. MR Job was writing files.
3. At some point SNN went down again while datanode processing the REGISTER command for SNN. 
4. Datanodes started retrying to connect to SNN to register at the following code  in BPServiceActor#retrieveNamespaceInfo() which will be called under synchronization.
{code}      try {
        nsInfo = bpNamenode.versionRequest();
        LOG.debug(this + "" received versionRequest response: "" + nsInfo);
        break;{code}
Unfortunately in all datanodes at same point this happened.
5. For next 7-8 min standby was down, and no blocks were reported to active NN at this point and writes have failed.


So culprit is {{BPOfferService#processCommandFromActor()}} is completely synchronized which is not required.",0
"In the testing of some failure scenarios for HBase MTTR, we have been simulating node failures via firewalling of nodes (where all communication ports would be firewalled except ssh's port). We have noticed that when a (data)node is firewalled, we lose certain other datanodes - those that were involved in some communication with the firewalled node before the latter was firewalled. Will attach jstack output from one of the lost datanodes. The heartbeating thread seems to be locked up.
This jira is to track a fix for the problem.",0
"Here is one scenario I have recently encountered in a hbase cluster.

The 1st datanode in a write pipeline's disk became extremely busy for many minutes and it caused block writes on the disk to slow down. The 2nd datanode's socket read from the 1st datanode timed out in 60 seconds and disconnected. This caused a block recovery. The problem was, the 1st datanode hasn't written the last packet, but the downstream nodes did and ACK was sent back to the client. For this reason, the block recovery was issued up to the ACKed size. 

During the recovery, the first datanode was told to do copyBlock(). Since it didn't have enough data on disk, it waited in waitForMinLength(), which didn't help, so the command failed. The connection was already established to the target node for the copy, but the target never received any data. The data packet was eventually written, but it was too late for the copyBlock() call.

The destination node for the copy had block metadata in memory, but no file was created on disk. When client contacted this node for block recovery, it too failed. 

There are few problems:
- The faulty (slow) node was not detected correctly. Instead, the 2nd DN was excluded. The 1st DN's packet responder could have done a better job. It didn't have any outstanding ACKs to receive.  Or the second DN could have tried to hint the 1st DN of what happened. 

- copyBlock() could probably wait longer than 3 seconds in waitForMinLength(). Or it could check the on-disk size early on and fail early even before trying to establish a connection to the target.

- Failed targets in block write/copy should clean up the record or make it recoverable.",0
"We suffered a cluster wide power failure after which HDFS lost data that it had acknowledged as closed and complete.

The client was HBase which compacted a set of HFiles into a new HFile, then after closing the file successfully, deleted the previous versions of the file.  The cluster then lost power, and when brought back up the newly created file was marked CORRUPT.

Based on reading the logs it looks like the replicas were created by the DataNodes in the 'blocksBeingWritten' directory.  Then when the file was closed they were moved to the 'current' directory.  After the power cycle those replicas were again in the blocksBeingWritten directory of the underlying file system (ext3).  When those DataNodes reported in to the NameNode it deleted those replicas and lost the file.

Some possible fixes could be having the DataNode fsync the directory(s) after moving the block from blocksBeingWritten to current to ensure the rename is durable or having the NameNode accept replicas from blocksBeingWritten under certain circumstances.

Log snippets from RS (RegionServer), NN (NameNode), DN (DataNode):
{noformat}
RS 2013-06-29 11:16:06,812 DEBUG org.apache.hadoop.hbase.util.FSUtils: Creating file=hdfs://hm3:9000/hbase/users-6/b5b0820cde759ae68e333b2f4015bb7e/.tmp/6e0cc30af6e64e56ba5a539fdf159c4c with permission=rwxrwxrwx
NN 2013-06-29 11:16:06,830 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /hbase/users-6/b5b0820cde759ae68e333b2f4015bb7e/.tmp/6e0cc30af6e64e56ba5a539fdf159c4c. blk_1395839728632046111_357084589
DN 2013-06-29 11:16:06,832 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving block blk_1395839728632046111_357084589 src: /10.0.5.237:14327 dest: /10.0.5.237:50010
NN 2013-06-29 11:16:11,370 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.0.6.1:50010 is added to blk_1395839728632046111_357084589 size 25418340
NN 2013-06-29 11:16:11,370 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.0.6.24:50010 is added to blk_1395839728632046111_357084589 size 25418340
NN 2013-06-29 11:16:11,385 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.0.5.237:50010 is added to blk_1395839728632046111_357084589 size 25418340
DN 2013-06-29 11:16:11,385 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Received block blk_1395839728632046111_357084589 of size 25418340 from /10.0.5.237:14327
DN 2013-06-29 11:16:11,385 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder 2 for block blk_1395839728632046111_357084589 terminating
NN 2013-06-29 11:16:11,385 INFO org.apache.hadoop.hdfs.StateChange: Removing lease on  file /hbase/users-6/b5b0820cde759ae68e333b2f4015bb7e/.tmp/6e0cc30af6e64e56ba5a539fdf159c4c from client DFSClient_hb_rs_hs745,60020,1372470111932
NN 2013-06-29 11:16:11,385 INFO org.apache.hadoop.hdfs.StateChange: DIR* NameSystem.completeFile: file /hbase/users-6/b5b0820cde759ae68e333b2f4015bb7e/.tmp/6e0cc30af6e64e56ba5a539fdf159c4c is closed by DFSClient_hb_rs_hs745,60020,1372470111932
RS 2013-06-29 11:16:11,393 INFO org.apache.hadoop.hbase.regionserver.Store: Renaming compacted file at hdfs://hm3:9000/hbase/users-6/b5b0820cde759ae68e333b2f4015bb7e/.tmp/6e0cc30af6e64e56ba5a539fdf159c4c to hdfs://hm3:9000/hbase/users-6/b5b0820cde759ae68e333b2f4015bb7e/n/6e0cc30af6e64e56ba5a539fdf159c4c
RS 2013-06-29 11:16:11,505 INFO org.apache.hadoop.hbase.regionserver.Store: Completed major compaction of 7 file(s) in n of users-6,\x12\xBDp\xA3,1359426311784.b5b0820cde759ae68e333b2f4015bb7e. into 6e0cc30af6e64e56ba5a539fdf159c4c, size=24.2m; total size for store is 24.2m

-------  CRASH, RESTART ---------

NN 2013-06-29 12:01:19,743 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: addStoredBlock request received for blk_1395839728632046111_357084589 on 10.0.6.1:50010 size 21978112 but was rejected: Reported as block being written but is a block of closed file.
NN 2013-06-29 12:01:19,743 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addToInvalidates: blk_1395839728632046111 is added to invalidSet of 10.0.6.1:50010
NN 2013-06-29 12:01:20,155 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: addStoredBlock request received for blk_1395839728632046111_357084589 on 10.0.5.237:50010 size 16971264 but was rejected: Reported as block being written but is a block of closed file.
NN 2013-06-29 12:01:20,155 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addToInvalidates: blk_1395839728632046111 is added to invalidSet of 10.0.5.237:50010
NN 2013-06-29 12:01:20,175 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: addStoredBlock request received for blk_1395839728632046111_357084589 on 10.0.6.24:50010 size 21913088 but was rejected: Reported as block being written but is a block of closed file.
NN 2013-06-29 12:01:20,175 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addToInvalidates: blk_1395839728632046111 is added to invalidSet of 10.0.6.24:50010
(note the clock on the server running DN is wrong after restart.  I believe timestamps are off by 6 hours:)
DN 2013-06-29 06:07:22,877 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Scheduling block blk_1395839728632046111_357084589 file /data/hadoop/dfs/data/blocksBeingWritten/blk_1395839728632046111 for deletion
DN 2013-06-29 06:07:24,952 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Deleted block blk_1395839728632046111_357084589 at file /data/hadoop/dfs/data/blocksBeingWritten/blk_1395839728632046111
{noformat}

There was some additional discussion on this thread on the mailing list:

http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-user/201307.mbox/%3CCA+qbEUPuf19PL_EVeWi1104+scLVrcS0LTFUvBPw=qcuXnZ8hQ@mail.gmail.com%3E",0
The primary namenode attempts to connect back to (incoming hostname):port regardless of how dfs.namenode.secondary.http-address is configured.,0
"For the same reasons as HDFS-1490, TransferFsImage of branch-1 should timeout.
",0
"We've seen a small handful of times a case where one of the NNs in an HA cluster ends up with an fsimage checkpoint that falls in the middle of an edit segment. We're not sure yet how this happens, but one issue can happen as a result:
- Node has fsimage_500. Cluster has edits_1-1000, edits_1001_inprogress
- Node restarts, loads fsimage_500
- Node wants to become active. It calls selectInputStreams(500). Currently, this API logs a WARN that 500 falls in the middle of the 1-1000 segment, but continues and returns no results.
- Node calls startLogSegment(501).

Currently, the QJM will accept this (incorrectly). The node then crashes when it first tries to journal a real transaction, but it ends up leaving the edits_501_inprogress lying around, potentially causing more issues later.",0
"We've seen the following behavior a couple times:
- SBN is running and somehow encounters an error in the middle of replaying an edit log in the tailer (eg the JN it's reading from crashes)
- SBN successfully has processed half of the edits in the segment it was reading.
- SBN saves a checkpoint, which now falls in the middle of a segment, and then restarts

Upon restart, the SBN will load this checkpoint which falls in the middle of a segment. {{selectInputStreams}} then fails when the SBN requests a mid-segment txid.

We should handle this case by downloading the right segment and fast-forwarding to the correct txid.",0
"NN starts a block recovery, which will synchronize block replicas on different DNs. In the end one of DNs will report the list of the nodes containing the consistent replicas to the NN via commitBlockSynchronization() call. The NPE happens if just before processing commitBlockSynchronization() NN removes from active one of DNs that are then reported in the call.",0
"Currently when QJM is used, running BootstrapStandby while the existing NN is active can get the following exception:
{code}
FATAL ha.BootstrapStandby: Unable to read transaction ids 6175397-6175405 from the configured shared edits storage. Please copy these logs into the shared edits storage or call saveNamespace on the active node.
Error: Gap in transactions. Expected to be able to read up until at least txid 6175405 but unable to find any edit logs containing txid 6175405
java.io.IOException: Gap in transactions. Expected to be able to read up until at least txid 6175405 but unable to find any edit logs containing txid 6175405
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.checkForGaps(FSEditLog.java:1300)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.selectInputStreams(FSEditLog.java:1258)
	at org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby.checkLogsAvailableForRead(BootstrapStandby.java:229)
{code}

Looks like the cause of the exception is that, when the active NN is queries by BootstrapStandby about the last written transaction ID, the in-progress edit log segment is included. However, when journal nodes are asked about the last written transaction ID, in-progress edit log is excluded. This causes BootstrapStandby#checkLogsAvailableForRead to complain gaps. 

To fix this, we can either let journal nodes take into account the in-progress editlog, or let active NN exclude the in-progress edit log segment.",0
"1. Add Credentials and Verifier definitions. 
2. Update Nfs3Interface.java and add SessionSecurityHandler to allow the support of different types of authentications.
3. Update RpcProgrammNfs3.java to move the original AUTH_SYS authentication into the new interfaces.",0
"If a large proportion of data nodes are being decommissioned, one or more racks may not be writable. However this is not taken into account when the default block placement policy module invokes getMaxNodesPerRack(). Some blocks, especially the ones with a high replication factor, may not be able to fully replicated until those nodes are taken out of dfs.include.  It can actually block decommissioning itself.",0
"Bug reported by [~arpitgupta]:

If the dfs.nameservices is set to arpit,
{code}
hdfs dfs -ls webhdfs://arpit/tmp
{code}
does not work. You have to provide the exact active namenode hostname. On an HA cluster using dfs client one should not need to provide the active nn hostname.

To fix this, we try to 
1) let WebHdfsFileSystem support logical NN service name
2) add failover_and_retry functionality in WebHdfsFileSystem for NN HA

",0
Namenode deadlocks after a while in use.,0
"I propose a authorized user impersonate mechanism for fine grain (path level) access control in HDFS.
In short, owner of data encrypt the path with a shared secret, and other user use the encrypted path to call namenode service (create/read/delete file). Namenode decrypt the path to validate the access and execute the operation as owner of the data if valid. It consists of:
1. a ACLFileSystem extends DistributedFileSystem, which wrap the create/open/delete/etc. RPC calls, and send the encrypted path to namenode
2. authenticator(embedded in namenode), which decrypt the path and execute the call as owner of the data
With authorized user impersonate, we can develop a authorization manager to check whether a path level access is permitted.
A detailed explanation can be found in maillist:
http://mail-archives.apache.org/mod_mbox/hive-dev/201308.mbox/%3CCACkoVCxm+=44kB_4eWtepHe_knkdm0Uzyh=0q-vfybYU8eLQxw@mail.gmail.com%3E",0
While running nn load generator with 15 threads for 20 mins the standby namenode deadlocked.,0
"With HA enabled, NN wo't start with ""-upgrade"". Since there has been a layout version change between 2.0.x and 2.1.x, starting NN in upgrade mode was necessary when deploying 2.1.x to an existing 2.0.x cluster. But the only way to get around this was to disable HA and upgrade. 

The NN and the cluster cannot be flipped back to HA until the upgrade is finalized. If HA is disabled only on NN for layout upgrade and HA is turned back on without involving DNs, things will work, but finaliizeUpgrade won't work (the NN is in HA and it cannot be in upgrade mode) and DN's upgrade snapshots won't get removed.

We will need a different ways of doing layout upgrade and upgrade snapshot.  I am marking this as a 2.1.1-beta blocker based on feedback from others.  If there is a reasonable workaround that does not increase maintenance window greatly, we can lower its priority from blocker to critical.",0
"While running namenode load generator with 100 threads for 10 mins namenode was being failed over ever 2 mins.
The standby namenode shut itself down as it ran out of memory and was not able to create another thread.
When we searched for 'Safe mode extension entered' in the standby log it was present 55000+ times",0
"There are certain commands in dfsadmin return the status of the first namenode specified in the configs rather than interacting with the active namenode

For example. Issue

hdfs dfsadmin -safemode get

and it will return the status of the first namenode in the configs rather than the active namenode.

I think all dfsadmin commands should determine which is the active namenode do the operation on it.",0
"The 2NN will avoid downloading/loading a new fsimage if its local copy of fsimage is the same as the version on the NN. However, the decision to *load* the fsimage from disk into memory is based only on the on-disk fsimage version. If an error occurs between downloading and loading the fsimage on the first checkpoint attempt, the 2NN will never load the fsimage, and then on subsequent checkpoint attempts it will not load the on-disk fsimage and thus will never checkpoint successfully.

Example error message in the first comment of this ticket.",0
"DataNode fails to startup if one of the data dirs configured is out of space. 


fails with following exception
{noformat}2013-09-11 17:48:43,680 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for block pool Block pool <registering> (storage id DS-308316523-xx.xx.xx.xx-64015-1378896293604) service to /nn1:65110
java.io.IOException: Mkdirs failed to create /opt/nish/data/current/BP-123456-1234567/tmp
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.<init>(BlockPoolSlice.java:105)
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.addBlockPool(FsVolumeImpl.java:216)
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.addBlockPool(FsVolumeList.java:155)
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.addBlockPool(FsDatasetImpl.java:1593)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:834)
        at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:311)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:217)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:660)
        at java.lang.Thread.run(Thread.java:662)
{noformat}


It should continue to start-up with other data dirs available.",0
"{{DistributedFileSystem#close}} closes the underlying {{DFSClient}} before the superclass call to {{FileSystem#close}}, which removes the instance from the cache.  This leaves a small window during which another thread can call {{FileSystem#get}} and receive the closed instance from the cache.",0
Trash has introduced new static method moveToAppropriateTrash which resolves to right filesystem. To be API compatible we need to check if Trash::moveToTrash can do what moveToAppropriateTrash does so that downstream users need not change code.,0
"Get a RemoteIterator from DistributedFileSystem.listFiles(..) with a relative path.  Then, it will result a NullPointerException when calling hasNext() from the RemoteIterator.

This bug was discovered by Arnaud:
http://hortonworks.com/community/forums/topic/new-bug-in-hdfs-listfiles-method/",0
"If the merge takes up more than 10 hours, the TGT of the SNN expires and SNN will fail to upload the image to the Namenode

",0
"The constructor of NameNode starts a HTTP server before the FSNameSystem is initialized. Currently there is a race where the HTTP server can access the uninitialized namesystem variable, throwing a NullPointerException.",0
"Run Distcp job using hsftp when ssl  is enabled. The job fails with ""java.net.SocketException: Unexpected end of file from server"" Error

Running: hadoop distcp hsftp://localhost:50070/f1 hdfs://localhost:19000/f5 
All the tasks fails with below error.

13/09/23 15:52:38 INFO mapreduce.Job: Task Id : attempt_1379976241507_0004_m_000000_0, Status : FAILED
Error: java.io.IOException: File copy failed: hsftp://localhost:50070/f1 --> hdfs://localhost:19000/f5
	at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:262)
	at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:229)
	at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:45)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1499)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:166)
Caused by: java.io.IOException: Couldn't run retriable-command: Copying hsftp://127.0.0.1:50070/f1 to hdfs://localhost:19000/f5
	at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:101)
	at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:258)
	... 10 more
Caused by: org.apache.hadoop.tools.mapred.RetriableFileCopyCommand$CopyReadException: java.io.IOException: HTTP_OK expected, received 500
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.readBytes(RetriableFileCopyCommand.java:233)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.copyBytes(RetriableFileCopyCommand.java:198)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.copyToTmpFile(RetriableFileCopyCommand.java:134)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doCopy(RetriableFileCopyCommand.java:101)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doExecute(RetriableFileCopyCommand.java:83)
	at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:87)
	... 11 more
Caused by: java.io.IOException: HTTP_OK expected, received 500
	at org.apache.hadoop.hdfs.HftpFileSystem$RangeHeaderUrlOpener.connect(HftpFileSystem.java:383)
	at org.apache.hadoop.hdfs.ByteRangeInputStream.openInputStream(ByteRangeInputStream.java:119)
	at org.apache.hadoop.hdfs.ByteRangeInputStream.getInputStream(ByteRangeInputStream.java:103)
	at org.apache.hadoop.hdfs.ByteRangeInputStream.read(ByteRangeInputStream.java:187)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:273)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
	at java.io.FilterInputStream.read(FilterInputStream.java:107)
	at org.apache.hadoop.tools.util.ThrottledInputStream.read(ThrottledInputStream.java:75)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.readBytes(RetriableFileCopyCommand.java:230)
	... 16 more
",0
"{{addBlock()}} call retry should return the LocatedBlock with locations if the block was created in previous call and failover/restart of namenode happened.

otherwise client will get {{ArrayIndexOutOfBoundsException}} while creating the block and write will fail.

{noformat}java.lang.ArrayIndexOutOfBoundsException: 0
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1118)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1078)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:511){noformat}
",0
"We noticed that when we set the value of 'dfs.client.socket-timeout' to 0, 
and start the HBase regionserver in the same node as the Datanode, we have a situation where the Datanode heap size just blows up in a very short span of time.

A jmap histogram of the live objects in the datanode yields this :

{noformat}
~/hbase_debug]$ head jmap.histo

 num     #instances         #bytes  class name
----------------------------------------------
   1:      46054779     1842191160  org.apache.hadoop.hdfs.server.datanode.BlockReceiver$Packet
   2:      46054878     1105317072  java.util.LinkedList$Entry
....
....
{noformat}

and again after a couple of seconds :

{noformat}
~/hbase_debug]$ head jmap2.histo

 num     #instances         #bytes  class name
----------------------------------------------
   1:      50504594     2020183760  org.apache.hadoop.hdfs.server.datanode.BlockReceiver$Packet
   2:      50504693     1212112632  java.util.LinkedList$Entry
....
....
{noformat}

We also see a very high rate of minor GCs happening and untimately, full GCs start with pause times of around 10 - 15 secs and this keeps increasing..

It looks like entries are being pushed into a linkedlist very rapidly and thus are not eligible for GC

On enabling debug logging for the DFS client and hadoop ipc on the HBase regionserver this is what we see :
{noformat}
2013-09-24 20:53:10,485 DEBUG org.apache.hadoop.ipc.HBaseServer: IPC Server handler 23 on 60020: has #26 from 192.168.0.67:33790
2013-09-24 20:53:10,485 DEBUG org.apache.hadoop.ipc.HBaseServer: IPC Server handler 23 on 60020: call #26 executing as NULL principal
2013-09-24 20:53:10,485 DEBUG org.apache.hadoop.ipc.HBaseServer.trace: Call #26; Served: HRegionInterface#get queueTime=0 processingTime=0 contents=1 Get, 9 bytes
2013-09-24 20:53:10,486 DEBUG org.apache.hadoop.ipc.HBaseServer: IPC Server Responder: responding to #26 from 192.168.0.67:33790
2013-09-24 20:53:10,486 DEBUG org.apache.hadoop.ipc.HBaseServer: IPC Server Responder: responding to #26 from 192.168.0.67:33790 Wrote 140 bytes.
2013-09-24 20:53:10,523 DEBUG org.apache.hadoop.hdfs.DFSClient: DataStreamer block BP-295691219-192.168.0.58-1380070220243:blk_2084430581332674588_1018 sending packet packet seqno:0 offsetInBlock:0 lastPacketInBlock:false lastByteOffsetInBlock: 326
2013-09-24 20:53:10,523 DEBUG org.apache.hadoop.hdfs.DFSClient: DataStreamer block BP-295691219-192.168.0.58-1380070220243:blk_2084430581332674588_1018 sending packet packet seqno:-1 offsetInBlock:0 lastPacketInBlock:false lastByteOffsetInBlock: 0
2013-09-24 20:53:10,523 DEBUG org.apache.hadoop.hdfs.DFSClient: DataStreamer block BP-295691219-192.168.0.58-1380070220243:blk_2084430581332674588_1018 sending packet packet seqno:-1 offsetInBlock:0 lastPacketInBlock:false lastByteOffsetInBlock: 0
2013-09-24 20:53:10,523 DEBUG org.apache.hadoop.hdfs.DFSClient: DataStreamer block BP-295691219-192.168.0.58-1380070220243:blk_2084430581332674588_1018 sending packet packet seqno:-1 offsetInBlock:0 lastPacketInBlock:false lastByteOffsetInBlock: 0
2013-09-24 20:53:10,523 DEBUG org.apache.hadoop.hdfs.DFSClient: DataStreamer block BP-295691219-192.168.0.58-1380070220243:blk_2084430581332674588_1018 sending packet packet seqno:-1 offsetInBlock:0 lastPacketInBlock:false lastByteOffsetInBlock: 0
2013-09-24 20:53:10,524 DEBUG org.apache.hadoop.hdfs.DFSClient: DataStreamer block BP-295691219-192.168.0.58-1380070220243:blk_2084430581332674588_1018 sending packet packet seqno:-1 offsetInBlock:0 lastPacketInBlock:false lastByteOffsetInBlock: 0
2013-09-24 20:53:10,524 DEBUG org.apache.hadoop.hdfs.DFSClient: DataStreamer block BP-295691219-192.168.0.58-1380070220243:blk_2084430581332674588_1018 sending packet packet seqno:-1 offsetInBlock:0 lastPacketInBlock:false lastByteOffsetInBlock: 0
....
....
....
{noformat}

The last line keeps repeating.. and the LOG files run into 100s of MBs really fast..

My assumption was that the HBase region server creates an hlog file at startup.. which it keeps open by sending a heartbeat (-1 seqno) packet... But we were stumped as to why packets were sent at this alarming rate.

Looking at the DFSOutputstream code, it looks like there is a section inside the DataStreamer class where the 'dfs.client.socket-timeout' is being used :

{code}
..
....
            while ((!streamerClosed && !hasError && dfsClient.clientRunning 
                && dataQueue.size() == 0 && 
                (stage != BlockConstructionStage.DATA_STREAMING || 
                 stage == BlockConstructionStage.DATA_STREAMING && 
                 now - lastPacket < dfsClient.getConf().socketTimeout/2)) || doSleep ) {
              long timeout = dfsClient.getConf().socketTimeout/2 - (now-lastPacket);
              timeout = timeout <= 0 ? 1000 : timeout;
              timeout = (stage == BlockConstructionStage.DATA_STREAMING)?
                 timeout : 1000;
              try {
                dataQueue.wait(timeout);
              } catch (InterruptedException  e) {
              }
              doSleep = false;
              now = Time.now();
            }
...
..
{code}

We see that this code path is never traversed and thus Datastreamer thread keeps sending packets without any delay...

Further more, on going thru the DataStreamer code, it looks like once the DataStreamer starts sending heartbeat packets, there is no code path that checks to see if there is any valid data in the dataQueue.. except the above piece... 

which implies that unless the absolute value of 'now - lastPacket' is less than 'dfs.client.socket-timeout', the client would hang...

Shouldnt there be a timed 'dataQueue.wait()' in each loop of the DataStreamer irrespective of the value of this parameter ?

Kindly do provide comments..



",0
"When Kerberos authentication is enabled, we are unable to browse to the data nodes using ( Name node web page --> Live Nodes --> Select any of the data nodes). The reason behind this is the delegation token is not provided as part of the url in the method (generateNodeDataHeader method of NodeListJsp)

{code}
      String url = HttpConfig.getSchemePrefix() + d.getHostName() + "":""
          + d.getInfoPort()
          + ""/browseDirectory.jsp?namenodeInfoPort="" + nnHttpPort + ""&dir=""
          + URLEncoder.encode(""/"", ""UTF-8"")
          + JspHelper.getUrlParam(JspHelper.NAMENODE_ADDRESS, nnaddr);
{code}

But browsing the file system using name node web page --> Browse the file system -> <any directory> is working fine as the redirectToRandomDataNode method of NamenodeJspHelper creates the delegation token

{code}
    redirectLocation = HttpConfig.getSchemePrefix() + fqdn + "":"" + redirectPort
        + ""/browseDirectory.jsp?namenodeInfoPort=""
        + nn.getHttpAddress().getPort() + ""&dir=/""
        + (tokenString == null ? """" :
           JspHelper.getDelegationTokenUrlParam(tokenString))
        + JspHelper.getUrlParam(JspHelper.NAMENODE_ADDRESS, addr);
{code}

I will work on providing a patch for this issue.",0
"Namenode get the wrong address when dfs.https.port is unspecified.

java.lang.IllegalArgumentException: Does not contain a valid host:port authority: 0.0.0.0:0.0.0.0:0
        at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:212)
        at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:163)
        at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:152)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:102)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:633)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:490)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:691)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:676)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1265)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1331)",0
"Each time, after server reboot a WRITE_COMMIT_VERF is created and passed to NFS client with write or commit response. If this verifier is not correctly set in the response, some NFS client(especially Linux client) could keep resending the same request, since it thinks the write/commit didn't succeed.",0
"This is observed in one of our env:
1. A MR Job was running which has created some temporary files and was writing to them.
2. Snapshot was taken
3. And Job was killed and temporary files were deleted.
4. Namenode restarted.
5. After restart Namenode was in safemode waiting for blocks

Analysis
---------
1. Since the snapshot taken also includes the temporary files which were open, and later original files are deleted.
2. UnderConstruction blocks count was taken from leases. not considered the UC blocks only inside snapshots
3. So safemode threshold count was more and NN did not come out of safemode",0
"Currently all HDFS RPCs performed by NNs/DNs/clients can be optionally encrypted. However, actual data read or written between DNs and clients (or DNs to DNs) is sent in the clear. When processing sensitive data on a shared cluster, confidentiality of the data read/written from/to HDFS may be desired.
This change is already done in Hadoop 2 and trunk. 
The task is to port this change to Hadoop 1.",0
"In our test, we saw NN immediately went into safemode after transitioning to active state. This can cause HBase region server to timeout and kill itself. We should allow clients to retry when HA is enabled and ANN is in SafeMode.

============================================
Some log snippets:

standby state to active transition
{code}
2013-10-02 00:13:49,482 INFO  ipc.Server (Server.java:run(2068)) - IPC Server handler 69 on 8020, call org.apache.hadoop.hdfs.protocol.ClientProtocol.renewLease from IP:33911 Call#1483 Retry#1: error: org.apache.hadoop.ipc.StandbyException: Operation category WRITE is not supported in state standby
2013-10-02 00:13:49,689 INFO  ipc.Server (Server.java:saslProcess(1342)) - Auth successful for nn/hostname@EXAMPLE.COM (auth:SIMPLE)
2013-10-02 00:13:49,696 INFO  authorize.ServiceAuthorizationManager (ServiceAuthorizationManager.java:authorize(111)) - Authorization successful for nn/hostname@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ha.HAServiceProtocol
2013-10-02 00:13:49,700 INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1013)) - Stopping services started for standby state
2013-10-02 00:13:49,701 WARN  ha.EditLogTailer (EditLogTailer.java:doWork(336)) - Edit log tailer interrupted
java.lang.InterruptedException: sleep interrupted
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:334)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$200(EditLogTailer.java:279)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:296)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:356)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1463)
        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:454)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTail
2013-10-02 00:13:49,704 INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(885)) - Starting services required for active state
2013-10-02 00:13:49,719 INFO  client.QuorumJournalManager (QuorumJournalManager.java:recoverUnfinalizedSegments(419)) - Starting recovery process for unclosed journal segments...
2013-10-02 00:13:49,755 INFO  ipc.Server (Server.java:saslProcess(1342)) - Auth successful for hbase/hostname@EXAMPLE.COM (auth:SIMPLE)
2013-10-02 00:13:49,761 INFO  authorize.ServiceAuthorizationManager (ServiceAuthorizationManager.java:authorize(111)) - Authorization successful for hbase/hostname@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol
2013-10-02 00:13:49,839 INFO  client.QuorumJournalManager (QuorumJournalManager.java:recoverUnfinalizedSegments(421)) - Successfully started new epoch 85
2013-10-02 00:13:49,839 INFO  client.QuorumJournalManager (QuorumJournalManager.java:recoverUnclosedSegment(249)) - Beginning recovery of unclosed segment starting at txid 887112
2013-10-02 00:13:49,874 INFO  client.QuorumJournalManager (QuorumJournalManager.java:recoverUnclosedSegment(258)) - Recovery prepare phase complete. Responses:
IP:8485: segmentState { startTxId: 887112 endTxId: 887531 isInProgress: true } lastWriterEpoch: 84 lastCommittedTxId: 887530
172.18.145.97:8485: segmentState { startTxId: 887112 endTxId: 887531 isInProgress: true } lastWriterEpoch: 84 lastCommittedTxId: 887530
2013-10-02 00:13:49,875 INFO  client.QuorumJournalManager (QuorumJournalManager.java:recover
{code}


And then we get into safemode

{code}
Construction[IP:1019|RBW]]} size 0
2013-10-02 00:13:50,277 INFO  BlockStateChange (BlockManager.java:logAddStoredBlock(2237)) - BLOCK* addStoredBlock: blockMap updated: IP:1019 is added to blk_IP157{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[IP:1019|RBW], ReplicaUnderConstruction[172.18.145.96:1019|RBW], ReplicaUnde
rConstruction[IP:1019|RBW]]} size 0
2013-10-02 00:13:50,279 INFO  hdfs.StateChange (FSNamesystem.java:reportStatus(4703)) - STATE* Safe mode ON.
The reported blocks 1071 needs additional 5 blocks to reach the threshold 1.0000 of total blocks 1075.
Safe mode will be turned off automatically
2013-10-02 00:13:50,279 INFO  BlockStateChange (BlockManager.java:logAddStoredBlock(2237)) - BLOCK* addStoredBlock: blockMap updated: IP:1019 is added to blk_IP158{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[172.18.145.99:1019|RBW], ReplicaUnderConstruction[172.18.145.97:1019|RBW], ReplicaUnderConstruction[IP:1019|RBW]]} size 0
2013-10-02 00:13:50,280 INFO  BlockStateChange (BlockManager.java:logAddStoredBlock(2237)) - BLOCK* addStoredBlock: blockMap updated: 172.18.145.99:1019 is added to blk_IP158{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[172.18.145.99:1019|RBW], ReplicaUnderConstruction[172.18.145.97:1019|RBW], ReplicaUnderConstruction[IP:1019|RBW]]} size 0
2013-10-02 00:13:50,281 INFO  BlockStateChange (BlockManager.java:logAddStoredBlock(2237)) - BLOCK* addStoredBlock: blockMap updated: 172.18.145.97:1019 is added to blk_IP158{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[172.18.145.99:1019|RBW], ReplicaUnderConstruction[172.18.145.97:1019|RBW], ReplicaUnderConstruction[IP:1019|RBW]]} size 0
{code}",0
"The NN returns a {{FileStatus}} containing the exact link target as specified by the user at creation.  However, {{DistributedFileSystem#getFileLinkStatus}} explicit overwrites the target with the fully scheme qualified path lookup.  This causes multiple issues such as:
# Prevents clients from discerning if the target is relative or absolute
# Mangles a target that is not intended to be a path
# Causes incorrect resolution with multi-layered filesystems - ie. the link should be resolved relative to a higher level fs (ie. viewfs, chroot, filtered, etc)",0
"DFSClient got hanged in updatedPipeline call to namenode when the failover happened at exactly sametime.


When we digged down, issue found to be with handling the RetryCache in updatePipeline.

Here are the steps :
1. Client was writing slowly.
2. One of the datanode was down and updatePipeline was called to ANN.
3. Call reached the ANN, while processing updatePipeline call it got shutdown.
3. Now Client retried (Since the api marked as AtMostOnce) to another NameNode. at that time still NN was in STANDBY and got StandbyException.
4. Now one more time client failover happened. 
5. Now SNN became Active.
6. Client called to current ANN again for updatePipeline, 

Now client call got hanged in NN, waiting for the cached call with same callid to be over. But this cached call is already got over last time with StandbyException.

Conclusion :
Always whenever the new entry is added to cache we need to update the result of the call before returning the call or throwing exception.
I can see similar issue multiple RPCs in FSNameSystem.",0
"FSNameSystem#deleteSnapshot() should not check owner in case of permissions disabled

{code:java}      checkOperation(OperationCategory.WRITE);
      if (isInSafeMode()) {
        throw new SafeModeException(
            ""Cannot delete snapshot for "" + snapshotRoot, safeMode);
      }
      FSPermissionChecker pc = getPermissionChecker();
      checkOwner(pc, snapshotRoot);

      BlocksMapUpdateInfo collectedBlocks = new BlocksMapUpdateInfo();
      List<INode> removedINodes = new ChunkedArrayList<INode>();
      dir.writeLock();{code}

should check owner only in case of permissions enabled as its done for all other operations.",0
This is the HDFS part of HADOOP-10022. This will serve as the umbrella jira for all the https related cleanup in HDFS.,0
"To enable https access, the datanode http server https port is needed in namenode web pages and redirects from the namenode. This jira adds an additional optional field to DatanodeIDProto in hdfs.proto and the corresponding DatanodeID java class.",0
"When dfs.https.enable is true and dfs.https.port is not configured, namenode does not pickup the default https port (50470), instead picks up random port. See:
boolean needClientAuth = conf.getBoolean(""dfs.https.need.client.auth"", false);
      InetSocketAddress secInfoSocAddr = NetUtils.createSocketAddr(infoHost + "":"" + conf.get(
        DFSConfigKeys.DFS_NAMENODE_HTTPS_PORT_KEY, ""0""));
      Configuration sslConf = new Configuration(false);
      if (certSSL) {
        sslConf.addResource(conf.get(DFSConfigKeys.DFS_SERVER_HTTPS_KEYSTORE_RESOURCE_KEY,
                                     ""ssl-server.xml""));
      }
Unless https port is specifically configured as 0, we should not be picking random port. This needs to be documented in hdfs-default.xml",0
"While running HA tests we have seen issues were we see HDFS delegation token not found in cache errors causing jobs running to fail.

{code}
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)
at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157)
|2013-10-06 20:14:51,193 INFO  [main] mapreduce.Job: Task Id : attempt_1381090351344_0001_m_000007_0, Status : FAILED
Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 11 for hrt_qa) can't be found in cache
at org.apache.hadoop.ipc.Client.call(Client.java:1347)
at org.apache.hadoop.ipc.Client.call(Client.java:1300)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
at com.sun.proxy.$Proxy10.getBlockLocations(Unknown Source)
{code}",0
"This is to support network protocols which can't use file name as the token to resume directory listing. 

NFS gateway doesn't know the name of startAfter and has to use inode id instead. NFS protocol uses readdir and readdirplus to list directory content. In the response, each dirent has an 8-byte number verifier.

To list the content of large directories, NFS client sends multiple readdir or readdirplus requests to NFS gateway with one verifier(resume point) in the request. The verifier is basically of the same usage as ""startAfter"" in getListing. Since NFSv3 uses file handle to communicate and doesn't know the file name. NFS gateway has to use the inode id path as ""startAfter"" when sending getList request to NN, however NN currently expects ""startAfter"" to be just a file name. As a result, NFS gateway can't list the content of large directories.",0
"The semantic of DFSOutputStream#close() is incorrect.
Close() will throw the last exception that it encounters during the flush operations when closing the stream. The exception that it remembers never get cleared therefore spurious exceptions arise when close() is called multiple times.

Here is the stack trace from the client

{code}
java.nio.channels.ClosedChannelException
at org.apache.hadoop.hdfs.DFSOutputStream.checkClosed(DFSOutputStream.java:1317)
at org.apache.hadoop.hdfs.DFSOutputStream.waitForAckedSeqno(DFSOutputStream.java:1810)
at org.apache.hadoop.hdfs.DFSOutputStream.flushInternal(DFSOutputStream.java:1789)
at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:1877)
at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:71)
at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:104)
at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:54)
at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:112)
at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:366)
at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:338)
at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289)
at org.apache.hadoop.mapreduce.JobSubmitter.copyRemoteFiles(JobSubmitter.java:139)
at org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles(JobSubmitter.java:212)
at org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles(JobSubmitter.java:300)
at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:387)
at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1268)
at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1265)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:396)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)
at org.apache.hadoop.mapreduce.Job.submit(Job.java:1265)
at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)
at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:396)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)
at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)
at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)
at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:425)
at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:136)
at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:151)
at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:65)
at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1414)
at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1192)
at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1020)
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:888)
at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259)
at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:348)
at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:446)
at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:456)
at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:737)
at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
Job Submission failed with exception 'java.nio.channels.ClosedChannelException(null)'
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
{code}",0
"On an insecure, HA setup, we see that this works:
{code}
[jenkins@hdfs-cdh5-ha-1 ~]$ hdfs dfs -ls webhdfs://ns1/
13/09/27 15:23:52 INFO web.WebHdfsFileSystem: Retrying connect to namenode: hdfs-cdh5-ha-1.ent.cloudera.com/10.20.190.104:20101. Already tried 0 time(s); retry policy is org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry@5ebc404e, delay 0ms.
Found 5 items
drwxr-xr-x   - hbase hbase               0 2013-09-23 09:04 webhdfs://ns1/hbase
drwxrwxr-x   - solr  solr                0 2013-09-18 12:07 webhdfs://ns1/solr
drwxr-xr-x   - hdfs  supergroup          0 2013-09-19 11:09 webhdfs://ns1/system
drwxrwxrwt   - hdfs  supergroup          0 2013-09-18 16:25 webhdfs://ns1/tmp
drwxr-xr-x   - hdfs  supergroup          0 2013-09-18 15:53 webhdfs://ns1/user
[jenkins@hdfs-cdh5-ha-1 ~]$
{code}

However, when security is enabled, we get the following error:

{code}
[jenkins@hdfs-cdh5-ha-secure-1 ~]$ hdfs dfs -ls webhdfs://ns1/
-ls: java.net.UnknownHostException: ns1
Usage: hadoop fs [generic options] -ls [-d] [-h] [-R] [<path> ...]
[jenkins@hdfs-cdh5-ha-secure-1 ~]$
{code}

I verified that we can use the hdfs://ns1/ URI on the cluster where I see the problem.

Also, I verified on a secure, non-HA cluster that we can use the webhdfs uri in secure mode:

{code}
[jenkins@hdfs-cdh5-secure-1 ~]$ hdfs dfs -ls webhdfs://hdfs-cdh5-secure-1.ent.cloudera.com:20101/
drwxr-xr-x   - hbase hbase               0 2013-09-25 10:33 webhdfs://hdfs-cdh5-secure-1.ent.cloudera.com:20101/hbase
drwxrwxr-x   - solr  solr                0 2013-09-25 10:34 webhdfs://hdfs-cdh5-secure-1.ent.cloudera.com:20101/solr
drwxrwxrwt   - hdfs  supergroup          0 2013-09-25 10:39 webhdfs://hdfs-cdh5-secure-1.ent.cloudera.com:20101/tmp
drwxr-xr-x   - hdfs  supergroup          0 2013-09-25 11:00 webhdfs://hdfs-cdh5-secure-1.ent.cloudera.com:20101/user
[jenkins@hdfs-cdh5-secure-1 ~]$
{code}",0
"first if we create one file with some file length and take the snapshot of that file,and again append some data through append method to that file,then if we do cat command operation on snapshot of that file,in general it should dispaly the data what we added with create operation,but it is displaying the total data i.e. create +_ appended data.
but if we do the same operation and if we read the contents of snapshot file through input stream it is just displaying the data created in snapshoted files.
in this the behaviour of cat command and reading through inputstream is getting different",0
"DataXceiver tries to establish secure channels via sasl when dfs.encrypt.data.transfer is turned on. However, domain socket traffic seems to be unencrypted therefore the client cannot communicate with the data node via domain sockets, which makes short circuit reads unfunctional.",0
"In https://issues.apache.org/jira/browse/HDFS-5367, fsimage may not write to all IMAGE dir, so we need to check whether fsimage exists before FSImage.getFsImageName returned.",0
"{{DFS_CLIENT_MMAP_CACHE_THREAD_RUNS_PER_TIMEOUT}} is set to the wrong value.  It should be set to {{dfs.client.mmap.cache.thread.runs.per.timeout}}, but instead it is set to {{dfs.client.mmap.cache.timeout.ms}}.  The result is that mmap segments cached in the {{ClientMmapManager}} will take much longer to timeout than they should.",0
"I faced this When i am doing some snapshot operations like createSnapshot,renameSnapshot,i restarted my NN,it is shutting down with exception,
2013-10-24 21:07:03,040 FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Exception in namenode join
java.lang.IllegalStateException
	at com.google.common.base.Preconditions.checkState(Preconditions.java:133)
	at org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$ChildrenDiff.replace(INodeDirectoryWithSnapshot.java:82)
	at org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$ChildrenDiff.access$700(INodeDirectoryWithSnapshot.java:62)
	at org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$DirectoryDiffList.replaceChild(INodeDirectoryWithSnapshot.java:397)
	at org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$DirectoryDiffList.access$900(INodeDirectoryWithSnapshot.java:376)
	at org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot.replaceChild(INodeDirectoryWithSnapshot.java:598)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedReplaceINodeFile(FSDirectory.java:1548)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.replaceINodeFile(FSDirectory.java:1537)
	at org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader.loadFilesUnderConstruction(FSImageFormat.java:855)
	at org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader.load(FSImageFormat.java:350)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:910)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:899)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile(FSImage.java:751)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:720)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:266)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:784)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:563)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:422)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:472)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:670)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:655)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1245)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1311)
2013-10-24 21:07:03,050 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2013-10-24 21:07:03,052 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
",0
"1. allow snapshots under dir /foo
2. create a file /foo/bar
3. create a snapshot s1 under /foo
4. delete the file /foo/bar
5. wait till checkpoint or do saveNameSpace
6. restart NN.
7. Now try to read the file from snapshot /foo/.snapshot/s1/bar

client will get BlockMissingException


Reason is 
While loading the deleted file list for a snashottable dir from fsimage, blocks were not updated in blocksmap",0
"1. allow snapshots under dir /foo
2. create a file /foo/test/bar and start writing to it
3. create a snapshot s1 under /foo after block is allocated and some data has been written to it
4. Delete the directory /foo/test
5. wait till checkpoint or do saveNameSpace
6. restart NN.

NN enters to safemode.

Analysis:
Snapshot nodes loaded from fsimage are always complete and all blocks will be in COMPLETE state. 
So when the Datanode reports RBW blocks those will not be updated in blocksmap.
Some of the FINALIZED blocks will be marked as corrupt due to length mismatch.",0
"The complete set of snapshottable directories are referenced both via the file system tree and in the SnapshotManager class. It's possible that when the 2NN performs a checkpoint, it will reload its in-memory state based on a new fsimage from the NN, but will not clear the set of snapshottable directories referenced by the SnapshotManager. In this case, the 2NN will write out an fsimage that cannot be loaded, since the integer written to the fsimage indicating the number of snapshottable directories will be out of sync with the actual number of snapshottable directories serialized to the fsimage.

This is basically the same as HDFS-3835, but for snapshottable directories instead of delegation tokens.",0
"The incremental block reports from data nodes and block commits are asynchronous. This becomes troublesome when the gen stamp for a block is changed during a write pipeline recovery.

* If an incremental block report is delayed from a node but NN had enough replicas already, a report with the old gen stamp may be received after block completion. This replica will be correctly marked corrupt. But if the node had participated in the pipeline recovery, a new (delayed) report with the correct gen stamp will come soon. However, this report won't have any effect on the corrupt state of the replica.

* If block reports are received while the block is still under construction (i.e. client's call to make block committed has not been received by NN), they are blindly accepted regardless of the gen stamp. If a failed node reports in with the old gen stamp while pipeline recovery is on-going, it will be accepted and counted as valid during commit of the block.

Due to the above two problems, correct replicas can be marked corrupt and corrupt replicas can be accepted during commit.  So far we have observed two cases in production.

* The client hangs forever to close a file. All replicas are marked corrupt.
* After the successful close of a file, read fails. Corrupt replicas are accepted during commit and valid replicas are marked corrupt afterward.
",0
"Namenode can stuck in safemode on restart if it crashes just after addblock logsync and after taking snapshot for such file. This issue is reported by Prakash and Sathish.

On looking into the issue following things are happening.
.
1) Client added block at NN and just did logsync
   So, NN has block ID persisted.
2)Before returning addblock response to client take a snapshot for root or parent directories for that file
3) Delete parent directory for that file
4) Now crash the NN with out responding success to client for that addBlock call

Now on restart of the Namenode, it will stuck in safemode.
",0
"Saw a stacktrace of datanode startup with a bad volume, where even listing directories would throw an IOException. The failed volume threshold was set to 1, but it would fatally error out in {{File#getCanonicalPath}} in {{getDataDirsFromURIs}}:

{code}
      File dir = new File(dirURI.getPath());
      try {
        dataNodeDiskChecker.checkDir(localFS, new Path(dir.toURI()));
        dirs.add(dir);
      } catch (IOException ioe) {
        LOG.warn(""Invalid "" + DFS_DATANODE_DATA_DIR_KEY + "" ""
            + dir + "" : "", ioe);
        invalidDirs.append(""\"""").append(dir.getCanonicalPath()).append(""\"" "");
      }
{code}

Since {{getCanonicalPath}} can need to do I/O and thus throw an IOException, this catch clause doesn't properly protect startup from a failed volume.",0
"When we deletesnapshot, we are deleting the blocks associated to that snapshot and after that we do logsync to editlog about deleteSnapshot.
There can be a chance that blocks removed from blocks map but before log sync if there is BR ,  NN may finds that block does not exist in blocks map and may invalidate that block. As part HB, invalidation info also can go. After this steps if Namenode shutdown before actually do logsync,  On restart it will still consider that snapshot Inodes and expect blocks to report from DN.

Simple solution is, we should simply move down that blocks removal after logsync only. Similar to delete op.",0
"Currently DstReference#destroyAndCollectBlocks may fail to clean the subtree under the DstReference node for file/directory/snapshot deletion.

Use case 1:
# rename under-construction file with 0-sized blocks after snapshot.
# delete the renamed directory.

We need to make sure we delete the 0-sized block.

Use case 2:
# create snapshot s0 for /
# create a new file under /foo/bar/
# rename foo --> foo2
# create snapshot s1
# delete bar and foo2
# delete snapshot s1

We need to make sure we delete the file under /foo/bar since it is not included in snapshot s0.",0
"1. HA installation, standby NN is down.
2. delete snapshot is called and it has deleted the blocks from blocksmap and all datanodes. log sync also happened.
3. before next log roll NN crashed
4. When the namenode restartes then it will fsimage and finalized edits from shared storage and set the safemode threshold. which includes blocks from deleted snapshot also. (because this edits is not yet read as namenode is restarted before the last edits segment is not finalized)
5. When it becomes active, it finalizes the edits and read the delete snapshot edits_op. but at this time, it was not reducing the safemode count. and it will continuing in safemode.
6. On next restart, as the edits is already finalized, on startup only it will read and set the safemode threshold correctly.

But one more restart will bring NN out of safemode.",0
"https://issues.apache.org/jira/browse/HDFS-5471 (CacheAdmin -listPools fails when user lacks permissions to view all pools) was recently resolved, but on a build with this fix, I am running into another error when using cacheadmin -listPools.

Now, when the user does not have permissions to view all the pools, the cacheadmin -listPools command will throw a NullPointerException.

On a system with a single pool ""root"" with a mode of 750, I see this as user schu:
{code}
[schu@hdfs-c5-nfs ~]$ hdfs cacheadmin -listPools
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.hadoop.hdfs.tools.CacheAdmin$ListCachePoolsCommand.run(CacheAdmin.java:745)
	at org.apache.hadoop.hdfs.tools.CacheAdmin.run(CacheAdmin.java:82)
	at org.apache.hadoop.hdfs.tools.CacheAdmin.main(CacheAdmin.java:87)
{code}

After we modify the root pool to 755, then -listPools works properly.
{code}
[schu@hdfs-c5-nfs ~]$ hdfs cacheadmin -listPools
Found 1 result.
NAME  OWNER  GROUP  MODE       WEIGHT 
root  root   root   rwxr-xr-x  100    
[schu@hdfs-c5-nfs ~]$ 
{code}",0
"WebHDFS requests do not require user name to be specified in the request URL even when in core-site configuration options HTTP authentication is set to simple, and anonymous authentication is disabled.",0
"After HDFS-4581 and HDFS-4699, {{checkDiskError()}} is not called when network errors occur during processing data node requests.  This appears to create problems when a disk is having problems, but not failing I/O soon. 

If I/O hangs for a long time, network read/write may timeout first and the peer may close the connection. Although the error was caused by a faulty local disk, disk check is not being carried out in this case. ",0
"Currently, the HDFS NFS Gateway only supports configuring a single subdirectory export via the dfs.nfs3.export.point configuration setting. Supporting multiple subdirectory exports can make data and security management easier when using the HDFS NFS Gateway.",0
"Current trunk layout version is -48.
Hadoop v2.2.0 layout version is -47.

If a cluster is upgraded from v2.2.0 (-47) to trunk (-48), the datanodes cannot start with -rollback.  It will fail with IncorrectVersionException.",0
"Currently, NFS gateway only supports AUTH_UNIX RPC authentication. 
AUTH_UNIX is easy to deploy and use but lack of strong security support. 
This JIRA is to track the effort of NFS gateway security enhancement, such as RPCSEC_GSS framework and end to end Kerberos support.",0
"I am on a HA-enabled cluster. The NameNodes are on host-1 and host-2.

In the configurations, we specify the host-1 NN first and the host-2 NN afterwards in the _dfs.ha.namenodes.ns1_ property (where _ns1_ is the name of the nameservice).

If the host-1 NN is Standby and the host-2 NN is Active, some CacheAdmins will fail complaining about operation not supported in standby state.

e.g.
{code}
bash-4.1$ hdfs cacheadmin -removeDirectives -path /user/hdfs2
Exception in thread ""main"" org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby
	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1501)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1082)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.listCacheDirectives(FSNamesystem.java:6892)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer$ServerSideCacheEntriesIterator.makeRequest(NameNodeRpcServer.java:1263)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer$ServerSideCacheEntriesIterator.makeRequest(NameNodeRpcServer.java:1249)
	at org.apache.hadoop.fs.BatchedRemoteIterator.makeRequest(BatchedRemoteIterator.java:77)
	at org.apache.hadoop.fs.BatchedRemoteIterator.makeRequestIfNeeded(BatchedRemoteIterator.java:85)
	at org.apache.hadoop.fs.BatchedRemoteIterator.hasNext(BatchedRemoteIterator.java:99)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.listCacheDirectives(ClientNamenodeProtocolServerSideTranslatorPB.java:1087)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1499)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)

	at org.apache.hadoop.ipc.Client.call(Client.java:1348)
	at org.apache.hadoop.ipc.Client.call(Client.java:1301)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy9.listCacheDirectives(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB$CacheEntriesIterator.makeRequest(ClientNamenodeProtocolTranslatorPB.java:1079)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB$CacheEntriesIterator.makeRequest(ClientNamenodeProtocolTranslatorPB.java:1064)
	at org.apache.hadoop.fs.BatchedRemoteIterator.makeRequest(BatchedRemoteIterator.java:77)
	at org.apache.hadoop.fs.BatchedRemoteIterator.makeRequestIfNeeded(BatchedRemoteIterator.java:85)
	at org.apache.hadoop.fs.BatchedRemoteIterator.hasNext(BatchedRemoteIterator.java:99)
	at org.apache.hadoop.hdfs.DistributedFileSystem$32.hasNext(DistributedFileSystem.java:1704)
	at org.apache.hadoop.hdfs.tools.CacheAdmin$RemoveCacheDirectiveInfosCommand.run(CacheAdmin.java:372)
	at org.apache.hadoop.hdfs.tools.CacheAdmin.run(CacheAdmin.java:84)
	at org.apache.hadoop.hdfs.tools.CacheAdmin.main(CacheAdmin.java:89)
{code}

After manually failing over from host-2 to host-1, the CacheAdmin commands succeed.


The affected commands are:

-listPools
-listDirectives
-removeDirectives",0
"When a block is reported from a data node while the block is under construction (i.e. not committed or completed), BlockManager calls BlockInfoUnderConstruction.addReplicaIfNotPresent() to update the reported replica state. But BlockManager is calling it with the stored block, not reported block.  This causes the recorded replicas' gen stamp to be that of BlockInfoUnderConstruction itself, not the one from reported replica.

When a pipeline recovery is done for the last packet of a block, the incremental block reports with the new gen stamp may come before the client calling updatePipeline(). If this happens, these replicas will be incorrectly recorded with the old gen stamp and get removed later.  The result is close or addAdditionalBlock failure.

If the last block is completed, but the penultimate block is not because of this issue, the file won't be closed. If this file is not cleared, but the client goes away, the lease manager will try to recover the lease/block, at which point it will crash. I will file a separate jira for this shortly.

The worst case is to reject all good ones and accepting a bad one. In this case, the block will get completed, but the data cannot be read until the next full block report containing one of the valid replicas is received.",0
"As mentioned in HDFS-5557, if a file has its last and penultimate block not completed and the file is being closed, the last block may be completed but the penultimate one might not. If this condition lasts long and the file is abandoned, LeaseManager will try to recover the lease and the block. But {{internalReleaseLease()}} will fail with invalid cast exception with this kind of file.

",0
"Currently we can't restrict what networks are allowed to transfer data using WebHDFS. Obviously we can use firewalls to block ports, but this can be complicated and problematic to maintain. Additionally, because all the jetty servlets run inside the same container, blocking access to jetty to prevent WebHDFS transfers also blocks the other servlets running inside that same jetty container.
I am requesting a deny/allow feature be added to WebHDFS. This is already done with the Apache HTTPD server, and is what I'd like to see the deny/allow list modeled after. Thanks.",0
"In recent [build|https://builds.apache.org/job/PreCommit-HDFS-Build/5592//testReport/org.apache.hadoop.hdfs.server.balancer/TestBalancerWithNodeGroup/testBalancerWithNodeGroup/] in HDFS-5574, TestBalancerWithNodeGroup timeout, this is also mentioned in HDFS-4376 [here|https://issues.apache.org/jira/browse/HDFS-4376?focusedCommentId=13799402&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13799402]. 
Looks like the bug is introduced by HDFS-3495.",0
"In a cluster with non-HA setup and dfs.persist.blocks set to false, we may have data loss in the following case:

# client creates file1 and requests a block from NN and get blk_id1_gs1
# client writes blk_id1_gs1 to DN
# NN is restarted and because persistBlocks is false, blk_id1_gs1 may not be persisted in disk
# another client creates file2 and NN will allocate a new block using the same block id blk_id1_gs1 since block ID and generation stamp are both increased sequentially.

Now we may have two versions (file1 and file2) of the blk_id1_gs1 (same id, same gs) in the system. It will case data loss.",0
Implement RPC stubs for both DistributedFileSystem and NameNodeRpcServer.,0
Implement and test GETACLS and SETACL in WebHDFS.,0
Currently JournalNode has only HTTP support only. This jira tracks the effort to add HTTPS support into JournalNode.,0
"we observed several warning logs like below from region server nodes:
2013-12-05,13:22:26,042 WARN org.apache.hadoop.hdfs.DFSClient: Failed to connect to /10.2.201.110:11402 for block, add to deadNodes and continue. org.apache.hadoop.security.token.SecretManager$InvalidToken: Block token with block_token_identifier (expiryDate=1386060141977, keyId=-333530248, userId=hbase_srv, blockPoolId=BP-1310313570-10.101.10.66-1373527541386, blockId=-190217754078101701, access modes=[READ]) is expired.
at org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.checkAccess(BlockTokenSecretManager.java:280)
at org.apache.hadoop.hdfs.security.token.block.BlockPoolTokenSecretManager.checkAccess(BlockPoolTokenSecretManager.java:88)
at org.apache.hadoop.hdfs.server.datanode.DataNode.checkBlockToken(DataNode.java:1082)
at org.apache.hadoop.hdfs.server.datanode.DataNode.getBlockLocalPathInfo(DataNode.java:1033)
at org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolServerSideTranslatorPB.getBlockLocalPathInfo(ClientDatanodeProtocolServerSideTranslatorPB.java:112)
at org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$ClientDatanodeProtocolService$2.callBlockingMethod(ClientDatanodeProtocolProtos.java:5104)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:453)
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:898)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1693)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1689)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:396)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1332)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1687)
org.apache.hadoop.security.token.SecretManager$InvalidToken: Block token with block_token_identifier (expiryDate=1386060141977, keyId=-333530248, userId=hbase_srv, blockPoolId=BP-1310313570-10.101.10.66-1373527541386, blockId=-190217754078101701, access modes=[READ]) is expired.
at org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.checkAccess(BlockTokenSecretManager.java:280)
at org.apache.hadoop.hdfs.security.token.block.BlockPoolTokenSecretManager.checkAccess(BlockPoolTokenSecretManager.java:88)
at org.apache.hadoop.hdfs.server.datanode.DataNode.checkBlockToken(DataNode.java:1082)
at org.apache.hadoop.hdfs.server.datanode.DataNode.getBlockLocalPathInfo(DataNode.java:1033)
at org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolServerSideTranslatorPB.getBlockLocalPathInfo(ClientDatanodeProtocolServerSideTranslatorPB.java:112)
at org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$ClientDatanodeProtocolService$2.callBlockingMethod(ClientDatanodeProtocolProtos.java:5104)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:453)
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:898)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1693)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1689)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:396)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1332)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1687)
at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:90)
at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:57)
at org.apache.hadoop.hdfs.DFSClient.getLocalBlockReader(DFSClient.java:771)
at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:888)
at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455)
at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:645)
at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:689)
at java.io.DataInputStream.read(DataInputStream.java:132)
at org.apache.hadoop.hbase.io.hfile.HFileBlock.readWithExtra(HFileBlock.java:614)
at org.apache.hadoop.hbase.io.hfile.HFileBlock$AbstractFSReader.readAtOffset(HFileBlock.java:1384)
at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderV2.readBlockDataInternal(HFileBlock.java:1829)
at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderV2.readBlockData(HFileBlock.java:1673)
at org.apache.hadoop.hbase.io.hfile.HFileReaderV2.readBlock(HFileReaderV2.java:341)
at org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader.loadDataBlockWithScanInfo(HFileBlockIndex.java:254)
at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.seekTo(HFileReaderV2.java:485)
at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.reseekTo(HFileReaderV2.java:535)
at org.apache.hadoop.hbase.regionserver.StoreFileScanner.reseekAtOrAfter(StoreFileScanner.java:246)
at org.apache.hadoop.hbase.regionserver.StoreFileScanner.reseek(StoreFileScanner.java:167)
at org.apache.hadoop.hbase.regionserver.NonLazyKeyValueScanner.doRealSeek(NonLazyKeyValueScanner.java:54)
at org.apache.hadoop.hbase.regionserver.KeyValueHeap.generalizedSeek(KeyValueHeap.java:352)
at org.apache.hadoop.hbase.regionserver.KeyValueHeap.reseek(KeyValueHeap.java:292)
at org.apache.hadoop.hbase.regionserver.StoreScanner.reseek(StoreScanner.java:586)
at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:446)
at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:349)
at org.apache.hadoop.hbase.regionserver.Store.compactStore(Store.java:1660)
at org.apache.hadoop.hbase.regionserver.Store.compact(Store.java:1080)
at org.apache.hadoop.hbase.regionserver.HRegion.compact(HRegion.java:1244)
at org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.run(CompactionRequest.java:258)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): Block token with block_token_identifier (expiryDate=1386060141977, keyId=-333530248, userId=hbase_srv, blockPoolId=BP-1310313570-10.101.10.66-1373527541386, blockId=-190217754078101701, access modes=[READ]) is expired.
at org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.checkAccess(BlockTokenSecretManager.java:280)
at org.apache.hadoop.hdfs.security.token.block.BlockPoolTokenSecretManager.checkAccess(BlockPoolTokenSecretManager.java:88)
at org.apache.hadoop.hdfs.server.datanode.DataNode.checkBlockToken(DataNode.java:1082)
at org.apache.hadoop.hdfs.server.datanode.DataNode.getBlockLocalPathInfo(DataNode.java:1033)
at org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolServerSideTranslatorPB.getBlockLocalPathInfo(ClientDatanodeProtocolServerSideTranslatorPB.java:112)
at org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$ClientDatanodeProtocolService$2.callBlockingMethod(ClientDatanodeProtocolProtos.java:5104)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:453)
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:898)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1693)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1689)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:396)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1332)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1687)
at org.apache.hadoop.ipc.Client.call(Client.java:1167)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:202)
at $Proxy21.getBlockLocalPathInfo(Unknown Source)
at org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolTranslatorPB.getBlockLocalPathInfo(ClientDatanodeProtocolTranslatorPB.java:215)
at org.apache.hadoop.hdfs.BlockReaderLocal.getBlockPathInfo(BlockReaderLocal.java:254)
at org.apache.hadoop.hdfs.BlockReaderLocal.newBlockReader(BlockReaderLocal.java:167)
at org.apache.hadoop.hdfs.DFSClient.getLocalBlockReader(DFSClient.java:767)
... 28 more
2013-12-05,13:22:26,047 INFO org.apache.hadoop.hdfs.DFSClient: Will fetch a new access token and retry, access token was invalid when connecting to /10.2.201.28:11402 : org.apache.hadoop.hdfs.security.token.block.InvalidBlockTokenException: Got access token error for OP_READ_BLOCK, self=/10.2.201.110:39179, remote=/10.2.201.28:11402, for file /hbase/lgsrv-micloud/phone_number_digest/58c0ed89dd38537a0077807523458404/C/6448605d86fb4aae8f9c773408a1ea6a, for pool BP-1310313570-10.101.10.66-1373527541386 block -190217754078101701_5305685
2013-12-05,13:22:26,049 INFO org.apache.hadoop.hdfs.DFSClient: Successfully connected to /10.2.201.28:11402 for block -190217754078101701
[work@lg-hadoop-srv-st10 regionserver]$ ifconfig
em1 Link encap:Ethernet HWaddr B8:CA:3A:F5:BB:61 
inet addr:10.2.201.110 Bcast:10.2.201.255 Mask:255.255.255.0",0
"A race condition between NFS gateway writeback executor thread and new write handler thread can cause writeback state check failure, e.g.,
{noformat}
2013-11-26 10:34:07,859 DEBUG nfs3.RpcProgramNfs3 (Nfs3Utils.java:writeChannel(113)) - WRITE_RPC_CALL_END______957880843
2013-11-26 10:34:07,863 DEBUG nfs3.OpenFileCtx (OpenFileCtx.java:offerNextToWrite(832)) - The asyn write task has no pending writes, fileId: 30938
2013-11-26 10:34:07,871 ERROR nfs3.AsyncDataService (AsyncDataService.java:run(136)) - Asyn data service got error:java.lang.IllegalStateException: The openFileCtx has false async status
        at com.google.common.base.Preconditions.checkState(Preconditions.java:145)
        at org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx.executeWriteBack(OpenFileCtx.java:890)
        at org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService$WriteBackTask.run(AsyncDataService.java:134)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

2013-11-26 10:34:07,901 DEBUG nfs3.RpcProgramNfs3 (RpcProgramNfs3.java:write(707)) - requesed offset=917504 and current filesize=917504
2013-11-26 10:34:07,902 DEBUG nfs3.WriteManager (WriteManager.java:handleWrite(131)) - handleWrite fileId: 30938 offset: 917504 length:65536 stableHow:0
{noformat}",0
HDFS-5284 introduces features as generic abstractions to extend the functionality of the inodes. The implementation of ACL should leverage the new abstractions.,0
"I tried to cache a file by ""hdfs cacheadmin -addDirective"".

I thought the file was cached because ""CacheUsed"" at jmx was more than 0.
{code}
{
    ""name"" : ""Hadoop:service=DataNode,name=FSDatasetState-DS-1043926324-172.28.0.102-50010-1385087929296"",
    ""modelerType"" : ""org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl"",
    ""Remaining"" : 5604772597760,
    ""StorageInfo"" : ""FSDataset{dirpath='[/hadoop/data1/dfs/data/current, /hadoop/data2/dfs/data/current, /hadoop/data3/dfs/data/current]'}"",
    ""Capacity"" : 5905374474240,
    ""DfsUsed"" : 11628544,
    ""CacheCapacity"" : 1073741824,
    ""CacheUsed"" : 360448,
    ""NumFailedVolumes"" : 0,
    ""NumBlocksCached"" : 1,
    ""NumBlocksFailedToCache"" : 0,
    ""NumBlocksFailedToUncache"" : 0
  },
{code}
But ""dfsadmin -report"" didn't output the same value as jmx.
{code}
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
{code}",0
"TestBPOfferService#testBPInitErrorHandling fails intermittently.  We believe it's because of inconsistent synchronization in BPOfferService.

stack trace:
{code}
Regression

org.apache.hadoop.hdfs.server.datanode.TestBPOfferService.testBPInitErrorHandling

Failing for the past 1 build (Since #5698 )
Took 0.16 sec.

Error Message

expected:<1> but was:<2>

Stacktrace

java.lang.AssertionError: expected:<1> but was:<2>
at org.junit.Assert.fail(Assert.java:93)
at org.junit.Assert.failNotEquals(Assert.java:647)
at org.junit.Assert.assertEquals(Assert.java:128)
{code}

see https://builds.apache.org/job/PreCommit-HDFS-Build/5698//testReport/org.apache.hadoop.hdfs.server.datanode/TestBPOfferService/testBPInitErrorHandling/",0
"In the following code in Storage#tryLock(), there is a possibility that {{file.getChannel().tryLock()}} returns null if the lock is acquired by some other process. In that case even though return value is null, a successfull message confuses.
{code}try {
        res = file.getChannel().tryLock();
        file.write(jvmName.getBytes(Charsets.UTF_8));
        LOG.info(""Lock on "" + lockF + "" acquired by nodename "" + jvmName);
      } catch(OverlappingFileLockException oe) {{code}",0
"This patch will include the core logic for modification of ACLs. This includes support for all user-facing APIs that modify ACLs. This will cover access ACLs, default ACLs, automatic mask calculations, automatic inference of unprovided default ACL entries, and validation to prevent creation of an invalid ACL.",0
"When using distcp command to copy files with -delete switch, running as user <xyz>,

hadoop distcp -p -i -update  -delete hdfs://srchost:<port>/user hdfs://dsthost:<port>/user

It fails with the following exception:

Copy failed: java.io.FileNotFoundException: File does not exist: hdfs://dsthost:<port>/user/xyz/.stagingdistcp_urjb0g/_distcp_src_files
        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:557)
        at org.apache.hadoop.tools.DistCp$CopyInputFormat.getSplits(DistCp.java:266)
        at org.apache.hadoop.mapred.JobClient.writeOldSplits(JobClient.java:1081)
        at org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:1073)
        at org.apache.hadoop.mapred.JobClient.access$700(JobClient.java:179)
        at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:983)
        at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:936)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:936)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:910)
        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1353)
        at org.apache.hadoop.tools.DistCp.copy(DistCp.java:667)
        at org.apache.hadoop.tools.DistCp.run(DistCp.java:881)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
        at org.apache.hadoop.tools.DistCp.main(DistCp.java:908)


",0
"When HA is implemented with QJM and using kerberos, it's not possible to set wire-encrypted data.
If it's set property hadoop.rpc.protection to something different to authentication it doesn't work propertly, getting the error:

ERROR security.UserGroupInformation: PriviledgedActionException as:principal@REALM (auth:KERBEROS) cause:javax.security.sasl.SaslException: No common protection layer between client and ser

With NFS as shared storage everything works like a charm",0
"DataNode fails to start when dfs.http.policy is set to HTTP_ONLY. This is because that the sanity checks in HttpServer.Builder do not match the behavior in secureMain in DataNode, therefore DataNode cannot start the HTTP server and bail out.",0
FsShell Cli: Add XML based End-to-End test for getfacl and setfacl commands,0
"HDFS-3987 added HTTPS support to webhdfs, using the new scheme swebhdfs://.
This JIRA is to add HTTPS support to HttpFS as well as supporting the DelegationTokens required by swebhdfs://",0
"From https://builds.apache.org/job/Hadoop-Hdfs-trunk/1626/testReport/org.apache.hadoop.hdfs.server.namenode/TestSecondaryNameNodeUpgrade/testChangeNsIDFails/ :
{code}
java.util.ConcurrentModificationException: null
	at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
	at java.util.HashMap$EntryIterator.next(HashMap.java:834)
	at java.util.HashMap$EntryIterator.next(HashMap.java:832)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.shutdown(FsVolumeImpl.java:251)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.shutdown(FsVolumeList.java:218)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.shutdown(FsDatasetImpl.java:1414)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:1309)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdownDataNodes(MiniDFSCluster.java:1464)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:1439)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:1423)
	at org.apache.hadoop.hdfs.server.namenode.TestSecondaryNameNodeUpgrade.doIt(TestSecondaryNameNodeUpgrade.java:97)
	at org.apache.hadoop.hdfs.server.namenode.TestSecondaryNameNodeUpgrade.testChangeNsIDFails(TestSecondaryNameNodeUpgrade.java:116)
{code}
The above happens when shutdown() is called in parallel to addBlockPool() or shutdownBlockPool().",0
"From https://builds.apache.org/job/hbase-0.96-hadoop2/166/testReport/junit/org.apache.hadoop.hbase.mapreduce/TestTableInputFormatScan1/org_apache_hadoop_hbase_mapreduce_TestTableInputFormatScan1/ :
{code}
2014-01-01 00:10:15,571 INFO  [IPC Server handler 2 on 50198] blockmanagement.BlockManager(1009): BLOCK* addToInvalidates: blk_1073741967_1143 127.0.0.1:40188 127.0.0.1:46149 127.0.0.1:41496 
2014-01-01 00:10:16,559 WARN  [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@93935b] namenode.FSDirectory(1854): Could not get full path. Corresponding file might have deleted already.
2014-01-01 00:10:16,560 FATAL [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@93935b] blockmanagement.BlockManager$ReplicationMonitor(3127): ReplicationMonitor thread received Runtime exception. 
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.getFullPathName(FSDirectory.java:1871)
	at org.apache.hadoop.hdfs.server.namenode.INode.getFullPathName(INode.java:482)
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.getName(INodeFile.java:316)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy.chooseTarget(BlockPlacementPolicy.java:118)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1259)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1167)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3158)
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3112)
	at java.lang.Thread.run(Thread.java:724)
{code}
Looks like getRelativePathINodes() returned null but getFullPathName() didn't check inodes against null, leading to NPE.",0
