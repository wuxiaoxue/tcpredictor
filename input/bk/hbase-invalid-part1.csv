HBASE-1249	"To discuss all the new and potential issues coming out of the change in key format (HBASE-1234): zero-copy reads, client binary protocol, update of API (HBASE-880), server optimizations, etc..."
HBASE-18472	"When I run mvn clean install -DskipTests on my local machine, lt always shows error below 
{quote}
WARNING] Rule 0: org.apache.maven.plugins.enforcer.EvaluateBeanshell failed with message:
License errors detected, for more detail find ERROR in hbase-assembly/target/maven-shared-archive-resources/META-INF/LICENSE

Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:1.4.1:enforce (check-aggregate-license) on project hbase-assembly: Some Enforcer rules have failed. Look above for specific messages explaining why the rule failed.
{quote}"
HBASE-17874	"HBASE-14978 added this size limiting so as to make sure the multi read requests do not retain two many blocks. This works well when the blocks are obtained from any where other than memory mode BucketCache. In case of on heap or off heap Bucket Cache, the entire cache area is split into N ByteBuffers each of size 4 MB. When we hit a block in this cache, we no longer do copy data into temp array. We use the same shared memory (BB).  Its  capacity is 4 MB.
The block size accounting logic is RSRpcServices is like below
{code}
 if (c instanceof ByteBufferCell) {
          ByteBufferCell bbCell = (ByteBufferCell) c;
          ByteBuffer bb = bbCell.getValueByteBuffer();
          if (bb != lastBlock) {
            context.incrementResponseBlockSize(bb.capacity());
            lastBlock = bb;
          }
        } else {
          // We're using the last block being the same as the current block as
          // a proxy for pointing to a new block. This won't be exact.
          // If there are multiple gets that bounce back and forth
          // Then it's possible that this will over count the size of
          // referenced blocks. However it's better to over count and
          // use two rpcs than to OOME the regionserver.
          byte[] valueArray = c.getValueArray();
          if (valueArray != lastBlock) {
            context.incrementResponseBlockSize(valueArray.length);
            lastBlock = valueArray;
          }
        }
		{code}
We take the BBCell's value buffer and takes its capacity. The cell is backed by the same BB that backs the HFileBlock. When the HFileBlock is created from the BC, we do as below duplicating and proper positioning and limiting the BB
{code}
 ByteBuffer bb = buffers[i].duplicate();
      if (i == startBuffer) {
        cnt = bufferSize - startBufferOffset;
        if (cnt > len) cnt = len;
        bb.limit(startBufferOffset + cnt).position(startBufferOffset);
		{code}
Still this BB's capacity is 4 MB.
This will make the size limit breach to happen too soon. What we expect is block size defaults to 64 KB and so we here by allow cells from different blocks to appear in response. We have a way to check whether we move from one block to next.
{code}
if (bb != lastBlock) {
...
            lastBlock = bb;
}
{code}
But already just by considering the 1st cell, we added 4 MB size!"
HBASE-15573	"Can't retrieve any information with hbase  rpc java client.
With hbase shell its possible to scan data and retrieve all the information normally.

But with any rpc client region server don't retrieve data, all data come with null values.

Region Server log:
DEBUG [RpcServer.reader=2,bindAddress=HBASE,port=16020] ipc.RpcServer: RpcServer.listener,port=16020: DISCONNECTING client SERVER:37088 because read count=-1
DEBUG [RpcServer.reader=2,bindAddress=HBASE,port=16020] ipc.RpcServer: RpcServer.listener,port=16020: DISCONNECTING client SERVER2:36997 because read count=-1

Master log:
2016-03-31 18:16:27,998 DEBUG [ProcedureExecutorTimeout] procedure2.ProcedureExecutor$CompletedProcedureCleaner: No completed procedures to cleanup.
2016-03-31 18:16:57,998 DEBUG [ProcedureExecutorTimeout] procedure2.ProcedureExecutor$CompletedProcedureCleaner: No completed procedures to cleanup.
2016-03-31 18:17:27,998 DEBUG [ProcedureExecutorTimeout] procedure2.ProcedureExecutor$CompletedProcedureCleaner: No completed procedures to cleanup
"
HBASE-14903	" I've been reading on Latest Reference Guide and try to translated into Chinese!
     I think this sentence ""When a table is in the process of splitting,"" should be ""When a Region is in the process of splitting,"" on chapter 銆?2.2. hbase:meta銆戙€?     By the way,is this document the latest?銆恏ttp://hbase.apache.org/book.html#arch.overview銆慖 will translate it锛?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-12-03 19:23:27.39,,false,,,,,,,,,,,,,9223372036854775807,,,,,Sun Dec 06 02:38:23 UTC 2015,,,,,,0|i2p5cf:,9223372036854775807,,,,,,,,03/Dec/15 19:23;apurtell;bq. I think this sentence """"When a table is in the process of splitting"
HBASE-14957	"for initializing nutch, after configuring nutch whent I am trying to kickoff start-hbase.sh I am getting the below error.

/cygdrive/c/users/gaurav.kandpal/desktop/test/hbase-0.94.9.tar/hbase-0.94.9/hbase-0.94.9/bin/../conf/hbase-env.sh: line 101: $'\r': command not found
/cygdrive/c/users/gaurav.kandpal/desktop/test/hbase-0.94.9.tar/hbase-0.94.9/hbase-0.94.9/bin/../conf/hbase-env.sh: line 104: $'\r': command not found
/cygdrive/c/users/gaurav.kandpal/desktop/test/hbase-0.94.9.tar/hbase-0.94.9/hbase-0.94.9/bin/../conf/hbase-env.sh: line 107: $'\r': command not found
/cygdrive/c/users/gaurav.kandpal/desktop/test/hbase-0.94.9.tar/hbase-0.94.9/hbase-0.94.9/bin/../conf/hbase-env.sh: line 110: $'\r': command not found
/cygdrive/c/users/gaurav.kandpal/desktop/test/hbase-0.94.9.tar/hbase-0.94.9/hbase-0.94.9/bin/../conf/hbase-env.sh: line 115: $'\r': command not found
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.
'nrecognized VM option 'UseConcMarkSweepGC
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.
'nrecognized VM option 'UseConcMarkSweepGC
starting master, logging to /cygdrive/c/users/gaurav.kandpal/desktop/test/hbase-0.94.9.tar/hbase-0.94.9/hbase-0.94.9/bin/../logs/hbase-Gaurav.Kandpal-master-gauravk.out
localhost: /cygdrive/c/users/gaurav.kandpal/desktop/test/hbase-0.94.9.tar/hbase-0.94.9/hbase-0.94.9/bin/regionservers.sh: line 64: ssh: command not found

reference URL is 

https://gist.github.com/xrstf/b48a970098a8e76943b9
 "
HBASE-2761	"Never seen this prior to the new meta prefetch stuff. Saw it tonight on a YCSB run after about an hour.

Exception in thread ""Thread-9"" java.lang.OutOfMemoryError: GC overhead limit exceeded
        at java.util.Hashtable.rehash(Hashtable.java:356)
        at java.util.Hashtable.put(Hashtable.java:412)
        at java.util.Properties.setProperty(Properties.java:143)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1337)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:1227)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:1156)
        at org.apache.hadoop.conf.Configuration.iterator(Configuration.java:1198)
        at org.apache.hadoop.hbase.HBaseConfiguration.hashCode(HBaseConfiguration.java:112)
        at org.apache.hadoop.hbase.client.HConnectionManager.getConnection(HConnectionManager.java:121)
        at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:130)
        at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:99)
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:102)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.prefetchRegionCache(HConnectionManager.java:733)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegionInMeta(HConnectionManager.java:784)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:678)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.processBatchOfPuts(HConnectionManager.java:1424)
        at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:660)
        at org.apache.hadoop.hbase.client.HTable.doPut(HTable.java:545)
"
HBASE-2731	"We're getting zillions of these in HMaster when it starts assigning regions at startup and it doesn't seem to progress.

{code}
2010-06-15 06:30:00,287 WARN org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: 
<org.apache.hadoop.hbase.master.HMaster>Failed to create ZNode /hbase/UNASSIGNED/1766164500 in ZooKeeper^D
org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /hbase/UNASSIGNED/1766164500^D
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:110)^D
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)^D
        at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:637)^D
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.createZNodeIfNotExists(ZooKeeperWrapper.java:974)^D
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.createUnassignedRegion(ZooKeeperWrapper.java:1053)^D
        at org.apache.hadoop.hbase.master.RegionManager.doRegionAssignment(RegionManager.java:355)^D
        at org.apache.hadoop.hbase.master.RegionManager.assignRegions(RegionManager.java:312)^D
        at org.apache.hadoop.hbase.master.RegionManager.assignRegionsToMultipleServers(RegionManager.java:292)^D
        at org.apache.hadoop.hbase.master.RegionManager.assignRegions(RegionManager.java:221)^D
        at org.apache.hadoop.hbase.master.ServerManager.processMsgs(ServerManager.java:500)^D
        at org.apache.hadoop.hbase.master.ServerManager.processRegionServerAllsWell(ServerManager.java:425)^D
        at org.apache.hadoop.hbase.master.ServerManager.regionServerReport(ServerManager.java:335)^D
        at org.apache.hadoop.hbase.master.HMaster.regionServerReport(HMaster.java:700)^D
        at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)^D
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)^D
        at java.lang.reflect.Method.invoke(Method.java:597)^D
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:576)^D
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:919)

{code}"
HBASE-3122	"10/10/18 16:26:44 INFO catalog.CatalogTracker: acer,60020,1287443908850 carrying .META.; unsetting .META. location
10/10/18 16:26:44 INFO catalog.CatalogTracker: Current cached META location is not valid, resetting
10/10/18 16:26:44 INFO handler.ServerShutdownHandler: Splitting logs for acer,60020,1287443908850
10/10/18 16:26:44 INFO zookeeper.ZKUtil: hconnection-0x12bc1a2f0a60001 Set watcher on existing znode /hbase/root-region-server
10/10/18 16:26:44 INFO catalog.RootLocationEditor: Unsetting ROOT region location in ZooKeeper
10/10/18 16:26:44 DEBUG zookeeper.ZKAssign: master:60000-0x12bc1a2f0a60000 Creating (or updating) unassigned node for 70236052 with OFFLINE state
10/10/18 16:26:44 WARN master.LoadBalancer: Wanted to do random assignment but no servers to assign to
10/10/18 16:26:44 ERROR executor.EventHandler: Caught throwable while processing event M_SERVER_SHUTDOWN
java.lang.NullPointerException
	at org.apache.hadoop.hbase.master.LoadBalancer$RegionPlan.toString(LoadBalancer.java:595)
	at java.lang.String.valueOf(String.java:2826)
	at java.lang.StringBuilder.append(StringBuilder.java:115)
	at org.apache.hadoop.hbase.master.AssignmentManager.getRegionPlan(AssignmentManager.java:803)
	at org.apache.hadoop.hbase.master.AssignmentManager.getRegionPlan(AssignmentManager.java:777)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:720)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:640)
	at org.apache.hadoop.hbase.master.AssignmentManager.assignRoot(AssignmentManager.java:922)
	at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:97)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:150)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
"
HBASE-13822	"Launching the shell from a build of {{branch-1.1}} gives me the following error

{noformat}
$ echo $HBASE_HOME
/Users/ndimiduk/repos/hbase
$ $HBASE_HOME/bin/hbase shell
NoMethodError: undefined method `getTerminal' for Java::Jline::Terminal:Module
  refresh_width at /Users/ndimiduk/repos/hbase/hbase-shell/src/main/ruby/shell/formatter.rb:33
     initialize at /Users/ndimiduk/repos/hbase/hbase-shell/src/main/ruby/shell/formatter.rb:46
         (root) at /Users/ndimiduk/repos/hbase/bin/hirb.rb:115
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/Users/ndimiduk/.m2/repository/org/slf4j/slf4j-log4j12/1.7.7/slf4j-log4j12-1.7.7.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
{noformat}"
HBASE-13934	"HBase Client get's stuck when I try to execute a PUT operation.

{code}
Thread [BenchmarkThread-0] (Suspended)  
    owns: BufferedMutatorImpl  (id=43)  
    Unsafe.park(boolean, long) line: not available [native method]  
    LockSupport.park(Object) line: 186  
    AbstractQueuedSynchronizer$ConditionObject.await() line: 2043   
    ArrayBlockingQueue<E>.take() line: 374  
    BoundedCompletionService<V>.take() line: 75 
    ScannerCallableWithReplicas.call(int) line: 190 
    ScannerCallableWithReplicas.call(int) line: 56  
    RpcRetryingCaller<T>.callWithoutRetries(RetryingCallable<T>, int) line: 200 
    ClientSmallReversedScanner.loadCache() line: 211    
    ClientSmallReversedScanner.next() line: 185 
    ConnectionManager$HConnectionImplementation.locateRegionInMeta(TableName, byte[], boolean, boolean, int) line: 1200 
    ConnectionManager$HConnectionImplementation.locateRegion(TableName, byte[], boolean, boolean, int) line: 1109   
    AsyncProcess.submit(ExecutorService, TableName, List<Row>, boolean, Callback<CResult>, boolean) line: 369   
    AsyncProcess.submit(TableName, List<Row>, boolean, Callback<CResult>, boolean) line: 320    
    BufferedMutatorImpl.backgroundFlushCommits(boolean) line: 206   
    BufferedMutatorImpl.flush() line: 183   
    HTable.flushCommits() line: 1436    
    HTable.put(Put) line: 1032  
{code}

Source code:

Connect:
{code}
        this.config = HBaseConfiguration.create();
        config.set(""hbase.zookeeper.quorum"", zookeeperHost);

        Connection connection = ConnectionFactory.createConnection(config);
        this.table = connection.getTable(TableName.valueOf(tableName));
{code}

Put:
{code}
        final Put put = new Put(Bytes.toBytes(key));
        for (Map.Entry<String, String> pair : columnValues.entrySet()) {
            final String column = pair.getKey();
            final String value = pair.getValue();
            put.addColumn(columnFamily, Bytes.toBytes(column), Bytes.toBytes(value));
        }

        try {
            table.put(put);
        } catch (IOException e) {
            throw new ClientException(""put error"", e);
        }
{code}

Client log:

{code}
17:00:58,193  INFO ZooKeeper:438 - Initiating client connection, connectString=nosql-x64-node-1.local:2181 sessionTimeout=90000 watcher=hconnection-0x3018fc1a0x0, quorum=nosql-x64-node-1.local:2181, baseZNode=/hbase
17:00:58,325  INFO ClientCnxn:975 - Opening socket connection to server 192.168.56.201/192.168.56.201:2181. Will not attempt to authenticate using SASL (unknown error)
17:00:58,329  INFO ClientCnxn:852 - Socket connection established to 192.168.56.201/192.168.56.201:2181, initiating session
17:00:58,346  INFO ClientCnxn:1235 - Session establishment complete on server 192.168.56.201/192.168.56.201:2181, sessionid = 0x14e06dbd6450020, negotiated timeout = 40000
{code}

Server's log:
{code}
2015-06-18 17:12:28,183 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181] server.NIOServerCnxn: Closed socket connection for client /192.168.56.1:35002 which had sessionid 0x14e06dbd6450020
2015-06-18 17:12:30,001 INFO  [SessionTracker] server.ZooKeeperServer: Expiring session 0x14e06dbd645001d, timeout of 40000ms exceeded
2015-06-18 17:12:30,002 INFO  [ProcessThread(sid:0 cport:-1):] server.PrepRequestProcessor: Processed session termination for sessionid: 0x14e06dbd645001d
2015-06-18 17:12:31,078 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181] server.NIOServerCnxnFactory: Accepted socket connection from /192.168.56.1:35130
2015-06-18 17:12:31,080 INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181] server.ZooKeeperServer: Client attempting to establish new session at /192.168.56.1:35130
2015-06-18 17:12:31,092 INFO  [SyncThread:0] server.ZooKeeperServer: Established session 0x14e06dbd6450021 with negotiated timeout 40000 for client /192.168.56.1:35130
{code}

Happens both with HBASE running in standalone and distributed mode.

Any idea what causing this?

HBase version: 1.0.1 (client + server)


"
HBASE-13444	"IntegrationTestBigLinkedList compiled on branch-1 can't talk to an hbase cluster running branch-1.0. See attached stack trace for more details.

Repro steps:
1) start a cluster from branch-1.0
2) copy configs from that cluster into another repo on branch-1
3) run IntegrationTestBigLinkedlist as follows:
./bin/hbase org.apache.hadoop.hbase.test.IntegrationTestBigLinkedList loop 1 1 100 /tmp/blah2 1 10 10 "
HBASE-13087	"{code}org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException: Column family table does not exist in region hbase:meta,,1.1588230740 in table 'hbase:meta', {TABLE_ATTRIBUTES => {IS_META => 'true', coprocessor$1 => '|org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint|536870911|'}, {NAME => 'info', BLOOMFILTER => 'NONE', VERSIONS => '10', IN_MEMORY => 'true', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZE => '8192', REPLICATION_SCOPE => '0'}
	at org.apache.hadoop.hbase.regionserver.HRegionServer.doBatchOp(HRegionServer.java:4513)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.doNonAtomicRegionMutation(HRegionServer.java:3687)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.java:3576)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:30816)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2029)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:107)
	at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:130)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:107)
	at java.lang.Thread.run(Thread.java:745)
: 1 time, 
	at org.apache.hadoop.hbase.client.AsyncProcess$BatchErrors.makeException(AsyncProcess.java:228)
	at org.apache.hadoop.hbase.client.AsyncProcess$BatchErrors.access$1700(AsyncProcess.java:208)
	at org.apache.hadoop.hbase.client.AsyncProcess.waitForAllPreviousOpsAndReset(AsyncProcess.java:1689)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.backgroundFlushCommits(BufferedMutatorImpl.java:208)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.flush(BufferedMutatorImpl.java:183)
	at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:1404)
	at org.apache.hadoop.hbase.client.HTable.put(HTable.java:1017)
	at org.apache.hadoop.hbase.MetaTableAccessor.put(MetaTableAccessor.java:1123)
	at org.apache.hadoop.hbase.MetaTableAccessor.putToMetaTable(MetaTableAccessor.java:1113)
	at org.apache.hadoop.hbase.MetaTableAccessor.updateTableState(MetaTableAccessor.java:1436)
	at org.apache.hadoop.hbase.MetaTableAccessor.updateTableState(MetaTableAccessor.java:948)
	at org.apache.hadoop.hbase.master.TableStateManager.writeMetaState(TableStateManager.java:195)
	at org.apache.hadoop.hbase.master.TableStateManager.setTableState(TableStateManager.java:69)
	at org.apache.hadoop.hbase.master.AssignmentManager.setEnabledTable(AssignmentManager.java:3427)
	at org.apache.hadoop.hbase.master.HMaster.assignMeta(HMaster.java:903)
	at org.apache.hadoop.hbase.master.HMaster.finishActiveMasterInitialization(HMaster.java:698)
	at org.apache.hadoop.hbase.master.HMaster.access$500(HMaster.java:166)
	at org.apache.hadoop.hbase.master.HMaster$1.run(HMaster.java:1494)
	at java.lang.Thread.run(Thread.java:745)
{code}"
HBASE-5971	"Here is what happens when we try to register server bean:

{code}
javax.management.NotCompliantMBeanException: org.apache.hadoop.hbase.master.MXBean: Method org.apache.hadoop.hbase.master.MXBean.getRegionServers has parameter or return type that cannot be translated into an open type
	at com.sun.jmx.mbeanserver.Introspector.throwException(Introspector.java:412)
	at com.sun.jmx.mbeanserver.MBeanAnalyzer.<init>(MBeanAnalyzer.java:101)
	at com.sun.jmx.mbeanserver.MBeanAnalyzer.analyzer(MBeanAnalyzer.java:87)
	at com.sun.jmx.mbeanserver.MXBeanIntrospector.getAnalyzer(MXBeanIntrospector.java:53)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.getPerInterface(MBeanIntrospector.java:163)
	at com.sun.jmx.mbeanserver.MBeanSupport.<init>(MBeanSupport.java:147)
	at com.sun.jmx.mbeanserver.MXBeanSupport.<init>(MXBeanSupport.java:48)
	at com.sun.jmx.mbeanserver.Introspector.makeDynamicMBean(Introspector.java:184)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:915)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:312)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:482)
	at org.apache.hadoop.metrics.util.MBeanUtil.registerMBean(MBeanUtil.java:58)
	at org.apache.hadoop.hbase.master.HMaster.registerMBean(HMaster.java:1926)
	at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:617)
	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:367)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.lang.IllegalArgumentException: Method org.apache.hadoop.hbase.master.MXBean.getRegionServers has parameter or return type that cannot be translated into an open type
	at com.sun.jmx.mbeanserver.ConvertingMethod.from(ConvertingMethod.java:32)
	at com.sun.jmx.mbeanserver.MXBeanIntrospector.mFrom(MXBeanIntrospector.java:63)
	at com.sun.jmx.mbeanserver.MXBeanIntrospector.mFrom(MXBeanIntrospector.java:33)
	at com.sun.jmx.mbeanserver.MBeanAnalyzer.initMaps(MBeanAnalyzer.java:118)
	at com.sun.jmx.mbeanserver.MBeanAnalyzer.<init>(MBeanAnalyzer.java:99)
	... 14 more
Caused by: javax.management.openmbean.OpenDataException: Cannot convert type: java.util.Map<java.lang.String, org.apache.hadoop.hbase.ServerLoad>
	at com.sun.jmx.mbeanserver.OpenConverter.openDataException(OpenConverter.java:1411)
	at com.sun.jmx.mbeanserver.OpenConverter.toConverter(OpenConverter.java:264)
	at com.sun.jmx.mbeanserver.ConvertingMethod.<init>(ConvertingMethod.java:184)
	at com.sun.jmx.mbeanserver.ConvertingMethod.from(ConvertingMethod.java:27)
	... 18 more
Caused by: javax.management.openmbean.OpenDataException: Cannot convert type: class org.apache.hadoop.hbase.ServerLoad
	at com.sun.jmx.mbeanserver.OpenConverter.openDataException(OpenConverter.java:1411)
	at com.sun.jmx.mbeanserver.OpenConverter.toConverter(OpenConverter.java:264)
	at com.sun.jmx.mbeanserver.OpenConverter.makeTabularConverter(OpenConverter.java:360)
	at com.sun.jmx.mbeanserver.OpenConverter.makeParameterizedConverter(OpenConverter.java:402)
	at com.sun.jmx.mbeanserver.OpenConverter.makeConverter(OpenConverter.java:296)
	at com.sun.jmx.mbeanserver.OpenConverter.toConverter(OpenConverter.java:262)
	... 20 more
Caused by: javax.management.openmbean.OpenDataException: Cannot convert type: java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$Coprocessor>
	at com.sun.jmx.mbeanserver.OpenConverter.openDataException(OpenConverter.java:1411)
	at com.sun.jmx.mbeanserver.OpenConverter.toConverter(OpenConverter.java:264)
	at com.sun.jmx.mbeanserver.OpenConverter.makeCompositeConverter(OpenConverter.java:467)
	at com.sun.jmx.mbeanserver.OpenConverter.makeConverter(OpenConverter.java:293)
	at com.sun.jmx.mbeanserver.OpenConverter.toConverter(OpenConverter.java:262)
	... 24 more
Caused by: javax.management.openmbean.OpenDataException: Cannot convert type: class org.apache.hadoop.hbase.protobuf.generated.HBaseProtos$Coprocessor
	at com.sun.jmx.mbeanserver.OpenConverter.openDataException(OpenConverter.java:1411)
	at com.sun.jmx.mbeanserver.OpenConverter.toConverter(OpenConverter.java:264)
	at com.sun.jmx.mbeanserver.OpenConverter.makeArrayOrCollectionConverter(OpenConverter.java:315)
	at com.sun.jmx.mbeanserver.OpenConverter.makeParameterizedConverter(OpenConverter.java:393)
	at com.sun.jmx.mbeanserver.OpenConverter.makeConverter(OpenConverter.java:296)
	at com.sun.jmx.mbeanserver.OpenConverter.toConverter(OpenConverter.java:262)
	... 27 more
Caused by: javax.management.openmbean.OpenDataException: Cannot convert type: java.util.Map<com.google.protobuf.Descriptors$FieldDescriptor, java.lang.Object>
	at com.sun.jmx.mbeanserver.OpenConverter.openDataException(OpenConverter.java:1411)
	at com.sun.jmx.mbeanserver.OpenConverter.toConverter(OpenConverter.java:264)
	at com.sun.jmx.mbeanserver.OpenConverter.makeCompositeConverter(OpenConverter.java:467)
	at com.sun.jmx.mbeanserver.OpenConverter.makeConverter(OpenConverter.java:293)
	at com.sun.jmx.mbeanserver.OpenConverter.toConverter(OpenConverter.java:262)
	... 31 more
Caused by: javax.management.openmbean.OpenDataException: Cannot convert type: class com.google.protobuf.Descriptors$FieldDescriptor
	at com.sun.jmx.mbeanserver.OpenConverter.openDataException(OpenConverter.java:1411)
	at com.sun.jmx.mbeanserver.OpenConverter.toConverter(OpenConverter.java:264)
	at com.sun.jmx.mbeanserver.OpenConverter.makeTabularConverter(OpenConverter.java:359)
	at com.sun.jmx.mbeanserver.OpenConverter.makeParameterizedConverter(OpenConverter.java:402)
	at com.sun.jmx.mbeanserver.OpenConverter.makeConverter(OpenConverter.java:296)
	at com.sun.jmx.mbeanserver.OpenConverter.toConverter(OpenConverter.java:262)
	... 34 more
Caused by: javax.management.openmbean.OpenDataException: Cannot convert type: class com.google.protobuf.Descriptors$Descriptor
	at com.sun.jmx.mbeanserver.OpenConverter.openDataException(OpenConverter.java:1411)
	at com.sun.jmx.mbeanserver.OpenConverter.toConverter(OpenConverter.java:264)
	at com.sun.jmx.mbeanserver.OpenConverter.makeCompositeConverter(OpenConverter.java:467)
	at com.sun.jmx.mbeanserver.OpenConverter.makeConverter(OpenConverter.java:293)
	at com.sun.jmx.mbeanserver.OpenConverter.toConverter(OpenConverter.java:262)
	... 38 more
Caused by: javax.management.openmbean.OpenDataException: Recursive data structure, including com.google.protobuf.Descriptors$Descriptor
	at com.sun.jmx.mbeanserver.OpenConverter.toConverter(OpenConverter.java:250)
	at com.sun.jmx.mbeanserver.OpenConverter.makeCompositeConverter(OpenConverter.java:467)
	at com.sun.jmx.mbeanserver.OpenConverter.makeConverter(OpenConverter.java:293)
	at com.sun.jmx.mbeanserver.OpenConverter.toConverter(OpenConverter.java:262)
	... 41 more
{code}

On the 'Recursive data structure', jmx open mbean is really helpful suggesting that you just need to rewrite the recursive data structure.

I'll have a go at this later.... tracking something else at mo."
HBASE-7092	"One instance of ""java.util.concurrent.ConcurrentHashMap"" loaded by ""<system class loader>"" occupies 3,972,154,848 (92.88%) bytes. The instance is referenced by org.apache.hadoop.hbase.regionserver.HRegionServer @ 0x7038d3798 , loaded by ""sun.misc.Launcher$AppClassLoader @ 0x703994668"". The memory is accumulated in one instance of ""java.util.concurrent.ConcurrentHashMap$Segment[]"" loaded by ""<system class loader>"".

Keywords
sun.misc.Launcher$AppClassLoader @ 0x703994668
java.util.concurrent.ConcurrentHashMap
java.util.concurrent.ConcurrentHashMap$Segment[]"
HBASE-6376	"I noticed that commands like ""bin/hbase shell"" doesn't work. The exception trace is:
{noformat}
bin/hbase shell
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/jruby/Main
Caused by: java.lang.ClassNotFoundException: org.jruby.Main
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
{noformat}
This is a trunk build (mvn package -DskipTests=true) and then I am trying to run the bin/hbase command from the root directory. (Am I missing something?)"
HBASE-6521	"This jira is to track a solution/patch to the mailing list thread titled ""Handling protocol versions"" - http://search-hadoop.com/m/6k7GUM028E/v=threaded."
HBASE-7433	"when i use the client of kettler insert data into hbase table by put operator,it is no problem when the table only has one region,but the data is also too much,so i want to load balancing,so i created many region when i created the table,this time problem is producted,exception is follow:

 Problem inserting row into HBase: Failed 1 action: servers with issues: slave2.hadoop:60020, 
 at org.pentaho.di.trans.steps.hbaseoutput.HBaseOutput.processRow(HBaseOutput.java:316)
 at org.pentaho.di.trans.step.RunThread.run(RunThread.java:50)
 at java.lang.Thread.run(Unknown Source)
Caused by: org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: servers with issues: slave2.hadoop:60020, 
at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.processBatchCallback(HConnectionManager.java:1601)
org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.processBatch(HConnectionManager.java:1377)
at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:916)
at org.apache.hadoop.hbase.client.HTable.doPut(HTable.java:772)
at org.apache.hadoop.hbase.client.HTable.put(HTable.java:747)
at org.pentaho.hbase.shim.common.CommonHBaseConnection.executeTargetTablePut(CommonHBaseConnection.java:732)
at org.pentaho.di.trans.steps.hbaseoutput.HBaseOutput.processRow(HBaseOutput.java:307)

we know the client find the rowkey belong to the region of the table,but can't connection the regionserver.but the regionserver is ok,but not running gc
so i can't understand the problem,please help me ,thanks
"
HBASE-6310	"We're still working the on the root cause here, but after the leap second armageddon we had a hard time getting our 0.94 cluster back up. This is what we saw in the logs until the master died by itself:

{noformat}
2012-07-01 23:01:52,149 DEBUG
org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation:
locateRegionInMeta parentTable=-ROOT-,
metaLocation={region=-ROOT-,,0.70236052, hostname=sfor3s28,
port=10304}, attempt=16 of 100 failed; retrying after sleep of 32000
because: HRegionInfo was null or empty in -ROOT-,
row=keyvalues={.META.,,1259448304806/info:server/1341124914705/Put/vlen=14/ts=0,
.META.,,1259448304806/info:serverstartcode/1341124914705/Put/vlen=8/ts=0}
{noformat}

(it's strage that we retry this)

This was really misleading because I could see the regioninfo in a scan:

{noformat}
hbase(main):002:0> scan '-ROOT-'
ROW                                           COLUMN+CELL
 .META.,,1                                    column=info:regioninfo,
timestamp=1331755381142, value={NAME => '.META.,,1', STARTKEY => '',
ENDKEY => '', ENCODED => 1028785192,}
 .META.,,1                                    column=info:server,
timestamp=1341183448693, value=sfor3s40:10304
 .META.,,1
column=info:serverstartcode, timestamp=1341183448693,
value=1341183444689
 .META.,,1                                    column=info:v,
timestamp=1331755419291, value=\x00\x00
 .META.,,1259448304806                        column=info:server,
timestamp=1341124914705, value=sfor3s24:10304
 .META.,,1259448304806
column=info:serverstartcode, timestamp=1341124914705,
value=1341124455863
{noformat}

Except that the devil is in the details, "".META.,,1"" is not "".META.,,1259448304806"". Basically something writes to .META. by directly creating the row key without caring if the row is in the old format. I did a deleteall in the shell and it fixed the issue... until some time later it was stuck again because the edits reappeared (still not sure why). This time the PostOpenDeployTasksThread were stuck in the RS trying to update .META. but there was no logging (saw it with a jstack). I deleted the row again to make it work.

I'm marking this as a blocker against 0.94.2 since we're trying to get 0.94.1 out, but I wouldn't recommend upgrading to 0.94 if your cluster was created before 0.89"
HBASE-5513	"Used Cloudera Manager 3.7.3 to install Cloudera CDH3U3 on a cluster of EC2 instances. After installation, HBase master cannot start. Both HDFS NN and DN have started successfully.

The HBase log shows 

PM 	INFO 	org.apache.hadoop.hbase.metrics 	

MetricsString added: url

Mar 2, 4:41:21 PM 	INFO 	org.apache.hadoop.hbase.metrics 	

MetricsString added: version

Mar 2, 4:41:21 PM 	INFO 	org.apache.hadoop.hbase.metrics 	

new MBeanInfo

Mar 2, 4:41:21 PM 	INFO 	org.apache.hadoop.hbase.metrics 	

new MBeanInfo

Mar 2, 4:41:21 PM 	INFO 	org.apache.hadoop.hbase.master.metrics.MasterMetrics 	

Initialized

Mar 2, 4:41:21 PM 	INFO 	org.apache.hadoop.hbase.master.ActiveMasterManager 	

Master=epoch-node-101:60000

Mar 2, 4:41:22 PM 	WARN 	org.apache.hadoop.hdfs.DFSClient 	

DataStreamer Exception: org.apache.hadoop.ipc.RemoteException: java.io.IOException: File /hbase/hbase.version could only be replicated to 0 nodes, instead of 1
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1520)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:665)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:557)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1434)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1430)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1157)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1428)

	at org.apache.hadoop.ipc.Client.call(Client.java:1107)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:226)
	at $Proxy6.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
	at $Proxy6.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.locateFollowingBlock(DFSClient.java:3553)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:3421)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2100(DFSClient.java:2627)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2822)

Mar 2, 4:41:22 PM 	WARN 	org.apache.hadoop.hdfs.DFSClient 	

Error Recovery for block null bad datanode[0] nodes == null

Mar 2, 4:41:22 PM 	WARN 	org.apache.hadoop.hdfs.DFSClient 	

Could not get block locations. Source file ""/hbase/hbase.version"" - Aborting...

Mar 2, 4:41:22 PM 	WARN 	org.apache.hadoop.hbase.util.FSUtils 	

Unable to create version file at hdfs://epoch-node-101:8020/hbase, retrying: java.io.IOException: File /hbase/hbase.version could only be replicated to 0 nodes, instead of 1
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1520)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:665)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:557)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1434)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1430)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1157)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1428)

"
HBASE-3804	"After putting some significant load on the sever (load around of 8 for approximatively 50 minutes) the following starts showing in the logs and then hbase is completely stuck: won't recover without a restart (I copy a large chunk of the master logs before the problem in case it helps; the bottom part about ROOT and META repeats forever):

9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305484717
2011-04-20 06:19:51,228 INFO org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter: Using syncFs -- HDFS-200
2011-04-20 06:19:51,228 DEBUG org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Creating writer path=hdfs://hqsnosq
l01:9000/hbase/Property/69e1f895ca9d6824b409015a9f9c03a3/recovered.edits/0000000000000098012 region=69e1f895ca9d6824b409
015a9f9c03a3
2011-04-20 06:19:51,539 DEBUG org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Pushed=38 entries from hdfs://HQSNO
SQL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305484717
2011-04-20 06:19:51,540 DEBUG org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Splitting hlog 42 of 43: hdfs://HQS
NOSQL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305492251, length=67522024
2011-04-20 06:19:51,540 INFO org.apache.hadoop.hbase.util.FSUtils: Recovering file hdfs://HQSNOSQL01:9000/hbase/.logs/HQ
SNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305492251
2011-04-20 06:19:52,543 INFO org.apache.hadoop.hbase.util.FSUtils: Finished lease recover attempt for hdfs://HQSNOSQL01:
9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305492251
2011-04-20 06:19:53,336 DEBUG org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Pushed=40 entries from hdfs://HQSNO
SQL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305492251
2011-04-20 06:19:54,894 DEBUG org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Splitting hlog 43 of 43: hdfs://HQS
NOSQL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305500805, length=75765855
2011-04-20 06:19:54,906 INFO org.apache.hadoop.hbase.util.FSUtils: Recovering file hdfs://HQSNOSQL01:9000/hbase/.logs/HQ
SNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305500805
2011-04-20 06:19:55,108 INFO org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter: Using syncFs -- HDFS-200
2011-04-20 06:19:55,108 DEBUG org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Creating writer path=hdfs://hqsnosq
l01:9000/hbase/Property/a21bf162e9ff2915ff036f486919c9b5/recovered.edits/0000000000000098062 region=a21bf162e9ff2915ff03
6f486919c9b5
2011-04-20 06:19:55,908 INFO org.apache.hadoop.hbase.util.FSUtils: Finished lease recover attempt for hdfs://HQSNOSQL01:
9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305500805
2011-04-20 06:19:56,842 DEBUG org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Pushed=43 entries from hdfs://HQSNO
SQL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305500805
2011-04-20 06:19:56,859 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305143661 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305143661
2011-04-20 06:19:56,863 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305152287 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305152287
2011-04-20 06:19:56,867 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305160979 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305160979
2011-04-20 06:19:56,871 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305170430 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305170430
2011-04-20 06:19:56,875 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305178662 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305178662
2011-04-20 06:19:56,879 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305188551 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305188551
2011-04-20 06:19:56,883 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305196703 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305196703
2011-04-20 06:19:56,887 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305205039 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305205039
2011-04-20 06:19:56,891 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305213412 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305213412
2011-04-20 06:19:56,895 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305220665 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305220665
2011-04-20 06:19:56,899 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305231697 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305231697
2011-04-20 06:19:56,913 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305239174 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305239174
2011-04-20 06:19:56,917 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305248013 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305248013
2011-04-20 06:19:56,921 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305256411 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305256411
2011-04-20 06:19:56,925 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305265663 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305265663
2011-04-20 06:19:56,929 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305274667 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305274667
2011-04-20 06:19:56,933 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305283140 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305283140
2011-04-20 06:19:56,937 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305291307 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305291307
2011-04-20 06:19:56,941 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305300632 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305300632
2011-04-20 06:19:56,945 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305309439 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305309439
2011-04-20 06:19:56,949 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305316405 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305316405
2011-04-20 06:19:56,953 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305322730 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305322730
2011-04-20 06:19:56,957 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305330453 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305330453
2011-04-20 06:19:56,961 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305337020 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305337020
2011-04-20 06:19:56,965 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305345309 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305345309
2011-04-20 06:19:56,969 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305352766 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305352766
2011-04-20 06:19:56,972 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305362335 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305362335
2011-04-20 06:19:56,976 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305370474 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305370474
2011-04-20 06:19:56,980 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305379360 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305379360
2011-04-20 06:19:56,984 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305387434 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305387434
2011-04-20 06:19:56,988 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305396557 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305396557
2011-04-20 06:19:56,992 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305403848 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305403848
2011-04-20 06:19:56,996 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305412950 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305412950
2011-04-20 06:19:57,000 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305420349 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305420349
2011-04-20 06:19:57,004 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305429358 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305429358
2011-04-20 06:19:57,008 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305437383 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305437383
2011-04-20 06:19:57,012 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305446715 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305446715
2011-04-20 06:19:57,016 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305460169 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305460169
2011-04-20 06:19:57,678 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305467377 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305467377
2011-04-20 06:19:57,685 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305476873 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305476873
2011-04-20 06:19:57,689 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305484717 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305484717
2011-04-20 06:19:57,693 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305492251 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305492251
2011-04-20 06:19:57,697 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Archived processed log hdfs://HQSNOS
QL01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146/HQSNOSQL01%3A60020.1303305500805 to hdfs://hqsnosql01:9000/hbase/.o
ldlogs/HQSNOSQL01%3A60020.1303305500805
2011-04-20 06:19:57,701 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Waiting for split writer threads to
finish
2011-04-20 06:19:57,874 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Split writers finished
2011-04-20 06:19:57,881 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs://hqsnosql01:9000/h
base/Property/36d89fc46915d8db56bad45c64a313a3/recovered.edits/0000000000000096979 (wrote 134 edits in 6107ms)
2011-04-20 06:19:57,885 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs://hqsnosql01:9000/h
base/Property/4499b405bae1047884256f39b7559376/recovered.edits/0000000000000096571 (wrote 30 edits in 1338ms)
2011-04-20 06:19:57,889 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs://hqsnosql01:9000/h
base/Property/465fb6b0a1f2d5306c7bb87145a32be2/recovered.edits/0000000000000096568 (wrote 22 edits in 909ms)
2011-04-20 06:19:57,893 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs://hqsnosql01:9000/h
base/Property/4bc3c0bd3a824fb6c7fabbd332ef1009/recovered.edits/0000000000000097151 (wrote 85 edits in 3982ms)
2011-04-20 06:19:57,897 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs://hqsnosql01:9000/h
base/Property/4bfff6ceb59cc4b6098eea42b49ee9ed/recovered.edits/0000000000000097336 (wrote 117 edits in 4810ms)
2011-04-20 06:19:57,901 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs://hqsnosql01:9000/h
base/Property/52dee69cbe0890b54aee56bda54577c9/recovered.edits/0000000000000096567 (wrote 63 edits in 4445ms)
2011-04-20 06:19:57,905 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs://hqsnosql01:9000/h
base/Property/539e4afe8064897395d4dc891605e702/recovered.edits/0000000000000097127 (wrote 53 edits in 3433ms)
2011-04-20 06:19:57,909 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs://hqsnosql01:9000/h
base/Property/5728cb0bb67c7c3d8701fc0679db97b9/recovered.edits/0000000000000097473 (wrote 134 edits in 6623ms)
2011-04-20 06:19:57,913 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs://hqsnosql01:9000/h
base/Property/5f92d0c1defc108140df27322257584d/recovered.edits/0000000000000096672 (wrote 77 edits in 7211ms)
2011-04-20 06:19:57,917 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs://hqsnosql01:9000/h
base/Property/69e1f895ca9d6824b409015a9f9c03a3/recovered.edits/0000000000000098012 (wrote 10 edits in 306ms)
2011-04-20 06:19:57,921 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs://hqsnosql01:9000/h
base/Property/70746a106ab152f290264d0b2b0f773c/recovered.edits/0000000000000096722 (wrote 117 edits in 7245ms)
2011-04-20 06:19:57,925 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs://hqsnosql01:9000/h
base/Property/906e3283419ae67052c34e31cc40ff05/recovered.edits/0000000000000097951 (wrote 31 edits in 2389ms)
2011-04-20 06:19:57,929 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs://hqsnosql01:9000/h
base/Property/94112f821b90271f9d693d5505d5eebf/recovered.edits/0000000000000097692 (wrote 71 edits in 4771ms)
2011-04-20 06:19:57,933 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs://hqsnosql01:9000/h
base/Property/9f519dfd7370990d017e7f6d187968d3/recovered.edits/0000000000000096697 (wrote 77 edits in 5551ms)
2011-04-20 06:19:57,937 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs://hqsnosql01:9000/h
base/Property/9f60e52d384d0fd9dfc2a2f3be42ac43/recovered.edits/0000000000000097877 (wrote 24 edits in 840ms)
2011-04-20 06:19:58,054 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs://hqsnosql01:9000/h
base/Property/a21bf162e9ff2915ff036f486919c9b5/recovered.edits/0000000000000098062 (wrote 8 edits in 1860ms)
2011-04-20 06:19:58,061 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs://hqsnosql01:9000/h
base/Property/abc26969d71b22de80c7e889cc9a6c12/recovered.edits/0000000000000096576 (wrote 5 edits in 281ms)
2011-04-20 06:19:58,065 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs://hqsnosql01:9000/h
base/Property/b5d7c2bd8383ea8871890f40b9827dcf/recovered.edits/0000000000000097554 (wrote 125 edits in 5240ms)
2011-04-20 06:19:58,073 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs://hqsnosql01:9000/h
base/Property/bf06b369cf84e5bccdd4e845636a2af6/recovered.edits/0000000000000096839 (wrote 1 edits in 17ms)
2011-04-20 06:19:58,077 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs://hqsnosql01:9000/h
base/Property/cf46d330cdaa1120c8e1a5fcf1899b0f/recovered.edits/0000000000000097442 (wrote 1 edits in 850ms)
2011-04-20 06:19:58,081 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs://hqsnosql01:9000/h
base/Property/d850f84bfdfa28c63934a0be5a41257e/recovered.edits/0000000000000096572 (wrote 53 edits in 2467ms)
2011-04-20 06:19:58,085 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs://hqsnosql01:9000/h
base/Property/d9b94307130a7096032b44b3aa18d3ea/recovered.edits/0000000000000096795 (wrote 155 edits in 10534ms)
2011-04-20 06:19:58,088 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs://hqsnosql01:9000/h
base/Property/da6540fdaae565a6d04cd78a145240dd/recovered.edits/0000000000000097580 (wrote 53 edits in 2992ms)
2011-04-20 06:19:58,092 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs://hqsnosql01:9000/h
base/Property/df02c5d923c466c5b89337fecee920cf/recovered.edits/0000000000000096924 (wrote 1 edits in 25ms)
2011-04-20 06:19:58,096 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs://hqsnosql01:9000/h
base/Property/e90c1c020e3148756a57d977133e31fc/recovered.edits/0000000000000096569 (wrote 96 edits in 3862ms)
2011-04-20 06:19:58,097 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: hlog file splitting completed in 866
02 ms for hdfs://hqsnosql01:9000/hbase/.logs/HQSNOSQL01,60020,1303302276146
2011-04-20 06:19:58,102 INFO org.apache.hadoop.hbase.catalog.RootLocationEditor: Unsetting ROOT region location in ZooKe
eper
2011-04-20 06:19:58,113 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x12f72de7aee0000 Creating (or up
dating) unassigned node for 70236052 with OFFLINE state
2011-04-20 06:19:58,133 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x12f72de7aee0000 Creating (or up
dating) unassigned node for 1028785192 with OFFLINE state
2011-04-20 06:19:58,139 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=M_ZK_REGION_OFFLINE,
 server=HQSNOSQL01:60000, region=1028785192/.META.
2011-04-20 06:20:36,363 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  .META.,
,1.1028785192 state=OFFLINE, ts=1303305598133
2011-04-20 06:20:36,364 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been OFFLINE for too long, rea
ssigning .META.,,1.1028785192 to a random server
2011-04-20 06:20:36,364 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  -ROOT-,
,0.70236052 state=OFFLINE, ts=1303305598113
2011-04-20 06:20:36,364 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been OFFLINE for too long, rea
ssigning -ROOT-,,0.70236052 to a random server
2011-04-20 06:20:36,364 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=.META.,,1.102878519
2 state=OFFLINE, ts=1303305598133
2011-04-20 06:20:36,364 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=-ROOT-,,0.70236052
state=OFFLINE, ts=1303305598113
2011-04-20 06:21:06,374 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  .META.,
,1.1028785192 state=OFFLINE, ts=1303305636364
2011-04-20 06:21:06,374 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been OFFLINE for too long, rea
ssigning .META.,,1.1028785192 to a random server
2011-04-20 06:21:06,374 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  -ROOT-,
,0.70236052 state=OFFLINE, ts=1303305636364
2011-04-20 06:21:06,374 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been OFFLINE for too long, rea
ssigning -ROOT-,,0.70236052 to a random server
2011-04-20 06:21:06,374 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=.META.,,1.102878519
2 state=OFFLINE, ts=1303305636364
2011-04-20 06:21:06,374 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=-ROOT-,,0.70236052
state=OFFLINE, ts=1303305636364
2011-04-20 06:21:36,385 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  .META.,
,1.1028785192 state=OFFLINE, ts=1303305666374
2011-04-20 06:21:36,385 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been OFFLINE for too long, rea
ssigning .META.,,1.1028785192 to a random server
2011-04-20 06:21:36,385 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  -ROOT-, 
,0.70236052 state=OFFLINE, ts=1303305666374
2011-04-20 06:21:36,385 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been OFFLINE for too long, rea
ssigning -ROOT-,,0.70236052 to a random server
2011-04-20 06:21:36,385 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=.META.,,1.102878519
2 state=OFFLINE, ts=1303305666374
2011-04-20 06:21:36,385 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=-ROOT-,,0.70236052
state=OFFLINE, ts=1303305666374
2011-04-20 06:22:06,393 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  .META.,
,1.1028785192 state=OFFLINE, ts=1303305696385
2011-04-20 06:22:06,393 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been OFFLINE for too long, rea
ssigning .META.,,1.1028785192 to a random server
2011-04-20 06:22:06,393 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  -ROOT-,
,0.70236052 state=OFFLINE, ts=1303305696385
2011-04-20 06:22:06,393 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been OFFLINE for too long, rea
ssigning -ROOT-,,0.70236052 to a random server
2011-04-20 06:22:06,393 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=.META.,,1.102878519
2 state=OFFLINE, ts=1303305696385
2011-04-20 06:22:06,393 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=-ROOT-,,0.70236052
state=OFFLINE, ts=1303305696385
2011-04-20 06:22:36,402 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  .META.,
,1.1028785192 state=OFFLINE, ts=1303305726393
2011-04-20 06:22:36,402 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been OFFLINE for too long, rea
ssigning .META.,,1.1028785192 to a random server
2011-04-20 06:22:36,402 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  -ROOT-,
,0.70236052 state=OFFLINE, ts=1303305726394
2011-04-20 06:22:36,402 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been OFFLINE for too long, rea
ssigning -ROOT-,,0.70236052 to a random server
2011-04-20 06:22:36,402 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=.META.,,1.102878519
2 state=OFFLINE, ts=1303305726393
2011-04-20 06:22:36,402 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=-ROOT-,,0.70236052
state=OFFLINE, ts=1303305726394
2011-04-20 06:23:06,411 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  .META.,
,1.1028785192 state=OFFLINE, ts=1303305756402
2011-04-20 06:23:06,411 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been OFFLINE for too long, rea
ssigning .META.,,1.1028785192 to a random server
2011-04-20 06:23:06,411 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  -ROOT-,
,0.70236052 state=OFFLINE, ts=1303305756402
2011-04-20 06:23:06,411 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been OFFLINE for too long, rea
ssigning -ROOT-,,0.70236052 to a random server
2011-04-20 06:23:06,412 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=.META.,,1.102878519
2 state=OFFLINE, ts=1303305756402
2011-04-20 06:23:06,412 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=-ROOT-,,0.70236052
state=OFFLINE, ts=1303305756402
2011-04-20 06:23:36,420 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  .META.,
,1.1028785192 state=OFFLINE, ts=1303305786412
2011-04-20 06:23:36,420 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been OFFLINE for too long, rea
ssigning .META.,,1.1028785192 to a random server
2011-04-20 06:23:36,420 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  -ROOT-,
,0.70236052 state=OFFLINE, ts=1303305786412
2011-04-20 06:23:36,420 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been OFFLINE for too long, rea
ssigning -ROOT-,,0.70236052 to a random server
2011-04-20 06:23:36,420 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=.META.,,1.102878519
2 state=OFFLINE, ts=1303305786412
2011-04-20 06:23:36,420 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=-ROOT-,,0.70236052
state=OFFLINE, ts=1303305786412
2011-04-20 06:24:06,429 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  .META.,
,1.1028785192 state=OFFLINE, ts=1303305816420
2011-04-20 06:24:06,430 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been OFFLINE for too long, rea
ssigning .META.,,1.1028785192 to a random server
2011-04-20 06:24:06,430 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  -ROOT-,
,0.70236052 state=OFFLINE, ts=1303305816420
2011-04-20 06:24:06,430 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been OFFLINE for too long, rea
ssigning -ROOT-,,0.70236052 to a random server
2011-04-20 06:24:06,430 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=.META.,,1.102878519"
HBASE-1033	"If there is a row with more timestamps than versions (which can happen if only some cells are newer than others in other columns), get and getRow fail to find columns with newer timestamps."
HBASE-1222	"The scripts in hbase/bin (e.g. start-hbase.sh) try to start Zookeeper. Why? There is no Zookeeper support in 0.19.x
"
HBASE-922	"Currently it is possible to enable a bloom filter after a table is created and data has been stored in it by disabling the table and modifying the column.

While this correctly sets the attribute in the column descriptor, it does not create bloom filters for existing data so that on the first compaction, an NPE will be thrown because the HStore expects each HStoreFile to have a bloom filter."
HBASE-863	"Whenever I make any request to the rest server, my http call hangs forever.

In the stacktrace, it looks like hbase waits on a lock it holds:

""SocketListener0-1"" prio=10 tid=0x00007fc88c1e2000 nid=0x4860 in Object.wait() [0x0000000041bdd000..0x0000000041bdea00]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x00007fc8ef59d700> (a org.apache.hadoop.ipc.Client$Call)
        at org.apache.hadoop.ipc.Client.call(Client.java:552)
        - locked <0x00007fc8ef59d700> (a org.apache.hadoop.ipc.Client$Call)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Invoker.invoke(HbaseRPC.java:230)
        at $Proxy1.getProtocolVersion(Unknown Source)
        at org.apache.hadoop.hbase.ipc.HbaseRPC.getProxy(HbaseRPC.java:340)
        at org.apache.hadoop.hbase.ipc.HbaseRPC.getProxy(HbaseRPC.java:327)
        at org.apache.hadoop.hbase.ipc.HbaseRPC.getProxy(HbaseRPC.java:364)
        at org.apache.hadoop.hbase.ipc.HbaseRPC.waitForProxy(HbaseRPC.java:302)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.getHRegionConnection(HConnectionManager.java:771)
        - locked <0x00007fc8ef067618> (a java.util.concurrent.ConcurrentHashMap)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegionInMeta(HConnectionManager.java:518)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:478)
        - locked <0x00007fc8ef067600> (a java.lang.Integer)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:438)
        at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:127)
        at org.apache.hadoop.hbase.rest.GenericHandler.getTable(GenericHandler.java:260)
        at org.apache.hadoop.hbase.rest.TableHandler.doGet(TableHandler.java:74)
        at org.apache.hadoop.hbase.rest.Dispatcher.doGet(Dispatcher.java:105)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:689)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)
        at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)
        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)
        at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)
        at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)
        at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)
        at org.mortbay.http.HttpServer.service(HttpServer.java:954)
        at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)
        at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)
        at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)
        at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)
        at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)
        at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)
"
HBASE-542	"As I was investigating HBASE-532, I became suspicious that scanners suffer from problems similar to those that getFull and getClosest suffered before. Namely, when there are multiple stores per columnfamily for a region (memcache and 1+ stofiles, empty memcache and 2+ storefiles), incorrect answers are produced. I will attach a test case that showcases this problem."
HBASE-560	"After HBASE-547 Master and region server will not start because they cannot find the jsp classes:
{code}
2008-04-03 18:38:49,051 ERROR [HMaster] hbase.HMaster(1216): Failed startup
java.io.IOException: Problem starting http server
        at org.apache.hadoop.hbase.util.InfoServer.start(InfoServer.java:227)
        at org.apache.hadoop.hbase.HMaster.startServiceThreads(HMaster.java:1201)
        at org.apache.hadoop.hbase.HMaster.run(HMaster.java:1063)
Caused by: org.mortbay.util.MultiException[java.lang.ClassNotFoundException: org
.apache.hadoop.hbase.generated.master.hql_jsp, java.lang.ClassNotFoundException:
 org.apache.hadoop.hbase.generated.master.master_jsp]
        at org.mortbay.http.HttpServer.doStart(HttpServer.java:731)
        at org.mortbay.util.Container.start(Container.java:72)
        at org.apache.hadoop.hbase.util.InfoServer.start(InfoServer.java:205)
        ... 2 more
{code}"
HBASE-21234	"Getting following exception during ChoreService runs in HBase Master logs. As a result we are accumulating a lot of data in archive folder as archive is not getting reclaimed.聽
{code:java}
Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos at org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest.internalGetFieldAccessorTable(SnapshotProtos.java:1190
{code}
聽Complete stack-trace
{code:java}
2018-09-26 10:15:06,188 ERROR [master01,16000,1536315941769_ChoreService_3] snapshot.SnapshotHFileCleaner: Exception while checking if files were valid, keeping them just in case. java.io.IOException: ExecutionException at org.apache.hadoop.hbase.snapshot.SnapshotManifestV2.loadRegionManifests(SnapshotManifestV2.java:161) at org.apache.hadoop.hbase.snapshot.SnapshotManifest.load(SnapshotManifest.java:364) at org.apache.hadoop.hbase.snapshot.SnapshotManifest.open(SnapshotManifest.java:130) at org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil.visitTableStoreFiles(SnapshotReferenceUtil.java:128) at org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil.getHFileNames(SnapshotReferenceUtil.java:357) at org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil.getHFileNames(SnapshotReferenceUtil.java:340) at org.apache.hadoop.hbase.master.snapshot.SnapshotHFileCleaner$1.filesUnderSnapshot(SnapshotHFileCleaner.java:88) at org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache.getSnapshotsInProgress(SnapshotFileCache.java:303) at org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache.getUnreferencedFiles(SnapshotFileCache.java:194) at org.apache.hadoop.hbase.master.snapshot.SnapshotHFileCleaner.getDeletableFiles(SnapshotHFileCleaner.java:63) at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteFiles(CleanerChore.java:287) at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteEntries(CleanerChore.java:211) at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteDirectory(CleanerChore.java:234) at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteEntries(CleanerChore.java:206) at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteDirectory(CleanerChore.java:234) at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteEntries(CleanerChore.java:206) at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteDirectory(CleanerChore.java:234) at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteEntries(CleanerChore.java:206) at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteDirectory(CleanerChore.java:234) at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteEntries(CleanerChore.java:206) at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteDirectory(CleanerChore.java:234) at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteEntries(CleanerChore.java:206) at org.apache.hadoop.hbase.master.cleaner.CleanerChore.chore(CleanerChore.java:130) at org.apache.hadoop.hbase.ScheduledChore.run(ScheduledChore.java:185) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos at org.apache.hadoop.hbase.protobuf.generated.SnapshotProtos$SnapshotRegionManifest.internalGetFieldAccessorTable(SnapshotProtos.java:1190) at com.google.protobuf.GeneratedMessage.getDescriptorForType(GeneratedMessage.java:98) at com.google.protobuf.AbstractMessage$Builder.findMissingFields(AbstractMessage.java:789) at com.google.protobuf.AbstractMessage$Builder.findMissingFields(AbstractMessage.java:780) at com.google.protobuf.AbstractMessage$Builder.newUninitializedMessageException(AbstractMessage.java:770) at com.google.protobuf.AbstractMessage.newUninitializedMessageException(AbstractMessage.java:237) at com.google.protobuf.AbstractParser.newUninitializedMessageException(AbstractParser.java:57) at com.google.protobuf.AbstractParser.checkMessageInitialized(AbstractParser.java:71) at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:217)
{code}"
HBASE-18006	"I have been reading the code for the new async scan paths excessively, and noticed that there is a problem in the retrying layer for openScan RPCs. 

In AsyncClientScanner#callOpenScanner() we are doing a open scan RPC. The retrying logic comes from using the single rpc retrying caller in openScanner(). However, we have the logic for failing the scanner if any of the RPC calls here: 
{code}
      stub.scan(controller, request, resp -> {
        if (controller.failed()) {
          future.completeExceptionally(controller.getFailed());
          return;
        }
        future.complete(new OpenScannerResponse(loc, isRegionServerRemote, stub, controller, resp));
      });
{code}

So, if the open scan gets an UnknownScannerException or something, instead of retrying, it just fails the whole scan. 

[~Apache9] FYI. "
HBASE-20341	There are even metrics from HBASE-12220 that expose counts. Talk them up and hedged reads in refguide.
HBASE-19312	See the discussion in HBASE-19266.
HBASE-19443	"When the loader is loading data to HBase, without any forewarning that client gets stuck and prints the below message continuously:

[ERROR] AsyncProcess[hconnection-0x71f5455a-shared--pool20-t40846] Cannot get replica 0 location for {""totalColumns"":20,""families"":{""v"":[{""timestamp"":1512584999999,""tag"":[],""qualifier"":""0"",""vlen"":4},{""timestamp"":1512584999999,""tag"":[],""qualifier"":""uid"",""vlen"":8},{""timestamp"":1512584999999,""tag"":[],""qualifier"":""1"",""vlen"":8},{""timestamp"":1512584999999,""tag"":[],""qualifier"":""2"",""vlen"":11}]},""row"":""C\\x99L[5\\x80\\x00\\x00""}

At this time no other loaders are impacted. Restart of the loader seems to solve the issue until next occurence.


Upon doing jstack at this time on the loader, it is observed that process is stuck on a object monitor inside 'AsyncProcess.java' class.
"
HBASE-20534	We need better user facing text to explain that they need to set hbase.localcluster.assign.random.ports to true. See HBASE-20224.
HBASE-19706	Deleted cells are always hiding the other cells even if the scan ran with time range having no delete marker.
HBASE-18246	"Before HBASE-18036, SSH would use round-robin to re-distribute regions during processing.  Round-robin assignment would loss data locality.  HBASE-18036 retains data locality if the dead region server has already restarted when the dead RS is processing.  

With Proc-V2 based AM, the change of HBASE-18036 in Apache HBASE 1.x releases is no longer possible.  We need to implement the same logic under Proc-V2 based AM."
HBASE-19409	"YARN ATSv2 leverages HBase as its data store. When ATSv2 is enabled, 
YARN NM will act as HBase clients to write data into HBase cluster.

Because YARN NM jvms already register jvmMetrics in the metrics system and 
no duplicate is allowed, when HBase client tries to register jvmMetrics again, NM will crash with the following exception.
{code}
ERROR org.apache.hadoop.yarn.server.nodemanager.NodeManager: Error starting NodeManager
org.apache.hadoop.service.ServiceStateException: java.io.IOException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:105)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:173)
        at org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorManager.serviceInit(TimelineCollectorManager.java:62)
        at org.apache.hadoop.yarn.server.timelineservice.collector.NodeTimelineCollectorManager.serviceInit(NodeTimelineCollectorManager.java:112)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
        at org.apache.hadoop.yarn.server.timelineservice.collector.PerNodeTimelineCollectorsAuxService.serviceInit(PerNodeTimelineCollectorsAuxService.java:87)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices.serviceInit(AuxServices.java:167)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
        at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:315)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
        at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:440)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:833)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:894)
Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:221)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:114)
        at org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineWriterImpl.serviceInit(HBaseTimelineWriterImpl.java:123)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
        ... 15 more
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:219)
        ... 18 more
Caused by: java.lang.RuntimeException: Could not create  interface org.apache.hadoop.hbase.zookeeper.MetricsZooKeeperSource Is the hadoop compatibility jar on the classpath?
        at org.apache.hadoop.hbase.CompatibilitySingletonFactory.getInstance(CompatibilitySingletonFactory.java:75)
        at org.apache.hadoop.hbase.zookeeper.MetricsZooKeeper.<init>(MetricsZooKeeper.java:38)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.<init>(RecoverableZooKeeper.java:130)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.connect(ZKUtil.java:137)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.<init>(ZooKeeperWatcher.java:134)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.<init>(ZooKeeperWatcher.java:108)
        at org.apache.hadoop.hbase.client.ZooKeeperKeepAliveConnection.<init>(ZooKeeperKeepAliveConnection.java:43)
        at org.apache.hadoop.hbase.client.ConnectionImplementation.getKeepAliveZooKeeperWatcher(ConnectionImplementation.java:1231)
        at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getClusterId(ZooKeeperRegistry.java:101)
        at org.apache.hadoop.hbase.client.ConnectionImplementation.retrieveClusterId(ConnectionImplementation.java:526)
        at org.apache.hadoop.hbase.client.ConnectionImplementation.<init>(ConnectionImplementation.java:288)
        ... 23 more
Caused by: java.util.ServiceConfigurationError: org.apache.hadoop.hbase.zookeeper.MetricsZooKeeperSource: Provider org.apache.hadoop.hbase.zookeeper.MetricsZooKeeperSourceImpl could not be instantiated
        at java.util.ServiceLoader.fail(ServiceLoader.java:232)
        at java.util.ServiceLoader.access$100(ServiceLoader.java:185)
        at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384)
        at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404)
        at java.util.ServiceLoader$1.next(ServiceLoader.java:480)
        at org.apache.hadoop.hbase.CompatibilitySingletonFactory.getInstance(CompatibilitySingletonFactory.java:59)
        ... 33 more
Caused by: org.apache.hadoop.metrics2.MetricsException: Metrics source JvmMetrics already exists!
        at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newSourceName(DefaultMetricsSystem.java:152)
        at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.sourceName(DefaultMetricsSystem.java:125)
        at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:229)
        at org.apache.hadoop.metrics2.source.JvmMetrics.create(JvmMetrics.java:111)
        at org.apache.hadoop.metrics2.source.JvmMetrics$Singleton.init(JvmMetrics.java:61)
        at org.apache.hadoop.metrics2.source.JvmMetrics.initSingleton(JvmMetrics.java:120)
        at org.apache.hadoop.hbase.metrics.BaseSourceImpl$DefaultMetricsSystemInitializer.init(BaseSourceImpl.java:52)
        at org.apache.hadoop.hbase.metrics.BaseSourceImpl.<init>(BaseSourceImpl.java:112)
        at org.apache.hadoop.hbase.zookeeper.MetricsZooKeeperSourceImpl.<init>(MetricsZooKeeperSourceImpl.java:56)
        at org.apache.hadoop.hbase.zookeeper.MetricsZooKeeperSourceImpl.<init>(MetricsZooKeeperSourceImpl.java:51)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at java.lang.Class.newInstance(Class.java:442)
        at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380)
        ... 36 more
{code}

"
HBASE-12757	"I excute mapreduce scan all table, sometimes map input value of rowkey is out of range on current Region (get from inputsplit ).
this mabey  lost data or get unused data.
ps. I want  to use ImportTSV translate table.....
eg. 
location=datanode11,start_row=D9CB114FD09A82A3_0000000000000000_m_43DAAA689D4AFC86 ,rowkey=D323E1D0A51E5185_0000000000000000_m_75686B8924108044 ,end_row=DB0C4FC44E6D80C1_0000000000000000_m_E956CC65322BA3E5"
HBASE-11156	"# hbase shell
2014-05-13 14:51:41,582 INFO  [main] Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
HBase Shell; enter 'help<RETURN>' for list of supported commands.
Type ""exit<RETURN>"" to leave the HBase Shell
Version 0.96.1.1-cdh5.0.0, rUnknown, Thu Mar 27 23:01:59 PDT 2014.

Not able to create table in Hbase. Please help
"
HBASE-18855	"We documented the workaround for folks using surefire needing to set a system property otherwise mini cluster won't start in HBASE-18849, but maybe we can do better.

Can we rewrite the SO? Or patch the java code to use the correct value instead of checking a system property for it? It should be ok to have a patch to hardcode this sine we're telling folks to use a static value anyway.

hbase-thirdparty already has mechanisms for patching the source libs (ref. protobuf) so the mechanics shouldn't be too complex."
HBASE-17556	"We noticed in our application, that sometimes when we interact with a table an operation will fail with an exception, an all operations that happen on the same region will also fail until the application is restarted.

It seems that when a merge or split happens on a region that is already in the clients cache, and the client is configured to retry operations, then there is no way for the client to detect this. In RpcRetryingCaller#callWithRetries if a call fails with RegionNotServingException then the cache will be cleared only if the retry parameter is equal to 1. This means the call will fail but the following calls will succeed.

RpcRetryingCaller#callWithoutRetries contains the comment ""It would be nice to clear the location cache here"". Additionally, the stale cache will cause this call to fail, even though the data is available.

See also HBASE-12534"
HBASE-11295	"Attached Files:

HRegionServer.java - instramented from 0.96.1.1-cdh5.0.0
HBaseLeaseTimeoutIT.java - reproducing JUnit 4 test
WaitFilter.java - Scan filter (extends FilterBase) that overrides filterRowKey() to sleep during invocation
SpliceFilter.proto - Protobuf defintiion for WaitFilter.java
OutOfOrderScann_InstramentedServer.log - instramented server log
Steps.txt - this note

Set up:

In HBaseLeaseTimeoutIT, create a scan, set the given filter (which sleeps in overridden filterRowKey() method) and set it on the scan, and scan the table.
This is done in test client_0x0_server_150000x10().

Here's what I'm seeing (see also attached log):

A new request comes into server (ID 1940798815214593802 - RpcServer.handler=96) and a RegionScanner is created for it, cached by ID, immediately looked up again and cached RegionScannerHolder's nextCallSeq incremeted (now at 1).
The RegionScan thread goes to sleep in WaitFilter#filterRowKey().

A short (variable) period later, another request comes into the server (ID 8946109289649235722 - RpcServer.handler=98) and the same series of events happen to this request.

At this point both RegionScanner threads are sleeping in WaitFilter.filterRowKey(). After another period, the client retries another scan request which thinks its next_call_seq is 0.  However, HRegionServer's cached RegionScannerHolder thinks the matching RegionScanner's nextCallSeq should be 1."
HBASE-17842	"The directions in the following section are not correct.

https://archive.cloudera.com/cdh5/cdh/5/hbase-1.2.0-cdh5.8.0/book.html#_integration_testing_with_an_hbase_mini_cluster

It is pointing to HBase 0.98.3 and `HBaseTestingUtility` can not be resolved in 1.2.0"
HBASE-17671	"We have a HBase Thrift2 server deployed on Windows, basically the physical view looks like:
QueryEngine <==> HBase Thrift2 <==> HBase cluster
Here QueryEngine is a C++ application, and HBase cluster is a about 50-nodes HBase cluster (CDH 5.3.3, namely Hbase version 0.98.6).

Our Thrift2 Java options looks like:
-server -Xms4096m -Xmx4096m -XX:MaxDirectMemorySize=8192m -XX:+HeapDumpOnOutOfMemoryError -XX:+UseG1GC -XX:+ParallelRefProcEnabled -XX:G1HeapRegionSize=4M -XX:InitiatingHeapOccupancyPercent=40 -XX:+PrintAdaptiveSizePolicy -XX:+PrintPromotionFailure -Dhbase.log.dir=d:\vhayu\thrift2\log -verbose:gc -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+PrintGCDetails -XX:PrintFLSStatistics=1 -Xloggc:log_gc.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=200M -Dhbase.log.file=hbase-thrift2.log  -Dhbase.home.dir=D:\vhayu\thrift2\hbase0.98 -Dhbase.id.str=root -Dlog4j.info -Dhbase.root.logger=INFO,DRFA -cp ""d:\vhayu\thrift2\hbase0.98\*;d:\vhayu\thrift2\conf"" org.apache.hadoop.hbase.thrift2.ThriftServer -b 127.0.0.1 -f framed start

The phenomenon of  the issue is that after some time running, Thrift2 sometimes reports OOM and heap dump file (.hprof) file was generated. The consequence of this will always trigger high latency form HBase cluster."
HBASE-15385	"When using Wsab file system, we found that a failed atomic folder rename operation can never recovery for the destination file deleted in Wasb filesystem. 
{quota}
ls: Attempting to complete rename of file hbase/azurtst-xiaomi/data/default/YCSBTest/.tabledesc during folder rename redo, and file was not found in source or destination.
{quote}

The reason is the the file is renamed to the destination file  before the crash, and the destination file is deleted by another process after crash. So the recovery is blocked during finishing the rename operation of this file when found the source and destination files all don't exist.

See: NativeAzureFileSystem.java #finishSingleFileRename

Another serious problem is that the recovery of atomic rename operation may delete new created file which is same name as the source file, because the file system don't check if there are rename operation need be redo.

Suggestions are welcomed~

"
HBASE-16395	"I had sended an email to user@hbase.apache.org,but received no help.

The cluster enabled shortCircuitLocalReads.
<property>
    <name>dfs.client.read.shortcircuit</name>
    <value>true</value>
</property>

When enabled replication,we found a large number of error logs.
1.shortCircuitLocalReads(fail everytime).
2.Try reading via the datanode on targetAddr(success).
How to make shortCircuitLocalReads successfully when enabled replication?

2016-08-03 10:46:21,721 DEBUG org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Opening log for replication dn7%2C60020%2C1470136216957.1470192327030 at 16999670
2016-08-03 10:46:21,723 WARN org.apache.hadoop.hdfs.DFSClient: BlockReaderLocal requested with incorrect offset: Offset 0 and length 17073479 don't match block blk_4137524355009640437_53760530 ( blockLen 16999670 )
2016-08-03 10:46:21,723 WARN org.apache.hadoop.hdfs.DFSClient: BlockReaderLocal: Removing blk_4137524355009640437_53760530 from cache because local file /sdd/hdfs/dfs/data/blocksBeingWritten/blk_4137524355009640437 could not be opened.
2016-08-03 10:46:21,724 INFO org.apache.hadoop.hdfs.DFSClient: Failed to read block blk_4137524355009640437_53760530 on local machinejava.io.IOException: Offset 0 and length 17073479 don't match block blk_4137524355009640437_53760530 ( blockLen 16999670 )
at org.apache.hadoop.hdfs.BlockReaderLocal.<init>(BlockReaderLocal.java:287)
at org.apache.hadoop.hdfs.BlockReaderLocal.newBlockReader(BlockReaderLocal.java:171)
at org.apache.hadoop.hdfs.DFSClient.getLocalBlockReader(DFSClient.java:358)
at org.apache.hadoop.hdfs.DFSClient.access$800(DFSClient.java:74)
at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:2073)
at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:2224)
at java.io.DataInputStream.read(DataInputStream.java:149)
at java.io.DataInputStream.readFully(DataInputStream.java:195)
at java.io.DataInputStream.readFully(DataInputStream.java:169)
at org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:1508)
at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1486)
at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1475)
at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1470)
at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader$WALReader.<init>(SequenceFileLogReader.java:55)
at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.init(SequenceFileLogReader.java:178)
at org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(HLog.java:734)
at org.apache.hadoop.hbase.replication.regionserver.ReplicationHLogReaderManager.openReader(ReplicationHLogReaderManager.java:69)
at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.openReader(ReplicationSource.java:574)
at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:364)
2016-08-03 10:46:21,724 INFO org.apache.hadoop.hdfs.DFSClient: Try reading via the datanode on /192.168.7.139:50010"
HBASE-15818	"Unable to create a table with multiple families (as suggested by the Examples)
also the shell is exiting. (I only tested 1.2 and 2.0 and have the problem, 1.1 seems to be ok)
{noformat}
hbase(main):001:0> create 鈥榯1鈥? 鈥榝1鈥? 鈥榝2鈥? 鈥榝3鈥?
ERROR: wrong number of arguments (0 for 1)

Examples:
  hbase> create 't1', 'f1', 'f2', 'f3'
{noformat}"
HBASE-15403	"hbase pe --nomapred --rows=100 --table='t4' randomWrite 10
# count on t4 gives 620 rows

hbase pe --nomapred --rows=200 --table='t5' randomWrite 10
# count on t5 gives 1257 rows

hbase pe --nomapred --table='t6' --rows=200 randomWrite 1
# count on t6 gives 126 rows

I was working with 1.2.0, but it's likely that it'll also be affecting master.
"
HBASE-1920	"Testing in HBASE-1908 uncovered an issue where I had a client that had cached a region location.  I killed that regionserver, waited for recovery/reassignment, and then tried to scan the table again from the same client w/o restarting it.

A client normally gets a NotServingRegionException when a region is reassigned, but since this server is dead the client just got Connection Refused type exceptions.  These didn't seem to trigger the client to ask META for a new region location."
HBASE-3266	"I was in the situation described by HBASE-3265, where I had a number of RS waiting on ROOT, but the master hadn't seen any RS checkins, so was waiting on checkins. To get past this, I restarted one of the region servers. The restarted server checked in, and the master began its startup.
At this point the master started scanning /hbase/.logs for things to split. It correctly identified that the RS on haus01 was running (this is the one I restarted):

2010-11-23 00:21:25,595 INFO org.apache.hadoop.hbase.master.MasterFileSystem: Log folder hdfs://haus01.sf.cloudera.com:11020/hbase-normal/.logs/haus01.sf.cloudera.com,60020,1290500443143 belongs to an existing region server

but then incorrectly decided that the RS on haus02 was down:

2010-11-23 00:21:25,595 INFO org.apache.hadoop.hbase.master.MasterFileSystem: Log folder hdfs://haus01.sf.cloudera.com:11020/hbase-normal/.logs/haus02.sf.cloudera.com,60020,1290498411450 doesn't belong to a known region server, splitting

However ZK shows that this RS is up:
[zk: haus01.sf.cloudera.com:2222(CONNECTED) 3] ls /hbase/rs
[haus04.sf.cloudera.com,60020,1290498411533, haus05.sf.cloudera.com,60020,1290498411520, haus03.sf.cloudera.com,60020,1290498411518, haus01.sf.cloudera.com,60020,1290500443143, haus02.sf.cloudera.com,60020,1290498411450]

splitLogsAfterStartup seems to check ServerManager.onlineServers, which best I can tell is derived from heartbeats and not from ZK (sorry if I got some of this wrong, still new to this new codebase)

Of course, the master went into an infinite splitting loop at this point since haus02 is up and renewing its DFS lease on its logs."
HBASE-3540	"I inserted 100G or so of data and got a region stuck in a CLOSED state. Different region servers kept trying to open it, but they failed to transition OFFLINE->OPENING because it was in a CLOSED state. "
HBASE-6266	"
Hi,
please find the below logs for same scenario:

java.lang.NullPointerException
	at com.onmobile.bmg.scheduler.mr.scheduler.SchedulerMR$SchedulerOutputCommitter.getStringData(SchedulerMR.java:1149)
	at com.onmobile.bmg.scheduler.mr.scheduler.SchedulerMR$SchedulerOutputCommitter.updateJobTable(SchedulerMR.java:945)
	at com.onmobile.bmg.scheduler.mr.scheduler.SchedulerMR$SchedulerOutputCommitter.commitTask(SchedulerMR.java:803)
	at org.apache.hadoop.mapred.Task.commit(Task.java:721)
	at org.apache.hadoop.mapred.Task.done(Task.java:644)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:460)
	at org.apache.hadoop.mapred.Child.main(Child.java:158)
java.lang.NullPointerException
	at com.onmobile.bmg.scheduler.mr.scheduler.SchedulerMR$SchedulerOutputCommitter.getStringData(SchedulerMR.java:1149)
	at com.onmobile.bmg.scheduler.mr.scheduler.SchedulerMR$SchedulerOutputCommitter.updateJobTableForReporting(SchedulerMR.java:1055)
	at com.onmobile.bmg.scheduler.mr.scheduler.SchedulerMR$SchedulerOutputCommitter.commitTask(SchedulerMR.java:804)
	at org.apache.hadoop.mapred.Task.commit(Task.java:721)
	at org.apache.hadoop.mapred.Task.done(Task.java:644)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:460)
	at org.apache.hadoop.mapred.Child.main(Child.java:158)"
HBASE-4548	"In HBASE-4377, Jon noticed that HConnectionManager.listTable now looks on HDFS for the table list. This seems incorrect, since the client may not have access to the hbase directory on HDFS (eg in a secure cluster). At the least, it should RPC to the master to find a table list, and have the master do the list on HDFS."
HBASE-4073	"See this thread: http://search-hadoop.com/?q=Errors+after+major+compaction&fc_project=HBase

In it, Eran has snippets from a log that show us being asked open a region we already have opened.

We need to make sure that the root issue is addressed -- the races around assignment and its timeouts -- and then after that do something like a check if we already have region open before we queue an open (we can't return message to the master from down inside the regionserver event handlers)."
HBASE-14419	"[root@localhost local]# ll
lrwxrwxrwx.  1 root root   11 Sep 12 21:34 hbase -> hbase-1.1.2
drwxr-xr-x. 30 root root 4096 Sep 12 21:34 hbase-1.1.2

[root@localhost local]# ./hbase/bin/start-hbase.sh
Error: Could not find or load main class org.apache.hadoop.hbase.util.HBaseConfTool
Error: Could not find or load main class org.apache.hadoop.hbase.zookeeper.ZKServerTool
starting master, logging to /usr/local/hbase/logs/hbase-root-master-localhost.out
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0
Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0
Error: Could not find or load main class org.apache.hadoop.hbase.master.HMaster
starting regionserver, logging to /usr/local/hbase/logs/hbase-root-1-regionserver-localhost.out
Error: Could not find or load main class org.apache.hadoop.hbase.regionserver.HRegionServer

why show this error ?

It exist 銆?[root@localhost local]# find ./ -name HBaseConfTool.class
./hbase-1.1.2/hbase-server/target/classes/org/apache/hadoop/hbase/util/HBaseConfTool.class


/etc/profle:
export JAVA_HOME=/usr/local/jdk1.8.0_20
export HBASE_HOME=/usr/local/hbase
export PATH=$JAVA_HOME/bin:$HBASE_HOME/bin:$PATH
export CLASSPATH=.:$JAVA_HOME/lib:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$HBASE_HOME/hbase-server/target/classes

I add  $HBASE_HOME/hbase-server/target/classes, but it still not find class file銆倂ersion 0.98xx has no this problem锛寃hy the version 1.1.2  so eggache锛?I am just a newer锛実eting start follow official docs, but  can not run銆侷 am so sad銆傘€傘€俿os銆傘€傘€?

"
HBASE-12967	"Refer to this thread
http://osdir.com/ml/general/2015-02/msg03547.html

A user tries to alter a table with a new split policy.  Due to an invalid classname the table does not get enabled and the table becomes unusable.  I think Procedure V2 is a long term soln for this but I think we atleast need to provide a work around or a set of steps to come out of this.  Any fix before Procedure V2 comes into place would useful for the already released versions."
HBASE-13756	"I have observed below exception when running Phoenix integration tests with HBase-1.1.0. I think same can happen in real cluster when RS reporting to master.
{noformat}
ABORTING region server 100.73.163.39,53415,1432394107922: Unhandled: org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos$ServerLoad$Builder.setNumberOfRequests(J)Lorg/apache/hadoop/hbase/protobuf/generated/ClusterStatusProtos$ServerLoad$Builder;
Cause:
java.lang.NoSuchMethodError: org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos$ServerLoad$Builder.setNumberOfRequests(J)Lorg/apache/hadoop/hbase/protobuf/generated/ClusterStatusProtos$ServerLoad$Builder;
        at org.apache.hadoop.hbase.regionserver.HRegionServer.buildServerLoad(HRegionServer.java:1165)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.tryRegionServerReport(HRegionServer.java:1127)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:944)
        at org.apache.hadoop.hbase.MiniHBaseCluster$MiniHBaseClusterRegionServer.runRegionServer(MiniHBaseCluster.java:156)
        at org.apache.hadoop.hbase.MiniHBaseCluster$MiniHBaseClusterRegionServer.access$000(MiniHBaseCluster.java:108)
        at org.apache.hadoop.hbase.MiniHBaseCluster$MiniHBaseClusterRegionServer$1.run(MiniHBaseCluster.java:140)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:356)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1594)
        at org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:306)
        at org.apache.hadoop.hbase.MiniHBaseCluster$MiniHBaseClusterRegionServer.run(MiniHBaseCluster.java:138)
        at java.lang.Thread.run(Thread.java:745)
{noformat}"
HBASE-5881	"This issue will occur only in hadoop 23.x & above/

In hadoop 0.20.x
{code}
public static void returnDecompressor(Decompressor decompressor) {
    if (decompressor == null) {
      return;
    }
    decompressor.reset();
    payback(decompressorPool, decompressor);
  }
{code}


In hadoop 0.23.x
{code}
  public static void returnDecompressor(Decompressor decompressor) {
    if (decompressor == null) {
      return;
    }
    // if the decompressor can't be reused, don't pool it.
    if (decompressor.getClass().isAnnotationPresent(DoNotPool.class)) {
      return;
    }
    decompressor.reset();
    payback(decompressorPool, decompressor);
  }
{code}

Here annotation has been added. By default this library will be loaded if there are no native library.
{code}
@DoNotPool
public class BuiltInGzipDecompressor
{code}

Due to this each time new compressor/decompressor will be loaded, this leads to native memory leak.
{noformat}
2012-04-25 22:11:48,093 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.gz]
2012-04-25 22:11:48,093 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.gz]
2012-04-25 22:11:48,093 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.gz]
{noformat}"
HBASE-5895	"Running a YCSB workload against trunk, the slow query log ends up logging the entire contents of ""mutate"" RPCs (in PB-encoded binary). This then makes the logging back up, which makes more slow queries, which makes the whole thing spin out of control. We should only summarize the RPC, rather than printing the whole contents."
HBASE-5702	"This bug is so easy to reproduce I'm wondering why it hasn't been reported yet. Stop any number of region servers on a 0.94/6 cluster and you'll see in the master interface one task per stopped region server saying the following:

|Processing schema change exclusion for region server = sv4r27s44,62023,1333402175340|RUNNING (since 5sec ago)|No schema change in progress. Skipping exclusion for server = sv4r27s44,62023,1333402175340 (since 5sec ago)|

It's gonna stay there until the master cleans it:

bq. WARN org.apache.hadoop.hbase.monitoring.TaskMonitor: Status Processing schema change exclusion for region server = sv4r27s44,62023,1333402175340: status=No schema change in progress. Skipping exclusion for server = sv4r27s44,62023,1333402175340, state=RUNNING, startTime=1333404636419, completionTime=-1 appears to have been leaked

It's not clear to me why it's using a MonitoredTask in the first place. "
HBASE-11913	"hbck shows errors on a perfectly fine cluster, because of an incorrect check if there have been errors. fix didn't change the printout, just the error detection."
HBASE-10647	"In MemStore.updateColumnValue, when removing old values, size is not decreased, but local variable addedSize is decreased, so size is greater than real value. This leads to the problem of negative value for HRegion.memstoreSize.
"
HBASE-9527	Go over all old APIs that take a table name and ensure that it is not possible to pass in a byte array that is a namespace + tablename; instead throw an exception.
HBASE-10271	"HBASE-9593 moved the creation of the ephemeral znode earlier in the region server startup process such that we don't have access to the ServerName from the Master's POV. HRS.getMyEphemeralNodePath() calls HRS.getServerName() which at that point will return this.isa.getHostName(). If you set hbase.regionserver.ipc.address to 0.0.0.0, you will create a znode with that address.

What happens next is that the RS will report for duty correctly but the master will do this:

{noformat}
2014-01-02 11:45:49,498 INFO  [master:172.21.3.117:60000] master.ServerManager: Registering server=0:0:0:0:0:0:0:0%0,60020,1388691892014
2014-01-02 11:45:49,498 INFO  [master:172.21.3.117:60000] master.HMaster: Registered server found up in zk but who has not yet reported in: 0:0:0:0:0:0:0:0%0,60020,1388691892014
{noformat}

The cluster is then unusable.

I think a better solution is to track the heartbeats for the region servers and expire those that haven't checked-in for some time. The 0.89-fb branch has this concept, and they also use it to detect rack failures: https://github.com/apache/hbase/blob/0.89-fb/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java#L1224. In this jira's scope I would just add the heartbeat tracking and add a unit test for the wildcard address.

What do you think [~rajesh23]?"
HBASE-2913	See this thread originated by Thomas Downing: http://www.mail-archive.com/user@hbase.apache.org/msg00992.html  In it he is finding that we leak sockets and filehandles and that they are never recovered during sustained high rate ingest.  Investigate.
HBASE-12040	"While testing 0.99.0RC1 release performances, compared to 0.98.6, figured that:
- FilteredScanTest is 100 times slower;
- RandomReadTest is 1.5 times slower;
- RandomSeekScanTest is 3.2 times slower;
- RandomScanWithRange10Test is 1,2 times slower;
- RandomScanWithRange100Test is 1,3 times slower;
- RandomScanWithRange1000Test is 4 times slower;
- SequentialReadTest is 1,7 times slower;
- SequentialWriteTest is just a bit faster;
- RandomWriteTest	 is just a bit faster;
- GaussianRandomReadBenchmark is just a beat slower;
- SequentialReadBenchmark is 1,1 times slower;
- SequentialWriteBenchmark is 1,1 times slower;
- UniformRandomReadBenchmark crashed;
- UniformRandomSmallScan is 1,3 times slower."
HBASE-12101	"log snippet.
 from one of the region server
2014-09-26 18:22:47,793 WARN  [RS_OPEN_REGION-b1-255-07:60020-0] regionserver.HStore: Failed flushing store file, retrying num=0
2014-09-26 18:24:58,554 WARN  [DataStreamer for file /hbase/data/default/tsdb/5b2e8261c6f88eb44c861fd24dbc2b42/.tmp/b4cb1ad7892440b6a7c13a7c30053d3f] hdfs.DFSClient: DataStreamer Exception
2014-09-26 18:24:58,555 WARN  [RS_OPEN_REGION-b1-255-07:60020-0] regionserver.HStore: Failed flushing store file, retrying num=1
2014-09-26 18:48:09,077 WARN  [DataStreamer for file /hbase/data/default/tsdb/5b2e8261c6f88eb44c861fd24dbc2b42/.tmp/a6f6f951de6047b0b691ea42ddd5e036] hdfs.DFSClient: DataStreamer Exception
2014-09-26 18:48:09,079 WARN  [RS_OPEN_REGION-b1-255-07:60020-0] regionserver.HStore: Failed flushing store file, retrying num=2
2014-09-26 18:48:53,363 WARN  [DataStreamer for file /hbase/data/default/tsdb/5b2e8261c6f88eb44c861fd24dbc2b42/.tmp/7e99efe5981a46ee9ba7ce4b7e50dd26] hdfs.DFSClient: DataStreamer Exception


Hdfs dir for hbase
-rwxr-xr-x   3 hbase hbase   41047119 2014-09-23 02:40 /hbase/data/default/tsdb/5b2e8261c6f88eb44c861fd24dbc2b42/t/fc0d247e689c40b598e2e4e1dfaa5f0f.

-rwxr-xr-x   3 hbase hbase 146163105792 2014-09-26 19:21 /hbase/data/default/tsdb/5b2e8261c6f88eb44c861fd24dbc2b42/.tmp/2ae6cce7b6ea446991405bc8c7382f03
-rwxr-xr-x   3 hbase hbase 146028888064 2014-09-26 19:21 /hbase/data/default/tsdb/5b2e8261c6f88eb44c861fd24dbc2b42/.tmp/4f8dc1f8e06844be8cc1a9d61a77624a
-rwxr-xr-x   3 hbase hbase 140794396672 2014-09-26 19:21 /hbase/data/default/tsdb/5b2e8261c6f88eb44c861fd24dbc2b42/.tmp/5bb43145bab1454d8fe1758cfaa91286
-rwxr-xr-x   3 hbase hbase 150994944000 2014-09-26 19:21 /hbase/data/default/tsdb/5b2e8261c6f88eb44c861fd24dbc2b42/.tmp/66f124990997420eb6001b86bd9cea30
-rwxr-xr-x   3 hbase hbase 141197049856 2014-09-26 19:21 /hbase/data/default/tsdb/5b2e8261c6f88eb44c861fd24dbc2b42/.tmp/88cede4cce514e97973b08ef5d0fc32c
-rwxr-xr-x   3 hbase hbase 144015622144 2014-09-26 19:21 /hbase/data/default/tsdb/5b2e8261c6f88eb44c861fd24dbc2b42/.tmp/aeb55b1f7e214da192c3d21f84a01ceb
-rwxr-xr-x   3 hbase hbase 152605556736 2014-09-26 19:21 /hbase/data/default/tsdb/5b2e8261c6f88eb44c861fd24dbc2b42/.tmp/c58c57e7aef74fe3b2a4332deec426a9
-rwxr-xr-x   3 hbase hbase 144552493056 2014-09-26 19:21 /hbase/data/default/tsdb/5b2e8261c6f88eb44c861fd24dbc2b42/.tmp/ed76cac79cc04adc866670fe261f5cc6 


Hbase master data
5b2e8261c6f88eb44c861fd24dbc2b42	tsdb,,1411066769435.5b2e8261c6f88eb44c861fd24dbc2b42. state=OPENING, ts=Fri Sep 26 19:21:02 IST 2014 (1311s ago), server=hostname.colo.xyz.com,60020,1411736155186	1311508"
HBASE-12026	"hi,all!
My Hadoop works very well execpt the HBASE.
It displayed that Hbase Ave Load work heavily,but i cann't find out which 
area is hot ......


"
HBASE-6469	"In Enable/DisableTableHandler code, if something goes wrong in handling, the table state in zk is left as ENABLING / DISABLING. After that we cannot force any more action from the API or CLI, and the only recovery path is restarting the master. 

{code}
    if (done) {
      // Flip the table to enabled.
      this.assignmentManager.getZKTable().setEnabledTable(
        this.tableNameStr);
      LOG.info(""Table '"" + this.tableNameStr
      + ""' was successfully enabled. Status: done="" + done);
    } else {
      LOG.warn(""Table '"" + this.tableNameStr
      + ""' wasn't successfully enabled. Status: done="" + done);
    }
{code}

Here, if done is false, the table state is not changed. There is also no way to set skipTableStateCheck from cli / api. 

We have run into this issue a couple of times before. "
HBASE-11595	"Reported using HBase 0.98.3 and HDFS 2.4.1

All data before failure has not yet been flushed so only exists in the WAL files. During distributed splitting, the WAL has either not been written out and synced in the same way as an unencrypted WAL or is unreadable:
{noformat}
2014-07-26 19:29:16,160 ERROR [RS_LOG_REPLAY_OPS-host1:60020-0] codec.BaseDecoder: Partial cell read caused by EOF: java.io.IOException: Premature EOF from inputStream
{noformat}

This file is still moved to oldWALs even though splitting failed. 

Setting 'hbase.regionserver.wal.encryption' to false allows data recovery.
"
HBASE-11584	"HBase file encryption some consistences observed and data loss happens after running the hbck tool,
the operation steps are as below.

Procedure:
1. Start the Hbase services (HMaster & region Server)
2. Enable HFile encryption and WAL file encryption as below, and perform 'table4-0' put operations (100 records added)
<property>
 <name>hbase.crypto.keyprovider</name>
 <value>org.apache.hadoop.hbase.io.crypto.KeyStoreKeyProvider</value>
</property>
<property>
 <name>hbase.crypto.keyprovider.parameters</name>
 <value>jceks:///opt/shankar1/kdc_keytab/hbase.jks?password=Hadoop@234</value>
</property>
<property>
 <name>hbase.crypto.master.key.name</name>
 <value>hdfs</value>
</property>
<property>
 <name>hfile.format.version</name>
 <value>3</value>
</property>
<property>
 <name>hbase.regionserver.hlog.reader.impl</name>
 <value>org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogReader</value>
</property>
<property>
 <name>hbase.regionserver.hlog.writer.impl</name>
 <value>org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogWriter</value>
</property>
<property>
 <name>hbase.regionserver.wal.encryption</name>
 <value>true</value>
</property>
 
3. Machine went down, so all process went down
4. We disabled the WAL file encryption for performance reason, and keep encryption only for Hfile, as below
<property>
 <name>hbase.crypto.keyprovider</name>
 <value>org.apache.hadoop.hbase.io.crypto.KeyStoreKeyProvider</value>
</property>
<property>
 <name>hbase.crypto.keyprovider.parameters</name>
 <value>jceks:///opt/shankar1/kdc_keytab/hbase.jks?password=Hadoop@234</value>
</property>
<property>
 <name>hbase.crypto.master.key.name</name>
 <value>hdfs</value>
</property>
<property>
 <name>hfile.format.version</name>
 <value>3</value>
</property>

5. Start the Region Server and query the 'table4-0' data
hbase(main):003:0> count 'table4-0'
ERROR: org.apache.hadoop.hbase.NotServingRegionException: Region table4-0,,1406207815456.fc10620a3dcc14e004ab034420f7d332. is not online on XX-XX-XX-XX,60020,1406209023146
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionByEncodedName(HRegionServer.java:2685)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:4119)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.scan(HRegionServer.java:3066)
        at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:29497)
        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2084)
        at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:98)
        at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.consumerLoop(SimpleRpcScheduler.java:168)
        at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.access$000(SimpleRpcScheduler.java:39)
        at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler$1.run(SimpleRpcScheduler.java:111)
        at java.lang.Thread.run(Thread.java:662)

6. Not able to read the data, so we decided to revert back the configuration (as original)

7. Kill/Stop the Region Server, revert all the configurations as original, as below

<property>
 <name>hbase.crypto.keyprovider</name>
 <value>org.apache.hadoop.hbase.io.crypto.KeyStoreKeyProvider</value>
</property>
<property>
 <name>hbase.crypto.keyprovider.parameters</name>
 <value>jceks:///opt/shankar1/kdc_keytab/hbase.jks?password=Hadoop@234</value>
</property>
<property>
 <name>hbase.crypto.master.key.name</name>
 <value>hdfs</value>
</property>
<property>
 <name>hfile.format.version</name>
 <value>3</value>
</property>
<property>
 <name>hbase.regionserver.hlog.reader.impl</name>
 <value>org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogReader</value>
</property>
<property>
 <name>hbase.regionserver.hlog.writer.impl</name>
 <value>org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogWriter</value>
</property>
<property>
 <name>hbase.regionserver.wal.encryption</name>
 <value>true</value>
</property>

7. Start the Region Server, and perform the 'table4-0' query 
hbase(main):003:0> count 'table4-0'
ERROR: org.apache.hadoop.hbase.NotServingRegionException: Region table4-0,,1406207815456.fc10620a3dcc14e004ab034420f7d332. is not online on XX-XX-XX-XX,60020,1406209023146
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionByEncodedName(HRegionServer.java:2685)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:4119)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.scan(HRegionServer.java:3066)
        at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:29497)
        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2084)
        at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:98)
        at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.consumerLoop(SimpleRpcScheduler.java:168)
        at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.access$000(SimpleRpcScheduler.java:39)
        at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler$1.run(SimpleRpcScheduler.java:111)
        at java.lang.Thread.run(Thread.java:662)

8. Run the hbase hbck to repair, as below
./hbase hbck -details
.........................
Summary:
  table1-0 is okay.
    Number of regions: 0
    Deployed on:
  table2-0 is okay.
    Number of regions: 0
    Deployed on:
  table3-0 is okay.
    Number of regions: 0
    Deployed on:
  table4-0 is okay.
    Number of regions: 0
    Deployed on:
  table5-0 is okay.
    Number of regions: 0
    Deployed on:
  table6-0 is okay.
    Number of regions: 0
    Deployed on:
  table7-0 is okay.
    Number of regions: 0
    Deployed on:
  table8-0 is okay.
    Number of regions: 0
    Deployed on:
  table9-0 is okay.
    Number of regions: 0
    Deployed on:
  hbase:meta is okay.
    Number of regions: 1
    Deployed on:  XX-XX-XX-XX,60020,1406209023146
  hbase:acl is okay.
    Number of regions: 0
    Deployed on:
  hbase:namespace is okay.
    Number of regions: 0
    Deployed on:
22 inconsistencies detected.
Status: INCONSISTENT
2014-07-24 19:13:05,532 INFO  [main] client.HConnectionManager$HConnectionImplementation: Closing master protocol: MasterService
2014-07-24 19:13:05,533 INFO  [main] client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x1475d1611611bcf
2014-07-24 19:13:05,533 DEBUG [main] zookeeper.ZooKeeper: Closing session: 0x1475d1611611bcf
2014-07-24 19:13:05,533 DEBUG [main] zookeeper.ClientCnxn: Closing client for session: 0x1475d1611611bcf
2014-07-24 19:13:05,546 DEBUG [main-SendThread(XX-XX-XX-XX:2181)] zookeeper.ClientCnxn: Reading reply sessionid:0x1475d1611611bcf, packet:: clientPath:null serverPath:null finished:false header:: 6,-11  replyHeader:: 6,4295102074,0  request:: null response:: null
2014-07-24 19:13:05,546 DEBUG [main] zookeeper.ClientCnxn: Disconnecting client for session: 0x1475d1611611bcf
2014-07-24 19:13:05,546 DEBUG [main-SendThread(XX-XX-XX-XX:2181)] zookeeper.ClientCnxn: An exception was thrown while closing send thread for session 0x1475d1611611bcf : Unable to read additional data from server sessionid 0x1475d1611611bcf, likely server has closed socket
2014-07-24 19:13:05,546 INFO  [main-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-07-24 19:13:05,546 INFO  [main] zookeeper.ZooKeeper: Session: 0x1475d1611611bcf closed
shankar1@XX-XX-XX-XX:~/DataSight/hbase/bin>


9. Fix the assignments as below
./hbase hbck -fixAssignments
Summary:
  table1-0 is okay.
    Number of regions: 1
    Deployed on:  XX-XX-XX-XX,60020,1406209023146
  table2-0 is okay.
    Number of regions: 1
    Deployed on:  XX-XX-XX-XX,60020,1406209023146
  table3-0 is okay.
    Number of regions: 1
    Deployed on:  XX-XX-XX-XX,60020,1406209023146
  table4-0 is okay.
    Number of regions: 1
    Deployed on:  XX-XX-XX-XX,60020,1406209023146
  table5-0 is okay.
    Number of regions: 1
    Deployed on:  XX-XX-XX-XX,60020,1406209023146
  table6-0 is okay.
    Number of regions: 1
    Deployed on:  XX-XX-XX-XX,60020,1406209023146
  table7-0 is okay.
    Number of regions: 1
    Deployed on:  XX-XX-XX-XX,60020,1406209023146
  table8-0 is okay.
    Number of regions: 1
    Deployed on:  XX-XX-XX-XX,60020,1406209023146
  table9-0 is okay.
    Number of regions: 1
    Deployed on:  XX-XX-XX-XX,60020,1406209023146
  hbase:meta is okay.
    Number of regions: 1
    Deployed on:  XX-XX-XX-XX,60020,1406209023146
  hbase:acl is okay.
    Number of regions: 1
    Deployed on:  XX-XX-XX-XX,60020,1406209023146
  hbase:namespace is okay.
    Number of regions: 1
    Deployed on:  XX-XX-XX-XX,60020,1406209023146
0 inconsistencies detected.
Status: OK
2014-07-24 19:44:55,194 INFO  [main] client.HConnectionManager$HConnectionImplementation: Closing master protocol: MasterService
2014-07-24 19:44:55,194 INFO  [main] client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x2475d15f7b31b73
2014-07-24 19:44:55,194 DEBUG [main] zookeeper.ZooKeeper: Closing session: 0x2475d15f7b31b73
2014-07-24 19:44:55,194 DEBUG [main] zookeeper.ClientCnxn: Closing client for session: 0x2475d15f7b31b73
2014-07-24 19:44:55,203 DEBUG [main-SendThread(XX-XX-XX-XX:2181)] zookeeper.ClientCnxn: Reading reply sessionid:0x2475d15f7b31b73, packet:: clientPath:null serverPath:null finished:false header:: 7,-11  replyHeader:: 7,4295102377,0  request:: null response:: null
2014-07-24 19:44:55,203 DEBUG [main] zookeeper.ClientCnxn: Disconnecting client for session: 0x2475d15f7b31b73
2014-07-24 19:44:55,204 DEBUG [main-SendThread(XX-XX-XX-XX:2181)] zookeeper.ClientCnxn: An exception was thrown while closing send thread for session 0x2475d15f7b31b73 : Unable to read additional data from server sessionid 0x2475d15f7b31b73, likely server has closed socket
2014-07-24 19:44:55,204 INFO  [main] zookeeper.ZooKeeper: Session: 0x2475d15f7b31b73 closed
2014-07-24 19:44:55,204 INFO  [main-EventThread] zookeeper.ClientCnxn: EventThread shut down

10. Fix the assignments as below
./hbase hbck -fixAssignments -fixMeta
Summary:
  table1-0 is okay.
    Number of regions: 1
    Deployed on:  XX-XX-XX-XX,60020,1406209023146
  table2-0 is okay.
    Number of regions: 1
    Deployed on:  XX-XX-XX-XX,60020,1406209023146
  table3-0 is okay.
    Number of regions: 1
    Deployed on:  XX-XX-XX-XX,60020,1406209023146
  table4-0 is okay.
    Number of regions: 1
    Deployed on:  XX-XX-XX-XX,60020,1406209023146
  table5-0 is okay.
    Number of regions: 1
    Deployed on:  XX-XX-XX-XX,60020,1406209023146
  table6-0 is okay.
    Number of regions: 1
    Deployed on:  XX-XX-XX-XX,60020,1406209023146
  table7-0 is okay.
    Number of regions: 1
    Deployed on:  XX-XX-XX-XX,60020,1406209023146
  table8-0 is okay.
    Number of regions: 1
    Deployed on:  XX-XX-XX-XX,60020,1406209023146
  table9-0 is okay.
    Number of regions: 1
    Deployed on:  XX-XX-XX-XX,60020,1406209023146
  hbase:meta is okay.
    Number of regions: 1
    Deployed on:  XX-XX-XX-XX,60020,1406209023146
  hbase:acl is okay.
    Number of regions: 1
    Deployed on:  XX-XX-XX-XX,60020,1406209023146
  hbase:namespace is okay.
    Number of regions: 1
    Deployed on:  XX-XX-XX-XX,60020,1406209023146
0 inconsistencies detected.
Status: OK
2014-07-24 19:46:16,290 INFO  [main] client.HConnectionManager$HConnectionImplementation: Closing master protocol: MasterService
2014-07-24 19:46:16,290 INFO  [main] client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x3475d1605321be9
2014-07-24 19:46:16,290 DEBUG [main] zookeeper.ZooKeeper: Closing session: 0x3475d1605321be9
2014-07-24 19:46:16,290 DEBUG [main] zookeeper.ClientCnxn: Closing client for session: 0x3475d1605321be9
2014-07-24 19:46:16,300 DEBUG [main-SendThread(XX-XX-XX-XX:2181)] zookeeper.ClientCnxn: Reading reply sessionid:0x3475d1605321be9, packet:: clientPath:null serverPath:null finished:false header:: 6,-11  replyHeader:: 6,4295102397,0  request:: null response:: null
2014-07-24 19:46:16,300 DEBUG [main] zookeeper.ClientCnxn: Disconnecting client for session: 0x3475d1605321be9
2014-07-24 19:46:16,300 DEBUG [main-SendThread(XX-XX-XX-XX:2181)] zookeeper.ClientCnxn: An exception was thrown while closing send thread for session 0x3475d1605321be9 : Unable to read additional data from server sessionid 0x3475d1605321be9, likely server has closed socket
2014-07-24 19:46:16,300 INFO  [main] zookeeper.ZooKeeper: Session: 0x3475d1605321be9 closed
2014-07-24 19:46:16,300 INFO  [main-EventThread] zookeeper.ClientCnxn: EventThread shut down

hbase(main):006:0> count 'table4-0'
0 row(s) in 0.0200 seconds

=> 0
hbase(main):007:0> 

Complete data loss happened,

WALs, oldWALs & /hbase/data/default/table4-0/ does not have any data
"
HBASE-1736	"What I saw was master shutting itself down because it had lost zk lease.  Fine.   The RS though doesn't look like it can deal with this situation.    We'll see stuff like this:

{code}
...failed on connection exception: java.net.ConnectException: Connection refused
    at org.apache.hadoop.hbase.ipc.HBaseClient.wrapException(HBaseClient.java:744)
    at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:722)
    at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:328)
    at $Proxy0.regionServerReport(Unknown Source)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:470)
    at java.lang.Thread.run(Unknown Source)
Caused by: java.net.ConnectException: Connection refused
    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
    at sun.nio.ch.SocketChannelImpl.finishConnect(Unknown Source)
    at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
    at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:404)
    at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupIOstreams(HBaseClient.java:305)
    at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:826)
    at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:707)
    ... 4 more
{code}

... all over the regionserver as it tries to send heartbeat to master on this broken connection.

On split, we close parent, add children to the catalog but then when we try to tell the master about the split, it fails.  Means the children never get deployed.  Meantime  the parent is offline.

This issue is about going through the regionserver and anytime it has a connection to master, make sure on fault that no damage is done the table and then that the regionserver puts a pause on splitting.
"
HBASE-10826	"{code}
    KeyValue kv1 = new KeyValue(Bytes.toBytes(""aaa""), Bytes.toBytes(""fam1""), Bytes.toBytes(""q1""),
        Bytes.toBytes(""val""));
    writer.append(kv1);
    KeyValue kv2 = new KeyValue(Bytes.toBytes(""aab""), Bytes.toBytes(""fam1""), Bytes.toBytes(""q1""),
        Bytes.toBytes(""val""));
    writer.append(kv2);
    KeyValue kv4 = new KeyValue(Bytes.toBytes(""aac""), Bytes.toBytes(""fam1""), Bytes.toBytes(""q1""),
        Bytes.toBytes(""val""));
    writer.append(kv4);
    KeyValue kv5 = new KeyValue(Bytes.toBytes(""aac""), Bytes.toBytes(""fam12""), Bytes.toBytes(""q2""),
        Bytes.toBytes(""val""));
writer.append(kv5);
{code}
{code}
    KeyValue toSeek = new KeyValue(Bytes.toBytes(""aac""), Bytes.toBytes(""fam1""),
        Bytes.toBytes(""q2""), Bytes.toBytes(""val""));
    StoreFileScanner s = reader.getStoreFileScanner(false, false);
    s.reseek(toSeek);
{code}
Now calling s.next() should point to the last KV - kv5.
Before calling s.next() it would have done a moveToPrevious since there is no KV that exactly matches with the toSeek key.
Incase of NONE, PREFIX, DIFF, FAST_DIFF things work fine as expected.
But in case of Prefix Tree, calling reseek() points to a key that is same as the toSeek key which does not exist at all. 
Will attach a testcase that shows this problem. 
"
HBASE-10702	"One of our user contacted me about an issue with Deletes.

Some of the deletes they do are not totally processed. Therefore, after the Delete, if they do a Get, from time to time, the Get return the row when it should have been deleted and should have returned nothing. After multiple Deletes, the row is finally deleted. If we don't retry after the 1st attempt, the row stays there. Even after a flush, a major_compact, etc.

I have been able to reproduce the issue in 0.94.2 (CDH4.2.0 EC2), 0.94.15(CDH4.6.0 EC2) and 0.94.17 (Apache version bare metal)

Here is a simple output from my test app.

1736509 Doing a delete for 0000099676 failed. Start to count
puts=311 deletes=64 retries=2

2281712 Doing a delete for 0000027606 failed. Start to count
puts=3679 deletes=247 retries=2

2388305 Doing a delete for 0000018306 failed. Start to count
puts=4744 deletes=290 retries=2

2532943 Doing a delete for 0000030446 failed. Start to count
puts=5678 deletes=337 retries=2

2551421 Doing a delete for 0000046304 failed. Start to count
puts=5845 deletes=345 retries=2

2561099 Doing a delete for 0000019619 failed. Start to count
puts=5869 deletes=347 retries=3

First field is the time in ms since the test started.  So first error occurs after about 30 minutes. Below are the number of puts and deletes done, and the numbers of required retries to get the value deleted.

Key is random number between  0000000000 and 0000100000.

Very simple test. Just doing more puts than deletes.

Tests are running on 0.96.1.1 for almost 1h now so it seems to be fine, but it's not on the same cluster, so I will keep that running for hours/days first."
HBASE-10557	"Flush triggered by hlog-replay(replayRecoveredEdits) and region-close(non-abort close) get processed directly by region without putting flush entry into flushQueue, hence not handled by MemStoreFlusher, So DroppedSnapshotException emitted from internalFlushcache is not handled properly"
HBASE-2647	"Saw this failure on my Hudson:

org.apache.hadoop.hbase.client.NoServerForRegionException: Timed out trying to locate root region because: Failed setting up proxy to /192.168.42.22:44708 after attempts=1
	at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRootRegion(HConnectionManager.java:1023)
	at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:630)
	at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:606)
	at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegionInMeta(HConnectionManager.java:676)
	at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:635)
	at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:606)
	at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:132)
	at org.apache.hadoop.hbase.HBaseClusterTestCase.hBaseClusterSetup(HBaseClusterTestCase.java:110)
	at org.apache.hadoop.hbase.HBaseClusterTestCase.setUp(HBaseClusterTestCase.java:147)
	at org.apache.hadoop.hbase.regionserver.wal.TestLogRolling.setUp(TestLogRolling.java:131)
"
HBASE-3490	"I am trying to run the Map Reduce job to upload the text file into the Hbase-0.89 from the HDFS file system by using the Sample code in the Hbase-0.20.6 API document. But it seems, BatchUpdate has been completely removed. Does any one have the sample code to upload the Bulk of data into Hbase using Map reduce Program. 

Thanks in advance



 "
HBASE-9912	
HBASE-8974	"I'm exercising the patch over on HBASE-8803 and I've noticed something in the logs: it looks like {{rolling-restart.sh}} is restarting all the region servers multiple times instead of just the current entry in the loop iteration.

The logic looks like this:

{noformat}
for each rs in active region server list:
  unload $rs // move all regions to other RS's
  restart all Region Servers // !?! bug?
  reload $rs // pile 'em back on
{noformat}

Shouldn't that step 2 be only {{restart $rs}}?

This is what I see in the logs. My cluster has 9 active RegionServers. Notice the bit in the middle where all 9 are stopped and started again after unloading the target RS.

{noformat}
$ time /usr/lib/hbase/bin/rolling-restart.sh --rs-only --graceful --maxthreads 30                                                                                                       
Gracefully restarting: hor18n39.gq1.ygridcore.net
Disabling balancer!
...
Unloading hor18n39.gq1.ygridcore.net region(s)
...
Valid region move targets: 
hor18n37.gq1.ygridcore.net,60020,1374094975268
hor17n37.gq1.ygridcore.net,60020,1374094975264
hor18n35.gq1.ygridcore.net,60020,1374094975327
hor17n39.gq1.ygridcore.net,60020,1374094975281
hor18n36.gq1.ygridcore.net,60020,1374094975254
hor17n36.gq1.ygridcore.net,60020,1374094975277
hor17n34.gq1.ygridcore.net,60020,1374094975291
hor18n38.gq1.ygridcore.net,60020,1374094975259
13/07/17 21:44:38 INFO region_mover: Moving 330 region(s) from hor18n39.gq1.ygridcore.net,60020,1374094975326 during this cycle
13/07/17 21:44:38 INFO region_mover: Moving region b59050cf97aabcef838e3c50e93e6d13 (1 of 330) to server=hor18n37.gq1.ygridcore.net,60020,1374094975268
...
13/07/17 21:54:20 INFO region_mover: Moving region d00026d7cc396bb3e6ea91106cc6ab55 (329 of 330) to server=hor18n37.gq1.ygridcore.net,60020,1374094975268
13/07/17 21:54:20 INFO region_mover: Moving region a722179b33e6ece8c9cee3fba3056acd (330 of 330) to server=hor17n37.gq1.ygridcore.net,60020,1374094975264
13/07/17 21:54:21 INFO region_mover: Wrote list of moved regions to /tmp/hor18n39.gq1.ygridcore.net
Unloaded hor18n39.gq1.ygridcore.net region(s)
hor18n35.gq1.ygridcore.net: stopping regionserver.
hor17n39.gq1.ygridcore.net: stopping regionserver.
hor18n36.gq1.ygridcore.net: stopping regionserver.
hor17n37.gq1.ygridcore.net: stopping regionserver.
hor17n34.gq1.ygridcore.net: stopping regionserver.
hor18n38.gq1.ygridcore.net: stopping regionserver.
hor18n37.gq1.ygridcore.net: stopping regionserver.
hor17n36.gq1.ygridcore.net: stopping regionserver.
hor18n39.gq1.ygridcore.net: stopping regionserver.
hor18n36.gq1.ygridcore.net: starting regionserver, logging to /grid/0/var/log/hbase/hbase-hbase-regionserver-hor18n36.gq1.ygridcore.net.out
hor17n36.gq1.ygridcore.net: starting regionserver, logging to /grid/0/var/log/hbase/hbase-hbase-regionserver-hor17n36.gq1.ygridcore.net.out
hor17n37.gq1.ygridcore.net: starting regionserver, logging to /grid/0/var/log/hbase/hbase-hbase-regionserver-hor17n37.gq1.ygridcore.net.out
hor18n37.gq1.ygridcore.net: starting regionserver, logging to /grid/0/var/log/hbase/hbase-hbase-regionserver-hor18n37.gq1.ygridcore.net.out
hor18n38.gq1.ygridcore.net: starting regionserver, logging to /grid/0/var/log/hbase/hbase-hbase-regionserver-hor18n38.gq1.ygridcore.net.out
hor17n34.gq1.ygridcore.net: starting regionserver, logging to /grid/0/var/log/hbase/hbase-hbase-regionserver-hor17n34.gq1.ygridcore.net.out
hor18n35.gq1.ygridcore.net: starting regionserver, logging to /grid/0/var/log/hbase/hbase-hbase-regionserver-hor18n35.gq1.ygridcore.net.out
hor18n39.gq1.ygridcore.net: starting regionserver, logging to /grid/0/var/log/hbase/hbase-hbase-regionserver-hor18n39.gq1.ygridcore.net.out
hor17n39.gq1.ygridcore.net: starting regionserver, logging to /grid/0/var/log/hbase/hbase-hbase-regionserver-hor17n39.gq1.ygridcore.net.out
Reloading hor18n39.gq1.ygridcore.net region(s)
...
13/07/17 21:54:27 INFO region_mover: Moving 330 regions to hor18n39.gq1.ygridcore.net,60020,1374098064602
13/07/17 21:56:47 INFO region_mover: Moving region 7d0a02f452c334a12026b45346a87d36 (1 of 330) to server=hor18n39.gq1.ygridcore.net,60020,1374098064602 in thread 0
13/07/17 21:56:54 INFO region_mover: Moving region af5448c90e78a8f0d935efb0b380502e (2 of 330) to server=hor18n39.gq1.ygridcore.net,60020,1374098064602 in thread 1
...
{noformat}"
HBASE-7670	"We found ZK event not be watched by master for a  long time in our testing.
It seems one ZK-Event-Handle thread block it.
Attaching some logs on master
{code}
2013-01-16 22:18:55,667 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENED, 
2013-01-16 22:18:56,270 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENED, 
...
2013-01-16 23:55:33,259 INFO org.apache.hadoop.hbase.catalog.CatalogTracker: Retrying
org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=100, exceptions:
        at org.apache.hadoop.hbase.client.ServerCallable.withRetries(ServerCallable.java:183)
        at org.apache.hadoop.hbase.client.HTable.get(HTable.java:676)
        at org.apache.hadoop.hbase.catalog.MetaReader.get(MetaReader.java:247)
        at org.apache.hadoop.hbase.catalog.MetaReader.getRegion(MetaReader.java:349)
        at org.apache.hadoop.hbase.catalog.MetaReader.readRegionLocation(MetaReader.java:289)
        at org.apache.hadoop.hbase.catalog.MetaReader.getMetaRegionLocation(MetaReader.java:276)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.getMetaServerConnection(CatalogTracker.java:424)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMeta(CatalogTracker.java:489)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMeta(CatalogTracker.java:451)
        at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:289)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:169)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
2013-01-16 23:55:33,261 WARN org.apache.hadoop.hbase.master.AssignmentManager: Attempted to handle region transition for server but server is not online
{code}

Between 2013-01-16 22:18:56 and 2013-01-16 23:55:33, there is no any logs about handling ZK Event.


{code}
this.metaNodeTracker = new MetaNodeTracker(zookeeper, throwableAborter) {
      public void nodeDeleted(String path) {
        if (!path.equals(node)) return;
        ct.resetMetaLocation();
      }
    }
public void resetMetaLocation() {
    LOG.debug(""Current cached META location, "" + metaLocation +
      "", is not valid, resetting"");
    synchronized(this.metaAvailable) {
      this.metaAvailable.set(false);
      this.metaAvailable.notifyAll();
    }
  }

private AdminProtocol getMetaServerConnection(){
synchronized (metaAvailable){
...
ServerName newLocation = MetaReader.getMetaRegionLocation(this);
...
}
}
{code}

From the above code, we would found that nodeDeleted() would wait synchronized (metaAvailable) until MetaReader.getMetaRegionLocation(this) done,
however, getMetaRegionLocation() could be retrying for a long time"
HBASE-6670	"Currently HbaseObjectWritable uses ProtobufUtil to perform serialization of Scan objects, ProtobufUtil.toParameter() calls HbaseObjectWritable.writeObject().

We should untangle such mixture and ultimately remove HbaseObjectWritable"
HBASE-4575	I've found some misnaming of certain ZK config options.  Make them consistent.
HBASE-2549	"Once we move to all Scans, the trackers could use a refresh.  There are often times where we return, for example, a MatchCode.SKIP (which just goes to the next KV not including the current one) where we could be sending a more optimal return code like MatchCode.SEEK_NEXT_ROW.

This is a jira to review all of this code after 2248 goes in."
HBASE-5154	"1. Call put to insert some value in column 'fm:a' like:
Put.add('fm', 'a', 1000, 'abc'), here timestamp = 1000.
2. Delete the column 'fm:a'
3. Try to do #1 again.(it doesn't work, but can insert put which use timestamp > 1000)"
HBASE-3809	"This is a duplicate of another issue but at the moment I cannot find the original.

If you had a 700 node cluster and then you ran something on the cluster which killed 100 nodes, and .META. had been running on one of those downed nodes, well, you'll have all of your master executors processing ServerShutdowns and more than likely non of the currently processing executors will be servicing the shutdown of the server that was carrying .META.

Well, for server shutdown to complete at the moment, an online .META. is required.  So, in the above case, we'll be stuck. The current executors will not be able to clear to make space for the processing of the server carrying .META. because they need .META. to complete.

We can make the master handlers have no bound so it will expand to accomodate all crashed servers -- so it'll have the one .META. in its queue -- or we can change it so shutdown handling doesn't require .META. to be on-line (its used to figure the regions the server was carrying); we could use the master's in-memory picture of the cluster (But IIRC, there may be holes ....TBD)"
HBASE-7364	"RCFile is not thread-safe, even if each reader is only used by one thread as intended, because it is possible to return decompressors to the pool multiple times by calling close on the reader multiple times. Then, different threads can pick up the same decompressor twice from the pool, resulting in decompression failures."
HBASE-7227	"without this, we're looking for deadlocks.

I'm looking at the code currently, I may add stuff to this jira "
HBASE-6464	"reportTableInFlux() gets all tables not in flux. However, when no table is found in getTable(numSkipped) function, it will return null. Then, errorReporter attempts to print the table number, which causes a NPE here."
HBASE-6498	"Hi Dear:

I met problem with starting Hbase:

I have 5 machines (Ubuntu)

109.123.121.23 rsmm-master.example.com
109.123.121.24 rsmm-slave-1.example.com
109.123.121.25 rsmm-slave-2.example.com
109.123.121.26 rsmm-slave-3.example.com
109.123.121.27 rsmm-slave-4.example.com

Hadoop 0.20.205.0
Zookeeper: zookeeper-3.3.5.jar
Hbase: hbase-0.94.0

The configuration file:
1. /etc/hosts seting is fine
#127.0.0.1      localhost
109.123.121.23 rsmm-master.example.com
109.123.121.24 rsmm-slave-1.example.com
109.123.121.25 rsmm-slave-2.example.com
109.123.121.26 rsmm-slave-3.example.com
109.123.121.27 rsmm-slave-4.example.com
# The following lines are desirable for IPv6 capable hosts
::1     ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters

2. /home/hduser/hadoop-0.20.205.0/conf/core-site.xml:
<property>
  <name>hadoop.tmp.dir</name>
  <value>/home/hduser/hadoop-0.20.205.0/data</value>
</property>
<property>
  <name>fs.default.name</name>
  <value>hdfs://rsmm-master.example.com:9000</value>
</property>


3. I use individual zookeeper. /home/hduser/zookeeper/conf/zoo.cfg 
tickTime=2000
dataDir=/home/hduser/zookeeper/conf
dataLogDir=/home/hduser/zookeeper/logs
clientPort=2181
initLimit=10
syncLimit=5
minSessionTimeout=10000
maxSessionTimeout=20000
server.1=rsmm-master.example.com:2888:3888
server.2=rsmm-slave-1.example.com:2888:3888
server.3=rsmm-slave-2.example.com:2888:3888
server.4=rsmm-slave-3.example.com:2888:3888
server.5=rsmm-slave-4.example.com:2888:3888

4. /home/hduser/hbase/conf/hbase-site.xml

  <property>
    <name>hbase.zookeeper.quorum</name>
    <value>rsmm-master.example.com,rsmm-slave-1.example.com,rsmm-slave-2.example.com,rsmm-slave-3.example.com,rsmm-slave-4.example.com</value>
  </property>
  <property>
    <name>hbase.master</name>
    <value>rsmm-master.example.com:60000</value>
  </property>
  <property>
    <name>hbase.zookeeper.property.dataDir</name>
    <value>/home/hduser/zookeeper/conf</value>
  </property>
  <property>
    <name>hbase.rootdir</name>
    <value>hdfs://rsmm-master.example.com:9000/hbase</value>
  </property>
   <property>
    <name>dfs.support.append</name>
    <value>true</value>
   </property>
   <property>
    <name>hbase.cluster.distributed</name>
    <value>true</value>
  </property> 
<property>
<name>hbase.master.maxclockskew</name>
<value>180000</value>
</property>
<property> 
<name>hbase.master.distributed.log.splitting</name> 
<value>false</value> 
</property>
<property>
<name>hbase.master.info.port</name>
<value>60010</value>
</property>
<property>
<name>hbase.master.port</name>
<value>60000</value></property>
<property>
<name>hbase.defaults.for.version.skip</name>
<value>true</value>
</property>

5.  /home/hduser/hbase/conf/hbase-env.sh
export JAVA_HOME=/usr/java/jre1.6.0_33
export HADOOP_HOME=/home/hduser/hadoop-0.20.205.0
export HBASE_HOME=/home/hduser/hbase
export PATH=$PATH:/home/hduser/hbase/bin
export HBASE_CLASSPATH=/home/hduser/hadoop-0.20.205.0/conf
export HBASE_HEAPSIZE=2048
export HBASE_OPTS=""-XX:+UseConcMarkSweepGC""
export HBASE_MANAGES_ZK=false

I have checked the configuration file, I believe it is set OK

=============================================================================

Then I start hadoop, it is OK
because I could visit:
http://109.123.121.23:50030/jobtracker.jsp
http://109.123.121.23:50070/dfshealth.jsp
http://109.123.121.24:50060/tasktracker.jsp
http://109.123.121.25:50060/tasktracker.jsp
http://109.123.121.26:50060/tasktracker.jsp
http://109.123.121.26:50060/tasktracker.jsp

I start zookeeper in all the clusters, 
runnig  /home/hduser/zookeeper/bin/zkServer.sh status
JMX enabled by default
Using config: /home/hduser/zookeeper/bin/../conf/zoo.cfg
Mode: follower

Then I log in zookeeper command line:
I could view, create ,etc..


Then  I start HBASE, then jps

2778 NameNode
19996 HMaster
3209 JobTracker
30732 Jps
3126 SecondaryNameNode
3382 QuorumPeerMain

in hbase shell: status
4 servers, 0 dead, 0.5000 average load

it looks good but the log shows: 

Tue Dec 12 15:04:21 CST 2028 Starting master on rsmm-master.example.com
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 26056
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 32768
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 26056
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
2028-12-12 15:04:21,916 INFO org.apache.hadoop.hbase.util.VersionInfo: HBase 0.94.0
2028-12-12 15:04:21,917 INFO org.apache.hadoop.hbase.util.VersionInfo: Subversion https://svn.apache.org/repos/asf/hbase/branches/0.94 -r 1332822
2028-12-12 15:04:21,917 INFO org.apache.hadoop.hbase.util.VersionInfo: Compiled by jenkins on Tue May  1 21:43:54 UTC 2012
2028-12-12 15:04:22,219 DEBUG org.apache.hadoop.hbase.master.HMaster: Set serverside HConnection retries=100
2028-12-12 15:04:23,181 INFO org.apache.hadoop.ipc.HBaseServer: Starting Thread-1
2028-12-12 15:04:23,181 INFO org.apache.hadoop.ipc.HBaseServer: Starting Thread-1
2028-12-12 15:04:23,182 INFO org.apache.hadoop.ipc.HBaseServer: Starting Thread-1
2028-12-12 15:04:23,182 INFO org.apache.hadoop.ipc.HBaseServer: Starting Thread-1
2028-12-12 15:04:23,183 INFO org.apache.hadoop.ipc.HBaseServer: Starting Thread-1
2028-12-12 15:04:23,184 INFO org.apache.hadoop.ipc.HBaseServer: Starting Thread-1
2028-12-12 15:04:23,184 INFO org.apache.hadoop.ipc.HBaseServer: Starting Thread-1
2028-12-12 15:04:23,185 INFO org.apache.hadoop.ipc.HBaseServer: Starting Thread-1
2028-12-12 15:04:23,185 INFO org.apache.hadoop.ipc.HBaseServer: Starting Thread-1
2028-12-12 15:04:23,186 INFO org.apache.hadoop.ipc.HBaseServer: Starting Thread-1
2028-12-12 15:04:23,204 INFO org.apache.hadoop.hbase.ipc.HBaseRpcMetrics: Initializing RPC Metrics with hostName=HMaster, port=60000
2028-12-12 15:04:23,468 INFO org.apache.zookeeper.ZooKeeper: Client environment:zookeeper.version=3.3.5-1301095, built on 03/15/2012 19:48 GMT
2028-12-12 15:04:23,468 INFO org.apache.zookeeper.ZooKeeper: Client environment:host.name=rsmm-master.example.com
2028-12-12 15:04:23,468 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.version=1.6.0_33
2028-12-12 15:04:23,468 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.vendor=Sun Microsystems Inc.
2028-12-12 15:04:23,468 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.home=/usr/java/jre1.6.0_33
2028-12-12 15:04:23,469 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.class.path=/home/hduser/hbase/conf:/usr/java/jre1.6.0_33/lib/tools.jar:/home/hduser/hbase:/home/hduser/hbase/hbase-0.94.0.jar:/home/hduser/hbase/hbase-0.94.0-tests.jar:/home/hduser/hbase/lib/activation-1.1.jar:/home/hduser/hbase/lib/asm-3.1.jar:/home/hduser/hbase/lib/avro-1.5.3.jar:/home/hduser/hbase/lib/avro-ipc-1.5.3.jar:/home/hduser/hbase/lib/commons-beanutils-1.7.0.jar:/home/hduser/hbase/lib/commons-beanutils-core-1.8.0.jar:/home/hduser/hbase/lib/commons-cli-1.2.jar:/home/hduser/hbase/lib/commons-codec-1.4.jar:/home/hduser/hbase/lib/commons-collections-3.2.1.jar:/home/hduser/hbase/lib/commons-configuration-1.6.jar:/home/hduser/hbase/lib/commons-digester-1.8.jar:/home/hduser/hbase/lib/commons-el-1.0.jar:/home/hduser/hbase/lib/commons-httpclient-3.1.jar:/home/hduser/hbase/lib/commons-io-2.1.jar:/home/hduser/hbase/lib/commons-lang-2.5.jar:/home/hduser/hbase/lib/commons-logging-1.1.1.jar:/home/hduser/hbase/lib/commons-math-2.1.jar:/home/hduser/hbase/lib/commons-net-1.4.1.jar:/home/hduser/hbase/lib/core-3.1.1.jar:/home/hduser/hbase/lib/guava-r09.jar:/home/hduser/hbase/lib/hadoop-core-0.20.205.0.jar:/home/hduser/hbase/lib/high-scale-lib-1.1.1.jar:/home/hduser/hbase/lib/httpclient-4.1.2.jar:/home/hduser/hbase/lib/httpcore-4.1.3.jar:/home/hduser/hbase/lib/jackson-core-asl-1.5.5.jar:/home/hduser/hbase/lib/jackson-jaxrs-1.5.5.jar:/home/hduser/hbase/lib/jackson-mapper-asl-1.5.5.jar:/home/hduser/hbase/lib/jackson-xc-1.5.5.jar:/home/hduser/hbase/lib/jamon-runtime-2.3.1.jar:/home/hduser/hbase/lib/jasper-compiler-5.5.23.jar:/home/hduser/hbase/lib/jasper-runtime-5.5.23.jar:/home/hduser/hbase/lib/jaxb-api-2.1.jar:/home/hduser/hbase/lib/jaxb-impl-2.1.12.jar:/home/hduser/hbase/lib/jersey-core-1.4.jar:/home/hduser/hbase/lib/jersey-json-1.4.jar:/home/hduser/hbase/lib/jersey-server-1.4.jar:/home/hduser/hbase/lib/jettison-1.1.jar:/home/hduser/hbase/lib/jetty-6.1.26.jar:/home/hduser/hbase/lib/jetty-util-6.1.26.jar:/home/hduser/hbase/lib/jruby-complete-1.6.5.jar:/home/hduser/hbase/lib/jsp-2.1-6.1.14.jar:/home/hduser/hbase/lib/jsp-api-2.1-6.1.14.jar:/home/hduser/hbase/lib/libthrift-0.8.0.jar:/home/hduser/hbase/lib/log4j-1.2.16.jar:/home/hduser/hbase/lib/netty-3.2.4.Final.jar:/home/hduser/hbase/lib/protobuf-java-2.4.0a.jar:/home/hduser/hbase/lib/servlet-api-2.5-6.1.14.jar:/home/hduser/hbase/lib/slf4j-api-1.5.8.jar:/home/hduser/hbase/lib/slf4j-log4j12-1.5.8.jar:/home/hduser/hbase/lib/snappy-java-1.0.3.2.jar:/home/hduser/hbase/lib/stax-api-1.0.1.jar:/home/hduser/hbase/lib/velocity-1.7.jar:/home/hduser/hbase/lib/xmlenc-0.52.jar:/home/hduser/hbase/lib/zookeeper-3.3.5.jar:/home/hduser/hadoop-0.20.205.0/conf:/home/hduser/hadoop-0.20.205.0/libexec/../conf:/usr/java/jre1.6.0_33/lib/tools.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/hadoop-core-0.20.205.0.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/asm-3.2.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/aspectjrt-1.6.5.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/aspectjtools-1.6.5.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/commons-beanutils-1.7.0.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/commons-beanutils-core-1.8.0.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/commons-cli-1.2.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/commons-codec-1.4.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/commons-collections-3.2.1.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/commons-configuration-1.6.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/commons-daemon-1.0.1.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/commons-digester-1.8.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/commons-el-1.0.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/commons-httpclient-3.0.1.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/commons-lang-2.4.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/commons-logging-1.1.1.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/commons-logging-api-1.0.4.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/commons-math-2.1.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/commons-net-1.4.1.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/core-3.1.1.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/hadoop-capacity-scheduler-0.20.205.0.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/hadoop-fairscheduler-0.20.205.0.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/hadoop-thriftfs-0.20.205.0.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/hsqldb-1.8.0.10.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/jackson-core-asl-1.0.1.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/jackson-mapper-asl-1.0.1.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/jasper-compiler-5.5.12.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/jasper-runtime-5.5.12.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/jdeb-0.8.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/jersey-core-1.8.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/jersey-json-1.8.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/jersey-server-1.8.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/jets3t-0.6.1.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/jetty-6.1.26.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/jetty-util-6.1.26.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/jsch-0.1.42.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/junit-4.5.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/kfs-0.2.2.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/log4j-1.2.15.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/mockito-all-1.8.5.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/oro-2.0.8.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/servlet-api-2.5-20081211.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/slf4j-api-1.5.8.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/slf4j-log4j12-1.4.3.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/slf4j-log4j12-1.5.8.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/xmlenc-0.52.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/jsp-2.1/jsp-2.1.jar:/home/hduser/hadoop-0.20.205.0/libexec/../share/hadoop/lib/jsp-2.1/jsp-api-2.1.jar:/home/hduser/hbase/hbase-0.94.0.jar:/home/hduser/zookeeper/zookeeper-3.3.5.jar
2028-12-12 15:04:23,469 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.library.path=/home/hduser/hadoop-0.20.205.0/libexec/../lib:/home/hduser/hbase/lib/native/Linux-i386-32
2028-12-12 15:04:23,469 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
2028-12-12 15:04:23,469 INFO org.apache.zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
2028-12-12 15:04:23,469 INFO org.apache.zookeeper.ZooKeeper: Client environment:os.name=Linux
2028-12-12 15:04:23,469 INFO org.apache.zookeeper.ZooKeeper: Client environment:os.arch=i386
2028-12-12 15:04:23,469 INFO org.apache.zookeeper.ZooKeeper: Client environment:os.version=3.0.0-24-generic
2028-12-12 15:04:23,469 INFO org.apache.zookeeper.ZooKeeper: Client environment:user.name=hduser
2028-12-12 15:04:23,469 INFO org.apache.zookeeper.ZooKeeper: Client environment:user.home=/home/hduser
2028-12-12 15:04:23,469 INFO org.apache.zookeeper.ZooKeeper: Client environment:user.dir=/home/hduser/hbase/bin
2028-12-12 15:04:23,470 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=rsmm-slave-3.example.com:2181,rsmm-slave-2.example.com:2181,rsmm-slave-1.example.com:2181,rsmm-master.example.com:2181,rsmm-slave-4.example.com:2181 sessionTimeout=180000 watcher=master:60000
2028-12-12 15:04:23,489 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server rsmm-slave-4.example.com/109.123.121.27:2181
2028-12-12 15:04:23,496 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to rsmm-slave-4.example.com/109.123.121.27:2181, initiating session
2028-12-12 15:04:23,496 INFO org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper: The identifier of this process is 10155@rsmm-master.example.com
2028-12-12 15:04:23,520 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server rsmm-slave-4.example.com/109.123.121.27:2181, sessionid = 0xffb11d36a60b0004, negotiated timeout = 20000
2028-12-12 15:04:23,598 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server Responder: starting
2028-12-12 15:04:23,602 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server listener on 60000: starting
2028-12-12 15:04:23,625 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 0 on 60000: starting
2028-12-12 15:04:23,626 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 1 on 60000: starting
2028-12-12 15:04:23,626 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 2 on 60000: starting
2028-12-12 15:04:23,626 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 3 on 60000: starting
2028-12-12 15:04:23,627 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 4 on 60000: starting
2028-12-12 15:04:23,627 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 5 on 60000: starting
2028-12-12 15:04:23,627 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 6 on 60000: starting
2028-12-12 15:04:23,627 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 7 on 60000: starting
2028-12-12 15:04:23,627 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 8 on 60000: starting
2028-12-12 15:04:23,628 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 9 on 60000: starting
2028-12-12 15:04:23,632 INFO org.apache.hadoop.metrics.jvm.JvmMetrics: Initializing JVM Metrics with processName=Master, sessionId=rsmm-master.example.com,60000,1860217463298
2028-12-12 15:04:23,640 INFO org.apache.hadoop.hbase.metrics: MetricsString added: revision
2028-12-12 15:04:23,640 INFO org.apache.hadoop.hbase.metrics: MetricsString added: hdfsUser
2028-12-12 15:04:23,640 INFO org.apache.hadoop.hbase.metrics: MetricsString added: hdfsDate
2028-12-12 15:04:23,640 INFO org.apache.hadoop.hbase.metrics: MetricsString added: hdfsUrl
2028-12-12 15:04:23,640 INFO org.apache.hadoop.hbase.metrics: MetricsString added: date
2028-12-12 15:04:23,640 INFO org.apache.hadoop.hbase.metrics: MetricsString added: hdfsRevision
2028-12-12 15:04:23,640 INFO org.apache.hadoop.hbase.metrics: MetricsString added: user
2028-12-12 15:04:23,640 INFO org.apache.hadoop.hbase.metrics: MetricsString added: hdfsVersion
2028-12-12 15:04:23,640 INFO org.apache.hadoop.hbase.metrics: MetricsString added: url
2028-12-12 15:04:23,640 INFO org.apache.hadoop.hbase.metrics: MetricsString added: version
2028-12-12 15:04:23,640 INFO org.apache.hadoop.hbase.metrics: new MBeanInfo
2028-12-12 15:04:23,641 INFO org.apache.hadoop.hbase.metrics: new MBeanInfo
2028-12-12 15:04:23,642 INFO org.apache.hadoop.hbase.master.metrics.MasterMetrics: Initialized
2028-12-12 15:04:23,658 INFO org.apache.hadoop.hbase.master.ActiveMasterManager: Deleting ZNode for /hbase/backup-masters/rsmm-master.example.com,60000,1860217463298 from backup master directory
2028-12-12 15:04:23,674 WARN org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper: Node /hbase/backup-masters/rsmm-master.example.com,60000,1860217463298 already deleted, and this is not a retry
2028-12-12 15:04:23,675 INFO org.apache.hadoop.hbase.master.ActiveMasterManager: Master=rsmm-master.example.com,60000,1860217463298
2028-12-12 15:04:24,081 DEBUG org.apache.hadoop.hbase.catalog.CatalogTracker: Starting catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@1c3e9ba
2028-12-12 15:04:24,142 INFO org.apache.hadoop.hbase.master.HMaster: Server active/primary master; rsmm-master.example.com,60000,1860217463298, sessionid=0xffb11d36a60b0004, cluster-up flag was=false
2028-12-12 15:04:24,184 DEBUG org.apache.hadoop.hbase.executor.ExecutorService: Starting executor service name=MASTER_OPEN_REGION-rsmm-master.example.com,60000,1860217463298, corePoolSize=5, maxPoolSize=5
2028-12-12 15:04:24,184 DEBUG org.apache.hadoop.hbase.executor.ExecutorService: Starting executor service name=MASTER_CLOSE_REGION-rsmm-master.example.com,60000,1860217463298, corePoolSize=5, maxPoolSize=5
2028-12-12 15:04:24,184 DEBUG org.apache.hadoop.hbase.executor.ExecutorService: Starting executor service name=MASTER_SERVER_OPERATIONS-rsmm-master.example.com,60000,1860217463298, corePoolSize=3, maxPoolSize=3
2028-12-12 15:04:24,185 DEBUG org.apache.hadoop.hbase.executor.ExecutorService: Starting executor service name=MASTER_META_SERVER_OPERATIONS-rsmm-master.example.com,60000,1860217463298, corePoolSize=5, maxPoolSize=5
2028-12-12 15:04:24,185 DEBUG org.apache.hadoop.hbase.executor.ExecutorService: Starting executor service name=MASTER_TABLE_OPERATIONS-rsmm-master.example.com,60000,1860217463298, corePoolSize=1, maxPoolSize=1
2028-12-12 15:04:24,187 DEBUG org.apache.hadoop.hbase.master.LogCleaner: Add log cleaner in chain: org.apache.hadoop.hbase.master.TimeToLiveLogCleaner
2028-12-12 15:04:24,259 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2028-12-12 15:04:24,348 INFO org.apache.hadoop.http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
2028-12-12 15:04:24,358 INFO org.apache.hadoop.http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 60010
2028-12-12 15:04:24,359 INFO org.apache.hadoop.http.HttpServer: listener.getLocalPort() returned 60010 webServer.getConnectors()[0].getLocalPort() returned 60010
2028-12-12 15:04:24,359 INFO org.apache.hadoop.http.HttpServer: Jetty bound to port 60010
2028-12-12 15:04:24,359 INFO org.mortbay.log: jetty-6.1.26
2028-12-12 15:04:24,865 INFO org.mortbay.log: Started SelectChannelConnector@0.0.0.0:60010
2028-12-12 15:04:24,865 DEBUG org.apache.hadoop.hbase.master.HMaster: Started service threads
2028-12-12 15:04:24,865 INFO org.apache.hadoop.hbase.master.ServerManager: Waiting for region servers count to settle; currently checked in 0, slept for 0 ms, expecting minimum of 1, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms.
2028-12-12 15:04:25,338 WARN org.apache.hadoop.hbase.master.ServerManager: Reported time for server rsmm-slave-3.example.com,60020,1860217439763 is out of sync with master by 25058ms. (Warning threshold is 10000ms; error threshold is 180000ms)
2028-12-12 15:04:25,339 INFO org.apache.hadoop.hbase.master.ServerManager: Registering server=rsmm-slave-3.example.com,60020,1860217439763
2028-12-12 15:04:25,366 INFO org.apache.hadoop.hbase.master.ServerManager: Waiting for region servers count to settle; currently checked in 1, slept for 501 ms, expecting minimum of 1, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms.
2028-12-12 15:04:25,580 INFO org.apache.hadoop.hbase.master.ServerManager: Registering server=rsmm-slave-1.example.com,60020,1860217460976
2028-12-12 15:04:25,603 WARN org.apache.hadoop.hbase.master.ServerManager: Reported time for server rsmm-slave-4.example.com,60020,1860217451403 is out of sync with master by 13667ms. (Warning threshold is 10000ms; error threshold is 180000ms)
2028-12-12 15:04:25,603 INFO org.apache.hadoop.hbase.master.ServerManager: Registering server=rsmm-slave-4.example.com,60020,1860217451403
2028-12-12 15:04:25,617 INFO org.apache.hadoop.hbase.master.ServerManager: Waiting for region servers count to settle; currently checked in 3, slept for 752 ms, expecting minimum of 1, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms.
2028-12-12 15:04:25,649 WARN org.apache.hadoop.hbase.master.ServerManager: Reported time for server rsmm-slave-2.example.com,60020,1860217448183 is out of sync with master by 16960ms. (Warning threshold is 10000ms; error threshold is 180000ms)
2028-12-12 15:04:25,650 INFO org.apache.hadoop.hbase.master.ServerManager: Registering server=rsmm-slave-2.example.com,60020,1860217448183
2028-12-12 15:04:25,668 INFO org.apache.hadoop.hbase.master.ServerManager: Waiting for region servers count to settle; currently checked in 4, slept for 803 ms, expecting minimum of 1, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms.
2028-12-12 15:04:27,170 INFO org.apache.hadoop.hbase.master.ServerManager: Finished waiting for region servers count to settle; checked in 4, slept for 2305 ms, expecting minimum of 1, maximum of 2147483647, master is running.
2028-12-12 15:04:27,176 INFO org.apache.hadoop.hbase.master.MasterFileSystem: Log folder hdfs://rsmm-master.example.com:9000/hbase/.logs/rsmm-slave-1.example.com,60020,1860217460976 belongs to an existing region server
2028-12-12 15:04:27,176 INFO org.apache.hadoop.hbase.master.MasterFileSystem: Log folder hdfs://rsmm-master.example.com:9000/hbase/.logs/rsmm-slave-2.example.com,60020,1860217448183 belongs to an existing region server
2028-12-12 15:04:27,176 INFO org.apache.hadoop.hbase.master.MasterFileSystem: Log folder hdfs://rsmm-master.example.com:9000/hbase/.logs/rsmm-slave-3.example.com,60020,1860217439763 belongs to an existing region server
2028-12-12 15:04:27,177 INFO org.apache.hadoop.hbase.master.MasterFileSystem: Log folder hdfs://rsmm-master.example.com:9000/hbase/.logs/rsmm-slave-4.example.com,60020,1860217451403 belongs to an existing region server
2028-12-12 15:04:27,177 INFO org.apache.hadoop.hbase.master.MasterFileSystem: No logs to split
2028-12-12 15:04:28,185 INFO org.apache.hadoop.hbase.catalog.RootLocationEditor: Unsetting ROOT region location in ZooKeeper
2028-12-12 15:04:28,193 WARN org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper: Node /hbase/root-region-server already deleted, and this is not a retry
2028-12-12 15:04:28,196 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0xffb11d36a60b0004 Creating (or updating) unassigned node for 70236052 with OFFLINE state
2028-12-12 15:04:28,223 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: No previous transition plan was found (or we are ignoring an existing plan) for -ROOT-,,0.70236052 so generated a random one; hri=-ROOT-,,0.70236052, src=, dest=rsmm-slave-2.example.com,60020,1860217448183; 4 (online=4, available=4) available servers
2028-12-12 15:04:28,223 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region -ROOT-,,0.70236052 to rsmm-slave-2.example.com,60020,1860217448183
2028-12-12 15:04:28,224 DEBUG org.apache.hadoop.hbase.master.ServerManager: New connection to rsmm-slave-2.example.com,60020,1860217448183
2028-12-12 15:04:28,225 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=rsmm-slave-3.example.com:2181,rsmm-slave-2.example.com:2181,rsmm-slave-1.example.com:2181,rsmm-master.example.com:2181,rsmm-slave-4.example.com:2181 sessionTimeout=180000 watcher=hconnection
2028-12-12 15:04:28,227 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server rsmm-slave-4.example.com/109.123.121.27:2181
2028-12-12 15:04:28,228 INFO org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper: The identifier of this process is 10155@rsmm-master.example.com
2028-12-12 15:04:28,230 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to rsmm-slave-4.example.com/109.123.121.27:2181, initiating session
2028-12-12 15:04:28,239 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server rsmm-slave-4.example.com/109.123.121.27:2181, sessionid = 0xffb11d36a60b0006, negotiated timeout = 20000
2028-12-12 15:04:28,440 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=rsmm-slave-2.example.com,60020,1860217448183, region=70236052/-ROOT-, which is more than 15 seconds late
2028-12-12 15:04:28,728 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=rsmm-slave-2.example.com,60020,1860217448183, region=70236052/-ROOT-, which is more than 15 seconds late
2028-12-12 15:04:28,754 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENED, server=rsmm-slave-2.example.com,60020,1860217448183, region=70236052/-ROOT-, which is more than 15 seconds late
2028-12-12 15:04:28,758 INFO org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Handling OPENED event for -ROOT-,,0.70236052 from rsmm-slave-2.example.com,60020,1860217448183; deleting unassigned node
2028-12-12 15:04:28,759 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0xffb11d36a60b0004 Deleting existing unassigned node for 70236052 that is in expected state RS_ZK_REGION_OPENED
2028-12-12 15:04:28,769 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: The znode of region -ROOT-,,0.70236052 has been deleted.
2028-12-12 15:04:28,769 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0xffb11d36a60b0004 Successfully deleted unassigned node for region 70236052 in expected state RS_ZK_REGION_OPENED
2028-12-12 15:04:28,771 INFO org.apache.hadoop.hbase.master.AssignmentManager: The master has opened the region -ROOT-,,0.70236052 that was online on rsmm-slave-2.example.com,60020,1860217448183
2028-12-12 15:04:28,776 INFO org.apache.hadoop.hbase.master.HMaster: -ROOT- assigned=1, rit=false, location=rsmm-slave-2.example.com,60020,1860217448183
2028-12-12 15:04:28,791 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation: Looked up root region location, connection=org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@76f2e8; serverName=rsmm-slave-2.example.com,60020,1860217448183
2028-12-12 15:04:28,794 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation: Looked up root region location, connection=org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@76f2e8; serverName=rsmm-slave-2.example.com,60020,1860217448183
2028-12-12 15:04:28,884 INFO org.apache.hadoop.hbase.catalog.CatalogTracker: Failed verification of .META.,,1 at address=rsmm-slave-4.example.com,60020,1860211448024; org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region is not online: .META.,,1
2028-12-12 15:04:28,936 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation: Looked up root region location, connection=org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@76f2e8; serverName=rsmm-slave-2.example.com,60020,1860217448183
2028-12-12 15:04:28,937 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation: Looked up root region location, connection=org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@76f2e8; serverName=rsmm-slave-2.example.com,60020,1860217448183
2028-12-12 15:04:28,943 INFO org.apache.hadoop.hbase.catalog.CatalogTracker: Failed verification of .META.,,1 at address=rsmm-slave-4.example.com,60020,1860211448024; org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region is not online: .META.,,1
2028-12-12 15:04:28,994 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation: Looked up root region location, connection=org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@76f2e8; serverName=rsmm-slave-2.example.com,60020,1860217448183
2028-12-12 15:04:28,995 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation: Looked up root region location, connection=org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@76f2e8; serverName=rsmm-slave-2.example.com,60020,1860217448183
2028-12-12 15:04:29,000 INFO org.apache.hadoop.hbase.catalog.CatalogTracker: Failed verification of .META.,,1 at address=rsmm-slave-4.example.com,60020,1860211448024; org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region is not online: .META.,,1
2028-12-12 15:04:29,051 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation: Looked up root region location, connection=org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@76f2e8; serverName=rsmm-slave-2.example.com,60020,1860217448183
2028-12-12 15:04:29,052 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation: Looked up root region location, connection=org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@76f2e8; serverName=rsmm-slave-2.example.com,60020,1860217448183
2028-12-12 15:04:29,057 INFO org.apache.hadoop.hbase.catalog.CatalogTracker: Failed verification of .META.,,1 at address=rsmm-slave-4.example.com,60020,1860211448024; org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region is not online: .META.,,1
2028-12-12 15:04:29,109 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation: Looked up root region location, connection=org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@76f2e8; serverName=rsmm-slave-2.example.com,60020,1860217448183
2028-12-12 15:04:29,110 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation: Looked up root region location, connection=org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@76f2e8; serverName=rsmm-slave-2.example.com,60020,1860217448183
2028-12-12 15:04:29,115 INFO org.apache.hadoop.hbase.catalog.CatalogTracker: Fa"
	
	
	
HBASE-6174	"I see this:

{code}

Tests in error: 
  testCalls(org.apache.hadoop.hbase.ipc.TestPBOnWritableRpc): java.lang.NoSuchMethodError: org.apache.hadoop.net.NetUtils.getInputStream(Ljava/net/Socket;)Ljava/io/InputStream;

{code}

when I do this:

{code}
$ ~/bin/mvn/bin/mvn -PlocalTests test -Dtest=TestPBOnWritableRpc -Dhadoop.profile=2.0
{code}

... is this "
HBASE-4767	"when i want to upgrade my dev box from hbase dompiled on wto, 25-10-2011 to current branch master got exeption:
java.io.IOException: HTD not found in input buffer
        at org.apache.hadoop.hbase.HRegionInfo.readFields(HRegionInfo.java:738)

trying to rebuild META by OfflineMetaRepair i had the same exception too"
HBASE-1163	"Mapreduce tasks based on TIF won't start. Clients trying to find regions by start key block indefinitely (Heritrix hbase writer eventually times out archiver). 

Master seems hung in root scan. I've dumped thread stacks 10 times in 10 minutes and the same HBaseClient$Call  object appears in the trace. See below:

Thread 21 (RegionManager.rootScanner):
  State: WAITING
  Blocked count: 500
  Waited count: 621
  Waiting on org.apache.hadoop.hbase.ipc.HBaseClient$Call@55a2896d
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.Object.wait(Object.java:485)
    org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:695)
    org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:321)
    $Proxy2.next(Unknown Source)
    org.apache.hadoop.hbase.master.BaseScanner.scanRegion(BaseScanner.java:161)
    org.apache.hadoop.hbase.master.RootScanner.scanRoot(RootScanner.java:55)
    org.apache.hadoop.hbase.master.RootScanner.maintenanceScan(RootScanner.java:80)
    org.apache.hadoop.hbase.master.BaseScanner.chore(BaseScanner.java:137)
    org.apache.hadoop.hbase.Chore.run(Chore.java:65)

I only see messages from the MetaScanner scanner in the master log, nothing from RootScanner."
HBASE-2536	"The hbase.hstore.blockingStoreFiles threshold is currently across the entire region, which means that if the number of column families multiplied by hbase.hstore.compactionThreshold is greater than the threshold, writes will freeze up, since we neither flush nor compact.

We should set this threshold per store, not per region, probably."
HBASE-1497	"Web UI seems to work when running in pseudo-distributed mode.  However, when running on our dev cluster the master UI does not work.

Navigating (any browser, tried many) to:  http://master:60010/master.jsp

{noformat}
HTTP ERROR: 404
/master.jsp
RequestURI=/master.jsp
Powered by Jetty://
{noformat}

http://master:60010/  gives a directory listing, showing webapps/

Eventually leading to http://dn0:60010/webapps/master/master.jsp which gives a 404 as above."
HBASE-1675	During cluster idle and rebalance both META and ROOT tables become unavailable leading to cascading NotServingRegion exceptions and cluster unavailability.
HBASE-774	"Looking at our 70-node cluster, whose current state is mostly read-only but for a service that is trickling in pages at about 5/600 an hour, network traffic goes through the roof every 30 minutes.  This is 0.1.3.  Regionserver logs are silent but for the optional flush that runs every 30 minutes.  The optional flush will put a few small files out on the filesystem tripping compactions of small files."
HBASE-151	"Here are the log lines for the region.  Was undergoing heavy load at the time.

{code}
...
2008-01-28 17:37:22,070 INFO org.apache.hadoop.hbase.HRegion: Unblocking updates for region test_table,000000000844535,1201569706879 'IPC Server handler 7 on 60020'
2008-01-28 17:37:22,116 INFO org.apache.hadoop.hbase.HRegion: Unblocking updates for region test_table,000000000844535,1201569706879 'IPC Server handler 8 on 60020'
2008-01-28 17:37:22,117 INFO org.apache.hadoop.hbase.HRegion: Unblocking updates for region test_table,000000000844535,1201569706879 'IPC Server handler 0 on 60020'
2008-01-28 17:37:56,713 INFO org.apache.hadoop.hbase.HRegion: compaction completed on region test_table,000000000844535,1201569706879. Took 9mins, 34sec
2008-01-28 17:37:56,713 INFO org.apache.hadoop.hbase.HRegion: starting compaction on region test_table,000000000697863,1201569706879
2008-01-28 17:37:56,905 INFO org.apache.hadoop.hbase.HRegion: Splitting test_table,000000000844535,1201569706879 because largest aggregate size is 1.3g and desired size is 256.0m
2008-01-28 17:37:56,960 DEBUG org.apache.hadoop.hbase.HRegion: waiting for cache flush to complete for region test_table,000000000844535,1201569706879
2008-01-28 17:38:26,498 DEBUG org.apache.hadoop.hbase.HRegion: Finished memcache flush for region test_table,000000000844535,1201569706879 in 68738ms, sequenceid=559406
2008-01-28 17:38:26,498 DEBUG org.apache.hadoop.hbase.HRegion: Started memcache flush for region test_table,000000000697863,1201569706879. Size 128.0m
2008-01-28 17:38:26,499 DEBUG org.apache.hadoop.hbase.HRegion: compactions and cache flushes disabled for region test_table,000000000844535,1201569706879
2008-01-28 17:38:26,500 DEBUG org.apache.hadoop.hbase.HRegion: new updates and scanners for region test_table,000000000844535,1201569706879 disabled
2008-01-28 17:38:26,500 DEBUG org.apache.hadoop.hbase.HRegion: no more active scanners for region test_table,000000000844535,1201569706879 
2008-01-28 17:38:26,500 DEBUG org.apache.hadoop.hbase.HRegion: no more row locks outstanding on region test_table,000000000844535,1201569706879
2008-01-28 17:38:26,647 DEBUG org.apache.hadoop.hbase.HRegionServer: test_table,000000000844535,1201569706879 closing (Adding to retiringRegions)
2008-01-28 17:38:26,647 DEBUG org.apache.hadoop.hbase.HRegion: Started memcache flush for region test_table,000000000844535,1201569706879. Size 77.7m
2008-01-28 17:38:36,316 INFO org.apache.hadoop.hbase.HRegion: Unblocking updates for region test_table,000000000697863,1201569706879 'IPC Server handler 9 on 60020'
2008-01-28 17:38:36,317 INFO org.apache.hadoop.hbase.HRegion: Unblocking updates for region test_table,000000000697863,1201569706879 'IPC Server handler 2 on 60020'
2008-01-28 17:38:36,321 INFO org.apache.hadoop.hbase.HRegion: Unblocking updates for region test_table,000000000697863,1201569706879 'IPC Server handler 4 on 60020'
2008-01-28 17:39:21,023 DEBUG org.apache.hadoop.hbase.HRegion: Finished memcache flush for region test_table,000000000697863,1201569706879 in 54525ms, sequenceid=572619
2008-01-28 17:39:21,077 DEBUG org.apache.hadoop.hbase.HRegion: Started memcache flush for region test_table,000000000697863,1201569706879. Size 12.4m
2008-01-28 17:39:51,129 DEBUG org.apache.hadoop.hbase.HRegion: Finished memcache flush for region test_table,000000000844535,1201569706879 in 84482ms, sequenceid=573888
2008-01-28 17:39:51,130 DEBUG org.apache.hadoop.hbase.HRegionServer: test_table,000000000844535,1201569706879 closed
2008-01-28 17:39:51,130 INFO org.apache.hadoop.hbase.HRegion: closed test_table,000000000844535,1201569706879 
2008-01-28 17:39:51,130 DEBUG org.apache.hadoop.hbase.HRegionServer: test_table,000000000844535,1201569706879 closed
2008-01-28 17:39:55,664 DEBUG org.apache.hadoop.hbase.HRegion: Finished memcache flush for region test_table,000000000697863,1201569706879 in 34587ms, sequenceid=5738892008-01-28 17:40:06,756 INFO org.apache.hadoop.hbase.HRegionServer: region split, META updated, and report to master all successful. Old region=test_table,000000000844535,1201569706879, new regions: test_table,000000000844535,1201570676957, test_table,000000000920444,1201570676959. Split took 1mins, 40sec
2008-01-28 17:44:16,033 INFO org.apache.hadoop.hbase.HRegion: compaction completed on region test_table,000000000697863,1201569706879. Took 6mins, 19sec
2008-01-28 17:44:16,262 DEBUG org.apache.hadoop.hbase.HRegion: region test_table,000000000844535,1201569706879 does not need compaction
2008-01-28 17:44:16,262 INFO org.apache.hadoop.hbase.HRegion: starting compaction on region test_table,000000000697863,1201569706879
2008-01-28 17:44:16,285 INFO org.apache.hadoop.hbase.HRegion: Splitting test_table,000000000697863,1201569706879 because largest aggregate size is 1.4g and desired size is 256.0m
2008-01-28 17:44:16,288 DEBUG org.apache.hadoop.hbase.HRegion: waiting for compaction to complete for region test_table,000000000697863,1201569706879
2008-01-28 17:49:33,196 INFO org.apache.hadoop.hbase.HRegion: compaction completed on region test_table,000000000697863,1201569706879. Took 5mins, 16sec
2008-01-28 17:49:33,196 DEBUG org.apache.hadoop.hbase.HRegion: compactions and cache flushes disabled for region test_table,000000000697863,1201569706879
2008-01-28 17:49:33,196 DEBUG org.apache.hadoop.hbase.HRegion: region test_table,000000000697863,1201569706879 does not need compaction
2008-01-28 17:49:33,196 DEBUG org.apache.hadoop.hbase.HRegion: new updates and scanners for region test_table,000000000697863,1201569706879 disabled
2008-01-28 17:49:33,196 DEBUG org.apache.hadoop.hbase.HRegion: no more active scanners for region test_table,000000000697863,1201569706879
2008-01-28 17:49:33,196 DEBUG org.apache.hadoop.hbase.HRegion: region test_table,000000000697863,1201569706879 does not need compaction
2008-01-28 17:49:33,196 DEBUG org.apache.hadoop.hbase.HRegion: no more row locks outstanding on region test_table,000000000697863,1201569706879
2008-01-28 17:49:33,196 DEBUG org.apache.hadoop.hbase.HRegionServer: test_table,000000000697863,1201569706879 closing (Adding to retiringRegions)
2008-01-28 17:49:33,196 DEBUG org.apache.hadoop.hbase.HRegion: region test_table,000000000844535,1201569706879 does not need compaction
2008-01-28 17:49:33,197 DEBUG org.apache.hadoop.hbase.HRegion: region test_table,000000000844535,1201569706879 does not need compaction
2008-01-28 17:49:33,197 DEBUG org.apache.hadoop.hbase.HRegion: region test_table,000000000844535,1201569706879 does not need compaction
2008-01-28 17:49:33,197 DEBUG org.apache.hadoop.hbase.HRegion: region test_table,000000000697863,1201569706879 does not need compaction
2008-01-28 17:49:33,197 DEBUG org.apache.hadoop.hbase.HRegionServer: test_table,000000000697863,1201569706879 closed
2008-01-28 17:49:33,197 INFO org.apache.hadoop.hbase.HRegion: closed test_table,000000000697863,1201569706879
2008-01-28 17:49:33,197 DEBUG org.apache.hadoop.hbase.HRegion: region test_table,000000000697863,1201569706879 does not need compaction
2008-01-28 17:49:33,197 DEBUG org.apache.hadoop.hbase.HRegionServer: test_table,000000000697863,1201569706879 closed
2008-01-28 17:49:33,198 DEBUG org.apache.hadoop.hbase.HRegion: region test_table,000000000844535,1201569706879 does not need compaction
2008-01-28 17:49:33,198 DEBUG org.apache.hadoop.hbase.HRegion: region test_table,000000000697863,1201569706879 does not need compaction
2008-01-28 17:49:33,198 DEBUG org.apache.hadoop.hbase.HRegion: region test_table,000000000697863,1201569706879 does not need compaction
2008-01-28 17:49:34,773 INFO org.apache.hadoop.hbase.HRegionServer: region split, META updated, and report to master all successful. Old region=test_table,000000000697863,1201569706879, new regions: test_table,000000000697863,1201571056287, test_table,000000000771750,1201571056288. Split took 1sec
2008-01-28 19:09:32,812 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 60020, call batchUpdate(test_table,000000000844535,1201569706879, 9223372036854775807, org.apache.hadoop.hbase.io.BatchUpdate@53533c55) from XX.XX.XX.58:59891: error: org.apache.hadoop.hbase.NotServingRegionException: test_table,000000000844535,1201569706879
org.apache.hadoop.hbase.NotServingRegionException: test_table,000000000844535,1201569706879
2008-01-28 19:09:32,813 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 60020, call batchUpdate(test_table,000000000844535,1201569706879, 9223372036854775807, org.apache.hadoop.hbase.io.BatchUpdate@170aeb17) from XX.XX.XX.58:59891: error: org.apache.hadoop.hbase.NotServingRegionException: test_table,000000000844535,1201569706879
org.apache.hadoop.hbase.NotServingRegionException: test_table,000000000844535,1201569706879
2008-01-28 19:09:39,574 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 60020, call batchUpdate(test_table,000000000844535,1201569706879, 9223372036854775807, org.apache.hadoop.hbase.io.BatchUpdate@57ee026e) from XX.XX.XX.32:50384: error: org.apache.hadoop.hbase.NotServingRegionException: test_table,000000000844535,1201569706879
org.apache.hadoop.hbase.NotServingRegionException: test_table,000000000844535,1201569706879
2008-01-28 19:09:39,583 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 60020, call batchUpdate(test_table,000000000844535,1201569706879, 9223372036854775807, org.apache.hadoop.hbase.io.BatchUpdate@4ae1b0db) from XX.XX.XX.32:50384: error: org.apache.hadoop.hbase.NotServingRegionException: test_table,000000000844535,1201569706879
org.apache.hadoop.hbase.NotServingRegionException: test_table,000000000844535,1201569706879
...
{code}"
HBASE-21576	"Master has killed an RS that was hosting meta due to some HDFS issue (most likely; I've lost the RS logs due to HBASE-21575).
RS took a very long time to die (again, might be a separate bug, I'll file if I see repro), and a long time to restart; meanwhile master never tried to reassign meta, and eventually killed itself not being able to update it.
It seems like a RS on a bad machine would be especially prone to slow abort/startup, as well as to issues causing master to kill it, so it would make sense for master to immediately relocate meta once meta-hosting RS is dead after a kill; or even when killing the RS. In the former case (if the RS needs to die for meta to be reassigned safely), perhaps the RS hosting meta in particular should try to die fast in such circumstances, and not do any cleanup.
{noformat}
2018-12-08 04:52:55,144 WARN  [RpcServer.default.FPBQ.Fifo.handler=39,queue=4,port=17000] master.MasterRpcServices: <server1>,17020,1544264858183 reported a fatal error:
***** ABORTING region server <server1>,17020,1544264858183: Replay of WAL required. Forcing server shutdown *****
.... [aborting for ~7 minutes]
2018-12-08 04:53:44,190 INFO  [PEWorker-7] client.RpcRetryingCallerImpl: Call exception, tries=6, retries=61, started=41190 ms ago, cancelled=false, msg=org.apache.hadoop.hbase.regionserver.RegionServerAbortedException: Server <server1>,17020,1544264858183 aborting, details=row '...' on table 'hbase:meta' at region=hbase:meta,,1.1588230740, hostname=<server1>,17020,1544264858183, seqNum=-1
... [starting for ~5]
2018-12-08 04:59:58,574 INFO  [RpcServer.default.FPBQ.Fifo.handler=45,queue=0,port=17000] client.RpcRetryingCallerImpl: Call exception, tries=10, retries=61, started=392702 ms ago, cancelled=false, msg=Call to <server1> failed on connection exception: org.apache.hbase.thirdparty.io.netty.channel.ConnectTimeoutException: connection timed out: <server1>, details=row '...' on table 'hbase:meta' at region=hbase:meta,,1.1588230740, hostname=<server1>,17020,1544264858183, seqNum=-1
... [re-initializing for at least ~7]
2018-12-08 05:04:17,271 INFO  [hconnection-0x4d58bcd4-shared-pool3-t1877] client.RpcRetryingCallerImpl: Call exception, tries=6, retries=61, started=41137 ms ago, cancelled=false, msg=org.apache.hadoop.hbase.ipc.ServerNotRunningYetException: Server <server1>,17020,1544274145387 is not running yet
...
2018-12-08 05:11:18,470 ERROR [RpcServer.default.FPBQ.Fifo.handler=38,queue=3,port=17000] master.HMaster: ***** ABORTING master ...,17000,1544230401860: FAILED persisting region=... state=OPEN *****^M
{noformat}

There are no signs of meta assignment activity at all in master logs"
HBASE-13535	"hbase-1.1 will be the first release with DLR on by default. I've been running  ITBLLs on a cluster trying to find issues with DLR. My first few runs ran nicely... but the current run failed complaining regions are not online and indeed recovery is stuck making no progress.

Upon examination, it looks to be an assignment rather than DLR issue. A server carring meta has its meta log replayed first but we are seemingly failing to assign regions after meta is back online.

Meantime, my regionserver logs are filling with spewing complaint that regions are not online (we should dampen our logging of region not being online... ) and then the split log workers are stuck:

{code}
Thread 13206 (RS_LOG_REPLAY_OPS-c2021:16020-1-Writer-2):
  State: TIMED_WAITING
  Blocked count: 45
  Waited count: 59
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.hbase.wal.WALSplitter$LogReplayOutputSink.waitUntilRegionOnline(WALSplitter.java:1959)
    org.apache.hadoop.hbase.wal.WALSplitter$LogReplayOutputSink.locateRegionAndRefreshLastFlushedSequenceId(WALSplitter.java:1857)
    org.apache.hadoop.hbase.wal.WALSplitter$LogReplayOutputSink.groupEditsByServer(WALSplitter.java:1761)
    org.apache.hadoop.hbase.wal.WALSplitter$LogReplayOutputSink.append(WALSplitter.java:1674)
    org.apache.hadoop.hbase.wal.WALSplitter$WriterThread.writeBuffer(WALSplitter.java:1104)
    org.apache.hadoop.hbase.wal.WALSplitter$WriterThread.doRun(WALSplitter.java:1096)
    org.apache.hadoop.hbase.wal.WALSplitter$WriterThread.run(WALSplitter.java:1066)
Thread 13205 (RS_LOG_REPLAY_OPS-c2021:16020-1-Writer-1):
  State: TIMED_WAITING
  Blocked count: 45
  Waited count: 59
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.hbase.wal.WALSplitter$LogReplayOutputSink.waitUntilRegionOnline(WALSplitter.java:1959)
    org.apache.hadoop.hbase.wal.WALSplitter$LogReplayOutputSink.locateRegionAndRefreshLastFlushedSequenceId(WALSplitter.java:1857)
    org.apache.hadoop.hbase.wal.WALSplitter$LogReplayOutputSink.groupEditsByServer(WALSplitter.java:1761)
    org.apache.hadoop.hbase.wal.WALSplitter$LogReplayOutputSink.append(WALSplitter.java:1674)
    org.apache.hadoop.hbase.wal.WALSplitter$WriterThread.writeBuffer(WALSplitter.java:1104)
    org.apache.hadoop.hbase.wal.WALSplitter$WriterThread.doRun(WALSplitter.java:1096)
    org.apache.hadoop.hbase.wal.WALSplitter$WriterThread.run(WALSplitter.java:1066)
Thread 13204 (RS_LOG_REPLAY_OPS-c2021:16020-1-Writer-0):
  State: TIMED_WAITING
  Blocked count: 50
  Waited count: 63
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.hbase.wal.WALSplitter$LogReplayOutputSink.waitUntilRegionOnline(WALSplitter.java:1959)
    org.apache.hadoop.hbase.wal.WALSplitter$LogReplayOutputSink.locateRegionAndRefreshLastFlushedSequenceId(WALSplitter.java:1857)
    org.apache.hadoop.hbase.wal.WALSplitter$LogReplayOutputSink.groupEditsByServer(WALSplitter.java:1761)
    org.apache.hadoop.hbase.wal.WALSplitter$LogReplayOutputSink.append(WALSplitter.java:1674)
    org.apache.hadoop.hbase.wal.WALSplitter$WriterThread.writeBuffer(WALSplitter.java:1104)
    org.apache.hadoop.hbase.wal.WALSplitter$WriterThread.doRun(WALSplitter.java:1096)
    org.apache.hadoop.hbase.wal.WALSplitter$WriterThread.run(WALSplitter.java:1066)
{code}


...complaining that:

2015-04-22 21:28:02,746 DEBUG [RS_LOG_REPLAY_OPS-c2021:16020-1] wal.WALSplitter: Used 134248328 bytes of buffered edits, waiting for IO threads...

The accounting seems off around here in SSH where it is moving regions that were on dead server to OFFLINE but is reporting no regions to assign:

{code}
143320 2015-04-21 17:05:07,571 INFO  [MASTER_SERVER_OPERATIONS-c2020:16000-0] handler.ServerShutdownHandler: Mark regions in recovery for crashed server c2024.halxg.cloudera.com,16020,1429660802192 before assignment; regions=[]
143321 2015-04-21 17:05:07,572 DEBUG [MASTER_SERVER_OPERATIONS-c2020:16000-0] master.RegionStates: Adding to processed servers c2024.halxg.cloudera.com,16020,1429660802192
143322 2015-04-21 17:05:07,575 INFO  [MASTER_SERVER_OPERATIONS-c2020:16000-0] master.RegionStates: Transition {8d63312bc39a39727afea627bb20fee4 state=OPEN, ts=1429660996054, server=c2024.halxg.cloudera.com,16020,1429660802192} to {8d63312bc39a39727afea627bb20fee4 state=OFFLINE, ts=1429661107575, server=c2024.halxg.cloudera.com,16020,1429660802192}
143323 2015-04-21 17:05:07,575 INFO  [MASTER_SERVER_OPERATIONS-c2020:16000-0] master.RegionStates: Transition {64c97bf39441e09977332c02e628a8c2 state=OPEN, ts=1429660996045, server=c2024.halxg.cloudera.com,16020,1429660802192} to {64c97bf39441e09977332c02e628a8c2 state=OFFLINE, ts=1429661107575, server=c2024.halxg.cloudera.com,16020,1429660802192}
143324 2015-04-21 17:05:07,575 INFO  [MASTER_SERVER_OPERATIONS-c2020:16000-0] master.RegionStates: Transition {3f4ea5ea14653cee6006f13c7d06d10b state=OPEN, ts=1429660996066, server=c2024.halxg.cloudera.com,16020,1429660802192} to {3f4ea5ea14653cee6006f13c7d06d10b state=OFFLINE, ts=1429661107575, server=c2024.halxg.cloudera.com,16020,1429660802192}
143325 2015-04-21 17:05:07,575 INFO  [MASTER_SERVER_OPERATIONS-c2020:16000-0] master.RegionStates: Transition {6eaf51e55c9c23356a697f286f473db8 state=OPEN, ts=1429660996051, server=c2024.halxg.cloudera.com,16020,1429660802192} to {6eaf51e55c9c23356a697f286f473db8 state=OFFLINE, ts=1429661107575, server=c2024.halxg.cloudera.com,16020,1429660802192}
143326 2015-04-21 17:05:07,575 INFO  [MASTER_SERVER_OPERATIONS-c2020:16000-0] master.RegionStates: Transition {1cbe63cd29c7709209adbf4305ebc746 state=OPEN, ts=1429660996062, server=c2024.halxg.cloudera.com,16020,1429660802192} to {1cbe63cd29c7709209adbf4305ebc746 state=OFFLINE, ts=1429661107575, server=c2024.halxg.cloudera.com,16020,1429660802192}
143327 2015-04-21 17:05:07,575 INFO  [MASTER_SERVER_OPERATIONS-c2020:16000-0] master.RegionStates: Transition {3a49d17e189b2a26eb73e1d43cf2d0ac state=OPEN, ts=1429660996053, server=c2024.halxg.cloudera.com,16020,1429660802192} to {3a49d17e189b2a26eb73e1d43cf2d0ac state=OFFLINE, ts=1429661107575, server=c2024.halxg.cloudera.com,16020,1429660802192}
143328 2015-04-21 17:05:07,576 INFO  [MASTER_SERVER_OPERATIONS-c2020:16000-0] handler.ServerShutdownHandler: Reassigning 0 region(s) that c2024.halxg.cloudera.com,16020,1429660802192 was carrying (and 0 regions(s) that were opening on this server)

{code}

... and indeed these regions are never assigned.  I see DLR on occasion timing out like this....

2015-04-22 21:27:31,327 ERROR [RS_LOG_REPLAY_OPS-c2021:16020-1-Writer-0] wal.WALSplitter: Exiting thread

....

Caused by:


Caused by: org.apache.hadoop.hbase.ipc.RemoteWithExtrasException(org.apache.hadoop.hbase.NotServingRegionException): org.apache.hadoop.hbase.NotServingRegionException: Region IntegrationTestBigLinkedList,\xB3333333(,1429660019160.8d63312bc39a39727afea627bb20fee4. is not online on c2024.halxg.cloudera.com,16020,1429661908290

... which is one of the regions in the set of OFFLINE but not assigned.

Digging...."
HBASE-21343	
HBASE-21324	"When there a few regions, the balancer seems to do a poorish job. My cluster is all out of whack and while the balancer runs, it doesn't do anything. So, I need to configure the balancer to run against large number of regions. There is no doc in reguide to help and there is no logging coming out of the balancer to help either. Let me fix."
HBASE-21337	"Cluster A-HDP 3.0.0(hadoop3.0, hbase2.0); cluster B-native hadoop2.7, hbase1.2.6. 
I want to replicate a table named T(only one CF named 'f') on cluster A and B with each other, that is, the table T data of A changes will be synchronized to B, and the table T data of B changes will be synchronized to A. I configured repplication both on Cluster A and B for table T using 'add_peer' and 'enable_table_replication' by Hbase shell(firstly A to B,2ndly B to A).Then锛孖 did test in Hbase shell as below,
1.Put a record by typing ""put 'T','r1','f:a','1'"" on A,then Scan table T,it's no problem,the record can be found on both A and B;
2.Put a record by typing ""put 'T','r2','f:a','1'"" on B,no problem,both found on A and B;
3.Put a record by typing ""put 'T','r1','f:a','2'"" to update the value to '2' on A,no problem, updated successfully on both A and B;
4.Put a record by typing ""put 'T','r1','f:a','3'"" to update the value to '3' on B,no problem, updated successfully on both A and B;
5.Put a record by typing ""put 'T','r1','f:a','4'"" to update the value to '4' on A,the problem was coming, there is no update both A and B, that means this 'Put'is not effected,the value is still '3' on A and B although the Hbase shell does not give an error when I 'put'.
6.after about 1 minute, I typing the 'Put' again to update the value to '4' on A, now, it's successful, the value is updated to '4' both on A and B.

May I ask what's the reason? Anything I missed to configure?Thx."
HBASE-14320	"Our normal Jenkins setup up on builds.apache.org (e.g. [this one|https://builds.apache.org/job/HBase-TRUNK/lastCompletedBuild/testReport/]) makes it easy to get test reports on the latest builds, as well as providing some [pretty history views|https://builds.apache.org/job/HBase-TRUNK/lastCompletedBuild/testReport/history/]. This functionality seems to have been inadvertently broken when we turned the [1.2|https://builds.apache.org/job/HBase-1.2/] and [1.3|https://builds.apache.org/job/HBase-1.3/] jobs into matrix jobs in order to test jdk7 and jdk8 in parallel. Should be an easy fix on the Jenkins side to sort this out..."
HBASE-14198	"After running 

mvn eclipse:eclipse I tried to import projects into Eclipse (Luna) and got multiple build errors, similar to:
{code}
Cannot nest output folder 'hbase-thrift/target/test-classes/META-INF' inside output folder 'hbase-thrift/target/test-classes'	hbase-thrift		
{code}"
HBASE-9725	"to me, seems lost brackets for createMethodTimeMetrics"
HBASE-880	"The current API does not scale very well. For each new feature, we have to add many methods to take care of all the overloads. Also, the need to batch row operations (gets, inserts, deletes) implies that we have to manage some ""entities"" like we are able to do with BatchUpdate but not with the other operations. The RowLock should be an attribute of such an entity.

The scope of this jira is only to replace current API with another feature-compatible one, other methods will be added in other issues."
HBASE-14760	"Over in https://issues.apache.org/jira/browse/HBASE-14758?focusedCommentId=14989134&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14989134 , [~carp84] found that since HBASE-14725, the likes of TestAsyncProcess no longer runs as part of general build.

It seems like any test that gets a category-based timer in that patch no longer runs as part of the build.

The test runs fine if I invoke on cmd-line.

Other category-based timeout tests seem to show in the general build.

Looking...."
HBASE-21053	"Steps to reproduce are as follows:

1. Create a table with region replication
{code}
create ""test"", ""cf"", \{REGION_REPLICATION => 2}
{code}

2. Load data to the table
{code}
(0...2000).each\{|i| put ""test"", ""row#{i}"", ""cf:col"", ""val""}
{code}

3. Split the table
{code}
split ""test""
{code}

After that, the state of the replica region of the split parent region (info:state_0001) is CLOSING, but the correct state is CLOSED.
{code}
hbase> scan 'hbase:meta', \{FILTER => ""PrefixFilter('test')""}
ROW COLUMN+CELL
 test column=table:state, timestamp=1534271346311, value=\x08\x00
 test,,1534271345843.cc6bd353efe820b6a5ede1abfa2b05e column=info:regioninfo, timestamp=1534271348477, value={ENCODED => cc6bd353efe820b6a5ede1abfa2b05ec, NAME => 'test,,1534271345843.cc6bd353efe820b6a5ede
 c. 1abfa2b05ec.', STARTKEY => '', ENDKEY => '', OFFLINE => true, SPLIT => true}
 test,,1534271345843.cc6bd353efe820b6a5ede1abfa2b05e column=info:seqnumDuringOpen, timestamp=1534271346308, value=\x00\x00\x00\x00\x00\x00\x00\x02
 c.
 test,,1534271345843.cc6bd353efe820b6a5ede1abfa2b05e column=info:seqnumDuringOpen_0001, timestamp=1534271346309, value=\x00\x00\x00\x00\x00\x00\x00\x02
 c.
 test,,1534271345843.cc6bd353efe820b6a5ede1abfa2b05e column=info:server, timestamp=1534271346308, value=10.0.1.6:16020
 c.
 test,,1534271345843.cc6bd353efe820b6a5ede1abfa2b05e column=info:server_0001, timestamp=1534271346309, value=10.0.1.6:16020
 c.
 test,,1534271345843.cc6bd353efe820b6a5ede1abfa2b05e column=info:serverstartcode, timestamp=1534271346308, value=1534267943127
 c.
 test,,1534271345843.cc6bd353efe820b6a5ede1abfa2b05e column=info:serverstartcode_0001, timestamp=1534271346309, value=\x00\x00\x01e9~\xE8\xD7
 c.
 test,,1534271345843.cc6bd353efe820b6a5ede1abfa2b05e column=info:sn, timestamp=1534271348271, value=10.0.1.6,16020,1534267943127
 c.
 test,,1534271345843.cc6bd353efe820b6a5ede1abfa2b05e column=info:sn_0001, timestamp=1534271348271, value=10.0.1.6,16020,1534267943127
 c.
 test,,1534271345843.cc6bd353efe820b6a5ede1abfa2b05e column=info:splitA, timestamp=1534271348477, value={ENCODED => fe68bd328e37ab8a9cfa5616d7a5bb14, NAME => 'test,,1534271348168.fe68bd328e37ab8a9cfa5616d
 c. 7a5bb14.', STARTKEY => '', ENDKEY => 'row78'}
 test,,1534271345843.cc6bd353efe820b6a5ede1abfa2b05e column=info:splitB, timestamp=1534271348477, value={ENCODED => b77c219092dbbe4376d72868099f1386, NAME => 'test,row78,1534271348168.b77c219092dbbe4376d7
 c. 2868099f1386.', STARTKEY => 'row78', ENDKEY => ''}
 test,,1534271345843.cc6bd353efe820b6a5ede1abfa2b05e column=info:state, timestamp=1534271348436, value=CLOSED
 c.
 test,,1534271345843.cc6bd353efe820b6a5ede1abfa2b05e column=info:state_0001, timestamp=1534271348271, value=CLOSING聽 <-----聽 Wrong state!
 c.
...
{code}"
HBASE-21079	"Go to any table from table.jsp and try compact/split a table region.

Even when the hbase is secure any user can split/compact the table.

The same behavior is not聽 from the hbase shell and work as expected.

聽

聽"
HBASE-20972	"Call queue size is the currently queued and running Calls bytes size. It gets incremented after we parse a call and before we add it to the queue of calls for the scheduler to use. It get decremented after we have 'run' the Call. 

When setting up a call, total size of it is added. So when a new call can not be dispatched by BlockingQueue full, the call queue size should be decremented. We shouldn't add size of rejected calls to the call queue size."
HBASE-19329	"
2017-11-16 02:50:33,474 WARN  [blackstone064030,16020,1510632966258_ChoreService_1] quotas.QuotaCache: Unable to read user from quota table
org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 3 actions: Table 'hbase:quota' was not found, got: hbase:namespace.: 3 times, servers with issues: null
,
	at org.apache.hadoop.hbase.quotas.QuotaTableUtil.doGet(QuotaTableUtil.java:330)
	at org.apache.hadoop.hbase.quotas.QuotaUtil.fetchUserQuotas(QuotaUtil.java:155)
	at org.apache.hadoop.hbase.quotas.QuotaCache$QuotaRefresherChore$3.fetchEntries(QuotaCache.java:256)
	at org.apache.hadoop.hbase.quotas.QuotaCache$QuotaRefresherChore.fetch(QuotaCache.java:290)
	at org.apache.hadoop.hbase.quotas.QuotaCache$QuotaRefresherChore.fetchUserQuotaState(QuotaCache.java:248)
	at org.apache.hadoop.hbase.quotas.QuotaCache$QuotaRefresherChore.chore(QuotaCache.java:213)
2017-11-16 02:55:33,453 WARN  [blackstone064030,16020,1510632966258_ChoreService_1] quotas.QuotaCache: Unable to read namespace from quota table
org.apache.hadoop.hbase.TableNotFoundException: Table 'hbase:quota' was not found, got: hbase:namespace.
	at org.apache.hadoop.hbase.quotas.QuotaTableUtil.doGet(QuotaTableUtil.java:330)
	at org.apache.hadoop.hbase.quotas.QuotaUtil.fetchGlobalQuotas(QuotaUtil.java:220)
	at org.apache.hadoop.hbase.quotas.QuotaUtil.fetchNamespaceQuotas(QuotaUtil.java:207)
	at org.apache.hadoop.hbase.quotas.QuotaCache$QuotaRefresherChore$1.fetchEntries(QuotaCache.java:226)
	at org.apache.hadoop.hbase.quotas.QuotaCache$QuotaRefresherChore.fetch(QuotaCache.java:290)
	at org.apache.hadoop.hbase.quotas.QuotaCache$QuotaRefresherChore.fetchNamespaceQuotaState(QuotaCache.java:218)
	at org.apache.hadoop.hbase.quotas.QuotaCache$QuotaRefresherChore.chore(QuotaCache.java:211)
2017-11-16 02:55:33,488 WARN  [blackstone064030,16020,1510632966258_ChoreService_1] quotas.QuotaCache: Unable to read table from quota table
org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 47 actions: Table 'hbase:quota' was not found, got: hbase:namespace.: 47 times, servers with issues: nu
ll,"
HBASE-19025	"MasterProcWALs getting pile up and getting below error in Hmaster log
2017-10-17 03:34:42,141 INFO  [ip-176-0-0-44:16000.activeMasterManager] hdfs.DFSClient: Successfully connected to /176.0.0.29:50010 for BP-1665161556-176.0.
0.44-1498805562805:blk_1073773522_32746
2017-10-17 03:34:42,182 INFO  [ip-176-0-0-44:16000.activeMasterManager] util.FSHDFSUtils: Recover lease on dfs file hdfs://192.168.168.1:9000/hbase2/MasterProcWALs/state-00000000000000005592.log
2017-10-17 03:34:42,182 INFO  [ip-176-0-0-44:16000.activeMasterManager] util.FSHDFSUtils: Recovered lease, attempt=0 on file=hdfs://ec2-34-195-113-113.compu
te-1.amazonaws.com:9000/hbase2/MasterProcWALs/state-00000000000000005592.log after 0ms
2017-10-17 03:34:42,184 WARN  [ip-176-0-0-44:16000.activeMasterManager] hdfs.BlockReaderFactory: I/O error constructing remote block reader.
java.io.IOException: Got error, status message opReadBlock BP-1665161556-176.0.0.44-1498805562805:blk_1073773523_32747 received exception java.io.FileNotFou
ndException: /data/datanode/data/current/BP-1665161556-176.0.0.44-1498805562805/current/finalized/subdir0/subdir123/blk_1073773523_32747.meta (Too many open
 files), for OP_READ_BLOCK, self=/176.0.0.44:59340, remote=/176.0.0.44:50010, for file /hbase2/MasterProcWALs/state-00000000000000005592.log, for pool BP-16
65161556-176.0.0.44-1498805562805 block 1073773523_32747
"
HBASE-20675	"we use [withStopRow|https://hbase.apache.org/2.0/apidocs/org/apache/hadoop/hbase/client/Scan.html#withStopRow-byte:A-boolean-] API to scan a rowkey range [startrow, stoprow], both inclusive, but the server can't return the last row including stoprow, however there is no exception.

For example, the there are the following rows in hbase:

||rowkey||CF+qualifier+value||
|1|{value1}|
|2|{value2}
|3|{value3}|


{code:java}
// Do scan like this
Scan scan = new Scan();
scan.withStartRow(bytes(1), true);
scan.withStopRow(bytes(3), true);
{code}
The result returned only contains the first two rows: 鈥?鈥?and 鈥?鈥? no 鈥?鈥?

Thanks.

"
HBASE-20762	"When a precommit run fails due to license issues, we get pointed to a file in our maven logs:

{noformat}
/testptch/hbase/hbase-assembly/target/maven-shared-archive-resources/META-INF/LICENSE
{noformat}

But we don't have that file saved, so we don't know what the actual failure was. So we should save that in our build artifacts. Or maybe we can print a snippet from that file directly into the maven log. Both would be acceptable."
HBASE-18586	"I have 2 HBase tables - one with a single column family, and other has 4 column families. Both tables are keyed by same rowkey, and the column families all have a single column qualifier each, with a json string as value (each json payload is about 10-20K in size). All column families use fast-diff encoding and gzip compression.

After loading about 60MM rows to each table, a scan test on (any) single column family in the 2nd table takes 4x the time to scan the single column family from the 1st table. In both cases, the scanner is bounded by a start and stop key to scan 1MM rows. Performance did not change much even after running a major compaction on both tables.

Though HBase doc and other tech forums recommend not using more than 1 column family per table, nothing I have read so far suggests scan performance will linearly degrade based on number of column families. Has anyone else experienced this, and is there a simple explanation for this?

To note, the reason second table has 4 column families is even though I only scan one column family at a time now, there are requirements to scan multiple column families from that table given a set of rowkeys.

Thanks for any insight into the performance question."
HBASE-19037	"I'm testing the speed. At the time of the request, I know part of the key.
```
scan 'id_bank', {STARTROW=>""24168557""+""\137"",STOPROW=>""24168557""+""\177"",COLUMNS => ['high', 'low'], BLOCKCACHE => 'true'}
```
When I run the scan, the response returns a short time, and if I make a second request, the answer is already returned quickly, why?"
HBASE-17948	"I have installed HBase (1.2.4) Hadoop(2.7.3) on Master slave multi cluster combination.

Master

/etc/hosts/
127.0.0.1 developer4
192.168.1.149  master 
192.168.1.161  slave

Slave

127.0.0.1  cilm063-ThinkCentre-A58
192.168.1.149   master
192.168.1.161   slave

I have properly installed Hbase and working fine.

When I run the below command then getting following error.



hbase hbck details

ERROR: RegionServer: cilm063-thinkcentre-a58,16020,1492865892520 Unable to fetch region information. java.net.UnknownHostException: cilm063-thinkcentre-a58


Please help me  where I am doing wrong.
"
HBASE-19600	Placeholder for myself. Running ITBLL on cluster w/ no chaos fails to complete. We are missing connections. Digging. Cluster is 7 node running 8 tasks of 25M each.
HBASE-19874	"hbase聽 configure聽 聽 bucketcache锛屄燩roduction environment聽 聽exist聽is not deleted銆?

deleted file聽 number聽reached more than a few hundred锛?and聽Growing銆偮犅燤emory is growing.

$ ll|grep delete
lr-x------ 1 data data 64 Jan 28 14:28 1048 -> /block4/hadoop/dfs/data/current/BP-1101579887-10.50.64.23-1497104043858/current/finalized/subdir247/subdir216/blk_3036141819 (deleted)
lr-x------ 1 data data 64 Jan 28 14:28 1050 -> /block4/hadoop/dfs/data/current/BP-1101579887-10.50.64.23-1497104043858/current/finalized/subdir247/subdir216/blk_3036141819_1962457009.meta (deleted)
lr-x------ 1 data data 64 Jan 28 14:28 1078 -> /block5/hadoop/dfs/data/current/BP-1101579887-10.50.64.23-1497104043858/current/finalized/subdir247/subdir62/blk_3036102314 (deleted)
lr-x------ 1 data data 64 Jan 28 14:28 1079 -> /block7/hadoop/dfs/data/current/BP-1101579887-10.50.64.23-1497104043858/current/finalized/subdir247/subdir95/blk_3036110832 (deleted)
lr-x------ 1 data data 64 Jan 28 14:28 1091 -> /block3/hadoop/dfs/data/current/BP-1101579887-10.50.64.23-1497104043858/current/finalized/subdir245/subdir53/blk_3035968976_1962284166.meta (deleted)
lr-x------ 1 data data 64 Jan 28 14:28 1092 -> /block9/hadoop/dfs/data/current/BP-1101579887-10.50.64.23-1497104043858/current/finalized/subdir247/subdir97/blk_3036111332 (deleted)
lr-x------ 1 data data 64 Jan 28 14:28 1093 -> /block9/hadoop/dfs/data/current/BP-1101579887-10.50.64.23-1497104043858/current/finalized/subdir247/subdir97/blk_3036111332_1962426522.meta (deleted)
lr-x------ 1 data data 64 Jan 28 14:28 1096 -> /block5/hadoop/dfs/data/current/BP-1101579887-10.50.64.23-1497104043858/current/finalized/subdir247/subdir62/blk_3036102314_1962417504.meta (deleted)
lr-x------ 1 data data 64 Jan 28 14:28 1100 -> /block7/hadoop/dfs/data/current/BP-1101579887-10.50.64.23-1497104043858/current/finalized/subdir247/subdir95/blk_3036110832_1962426022.meta (deleted)
lr-x------ 1 data data 64 Jan 28 14:28 1101 -> /block10/hadoop/dfs/data/current/BP-1101579887-10.50.64.23-1497104043858/current/finalized/subdir245/subdir13/blk_3035958550 (deleted)
lr-x------ 1 data data 64 Jan 28 14:28 1102 -> /block10/hadoop/dfs/data/current/BP-1101579887-10.50.64.23-1497104043858/current/finalized/subdir245/subdir13/blk_3035958550_1962273740.meta (deleted)

聽"
HBASE-19669	"Online HBase services, large requests锛?  hbase version:  1.3.1    hadoop version:  2.7.4   os  linux version: centos6.5

hdfs-site.xml  conf:   
<property>
<name>dfs.client.read.shortcircuit</name>
<value>true</value>
<source>hdfs-site.xml</source>
</property>
<property>
<name>dfs.domain.socket.path</name>
<value>/var/run/hadoop-hdfs/dn_socket</value>
<source>hdfs-site.xml</source>
</property>
 
os   linux  command :    netstat -an  

unix  2      [ ACC ]     STREAM     LISTENING     10576083 /var/run/hadoop-hdfs/dn_socket  

*{color:#d04437}problem :    /var/run/hadoop-hdfs/dn_socket   only   one   LISTENING锛? but  no  CONNECTED {color}*"
HBASE-19800	"while using list command in hbase shell, most of all works well except which one contains 'd' char, as well as hbase program api prefix regex.

eg.聽 list 'd.\*' wont work, but '^d\[\da-f\]\{31\}' works well. and 'd.\*' performs just listing all of the tables.

!image-2018-01-16-12-13-25-723.png!

聽"
HBASE-19727	"I have started a cluster with 5 RSes, but after only two RSes were online. I've checked one of the logs, it keeps trying to report to the backup HMaster."
HBASE-18969	"{code}RowFilter(=, 'regexstring:(?s)^.{4}\\Q\xc4\x98\\E.{1}(?:.{6})*\\Q\x00\x00\x0a\x00\x00\x05\\E(?:.{6})*$'){code}

can  match 

{code}\x00\x00\x03Y\xC4\x98\xD0\x00\x00\x0A\x00\x00\x05 column=t:[\xC0\x8C1, timestamp=1507530755374, value={\x00\xE1\x00 {code}

but
{code}RowFilter(=, 'regexstring:(?s)^.{4}\\Q\xc4\\E.{2}(?:.{6})*\\Q\x00\x00\x0a\x00\x00\x05\\E(?:.{6})*$'){code}
can not match"
HBASE-18947	"Take backup of test1,test2,test3,test11,test12,test13 

and then take backup of only test2

{code}./hbase backup -d create incremental hdfs://localhost:8020/test/ -t test2{code}

It should only backup test2 but it backup all tables once backed up. This can be seen in hdfs as backed up tables and logs show the same : 

Logs show :
2017-09-25 19:29:39,170 DEBUG [main] impl.IncrementalTableBackupClient: For incremental backup, current table set is [test1,test2,test3,test11, test12,test13]

"
HBASE-18074	"HBASE-12751 did this:

{code}
...
         // If we haven't got any rows in our batch, we should block to
         // get the next one.
-        boolean shouldBlock = numReadyToWrite == 0;
         RowLock rowLock = null;
         try {
-          rowLock = getRowLockInternal(mutation.getRow(), shouldBlock);
+          rowLock = getRowLock(mutation.getRow(), true);
         } catch (IOException ioe) {
           LOG.warn(""Failed getting lock in batch put, row=""
             + Bytes.toStringBinary(mutation.getRow()), ioe);
         }
         if (rowLock == null) {
           // We failed to grab another lock
..
{code}

In old codebase, getRowLock with a true meant do not wait on row lock. In the HBASE-12751 codebase, the flag is read/write. So, we get a read lock on every mutation in the batch. If ten mutations in a batch on average, then we'll 10x the amount of locks.

I'm in here because interesting case where increments and batch going into same row seem to backup and stall trying to get locks. Looks like this where all handlers are one of either of the below:

{code}
""RpcServer.FifoWFPBQ.default.handler=190,queue=10,port=60020"" #243 daemon prio=5 os_prio=0 tid=0x00007fbb58691800 nid=0x2d2527 waiting on condition [0x00007fbb4ca49000]
   java.lang.Thread.State: TIMED_WAITING (parking)
  at sun.misc.Unsafe.park(Native Method)
  - parking to wait for  <0x00000007c6001b38> (a java.util.concurrent.locks.ReentrantReadWriteLock$FairSync)
  at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
  at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireNanos(AbstractQueuedSynchronizer.java:934)
  at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireNanos(AbstractQueuedSynchronizer.java:1247)
  at java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.tryLock(ReentrantReadWriteLock.java:1115)
  at org.apache.hadoop.hbase.regionserver.HRegion.getRowLockInternal(HRegion.java:5171)
  at org.apache.hadoop.hbase.regionserver.HRegion.doIncrement(HRegion.java:7453)
...
{code}

{code}
""RpcServer.FifoWFPBQ.default.handler=180,queue=0,port=60020"" #233 daemon prio=5 os_prio=0 tid=0x00007fbb586ed800 nid=0x2d251d waiting on condition [0x00007fbb4d453000]
   java.lang.Thread.State: TIMED_WAITING (parking)
  at sun.misc.Unsafe.park(Native Method)
  - parking to wait for  <0x0000000354976c00> (a java.util.concurrent.locks.ReentrantReadWriteLock$FairSync)
  at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
  at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1037)
  at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1328)
  at java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock.tryLock(ReentrantReadWriteLock.java:871)
  at org.apache.hadoop.hbase.regionserver.HRegion.getRowLockInternal(HRegion.java:5171)
  at org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchMutation(HRegion.java:3017)
...
{code}

It gets so bad it looks like deadlock but if you give it a while, we move on (I put it down to safe point giving a misleading view on what is happening).

Let me put back the optimization."
HBASE-20629	"{code:java}
bin/oozied.sh start{code}
聽
{code:java}
Setting up oozie DB

Validate DB Connection
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/util/ReflectionUtils
at org.apache.oozie.service.Services.setServiceInternal(Services.java:377)
at org.apache.oozie.service.Services.<init>(Services.java:111)
at org.apache.oozie.tools.OozieDBCLI.getJdbcConf(OozieDBCLI.java:169)
at org.apache.oozie.tools.OozieDBCLI.createConnection(OozieDBCLI.java:918)
at org.apache.oozie.tools.OozieDBCLI.validateConnection(OozieDBCLI.java:926)
at org.apache.oozie.tools.OozieDBCLI.createDB(OozieDBCLI.java:188)
at org.apache.oozie.tools.OozieDBCLI.run(OozieDBCLI.java:131)
at org.apache.oozie.tools.OozieDBCLI.main(OozieDBCLI.java:79)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.util.ReflectionUtils
at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
... 8 more{code}
{code:java}
/opt/oozie/oozie-5.1.0-SNAPSHOT/bin/oozie-setup.sh sharelib create -fs hdfs://hadoop.example.com:8020 -locallib ~/share/{code}
{code:java}
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/fs/Path
at org.apache.oozie.tools.OozieSharelibCLI.run(OozieSharelibCLI.java:153)
at org.apache.oozie.tools.OozieSharelibCLI.main(OozieSharelibCLI.java:67)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.fs.Path
at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
... 2 more{code}"
HBASE-19668	"Online HBase services, large requests锛?  hbase version:  1.3.1    hadoop version:  2.7.4   os  linux version: centos6.5

hdfs-site.xml  conf:   
<property>
<name>dfs.client.read.shortcircuit</name>
<value>true</value>
<source>hdfs-site.xml</source>
</property>
<property>
<name>dfs.domain.socket.path</name>
<value>/var/run/hadoop-hdfs/dn_socket</value>
<source>hdfs-site.xml</source>
</property>
 
os   linux  command :    netstat -an  

unix  2      [ ACC ]     STREAM     LISTENING     10576083 /var/run/hadoop-hdfs/dn_socket  

*{color:#d04437}problem :    /var/run/hadoop-hdfs/dn_socket   only   one   LISTENING锛? but  no  CONNECTED {color}*"
HBASE-20696	"{code}
hbase(main):020:0> list_peers
 PEER_ID CLUSTER_KEY ENDPOINT_CLASSNAME REMOTE_ROOT_DIR SYNC_REPLICATION_STATE STATE REPLICATE_ALL NAMESPACES TABLE_CFS BANDWIDTH SERIAL
 1 lg-hadoop-tst-st01.bj:10010,lg-hadoop-tst-st02.bj:10010,lg-hadoop-tst-st03.bj:10010:/hbase/test-hbase-slave nil hdfs://lg-hadoop-tst-st01.bj:20100/hbase/test-hbase-slave/remoteWALs ACTIVE ENABLED false  default.ycsb-test 0 false
1 row(s)
Took 0.0446 seconds                                                                                                    
=> #<Java::JavaUtil::ArrayList:0x43ab9ae9>    ----> It's useless .. 
{code}

Interested contributors are welcome to fix this bug..."
HBASE-20351	"{code}
stack@ve0524:~$ ./hbase/bin/hbase --config conf_hbase shell
2018-04-04 19:58:02,187 DEBUG [main] logging.InternalLoggerFactory: Using SLF4J as the default logging framework
2018-04-04 19:58:02,191 DEBUG [main] util.ResourceLeakDetector: -Dorg.apache.hbase.thirdparty.io.netty.leakDetection.level: simple
2018-04-04 19:58:02,192 DEBUG [main] util.ResourceLeakDetector: -Dorg.apache.hbase.thirdparty.io.netty.leakDetection.targetRecords: 4
2018-04-04 19:58:02,214 DEBUG [main] internal.PlatformDependent0: -Dio.netty.noUnsafe: false
2018-04-04 19:58:02,215 DEBUG [main] internal.PlatformDependent0: Java version: 8
2018-04-04 19:58:02,216 DEBUG [main] internal.PlatformDependent0: sun.misc.Unsafe.theUnsafe: available
2018-04-04 19:58:02,216 DEBUG [main] internal.PlatformDependent0: sun.misc.Unsafe.copyMemory: available
2018-04-04 19:58:02,217 DEBUG [main] internal.PlatformDependent0: java.nio.Buffer.address: available
2018-04-04 19:58:02,217 DEBUG [main] internal.PlatformDependent0: direct buffer constructor: available
2018-04-04 19:58:02,218 DEBUG [main] internal.PlatformDependent0: java.nio.Bits.unaligned: available, true
2018-04-04 19:58:02,218 DEBUG [main] internal.PlatformDependent0: jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
2018-04-04 19:58:02,218 DEBUG [main] internal.PlatformDependent0: java.nio.DirectByteBuffer.<init>(long, int): available
2018-04-04 19:58:02,218 DEBUG [main] internal.PlatformDependent: sun.misc.Unsafe: available
2018-04-04 19:58:02,218 DEBUG [main] internal.PlatformDependent: -Dio.netty.tmpdir: /tmp (java.io.tmpdir)
2018-04-04 19:58:02,218 DEBUG [main] internal.PlatformDependent: -Dio.netty.bitMode: 64 (sun.arch.data.model)
2018-04-04 19:58:02,219 DEBUG [main] internal.PlatformDependent: -Dio.netty.noPreferDirect: false
2018-04-04 19:58:02,219 DEBUG [main] internal.PlatformDependent: -Dio.netty.maxDirectMemory: 1073741824 bytes
2018-04-04 19:58:02,219 DEBUG [main] internal.PlatformDependent: -Dio.netty.uninitializedArrayAllocationThreshold: -1
2018-04-04 19:58:02,220 DEBUG [main] internal.CleanerJava6: java.nio.ByteBuffer.cleaner(): available
2018-04-04 19:58:02,220 DEBUG [main] util.ResourceLeakDetectorFactory: Loaded default ResourceLeakDetector: org.apache.hbase.thirdparty.io.netty.util.ResourceLeakDetector@7dbae40
2018-04-04 19:58:02,229 DEBUG [main] internal.PlatformDependent: org.jctools-core.MpscChunkedArrayQueue: available
2018-04-04 19:58:02,260 DEBUG [main] channel.MultithreadEventLoopGroup: -Dio.netty.eventLoopThreads: 96
2018-04-04 19:58:02,282 DEBUG [main] nio.NioEventLoop: -Dio.netty.noKeySetOptimization: false
2018-04-04 19:58:02,282 DEBUG [main] nio.NioEventLoop: -Dio.netty.selectorAutoRebuildThreshold: 512
HBase Shell
Use ""help"" to get list of supported commands.
Use ""exit"" to quit this interactive shell.
Version 2.0.0, r0db342d312784a6663b406fdb0f7b3b3c1fa928d, Mon Apr  2 22:54:56 PDT 2018
Took 0.0028 seconds
hbase(main):001:0>
{code}


Does it each time I run a command



{code}
hbase(main):001:0> describe 'ycsb'
2018-04-04 19:59:00,084 DEBUG [main] buffer.AbstractByteBuf: -Dorg.apache.hbase.thirdparty.io.netty.buffer.bytebuf.checkAccessible: true
2018-04-04 19:59:00,084 DEBUG [main] util.ResourceLeakDetectorFactory: Loaded default ResourceLeakDetector: org.apache.hbase.thirdparty.io.netty.util.ResourceLeakDetector@66ab924
2018-04-04 19:59:00,121 DEBUG [main] channel.DefaultChannelId: -Dio.netty.processId: 697 (auto-detected)
2018-04-04 19:59:00,123 DEBUG [main] util.NetUtil: -Djava.net.preferIPv4Stack: true
2018-04-04 19:59:00,123 DEBUG [main] util.NetUtil: -Djava.net.preferIPv6Addresses: false
2018-04-04 19:59:00,124 DEBUG [main] util.NetUtil: Loopback interface: lo (lo, 127.0.0.1)
2018-04-04 19:59:00,125 DEBUG [main] util.NetUtil: /proc/sys/net/core/somaxconn: 128
2018-04-04 19:59:00,125 DEBUG [main] channel.DefaultChannelId: -Dio.netty.machineId: 00:1e:67:ff:fe:c5:54:b4 (auto-detected)
2018-04-04 19:59:00,130 DEBUG [main] internal.InternalThreadLocalMap: -Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
2018-04-04 19:59:00,131 DEBUG [main] internal.InternalThreadLocalMap: -Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
2018-04-04 19:59:00,151 DEBUG [main] buffer.PooledByteBufAllocator: -Dio.netty.allocator.numHeapArenas: 82
2018-04-04 19:59:00,151 DEBUG [main] buffer.PooledByteBufAllocator: -Dio.netty.allocator.numDirectArenas: 10
2018-04-04 19:59:00,151 DEBUG [main] buffer.PooledByteBufAllocator: -Dio.netty.allocator.pageSize: 8192
2018-04-04 19:59:00,151 DEBUG [main] buffer.PooledByteBufAllocator: -Dio.netty.allocator.maxOrder: 11
2018-04-04 19:59:00,151 DEBUG [main] buffer.PooledByteBufAllocator: -Dio.netty.allocator.chunkSize: 16777216
2018-04-04 19:59:00,152 DEBUG [main] buffer.PooledByteBufAllocator: -Dio.netty.allocator.tinyCacheSize: 512
2018-04-04 19:59:00,152 DEBUG [main] buffer.PooledByteBufAllocator: -Dio.netty.allocator.smallCacheSize: 256
2018-04-04 19:59:00,152 DEBUG [main] buffer.PooledByteBufAllocator: -Dio.netty.allocator.normalCacheSize: 64
2018-04-04 19:59:00,152 DEBUG [main] buffer.PooledByteBufAllocator: -Dio.netty.allocator.maxCachedBufferCapacity: 32768
2018-04-04 19:59:00,152 DEBUG [main] buffer.PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimInterval: 8192
2018-04-04 19:59:00,152 DEBUG [main] buffer.PooledByteBufAllocator: -Dio.netty.allocator.useCacheForAllThreads: true
2018-04-04 19:59:00,161 DEBUG [main] buffer.ByteBufUtil: -Dio.netty.allocator.type: pooled
2018-04-04 19:59:00,161 DEBUG [main] buffer.ByteBufUtil: -Dio.netty.threadLocalDirectBufferSize: 65536
2018-04-04 19:59:00,161 DEBUG [main] buffer.ByteBufUtil: -Dio.netty.maxThreadLocalCharBufferSize: 16384
2018-04-04 19:59:00,189 DEBUG [Default-IPC-NioEventLoopGroup-1-1] util.Recycler: -Dio.netty.recycler.maxCapacityPerThread: 32768
2018-04-04 19:59:00,190 DEBUG [Default-IPC-NioEventLoopGroup-1-1] util.Recycler: -Dio.netty.recycler.maxSharedCapacityFactor: 2
2018-04-04 19:59:00,190 DEBUG [Default-IPC-NioEventLoopGroup-1-1] util.Recycler: -Dio.netty.recycler.linkCapacity: 16
2018-04-04 19:59:00,190 DEBUG [Default-IPC-NioEventLoopGroup-1-1] util.Recycler: -Dio.netty.recycler.ratio: 8
Table ycsb is ENABLED
ycsb
COLUMN FAMILIES DESCRIPTION
{NAME => 'family', VERSIONS => '1', EVICT_BLOCKS_ON_CLOSE => 'false', NEW_VERSION_BEHAVIOR => 'false', KEEP_DELETED_CELLS => 'FALSE', CACHE_DATA_ON_WRITE => 'false', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', MIN_VERSIONS => '0', REPLICATION_SCOPE => '0', BLOOMFILTER => 'ROW', CACHE_INDEX_ON_WRITE =>
'false', IN_MEMORY => 'false', CACHE_BLOOMS_ON_WRITE => 'false', PREFETCH_BLOCKS_ON_OPEN => 'false', COMPRESSION => 'NONE', BLOCKCACHE => 'true', BLOCKSIZE => '65536', METADATA => {'IN_MEMORY_COMPACTION' => 'NONE'}}
1 row(s)
Took 0.6422 seconds
hbase(main):002:0>
{code}

"
HBASE-19690	"I use the following command in master branch :
{code}
mvn clean verify install javadoc:aggregate package assembly:single -DskipTests=true -Dmaven.javadoc.skip=true -Dhadoop.profile=3.0
{code}
It fails with the following error:
{code}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-checkstyle-plugin:2.17:check (checkstyle) on project hbase-error-prone: Failed during checkstyle execution: Unable to process suppressions file location: hbase/checkstyle-suppressions.xml: Cannot create file-based resource:invalid distance too far back -> [Help 1]
{code}"
HBASE-19405	"Looks like between branches this is very different.  
In master the client extends the correct exception but it is not called from the StoreScanner.
Looking quickly at 1.4 it does not look to extend the correct exception and it is not called from anywhere. "
HBASE-19331	"when a region split happens on a key with trailing byte equals zero, the end key of the first resulting region and and start key of the second resulting region in meta table gets corrupted.

Here is the link to code to reproduce this issue
https://bitbucket.org/flytxt/hbase-meta-corruption-test
 


*+Test Result+*

[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running com.flytxt.HbaseRegionMetaTest
log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
18:23:54.346 [main] INFO com.flytxt.HbaseRegionMetaTest - Dropping table SAMPLE_TBL_1
18:23:56.094 [main] INFO com.flytxt.HbaseRegionMetaTest - Dropping table SAMPLE_TBL_2
18:23:58.107 [main] INFO com.flytxt.HbaseRegionMetaTest - Creating new table SAMPLE_TBL_1
18:23:58.658 [main] INFO com.flytxt.HbaseRegionMetaTest - Creating new table SAMPLE_TBL_1
18:23:59.212 [main] INFO com.flytxt.HbaseRegionMetaTest - Starting puts to table SAMPLE_TBL_1
18:24:00.046 [main] INFO com.flytxt.HbaseRegionMetaTest - Puts complete .. lets split SAMPLE_TBL_1
18:24:00.500 [main] INFO com.flytxt.HbaseRegionMetaTest - Starting puts to table SAMPLE_TBL_2
18:24:02.073 [main] INFO com.flytxt.HbaseRegionMetaTest - Puts complete .. lets split SAMPLE_TBL_2
18:24:02.753 [main] INFO com.flytxt.HbaseRegionMetaTest - region split complete .. Lets verify region infos for table SAMPLE_TBL_1 
18:24:02.754 [main] INFO com.flytxt.HbaseRegionMetaTest - ===========================================================
18:24:02.754 [main] INFO com.flytxt.HbaseRegionMetaTest - Region Name : SAMPLE_TBL_1,,1511355240515.56c8fd8e42228c3c1ec71f9a4da65f5f.
18:24:02.755 [main] INFO com.flytxt.HbaseRegionMetaTest - Region Id :1511355240515
18:24:02.755 [main] INFO com.flytxt.HbaseRegionMetaTest - Region start key :[]  , Key length :0
18:24:02.755 [main] INFO com.flytxt.HbaseRegionMetaTest - Region end key : [0, 0, 0, 0, 0, 19, -76]  , Key length :7
18:24:02.755 [main] INFO com.flytxt.HbaseRegionMetaTest - -----------------------------------------------------------
18:24:02.762 [main] INFO com.flytxt.HbaseRegionMetaTest - ===========================================================
18:24:02.762 [main] INFO com.flytxt.HbaseRegionMetaTest - Region Name : SAMPLE_TBL_1,\x00\x00\x00\x00\x00\x13\xB4,1511355240515.c06afed17b2a5c4fb54bacf704dd8a9e.
18:24:02.762 [main] INFO com.flytxt.HbaseRegionMetaTest - Region Id :1511355240515
18:24:02.762 [main] INFO com.flytxt.HbaseRegionMetaTest - Region start key :[0, 0, 0, 0, 0, 19, -76]  , Key length :7
18:24:02.763 [main] INFO com.flytxt.HbaseRegionMetaTest - Region end key : []  , Key length :0
18:24:02.763 [main] INFO com.flytxt.HbaseRegionMetaTest - -----------------------------------------------------------
18:24:03.005 [main] INFO com.flytxt.HbaseRegionMetaTest - region split complete .. Lets verify region infos for table SAMPLE_TBL_2 
18:24:03.006 [main] INFO com.flytxt.HbaseRegionMetaTest - ===========================================================
18:24:03.006 [main] INFO com.flytxt.HbaseRegionMetaTest - Region Name : SAMPLE_TBL_2,,1511355242363.0679851100e16aad005c743af618452e.
18:24:03.006 [main] INFO com.flytxt.HbaseRegionMetaTest - Region Id :1511355242363
18:24:03.007 [main] INFO com.flytxt.HbaseRegionMetaTest - Region start key :[]  , Key length :0
18:24:03.008 [main] INFO com.flytxt.HbaseRegionMetaTest - Region end key : [0, 0, 0, 0, 0, 0, 19, -57]  , Key length :8
18:24:03.008 [main] INFO com.flytxt.HbaseRegionMetaTest - -----------------------------------------------------------
18:24:03.008 [main] INFO com.flytxt.HbaseRegionMetaTest - ===========================================================
18:24:03.008 [main] INFO com.flytxt.HbaseRegionMetaTest - Region Name : SAMPLE_TBL_2,\x00\x00\x00\x00\x00\x00\x13\xC7,1511355242363.326b5175036efdeae89f9493b436e114.
18:24:03.008 [main] INFO com.flytxt.HbaseRegionMetaTest - Region Id :1511355242363
18:24:03.008 [main] INFO com.flytxt.HbaseRegionMetaTest - Region start key :[0, 0, 0, 0, 0, 0, 19, -57]  , Key length :8
18:24:03.008 [main] INFO com.flytxt.HbaseRegionMetaTest - Region end key : []  , Key length :0
18:24:03.009 [main] INFO com.flytxt.HbaseRegionMetaTest - -----------------------------------------------------------
[ERROR] Tests run: 3, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 10.777 s <<< FAILURE! - in com.flytxt.HbaseRegionMetaTest
[ERROR] testTableRegionStartAndEndKey[0](com.flytxt.HbaseRegionMetaTest)  Time elapsed: 0.228 s  <<< FAILURE!
java.lang.AssertionError: 
Region end key error for SAMPLE_TBL_1,,1511355240515.56c8fd8e42228c3c1ec71f9a4da65f5f.
Expected: is <8>
     but: was <7>
	at com.flytxt.HbaseRegionMetaTest.testTableRegionStartAndEndKey(HbaseRegionMetaTest.java:170)

[ERROR] testTableRegionStartAndEndKey[0](com.flytxt.HbaseRegionMetaTest)  Time elapsed: 0.23 s  <<< FAILURE!
java.lang.AssertionError: 
Region start key error for SAMPLE_TBL_1,\x00\x00\x00\x00\x00\x13\xB4,1511355240515.c06afed17b2a5c4fb54bacf704dd8a9e.
Expected: is <8>
     but: was <7>
	at com.flytxt.HbaseRegionMetaTest.testTableRegionStartAndEndKey(HbaseRegionMetaTest.java:167)

[INFO] 
[INFO] Results:
[INFO] 
[ERROR] Failures: 
[ERROR] com.flytxt.HbaseRegionMetaTest.testTableRegionStartAndEndKey[0](com.flytxt.HbaseRegionMetaTest)
[ERROR]   Run 1: HbaseRegionMetaTest.testTableRegionStartAndEndKey:170 Region end key error for SAMPLE_TBL_1,,1511355240515.56c8fd8e42228c3c1ec71f9a4da65f5f.
Expected: is <8>
     but: was <7>
[ERROR]   Run 2: HbaseRegionMetaTest.testTableRegionStartAndEndKey:167 Region start key error for SAMPLE_TBL_1,\x00\x00\x00\x00\x00\x13\xB4,1511355240515.c06afed17b2a5c4fb54bacf704dd8a9e.
Expected: is <8>
     but: was <7>
[INFO] 
[INFO] 
[ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0




  "
HBASE-17901	"Once per several days region server fails to flush a memstore and stops.

April, 8:
{code}
2017-04-08 00:10:57,737 WARN  [MemStoreFlusher.1] regionserver.HStore: Failed flushing store file, retrying num=9
java.io.IOException: ScanWildcardColumnTracker.checkColumn ran into a column actually smaller than the previous column: 
	at org.apache.hadoop.hbase.regionserver.ScanWildcardColumnTracker.checkVersions(ScanWildcardColumnTracker.java:117)
	at org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.match(ScanQueryMatcher.java:464)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:529)
	at org.apache.hadoop.hbase.regionserver.StoreFlusher.performFlush(StoreFlusher.java:119)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:74)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:915)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:2271)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushCacheAndCommit(HRegion.java:2375)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2105)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2067)
	at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:1958)
	at org.apache.hadoop.hbase.regionserver.HRegion.flush(HRegion.java:1884)
	at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:510)
	at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushOneForGlobalPressure(MemStoreFlusher.java:215)
	at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.access$600(MemStoreFlusher.java:75)
	at org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushHandler.run(MemStoreFlusher.java:244)
	at java.lang.Thread.run(Thread.java:745)
2017-04-08 00:10:57,737 FATAL [MemStoreFlusher.1] regionserver.HRegionServer: ABORTING region server datanode13.webmeup.com,16020,1491573320653: Replay of WAL required. Forcing server shutdown
org.apache.hadoop.hbase.DroppedSnapshotException: region: di_ordinal_tmp,gov.ok.data/browse?page=2&category=Natural%20Resources&limitTo=datasets&tags=ed,1489764397211.9d7ca11018672c4aace7f30c8f4253f3.
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushCacheAndCommit(HRegion.java:2428)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2105)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2067)
	at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:1958)
	at org.apache.hadoop.hbase.regionserver.HRegion.flush(HRegion.java:1884)
	at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:510)
	at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushOneForGlobalPressure(MemStoreFlusher.java:215)
	at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.access$600(MemStoreFlusher.java:75)
	at org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushHandler.run(MemStoreFlusher.java:244)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: ScanWildcardColumnTracker.checkColumn ran into a column actually smaller than the previous column: 
	at org.apache.hadoop.hbase.regionserver.ScanWildcardColumnTracker.checkVersions(ScanWildcardColumnTracker.java:117)
	at org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.match(ScanQueryMatcher.java:464)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:529)
	at org.apache.hadoop.hbase.regionserver.StoreFlusher.performFlush(StoreFlusher.java:119)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:74)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:915)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:2271)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushCacheAndCommit(HRegion.java:2375)
	... 9 more
{code}

After region server restart it functioned properly for a couple of days.

April, 10:
{code}
2017-04-10 22:36:32,147 WARN  [MemStoreFlusher.0] regionserver.HStore: Failed flushing store file, retrying num=9
java.io.IOException: Non-increasing Bloom keys: de.tina-eicke.blog/category/garten/\x09h after de.uina-eicke.blog/category/fruehling/\x09h
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.appendGeneralBloomfilter(StoreFile.java:936)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.append(StoreFile.java:969)
	at org.apache.hadoop.hbase.regionserver.StoreFlusher.performFlush(StoreFlusher.java:125)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:74)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:915)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:2271)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushCacheAndCommit(HRegion.java:2375)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2105)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2067)
	at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:1958)
	at org.apache.hadoop.hbase.regionserver.HRegion.flush(HRegion.java:1884)
	at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:510)
	at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:471)
	at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.access$900(MemStoreFlusher.java:75)
	at org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushHandler.run(MemStoreFlusher.java:259)
	at java.lang.Thread.run(Thread.java:745)
2017-04-10 22:36:32,147 FATAL [MemStoreFlusher.0] regionserver.HRegionServer: ABORTING region server datanode13.webmeup.com,16020,1491828707088: Replay of WAL required. Forcing server shutdown
org.apache.hadoop.hbase.DroppedSnapshotException: region: di_ordinal_tmp,de.thschroeer/lmo/lmo.php?action=results&file=archiv/BLW2-2013.l98&endtab=8&st=8&tabtype=2\x09hw,1489764397211.b07eaba657affc2ba29f84b59c672836.
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushCacheAndCommit(HRegion.java:2428)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2105)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2067)
	at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:1958)
	at org.apache.hadoop.hbase.regionserver.HRegion.flush(HRegion.java:1884)
	at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:510)
	at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:471)
	at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.access$900(MemStoreFlusher.java:75)
	at org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushHandler.run(MemStoreFlusher.java:259)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Non-increasing Bloom keys: de.tina-eicke.blog/category/garten/\x09h after de.uina-eicke.blog/category/fruehling/\x09h
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.appendGeneralBloomfilter(StoreFile.java:936)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.append(StoreFile.java:969)
	at org.apache.hadoop.hbase.regionserver.StoreFlusher.performFlush(StoreFlusher.java:125)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:74)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:915)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:2271)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushCacheAndCommit(HRegion.java:2375)
	... 9 more 
{code}

April, 13
{code}
2017-04-13 14:06:30,189 WARN  [MemStoreFlusher.1] regionserver.HStore: Failed flushing store file, retrying num=9
java.io.IOException: ScanWildcardColumnTracker.checkColumn ran into a column actually smaller than the previous column: p
        at org.apache.hadoop.hbase.regionserver.ScanWildcardColumnTracker.checkVersions(ScanWildcardColumnTracker.java:117)
        at org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.match(ScanQueryMatcher.java:464)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:529)
        at org.apache.hadoop.hbase.regionserver.StoreFlusher.performFlush(StoreFlusher.java:119)
        at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:74)
        at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:915)
        at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:2271)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushCacheAndCommit(HRegion.java:2375)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2105)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2067)
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:1958)
        at org.apache.hadoop.hbase.regionserver.HRegion.flush(HRegion.java:1884)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:510)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:471)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.access$900(MemStoreFlusher.java:75)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushHandler.run(MemStoreFlusher.java:259)
        at java.lang.Thread.run(Thread.java:745)
2017-04-13 14:06:30,190 INFO  [regionserver/datanode13.webmeup.com/88.99.58.169:16020-longCompactions-1491986731568] compress.CodecPool: Got brand-new decompressor [.gz]
2017-04-13 14:06:30,190 INFO  [regionserver/datanode13.webmeup.com/88.99.58.169:16020-shortCompactions-1491986746336] compress.CodecPool: Got brand-new decompressor [.gz]
2017-04-13 14:06:30,190 FATAL [MemStoreFlusher.1] regionserver.HRegionServer: ABORTING region server datanode13.webmeup.com,16020,1491986730362: Replay of WAL required. Forcing server shutdown
org.apache.hadoop.hbase.DroppedSnapshotException: region: di_ordinal_tmp,net.teleklik.lepavina/citati_izreke/index.php?page=2\x09h,1491054654848.ddb53de0e924251818ac2cb0c07b072f.
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushCacheAndCommit(HRegion.java:2428)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2105)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2067)
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:1958)
        at org.apache.hadoop.hbase.regionserver.HRegion.flush(HRegion.java:1884)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:510)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:471)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.access$900(MemStoreFlusher.java:75)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushHandler.run(MemStoreFlusher.java:259)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: ScanWildcardColumnTracker.checkColumn ran into a column actually smaller than the previous column: p
        at org.apache.hadoop.hbase.regionserver.ScanWildcardColumnTracker.checkVersions(ScanWildcardColumnTracker.java:117)
        at org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.match(ScanQueryMatcher.java:464)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:529)
        at org.apache.hadoop.hbase.regionserver.StoreFlusher.performFlush(StoreFlusher.java:119)
        at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:74)
        at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:915)
        at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:2271)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushCacheAndCommit(HRegion.java:2375)
        ... 9 more
2017-04-13 14:06:30,190 FATAL [MemStoreFlusher.1] regionserver.HRegionServer: RegionServer abort: loaded coprocessors are: []
{code}

Table description:
{code}
'di_ordinal_tmp', {TABLE_ATTRIBUTES => {DURABILITY => 'ASYNC_WAL', MAX_FILESIZE => '8589934592'}, {NAME => 'di', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'FAST_DIFF', TTL => '10368000 SECONDS (120 DAYS)', COMPRESSION => 'GZ', MIN_VERSIONS => '0', BLOCKCACHE => 'false', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0', METADATA => {'COMPRESSION_COMPACT' => 'GZ'}}
{code}

The table is being populated only using put operations. There has never been any bulk loading into this table."
HBASE-20025	HBASE-20013 addressed flakeyness in branch-2. This issue is about backport for branch-1. The patch on HBASE-20013 and perhaps some portion of HBASE-20020 would help?
HBASE-20596	"杩愯鐜(environment)锛歬ubernetes+docker(centos6.9)

杩愯鏈嶅姟锛坰ervices锛夛細

hadoop锛?.7锛?zk(3.4.6)+hbase(1.1.2)

閰嶇疆淇℃伅(conf):

1銆佷笉鍩轰簬hosts鏂囦欢鍋氫富鏈哄悕瑙ｆ瀽锛坣o /etc/hosts锛夛紝浣跨敤kube-dns瑙ｆ瀽涓绘満鍚?

2銆乭base-site.xml 濡備笅

!image-2018-05-17-10-11-30-540.png!

3銆乺egionservers鏂囦欢濡備笅

!image-2018-05-17-10-13-38-856.png!

4銆乺egionserver 鍚姩log 鎵撳嵃ERROR锛屼絾鏄惎鍔ㄦ垚鍔?

!image-2018-05-17-10-14-08-046.png!

4銆乄ebUI鏄剧ず寮傚父锛堝浜嗕竴鍊嶇殑regionserver锛?

!image-2018-05-17-10-14-40-134.png!

聽

聽

5銆乭base shell 寮傚父

!image-2018-05-17-10-15-12-225.png!

6銆侀噸鍚痳egionserver鍚庯紝log涓?涓婅堪寮傚父渚濈劧瀛樺湪锛屼絾鏄痺ebUI姝ｅ父锛宻hell client姝ｅ父

!image-2018-05-17-10-15-52-051.png!

聽

聽

!image-2018-05-17-10-16-01-313.png!

聽

聽"
HBASE-20339	"Our program analyzer has detected a potential security issue as follows聽
{code:java}
PrintWriter out = ServletUtil.initHTML(response, ""Log Level"");
String logName = ServletUtil.getParameter(request, ""log"");
String level = ServletUtil.getParameter(request, ""level"");

if (logName != null) {
   out.println(""<br /><hr /><h3>Results</h3>"");
   out.println(MARKER
        + ""Submitted Log Name: <b>"" + logName + ""</b><br />"");
  ...
}{code}
Above is the code piece. Seems that the log name is directly collected from the web request, and only whether the data is null is checked. So an attacker may provide a ""logName"" with a piece of injected code, leading to cross-site attacks. And besides, the variable ""level"" may also have such vulnerability.

聽

(org.apache.hadoop.hbase.http.log.LogLevel.java Line 111/118)

Linkage to the code is here:

[https://github.com/apache/hbase/blob/9e9b347d667e1fc6165c9f8ae5ae7052147e8895/hbase-http/src/main/java/org/apache/hadoop/hbase/http/log/LogLevel.java#L111]

聽

SourceBrella inc."
HBASE-17889	"We run into one case with read-replica, when the server hosting the primary region is shutdown, we see Get did not go to replica region and it paused for about 50 seconds before Get was resumed. 

More debugging finds out that when the server is down, one of the threads was stuck at the write, it holds lock at 
https://github.com/apache/hbase/blob/branch-1.3/hbase-client/src/main/java/org/apache/hadoop/hbase/ipc/RpcClientImpl.java#L916.
The later write threads were waiting on this lock until all threads in the connection's thread pool were stuck on this lock. At that moment, no work will be done. After socket write times out, it frees up all threads and it continues.

When QueueingFuture#cancel() is called, it does not interrupt the working thread and return the thread to the pool.

Attaching the jstack trace."
HBASE-20436	"Saw the following compilation error in hbase-spark-it module:
{code}
[ERROR] COMPILATION ERROR :
[INFO] -------------------------------------------------------------
[ERROR] /hbase/hbase-spark-it/src/test/java/org/apache/hadoop/hbase/spark/IntegrationTestSparkBulkLoad.java:[638,10] abstract method processOptions(org.apache.hbase.thirdparty.org.apache.commons.cli.CommandLine) in org.apache.hadoop.hbase.util.AbstractHBaseTool cannot be accessed directly
{code}
The processOptions method of AbstractHBaseTool is abstract."
HBASE-17891	"As it stayed in the API document, the the result in the postScannerNext() to return to the client, can be modified. How can I do this in the java code? Thanks a lot.

Best regards,
land Li"
HBASE-20028	"{noformat}
2018-02-20 16:36:41,794 ERROR [Thread-85] assignment.AssignmentManager: java.lang.NullPointerException
java.lang.NullPointerException
	at org.apache.hadoop.hbase.util.VersionInfo.compareVersion(VersionInfo.java:122)
	at org.apache.hadoop.hbase.master.assignment.AssignmentManager.lambda$getExcludedServersForSystemTable$5(AssignmentManager.java:1860)
	at java.util.Collections.max(Collections.java:712)
	at org.apache.hadoop.hbase.master.assignment.AssignmentManager.getExcludedServersForSystemTable(AssignmentManager.java:1859)
	at org.apache.hadoop.hbase.master.assignment.AssignmentManager.lambda$checkIfShouldMoveSystemRegionAsync$0(AssignmentManager.java:464){noformat}
Looks like a race condition around an RS losing its ZK lock. If AM tries to see if it should move a Region to a server who we've seen that the lock was lost but the RS hasn't yet been processed as ""dead"", we can get into a situation where {{HMaster.getRegionServerVersion()}} returns null and causes this to fail.

Looks like a simple filter on the servers to preclude null versions would fix the problem."
HBASE-19305	"when i alter hbase ttl, i found the alter cannot success for more than one day, the table size is not large  enough.
The the table size is : 303.6 G  /hbase/data/default/SqlMetaData_Ver2
The alter begin time is :
2017-11-17 15:14:22,128
but util 2017-11-20 18:00, the alter action has not been completed,so I do not know what went wrong ?
Someone can help me? Thank you very much!"
HBASE-18366	"It worked for a few days after enabling it with HBASE-18278. But started failing after commits:

6786b2b
68436c9
75d2eca
50bb045
df93c13

It works with one commit before: c5abb6c. Need to see what changed with those commits.

Currently it fails with TableNotFoundException."
HBASE-19872	"hbase 1.3.1聽 regionserver聽 crash ,聽 configure bucketcache.

error log :

聽FATAL [RpcServer.FifoWFPBQ.default.handler=42,queue=2,port=16020] regionserver.RSRpcServices: Run out of memory; RSRpcServices will abort itself immediately

聽

聽hbase-env.sh:

export HBASE_REGIONSERVER_OPTS=""-XX:+UseG1GC -XX:+UnlockExperimentalVMOptions -Xmx24g -Xms24g -XX:MetaspaceSize=256M -XX:MaxMetaspaceSize=512M -XX:MaxGCPauseMillis=100 -XX:G1NewSizePercent=5 -XX:ConcGCThreads=4 -XX:ParallelGCThreads=16 -XX:-ResizePLAB -XX:+ParallelRefProcEnabled -XX:InitiatingHeapOccupancyPercent=65 -XX:G1HeapRegionSize=32M -XX:G1MixedGCCountTarget=64 -XX:G1OldCSetRegionThresholdPercent=5 -XX:MaxTenuringThreshold=1 -XX:MaxDirectMemorySize=28g -XX:ReservedCodeCacheSize=512M -XX:+DisableExplicitGC -Xloggc:${HBASE_LOG_DIR}/regionserver.gc.log""

聽

hbase-site.xml:

聽

<property>
 聽聽聽聽聽 <name>hbase.bucketcache.combinedcache.enabled</name>
 聽聽聽聽聽 <value>true</value>
 聽聽聽 </property>
 聽聽 聽<property>
 聽聽聽 聽聽 <name>hbase.bucketcache.ioengine</name>
 聽 聽聽聽聽 <value>offheap</value>
 聽聽聽 </property>
 <property>
 聽聽聽聽 聽 <name>hbase.bucketcache.size</name>
 聽 聽聽聽聽 <value>25600</value>
 聽聽聽 </property>
 聽聽 聽<property>
 聽聽 聽聽 聽<name>hbase.bucketcache.writer.queuelength</name>
 聽聽聽聽聽 <value>64</value>
 聽聽 聽</property>
 聽聽聽 <property>
 聽聽聽聽聽 <name>hbase.bucketcache.writer.threads</name>
 聽聽聽聽聽 <value>3</value>
 聽聽聽 </property>
 <property>
 <name>hfile.block.cache.size</name>
 <value>0.3</value>
 </property>

聽"
HBASE-18198	"If null ACLs are passed then zk node creation fails with NPE
{code}
java.lang.NullPointerException
	at org.apache.zookeeper.server.PrepRequestProcessor.removeDuplicates(PrepRequestProcessor.java:1301)
	at org.apache.zookeeper.server.PrepRequestProcessor.fixupACL(PrepRequestProcessor.java:1341)
	at org.apache.zookeeper.server.PrepRequestProcessor.pRequest2Txn(PrepRequestProcessor.java:519)
	at org.apache.zookeeper.server.PrepRequestProcessor.pRequest(PrepRequestProcessor.java:1126)
	at org.apache.zookeeper.server.PrepRequestProcessor.run(PrepRequestProcessor.java:178)
{code}

Below APIs have problem.
{code}
public void create(final String path, byte data[], List<ACL> acl,
            CreateMode createMode, StringCallback cb, Object ctx)

public void create(final String path, byte data[], List<ACL> acl,
            CreateMode createMode, Create2Callback cb, Object ctx)
{code}

Solution:  Need to handle NULL acl in removeDuplicates method in server."
HBASE-19585	"Testing, RS crashes soon after startup with below cryptic mess. This is the branch-2 started over a 0.98 data. [~Apache9] You have a clue sir?

{code}
595126 2017-12-21 13:09:38,058 INFO  [regionserver/ve0528.halxg.cloudera.com/10.17.240.22:16020] wal.AbstractFSWAL: WAL configuration: blocksize=128 MB, rollsize=121.60 MB, prefi       x=ve0528.halxg.cloudera.com%2C16020%2C1513890565537, suffix=, logDir=hdfs://ve0524.halxg.cloudera.com:8020/hbase/WALs/ve0528.halxg.cloudera.com,16020,1513890565537, archiv       eDir=hdfs://ve0524.halxg.cloudera.com:8020/hbase/oldWALs
595127 2017-12-21 13:09:38,107 ERROR [regionserver/ve0528.halxg.cloudera.com/10.17.240.22:16020] regionserver.HRegionServer: ***** ABORTING region server ve0528.halxg.cloudera.co       m,16020,1513890565537: Unhandled: Bad type on operand stack
595128 Exception Details:
595129   Location:
595130     org/apache/hadoop/hdfs/protocolPB/PBHelperClient.convert(Lorg/apache/hadoop/hdfs/protocol/proto/HdfsProtos$ContentSummaryProto;)Lorg/apache/hadoop/fs/ContentSummary; @       98: invokestatic
595131   Reason:
595132     Type 'org/apache/hadoop/fs/ContentSummary$Builder' (current frame, stack[1]) is not assignable to 'org/apache/hadoop/fs/QuotaUsage$Builder'
595133   Current Frame:
595134     bci: @98
595135     flags: { }
595136     locals: { 'org/apache/hadoop/hdfs/protocol/proto/HdfsProtos$ContentSummaryProto', 'org/apache/hadoop/fs/ContentSummary$Builder' }
595137     stack: { 'org/apache/hadoop/hdfs/protocol/proto/HdfsProtos$StorageTypeQuotaInfosProto', 'org/apache/hadoop/fs/ContentSummary$Builder' }
595138   Bytecode:
595139     0x0000000: 2ac7 0005 01b0 bb03 3159 b703 324c 2b2a
595140     0x0000010: b603 33b6 0334 2ab6 0335 b603 362a b603
595141     0x0000020: 37b6 0338 2ab6 0339 b603 3a2a b603 3bb6
595142     0x0000030: 033c 2ab6 033d b603 3e2a b603 3fb6 0340
595143     0x0000040: 2ab6 0341 b603 422a b603 43b6 0344 2ab6
595144     0x0000050: 0345 b603 4657 2ab6 0347 9900 0b2a b603
595145     0x0000060: 482b b803 492b b603 4ab0
595146   Stackmap Table:
595147     same_frame(@6)
595148     append_frame(@101,Object[#2126])
595149  *****
{code}

2.8.2 hadoop."
HBASE-19294	"when create table failed in execute phase master is getting aborted.

DEBUG [RpcServer.FifoWFPBQ.default.handler=8,queue=3,port=16000] ipc.RpcServer: RpcServer.FifoWFPBQ.default.handler=8,queue=3,port=16000: callId: 443 service: MasterService methodName: ListTableDescriptorsByNamespace size: 51 connection: 
java.io.InterruptedIOException: Retry interrupted

java.lang.RuntimeException: HMaster Aborted
        at org.apache.hadoop.hbase.master.HMasterCommandLine.startMaster(HMasterCommandLine.java:239)
        at org.apache.hadoop.hbase.master.HMasterCommandLine.run(HMasterCommandLine.java:137)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)
        at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:3207)


"
HBASE-16298	"For policy/compliance reasons, we removed the tests jars from lib/ directory on HBase Master. Everything was working fine from 1.0 to 1.1.3.

When I upgraded from 1.1.3 to 1.1.5, the /master-status page started to return an error 500: {{java.lang.IllegalArgumentException: Failed to load ESAPI.properties as a classloader resource.}}

After some search, I found out that ESAPI has been added following HBASE-15122 which also added the ESAPI.properties files into src/main/resources.

However, it seems an exclusion has been put on packaging: the file is absent from hbase-server-1.1.5.jar, but present in hbase-server-1.1.5-tests.jar, which is in the lib/ directory in the tar.gz distribution.

Our workaround is to deploy back hbase-server-1.1.5-tests.jar in lib/. However, it does not seem right to require tests jar to have HBase master propertly work.

Even if it is the current HBase policy to keep those jars, I think the hbase-server.jar should contain ESAPI.properties.

The same thing applies for 1.2 branch."
HBASE-16170	"Hello HBase Team,

I am facing hanging problem while building ""Phoenix4.4-HBase1.1"" on RHEL 7.2 ppc64le which is dependent on HBase.

I am having IOP setup done with environment setup as open jdk 1.8 and maven 3.3.9, Hbase 1.1.1 installed and IOP hadoop services with ambari running on it.

When I build ""Phoenix4.4-HBase 1.1"" than there occurs hang at below point without any error logs, hanging occurs each time at different points.
For 1st build, below are the lines where hang up occurs for infinite time:
i.e. Running org.apache.hadoop.hbase.regionserver.PhoenixRpcSchedulerFactoryTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.017 sec - in org.apache.hadoop.hbase.regionserver.PhoenixRpcSchedulerFactoryTest

2nd time when I build,hang up occurs at different point. It is suspected to be HBase issue.

Team: Can you please help us in this and let us know the reason for hanging. Thank You.

Thanks & Regards,
Sonali Shrivastava

"
HBASE-16439	"Some rows is missing when put them into a table using mutateRow method and each row has thousands of columns. The code excerpt:
{code}
try (HConnection hc = HConnectionManager.createConnection(conf)) {
	try (HTableInterface table = hc.getTable(tableName)) {
		final LocalDate startDate = LocalDate.of(1980, 01, 01); 
		for (int i = 0; i < 15000; i++) {
			byte[] row = Bytes.toBytes(Integer.toString(i));

			long ts = System.currentTimeMillis();
			Put put = new Put(row, ts + 1);
			LocalDate date = startDate; 
			for (int j = 0; j < 5000; j++) {
				put.add(
					family,
					Bytes.toBytes(DateTimeFormatter.BASIC_ISO_DATE.format(date)),
					Bytes.toBytes(Integer.toString(j))
				);
				date = date.plusDays(1);
			}
			
			RowMutations rm = new RowMutations(row);

			rm.add(put);
			
			table.mutateRow(rm);
		}
	}
}
{code}
Resulting number of rows varies - sometimes it 200, sometimes it 8000. But never expected 15000.
The full code of test application is attached."
HBASE-13566	"As hbase run MetaLogRoller thread for writting meta hlog, I got below error

ERROR [RS_OPEN_META-xxx:60020-0-MetaLogRoller] wal.ProtobufLogWriter: Got IOException while writing trailer
java.io.IOException: Failing write. Tried pipeline recovery 5 times without success.
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:918)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:486)
2015-04-25 19:52:05,463 ERROR [RS_OPEN_META-host152:60020-0-MetaLogRoller] wal.FSHLog: Failed close of HLog writer
java.io.IOException: Failing write. Tried pipeline recovery 5 times without success.
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:918)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:486)
2015-04-25 19:52:05,463 FATAL [RS_OPEN_META-host152:60020-0-MetaLogRoller] regionserver.HRegionServer: ABORTING region server host152,60020,1429927886571: Failed log close in log roller
org.apache.hadoop.hbase.regionserver.wal.FailedLogCloseException: #1429959124806
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.cleanupCurrentWriter(FSHLog.java:777)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.rollWriter(FSHLog.java:565)
        at org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:97)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failing write. Tried pipeline recovery 5 times without success.
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:918)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:486)

note:
I hava checked namenode log, there is no error,just close the write socket
and hadoop and hbase file is all fine"
HBASE-14019	"hbase-0.98.9-hadoop2/bin/hbase org.apache.hadoop.hbase.mapreduce.Import item_restore /data/item_backup 
Fails with numerous RetriesExhaustedException

The export process eg "" hbase-0.98.9-hadoop2/bin/hbase org.apache.hadoop.hbase.mapreduce.Export item /data/item_backup ""
works flawlessly and the file item_backup is created import of the same file to a table of a different name, fails.

Please see attached job log"
HBASE-14018	"+ Pseudo-distributed Hadoop (2.6.0), ZK_HBASE_MANAGE = true (1 master, 1 regionserver).
+ Put data to OpenTSDB, 1000 records / s, for 2000 seconds.
+ RegionServer is aborted.

=== RegionServer logs ===
2015-07-03 16:37:37,332 INFO  [LruBlockCacheStatsExec"
HBASE-14180	"HBase keeps throwing a timeout exception I have tryed every configuration I could think about to increase it.

Partial stacktrace:
{quote}
Caused by: java.io.IOException: Call to hdp-w-1.c.dks-hadoop.internal/10.240.2.235:16020 failed on local exception: org.apache.hadoop.hbase.ipc.CallTimeoutException: Call id=43, waitTime=60001, operationTimeout=60000 expired.
        at org.apache.hadoop.hbase.ipc.RpcClientImpl.wrapException(RpcClientImpl.java:1242)
        at org.apache.hadoop.hbase.ipc.RpcClientImpl.call(RpcClientImpl.java:1210)
        at org.apache.hadoop.hbase.ipc.AbstractRpcClient.callBlockingMethod(AbstractRpcClient.java:213)
        at org.apache.hadoop.hbase.ipc.AbstractRpcClient$BlockingRpcChannelImplementation.callBlockingMethod(AbstractRpcClient.java:287)
        at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.scan(ClientProtos.java:32651)
        at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:213)
        at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:62)
        at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:200)
        at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:369)
        at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:343)
        at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:126)
        ... 4 more
Caused by: org.apache.hadoop.hbase.ipc.CallTimeoutException: Call id=43, waitTime=60001, operationTimeout=60000 expired.
        at org.apache.hadoop.hbase.ipc.Call.checkAndSetTimeout(Call.java:70)
        at org.apache.hadoop.hbase.ipc.RpcClientImpl.call(RpcClientImpl.java:1184)
        ... 13 more
{quote}

I've tryed editing config files and also setting config in Ambari with the next keys to increase the timeout with no success:
- hbase.rpc.timeout
- dfs.socket.timeout
- dfs.client.socket-timeout
- zookeeper.session.timeout

Also the Phoenix properties, but I think it's mostly an HBase issue:
- phoenix.query.timeoutMs
- phoenix.query.keepAliveMs

Full stack trace: 
{quote}
Error: Encountered exception in sub plan [0] execution. (state=,code=0)
java.sql.SQLException: Encountered exception in sub plan [0] execution.
        at org.apache.phoenix.execute.HashJoinPlan.iterator(HashJoinPlan.java:157)
        at org.apache.phoenix.jdbc.PhoenixStatement$1.call(PhoenixStatement.java:251)
        at org.apache.phoenix.jdbc.PhoenixStatement$1.call(PhoenixStatement.java:241)
        at org.apache.phoenix.call.CallRunner.run(CallRunner.java:53)
        at org.apache.phoenix.jdbc.PhoenixStatement.executeQuery(PhoenixStatement.java:240)
        at org.apache.phoenix.jdbc.PhoenixStatement.execute(PhoenixStatement.java:1250)
        at sqlline.Commands.execute(Commands.java:822)
        at sqlline.Commands.sql(Commands.java:732)
        at sqlline.SqlLine.dispatch(SqlLine.java:808)
        at sqlline.SqlLine.begin(SqlLine.java:681)
        at sqlline.SqlLine.start(SqlLine.java:398)
        at sqlline.SqlLine.main(SqlLine.java:292)
Caused by: org.apache.phoenix.exception.PhoenixIOException: org.apache.phoenix.exception.PhoenixIOException: Failed after attempts=36, exceptions:
Mon Aug 03 16:47:06 UTC 2015, null, java.net.SocketTimeoutException: callTimeout=60000, callDuration=60303: row '' on table 'hive_post_topics' at region=hive_post_topics,,1438084107396.cdbdc246ff0b7dfed31d481e0bccd2b5., hostname=hdp-w-1.c.dks-hadoop.internal,16020,1438619912282, seqNum=45322

        at org.apache.phoenix.util.ServerUtil.parseServerException(ServerUtil.java:108)
        at org.apache.phoenix.iterate.BaseResultIterators.getIterators(BaseResultIterators.java:542)
        at org.apache.phoenix.iterate.RoundRobinResultIterator.getIterators(RoundRobinResultIterator.java:176)
        at org.apache.phoenix.iterate.RoundRobinResultIterator.next(RoundRobinResultIterator.java:91)
        at org.apache.phoenix.join.HashCacheClient.serialize(HashCacheClient.java:106)
        at org.apache.phoenix.join.HashCacheClient.addHashCache(HashCacheClient.java:82)
        at org.apache.phoenix.execute.HashJoinPlan$HashSubPlan.execute(HashJoinPlan.java:339)
        at org.apache.phoenix.execute.HashJoinPlan$1.call(HashJoinPlan.java:136)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at org.apache.phoenix.job.JobManager$InstrumentedJobFutureTask.run(JobManager.java:172)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.concurrent.ExecutionException: org.apache.phoenix.exception.PhoenixIOException: Failed after attempts=36, exceptions:
Mon Aug 03 16:47:06 UTC 2015, null, java.net.SocketTimeoutException: callTimeout=60000, callDuration=60303: row '' on table 'hive_post_topics' at region=hive_post_topics,,1438084107396.cdbdc246ff0b7dfed31d481e0bccd2b5., hostname=hdp-w-1.c.dks-hadoop.internal,16020,1438619912282, seqNum=45322

        at java.util.concurrent.FutureTask.report(FutureTask.java:122)
        at java.util.concurrent.FutureTask.get(FutureTask.java:202)
        at org.apache.phoenix.iterate.BaseResultIterators.getIterators(BaseResultIterators.java:538)
        ... 11 more
Caused by: org.apache.phoenix.exception.PhoenixIOException: Failed after attempts=36, exceptions:
Mon Aug 03 16:47:06 UTC 2015, null, java.net.SocketTimeoutException: callTimeout=60000, callDuration=60303: row '' on table 'hive_post_topics' at region=hive_post_topics,,1438084107396.cdbdc246ff0b7dfed31d481e0bccd2b5., hostname=hdp-w-1.c.dks-hadoop.internal,16020,1438619912282, seqNum=45322

        at org.apache.phoenix.util.ServerUtil.parseServerException(ServerUtil.java:108)
        at org.apache.phoenix.iterate.ScanningResultIterator.next(ScanningResultIterator.java:56)
        at org.apache.phoenix.iterate.TableResultIterator.next(TableResultIterator.java:104)
        at org.apache.phoenix.iterate.LookAheadResultIterator$1.advance(LookAheadResultIterator.java:47)
        at org.apache.phoenix.iterate.LookAheadResultIterator.init(LookAheadResultIterator.java:59)
        at org.apache.phoenix.iterate.LookAheadResultIterator.peek(LookAheadResultIterator.java:73)
        at org.apache.phoenix.iterate.ParallelIterators$1.call(ParallelIterators.java:97)
        at org.apache.phoenix.iterate.ParallelIterators$1.call(ParallelIterators.java:85)
        ... 5 more
Caused by: org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=36, exceptions:
Mon Aug 03 16:47:06 UTC 2015, null, java.net.SocketTimeoutException: callTimeout=60000, callDuration=60303: row '' on table 'hive_post_topics' at region=hive_post_topics,,1438084107396.cdbdc246ff0b7dfed31d481e0bccd2b5., hostname=hdp-w-1.c.dks-hadoop.internal,16020,1438619912282, seqNum=45322

        at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.throwEnrichedException(RpcRetryingCallerWithReadReplicas.java:271)
        at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:223)
        at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:61)
        at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:200)
        at org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:320)
        at org.apache.hadoop.hbase.client.ClientScanner.loadCache(ClientScanner.java:403)
        at org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:364)
        at org.apache.phoenix.iterate.ScanningResultIterator.next(ScanningResultIterator.java:50)
        ... 11 more
Caused by: java.net.SocketTimeoutException: callTimeout=60000, callDuration=60303: row '' on table 'hive_post_topics' at region=hive_post_topics,,1438084107396.cdbdc246ff0b7dfed31d481e0bccd2b5., hostname=hdp-w-1.c.dks-hadoop.internal,16020,1438619912282, seqNum=45322
        at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:159)
        at org.apache.hadoop.hbase.client.ResultBoundedCompletionService$QueueingFuture.run(ResultBoundedCompletionService.java:64)
        ... 3 more
Caused by: java.io.IOException: Call to hdp-w-1.c.dks-hadoop.internal/10.240.2.235:16020 failed on local exception: org.apache.hadoop.hbase.ipc.CallTimeoutException: Call id=43, waitTime=60001, operationTimeout=60000 expired.
        at org.apache.hadoop.hbase.ipc.RpcClientImpl.wrapException(RpcClientImpl.java:1242)
        at org.apache.hadoop.hbase.ipc.RpcClientImpl.call(RpcClientImpl.java:1210)
        at org.apache.hadoop.hbase.ipc.AbstractRpcClient.callBlockingMethod(AbstractRpcClient.java:213)
        at org.apache.hadoop.hbase.ipc.AbstractRpcClient$BlockingRpcChannelImplementation.callBlockingMethod(AbstractRpcClient.java:287)
        at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.scan(ClientProtos.java:32651)
        at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:213)
        at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:62)
        at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:200)
        at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:369)
        at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:343)
        at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:126)
        ... 4 more
Caused by: org.apache.hadoop.hbase.ipc.CallTimeoutException: Call id=43, waitTime=60001, operationTimeout=60000 expired.
        at org.apache.hadoop.hbase.ipc.Call.checkAndSetTimeout(Call.java:70)
        at org.apache.hadoop.hbase.ipc.RpcClientImpl.call(RpcClientImpl.java:1184)
        ... 13 more
{quote}
"
HBASE-15966	"In a YARN job, I am creating HFiles with code that has been cribbed from the TableOutputFormat class and bulkloading them with LoadIncrementalHFiles.doBulkLoad.

On other clusters, where fs.defaultFS is set to an hdfs: URI, and my HFiles are placed in an hdfs: URI, the bulkload works as intended.

On this particular cluster, where fs.defaultFS is set to a wasb: URI and my HFiles are placed in a wasb: URI, the bulkload also works as intended.

However, on this same cluster, whenever I place the HFiles in an hdfs: URI, I get the following logs in my application from the HBase client logging repeatedly:

[02 Jun 23:23:26.002](20259/140062246807296) Info2:org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles: Trying to load hfile=hdfs://[my cluster]/[my path] first=\x00\x00\x11\x06 last=;\x8B\x85\x18
[02 Jun 23:23:26.002](20259/140062245754624) Info3:org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles: Going to connect to server region=[my namespace]:[my table],,1464909723920.00eafdb73989312bd8864f0913255f50., hostname=10.0.1.6,16020,1464698786237, seqNum=2 for row  with hfile group [{[B@4d0409e7,hdfs://[my cluster]/[my path]}]
[02 Jun 23:23:26.012](20259/140062245754624) Info1:org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles: Attempt to bulk load region containing  into table [my namespace]:[my table] with files [family:[my family] path:hdfs://[my cluster]/[my path]] failed.  This is recoverable and they will be retried.
[02 Jun 23:23:26.019](20259/140061634982912) Info2:org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles: Split occured while grouping HFiles, retry attempt 2 with 1 files remaining to group or split

And when I look at the appropriate region server's log, I find the following exception repeatedly:

2016-06-02 20:22:50,771 ERROR [B.DefaultRpcServer.handler=22,queue=2,port=16020] access.SecureBulkLoadEndpoint: Failed to complete bulk load
java.io.FileNotFoundException: File doesn't exist: hdfs://[my cluster]/[my path]      at org.apache.hadoop.fs.azure.NativeAzureFileSystem.setPermission(NativeAzureFileSystem.java:2192)
       at org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint$1.run(SecureBulkLoadEndpoint.java:280)
       at org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint$1.run(SecureBulkLoadEndpoint.java:270)
       at java.security.AccessController.doPrivileged(Native Method)
       at javax.security.auth.Subject.doAs(Subject.java:356)
       at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1651)
       at org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint.secureBulkLoadHFiles(SecureBulkLoadEndpoint.java:270)
       at org.apache.hadoop.hbase.protobuf.generated.SecureBulkLoadProtos$SecureBulkLoadService.callMethod(SecureBulkLoadProtos.java:4631)
       at org.apache.hadoop.hbase.regionserver.HRegion.execService(HRegion.java:6986)
       at org.apache.hadoop.hbase.regionserver.HRegionServer.execServiceOnRegion(HRegionServer.java:3456)
       at org.apache.hadoop.hbase.regionserver.HRegionServer.execService(HRegionServer.java:3438)
       at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:29998)
       at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2080)
       at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:108)
       at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:114)
       at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:94)
       at java.lang.Thread.run(Thread.java:745)

Looking at the appropriate code in SecureBulkLoadEndpoint.java, I'm finding the following:

        public Boolean run() {
          FileSystem fs = null;
         try {
            Configuration conf = env.getConfiguration();
            fs = FileSystem.get(conf);
            for(Pair<byte[], String> el: familyPaths) {
              Path p = new Path(el.getSecond());
              Path stageFamily = new Path(bulkToken, Bytes.toString(el.getFirst()));
              if(!fs.exists(stageFamily)) {
                fs.mkdirs(stageFamily);
                fs.setPermission(stageFamily, PERM_ALL_ACCESS);
              }
            }


The call to FileSystem.get is obviously the culprit, since it gets the FileSystem object based on fs.defaultFS, which is suboptimal in this case and other cases where the HFiles are located on a different type of filesystem than the defaultFS."
HBASE-15681	"Unable to read remote HBase tables from a local java Client due to a timeOut error. Seeing the following error:

java.io.IOException: org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=36, exceptions:
Wed Apr 20 10:32:43 WEST 2016, null, java.net.SocketTimeoutException: callTimeout=60000, callDuration=75181: row 'pentaho_mappings,,' on table 'hbase:meta' at region=hbase:meta,,1.1588230740, hostname=localhost,16020,1461071963695, seqNum=0


	at com.pentaho.big.data.bundles.impl.shim.hbase.table.HBaseTableImpl.exists(HBaseTableImpl.java:71)

	at org.pentaho.big.data.kettle.plugins.hbase.mapping.MappingAdmin.getMappedTables(MappingAdmin.java:502)

	at org.pentaho.big.data.kettle.plugins.hbase.output.HBaseOutputDialog.setupMappedTableNames(HBaseOutputDialog.java:818)

	at org.pentaho.big.data.kettle.plugins.hbase.output.HBaseOutputDialog.access$900(HBaseOutputDialog.java:88)

	at org.pentaho.big.data.kettle.plugins.hbase.output.HBaseOutputDialog$7.widgetSelected(HBaseOutputDialog.java:398)

	at org.eclipse.swt.widgets.TypedListener.handleEvent(Unknown Source)

	at org.eclipse.swt.widgets.EventTable.sendEvent(Unknown Source)

	at org.eclipse.swt.widgets.Widget.sendEvent(Unknown Source)

	at org.eclipse.swt.widgets.Display.runDeferredEvents(Unknown Source)

	at org.eclipse.swt.widgets.Display.readAndDispatch(Unknown Source)

	at org.pentaho.big.data.kettle.plugins.hbase.output.HBaseOutputDialog.open(HBaseOutputDialog.java:603)

	at org.pentaho.di.ui.spoon.delegates.SpoonStepsDelegate.editStep(SpoonStepsDelegate.java:125)

	at org.pentaho.di.ui.spoon.Spoon.editStep(Spoon.java:8783)

	at org.pentaho.di.ui.spoon.trans.TransGraph.editStep(TransGraph.java:3072)

	at org.pentaho.di.ui.spoon.trans.TransGraph.mouseDoubleClick(TransGraph.java:755)

	at org.eclipse.swt.widgets.TypedListener.handleEvent(Unknown Source)

	at org.eclipse.swt.widgets.EventTable.sendEvent(Unknown Source)

	at org.eclipse.swt.widgets.Widget.sendEvent(Unknown Source)

	at org.eclipse.swt.widgets.Display.runDeferredEvents(Unknown Source)

	at org.eclipse.swt.widgets.Display.readAndDispatch(Unknown Source)

	at org.pentaho.di.ui.spoon.Spoon.readAndDispatch(Spoon.java:1347)

	at org.pentaho.di.ui.spoon.Spoon.waitForDispose(Spoon.java:7989)

	at org.pentaho.di.ui.spoon.Spoon.start(Spoon.java:9269)

	at org.pentaho.di.ui.spoon.Spoon.main(Spoon.java:662)

	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)

	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)

	at java.lang.reflect.Method.invoke(Unknown Source)

	at org.pentaho.commons.launcher.Launcher.main(Launcher.java:92)

Caused by: org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=36, exceptions:
Wed Apr 20 10:32:43 WEST 2016, null, java.net.SocketTimeoutException: callTimeout=60000, callDuration=75181: row 'pentaho_mappings,,' on table 'hbase:meta' at region=hbase:meta,,1.1588230740, hostname=localhost,16020,1461071963695, seqNum=0


	at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.throwEnrichedException(RpcRetryingCallerWithReadReplicas.java:270)

	at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:225)

	at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:63)

	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:200)

	at org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:314)

	at org.apache.hadoop.hbase.client.ClientScanner.nextScanner(ClientScanner.java:289)

	at org.apache.hadoop.hbase.client.ClientScanner.initializeScannerInConstruction(ClientScanner.java:161)

	at org.apache.hadoop.hbase.client.ClientScanner.<init>(ClientScanner.java:156)

	at org.apache.hadoop.hbase.client.HTable.getScanner(HTable.java:888)

	at org.apache.hadoop.hbase.MetaTableAccessor.fullScan(MetaTableAccessor.java:601)

	at org.apache.hadoop.hbase.MetaTableAccessor.tableExists(MetaTableAccessor.java:365)

	at org.apache.hadoop.hbase.client.HBaseAdmin.tableExists(HBaseAdmin.java:310)

	at org.pentaho.hadoop.hbase.factory.HBase10Admin.tableExists(HBase10Admin.java:41)

	at org.pentaho.hbase.shim.common.CommonHBaseConnection.tableExists(CommonHBaseConnection.java:206)

	at org.pentaho.hbase.shim.common.HBaseConnectionImpl.access$801(HBaseConnectionImpl.java:35)

	at org.pentaho.hbase.shim.common.HBaseConnectionImpl$9.call(HBaseConnectionImpl.java:185)

	at org.pentaho.hbase.shim.common.HBaseConnectionImpl$9.call(HBaseConnectionImpl.java:181)

	at org.pentaho.hbase.shim.common.HBaseConnectionImpl.doWithContextClassLoader(HBaseConnectionImpl.java:76)

	at org.pentaho.hbase.shim.common.HBaseConnectionImpl.tableExists(HBaseConnectionImpl.java:181)

	at com.pentaho.big.data.bundles.impl.shim.hbase.HBaseConnectionWrapper.tableExists(HBaseConnectionWrapper.java:72)

	at com.pentaho.big.data.bundles.impl.shim.hbase.table.HBaseTableImpl.exists(HBaseTableImpl.java:69)

	... 28 more

Caused by: java.net.SocketTimeoutException: callTimeout=60000, callDuration=75181: row 'pentaho_mappings,,' on table 'hbase:meta' at region=hbase:meta,,1.1588230740, hostname=localhost,16020,1461071963695, seqNum=0

	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:159)

	at org.apache.hadoop.hbase.client.ResultBoundedCompletionService$QueueingFuture.run(ResultBoundedCompletionService.java:64)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)

	at java.lang.Thread.run(Unknown Source)

Caused by: java.net.ConnectException: Connection refused: no further information

	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)

	at sun.nio.ch.SocketChannelImpl.finishConnect(Unknown Source)

	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)

	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)

	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)

	at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.setupConnection(RpcClientImpl.java:404)

	at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.setupIOstreams(RpcClientImpl.java:710)

	at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.writeRequest(RpcClientImpl.java:890)

	at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.tracedWriteRequest(RpcClientImpl.java:859)

	at org.apache.hadoop.hbase.ipc.RpcClientImpl.call(RpcClientImpl.java:1193)

	at org.apache.hadoop.hbase.ipc.AbstractRpcClient.callBlockingMethod(AbstractRpcClient.java:216)

	at org.apache.hadoop.hbase.ipc.AbstractRpcClient$BlockingRpcChannelImplementation.callBlockingMethod(AbstractRpcClient.java:300)

	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.scan(ClientProtos.java:32651)

	at org.apache.hadoop.hbase.client.ScannerCallable.openScanner(ScannerCallable.java:372)

	at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:199)

	at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:62)

	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:200)

	at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:371)

	at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:345)

	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:126)

	... 4 more



hbase-site.xml :

<?xml version=""1.0""?>
<?xml-stylesheet type=""text/xsl"" href=""configuration.xsl""?>
<!--
/**
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * ""License""); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
-->
<configuration>
<property>
   <name>hbase.cluster.distributed</name>
   <value>true</value>
</property>
<property>
   <name>hbase.rootdir</name>
   <value>hdfs://master-sigma:54310/hbase</value>
</property>
<property>
    <name>hbase.zookeeper.quorum</name>
    <value>master-sigma</value>
</property>
<property>
  <name>hbase.master.ipc.address</name>
  <value>0.0.0.0</value>
</property>
<property>
  <name>hbase.regionserver.ipc.address</name>
  <value>0.0.0.0</value>
</property>
<property>
  <name>hbase.master</name>
  <value>master-sigma:16000</value>
</property>

</configuration>

Your help please
thanks"
HBASE-17687	"First , I created a table on phoenix, as this:
---------------------------------------------------------------------------
DROP TABLE IF EXISTS bidwd_test01 CASCADE;
CREATE TABLE IF NOT EXISTS bidwd_test01(
   rk VARCHAR,
   c1 integer,
   c2 VARCHAR,
   c3 VARCHAR,
   c4 VARCHAR
   constraint bidwd_test01_pk primary key(rk)
)
COMPRESSION='SNAPPY'
;
---------------------------------------------------------------------------
And then , I upserted two rows into the table:
---------------------------------------------------------------------------
upsert into bidwd_test01 values('001',1,'zhangsan','20170217','2017-02-17 12:34:22');
upsert into bidwd_test01 values('002',2,'lisi','20170216','2017-02-16 12:34:22');
---------------------------------------------------------------------------
At last , I scaned the table like this:
---------------------------------------------------------------------------
select * from bidwd_test01;
---------------------------------------------------------------------------

It's OK by now, but, I want to create a hive on hbase table ,that mapping to the phoenix table , the script likes this:
---------------------------------------------------------------------------
USE BIDWD;
DROP TABLE test01;
CREATE EXTERNAL TABLE test01
(
 rk string,
 id int,
 name string,
 datekey string,
 time_stamp string
)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'  
WITH SERDEPROPERTIES (""hbase.columns.mapping"" = "":key,0:C1,0:C2,0:C3,0:C4"")  
TBLPROPERTIES (""hbase.table.name"" = ""BIDWD_TEST01"");
---------------------------------------------------------------------------

So,I also try to insert some data into the table,and scan this table:
---------------------------------------------------------------------------
set hive.execution.engine=mr;
insert into test01 values('003',3,'lisi2','20170215','2017-02-15 12:34:22');
select * from test01;
---------------------------------------------------------------------------

But,there are some problems like this:
+------------+------------+--------------+-----------------+----------------------+--+
| test01.rk  | test01.id  | test01.name  | test01.datekey  |  test01.time_stamp   |
+------------+------------+--------------+-----------------+----------------------+--+
| 001        | NULL       | zhangsan     | 20170217        | 2017-02-17 12:34:22  |
| 002        | NULL       | lisi         | 20170216        | 2017-02-16 12:34:22  |
| 003        | 3          | lisi2        | 20170215        | 2017-02-15 12:34:22  |
+------------+------------+--------------+-----------------+----------------------+--+

the column ""id"" 's value was null,only the last row is ok.
but,when I scan data in the phoenix ,there are some errors like this:
Error: ERROR 201 (22000): Illegal data. Expected length of at least 115 bytes, but had 31 (state=22000,code=201)
java.sql.SQLException: ERROR 201 (22000): Illegal data. Expected length of at least 115 bytes, but had 31
	at org.apache.phoenix.exception.SQLExceptionCode$Factory$1.newException(SQLExceptionCode.java:389)
	at org.apache.phoenix.exception.SQLExceptionInfo.buildException(SQLExceptionInfo.java:145)
	at org.apache.phoenix.schema.KeyValueSchema.next(KeyValueSchema.java:211)
	at org.apache.phoenix.expression.ProjectedColumnExpression.evaluate(ProjectedColumnExpression.java:113)
	at org.apache.phoenix.compile.ExpressionProjector.getValue(ExpressionProjector.java:69)
	at org.apache.phoenix.jdbc.PhoenixResultSet.getString(PhoenixResultSet.java:591)
	at sqlline.Rows$Row.<init>(Rows.java:183)
	at sqlline.BufferedRows.<init>(BufferedRows.java:38)
	at sqlline.SqlLine.print(SqlLine.java:1546)
	at sqlline.Commands.execute(Commands.java:833)
	at sqlline.Commands.sql(Commands.java:732)
	at sqlline.SqlLine.dispatch(SqlLine.java:702)
	at sqlline.SqlLine.begin(SqlLine.java:575)
	at sqlline.SqlLine.start(SqlLine.java:292)
	at sqlline.SqlLine.main(SqlLine.java:194)

So,I don't know why? How can I solve this problem?

"
HBASE-5742	"In 0.90 the LoadIncrementalHFiles constructor did not throw an exception, now it throws Exception.  The constructor should ether not throw an exception or throw ZooKeeperConnectionException, and MasterNotRunningException since those come from the HBaseAdmin call.

https://github.com/apache/hbase/blob/trunk/src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java#L105"
HBASE-11790	While debugging HBASE-11591 it was found that we in case of bulk load either thro TSV and by using LoadIncrementalHFiles we should be able to use the HFileOutputFormat2 so that the the metadata BULKLOAD_TIME_KEY is added to the HFile.  If not this file will not be considered as part of bulk load and we end up in not reading from these files.  
HBASE-14236	"This is an error on online documation.  (http://hbase.apache.org/book.html#_get_started_with_hbase ).

To locate the error you can go to 
2.2. Get Started with HBase
Example 2. Example hbase-site.xml for Standalone HBase

The config example here do not specify a port for the region server. So When you go to the 2.4 Advanced - Fully Distributed (http://hbase.apache.org/book.html#quickstart_fully_distributed). You will hit error.
Because the region server will conflict with backup master or master. As they use the same port.

People who read this section are always newbee, this will confuse them.


"
HBASE-11850	"we don't stop until the map in 'connections' is empty.
the new connection is put in this map at creation, but if this connection is not used it will never be removed.

No totally sure of the fix yet. Probability is low (but not zero. I saw it happening).

The code is different in 0.98. It's 0.99+ issue only."
HBASE-11894	"As part of HBASE-9249 & HBASE-9489, added new hooks in split and merge which take meta entries from coprocessor hooks if any. These meta entries helps to ensure atomicity of split(merge) of regions by server and split(merge) of the regions handled in coprocessors(This is required in secondary indexing case).

After HBASE-11611 the meta entries are not getting added to meta both in split and merge.
{code}
    @MetaMutationAnnotation
    List<Mutation> metaEntries = new ArrayList<Mutation>();
    if (rsCoprocessorHost != null) {
      if (rsCoprocessorHost.preMergeCommit(this.region_a, this.region_b, metaEntries)) {
        throw new IOException(""Coprocessor bypassing regions "" + this.region_a + "" ""
            + this.region_b + "" merge."");
      }
      try {
        for (Mutation p : metaEntries) {
          HRegionInfo.parseRegionName(p.getRow());
        }
      } catch (IOException e) {
        LOG.error(""Row key of mutation from coprocessor is not parsable as region name.""
            + ""Mutations from coprocessor should only be for hbase:meta table."", e);
        throw e;
      }
    }

    // This is the point of no return. Similar with SplitTransaction.
    // IF we reach the PONR then subsequent failures need to crash out this
    // regionserver
    this.journal.add(JournalEntry.PONR);

    // Add merged region and delete region_a and region_b
    // as an atomic update. See HBASE-7721. This update to hbase:meta makes the region
    // will determine whether the region is merged or not in case of failures.
    // If it is successful, master will roll-forward, if not, master will
    // rollback
    if (services != null && !services.reportRegionStateTransition(TransitionCode.MERGE_PONR,
        mergedRegionInfo, region_a.getRegionInfo(), region_b.getRegionInfo())) {
      // Passed PONR, let SSH clean it up
      throw new IOException(""Failed to notify master that merge passed PONR: ""
        + region_a.getRegionInfo().getRegionNameAsString() + "" and ""
        + region_b.getRegionInfo().getRegionNameAsString());
    }
{code}

I think while reporting region state transition to master we need to pass meta entries also so that we can add them to meta along with split or merge updates."
HBASE-8997	hbase-8996 disabled the test because it is flakey.  See hbase-8996 for the thread dump when test was hung.
HBASE-17339	"The current implementation of a get operation (to retrieve values for a specific key) scans through all relevant stores of the region; for each store both memory components (memstores segments) and disk components (hfiles) are scanned in parallel.
We suggest to apply an optimization that speculatively scans memory-only components first and only if the result is incomplete scans both memory and disk."
HBASE-13293	"https://builds.apache.org/job/HBase-TRUNK-jacoco/15/testReport/junit/org.apache.hadoop.hbase.trace/TestHTraceHooks/testTraceCreateTable/

{noformat}
java.util.ConcurrentModificationException: null
	at java.util.HashMap$HashIterator.nextEntry(HashMap.java:926)
	at java.util.HashMap$KeyIterator.next(HashMap.java:960)
	at org.apache.htrace.TraceTree$SpansByProcessId.<init>(TraceTree.java:111)
	at org.apache.htrace.TraceTree.<init>(TraceTree.java:151)
	at org.apache.hadoop.hbase.trace.TestHTraceHooks.testTraceCreateTable(TestHTraceHooks.java:120)
{noformat}

This should be a benefit of running unit test slow..."
HBASE-12225	Went zombie in HBase-TRUNK #5644  Add timeout so fails faster.
HBASE-16909	See linked jira for comments. Change name as seems appropriate.
HBASE-5617	"With coprocessors hooks while put happens we have the provision to create new puts to other tables or regions.  These puts can be done with writeToWal as false.
In 0.94 and above the puts are first written to memstore and then to WAL.  If any failure in the WAL append or sync the memstore is rollbacked.  
Now the problem is that if the put that happens in the main flow fails there is no way to rollback the 
puts that happened in the prePut.

We can add coprocessor hooks to like pre/postRoolBackMemStore.  Is any one hook enough here?"
HBASE-12941	CompactionRequestor is a 'interface audience private' class with no users in the HBase code base.  Unused things should be deleted.
HBASE-12705	"Among the comments in HBASE-12294 was reference to the fact that you can't just build HBase by running {{mvn package}} from the top-level directory because of dependencies of certain goals on the hbase-annotations and hbase-checkstyle modules. I'd like to take a stab at reorganizing the project layout so that it's no longer necessary to do a {{mvn install}} on these modules before your local build will succeed.

Down the rabbit hole I go, unless someone objects. :)"
HBASE-15110	"right now the ref guide lists Hadoop 1.0 as ""X"" for hbase 0.94. the 0.94 version of the book listed it as ""S"".

It's also the default version used in the 0.94 pom, so we should update the ref guide to show it as a minimum of NT."
HBASE-6322	"From a mailing list question:

While generating some load against a library that makes extensive use of HTablePool in 0.92, I noticed that the largest heap consumer was java.lang.ref.Finalizer.  Digging in, I discovered that HTablePool's internal PooledHTable extends HTable, which instantiates a ThreadPoolExecutor and supporting objects every time a pooled HTable is retrieved.  Since ThreadPoolExecutor has a finalizer, it and its dependencies can't get garbage collected until the finalizer runs.  The result is by using HTablePool, we're creating a ton of objects to be finalized that are stuck on the heap longer than they should be, creating our largest source of pressure on the garbage collector.  It looks like this will also be a problem in 0.94 and trunk.

The easy fix is just to have PooledHTable implement HTableInterface (rather than subclass HTable), but this does break a unit test that explicitly checks that PooledHTable implements HTable -- I can only assume this test is there for some historical passivity reason."
HBASE-15749	"HBase codebase uses Guava library extensively.

There have been JIRAs such as HBASE-14963 which tried to make compatibility story around Guava better.

Long term fix, as suggested over in HBASE-14963, is to shade Guava dependency.
Future use of Guava in HBase would be more secure once shading is done."
HBASE-18128	"The sequence for a compaction are as follows:
1. Compaction writes new files under region/.tmp directory (compaction output)
2. Compaction atomically moves the temporary file under region directory
3. Compaction appends a WAL edit containing the compaction input and output files. Forces sync on WAL.
4. Compaction deletes the input files from the region directory.

But if a flush happened between 3 and 4, then the regionserver crushed. The compaction marker will be skipped when splitting log because the sequence id of compaction marker is smaller than lastFlushedSequenceId.
{code}
        if (lastFlushedSequenceId >= entry.getKey().getLogSeqNum()) {
          editsSkipped++;
          continue;
        }
{code}"
HBASE-17742	"As titled, reverse scan with empty start row should return empty result but doesn't in latest master code base."
HBASE-17811	"Module Thrift fails with this output:
Results :

Failed tests:
  TestThriftHttpServer.testRunThriftServerWithHeaderBufferLength
Expected: (an instance of org.apache.thrift.transport.TTransportException and exception with message a string containing ""HTTP Response code: 413"")
     but: exception with message a string containing ""HTTP Response code: 413"" message was ""java.net.ConnectException: Connection refused (Connection refused)""
Stacktrace was: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused (Connection refused)
        at org.apache.thrift.transport.THttpClient.flush(THttpClient.java:356)
        at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:73)
        at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:62)
        at org.apache.hadoop.hbase.thrift.generated.Hbase$Client.send_getTableNames(Hbase.java:901)
        at org.apache.hadoop.hbase.thrift.generated.Hbase$Client.getTableNames(Hbase.java:894)
        at org.apache.hadoop.hbase.thrift.TestThriftServer.checkTableList(TestThriftServer.java:248)
        at org.apache.hadoop.hbase.thrift.TestThriftHttpServer.talkToThriftServer(TestThriftHttpServer.java:187)
        at org.apache.hadoop.hbase.thrift.TestThriftHttpServer.runThriftServer(TestThriftHttpServer.java:147)
        at org.apache.hadoop.hbase.thrift.TestThriftHttpServer.testRunThriftServerWithHeaderBufferLength(TestThriftHttpServer.java:121)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
        at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused (Connection refused)
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
        at java.net.Socket.connect(Socket.java:589)
        at java.net.Socket.connect(Socket.java:538)
        at sun.net.NetworkClient.doConnect(NetworkClient.java:180)
        at sun.net.www.http.HttpClient.openServer(HttpClient.java:432)
        at sun.net.www.http.HttpClient.openServer(HttpClient.java:527)
        at sun.net.www.http.HttpClient.<init>(HttpClient.java:211)
        at sun.net.www.http.HttpClient.New(HttpClient.java:308)
        at sun.net.www.http.HttpClient.New(HttpClient.java:326)
        at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1202)
        at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1138)
        at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1032)
        at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:966)
        at org.apache.thrift.transport.THttpClient.flush(THttpClient.java:344)
        ... 20 more


Tests run: 88, Failures: 1, Errors: 0, Skipped: 0
"
HBASE-17781	"Command:
mvn clean install -X

Previous History:
After applying patch HBASE-17723, the test ""TestSimpleRpcScheduler"" passes but, the below test fails.

Final Output:

Tests in error:
  TestAcidGuarantees.testGetAtomicity:418->runTestAtomicity:315->runTestAtomicity:324->runTestAtomicity:388 禄 Runtime
  TestAcidGuarantees.testMobGetAtomicity:435->runTestAtomicity:388 禄 Runtime Def...

Tests run: 1774, Failures: 0, Errors: 2, Skipped: 6
"
HBASE-17789	", if we try to a hbase using ""HBaseAdmin"" by reading a value of """"hbase.zookeeper.quorum"" from list peers with value as ""a:2181,b:2181c:2181:/hbase"". org.apache.hadoop.hbase.zookeeper.ZKConfig.getZKQuorumServersStringFromHbaseConfig returns a incorrect queryString which causes following exception Because last host would see  :/hbase also in the string. This method was existing earlier but never being called before above change"
HBASE-17767	"cms gc pause serious program:

2017-03-10T10:15:29.524+0800: 4555920.160: [GC2017-03-10T10:15:29.524+0800: 4555920.160: [ParNew: 3067133K->340736K(3067136K), 2.0586980 secs] 80328431K->78058138K(100322560K), 2.0590280 secs] [Times: user=3.94 sys=0.34, real=2.05 secs]
2017-03-10T10:15:32.911+0800: 4555923.547: [CMS-concurrent-sweep: 1441.773/1618.869 secs] [Times: user=2518.60 sys=59.25, real=1618.62 secs]
2017-03-10T10:15:32.911+0800: 4555923.547: [CMS-concurrent-reset-start]
2017-03-10T10:15:33.126+0800: 4555923.762: [CMS-concurrent-reset: 0.215/0.215 secs] [Times: user=1.23 sys=0.08, real=0.22 secs]
2017-03-10T10:15:33.236+0800: 4555923.873: [GC2017-03-10T10:15:33.237+0800: 4555923.873: [ParNew: 3067011K->340736K(3067136K), 2.4140270 secs] 80615855K->78315999K(100322560K), 2.4144230 secs] [Times: user=4.63 sys=0.36, real=2.41 secs]
2017-03-10T10:15:35.655+0800: 4555926.292: [GC [1 CMS-initial-mark: 77975263K(97255424K)] 78316286K(100322560K), 0.0149650 secs] [Times: user=0.01 sys=0.00, real=0.01 secs]
2017-03-10T10:15:35.671+0800: 4555926.307: [CMS-concurrent-mark-start]
2017-03-10T10:15:36.098+0800: 4555926.734: [CMS-concurrent-mark: 0.427/0.427 secs] [Times: user=5.72 sys=0.05, real=0.43 secs]
2017-03-10T10:15:36.098+0800: 4555926.734: [CMS-concurrent-preclean-start]
2017-03-10T10:15:36.291+0800: 4555926.928: [CMS-concurrent-preclean: 0.192/0.193 secs] [Times: user=0.80 sys=0.03, real=0.19 secs]
2017-03-10T10:15:36.291+0800: 4555926.928: [CMS-concurrent-abortable-preclean-start]
2017-03-10T10:15:37.378+0800: 4555928.014: [GC2017-03-10T10:15:37.378+0800: 4555928.014: [ParNew: 3067083K->340736K(3067136K), 2.6221190 secs] 81042347K->78771078K(100322560K), 2.6224970 secs] [Times: user=4.79 sys=0.48, real=2.62 secs]
2017-03-10T10:15:41.012+0800: 4555931.648: [CMS-concurrent-abortable-preclean: 2.083/4.721 secs] [Times: user=13.51 sys=0.87, real=4.72 secs]
2017-03-10T10:15:41.015+0800: 4555931.652: [GC[YG occupancy: 2011637 K (3067136 K)]2017-03-10T10:15:41.016+0800: 4555931.652: [GC2017-03-10T10:15:41.016+0800: 4555931.652: [ParNew: 2011637K->340736K(3067136K), 2.0773980 secs] 80441979K->79117650K(100322560K), 2.0777380 secs] [Times: user=4.09 sys=0.38, real=2.07 secs]






regionserver  JVM config:

export HBASE_REGIONSERVER_OPTS=""$HBASE_REGIONSERVER_OPTS -XX:PermSize=256m -XX:MaxPermSize=256m -Xms96G -Xmx96G""
export HBASE_OPTS=""$HBASE_OPTS -Djava.net.preferIPv4Stack=true
-XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=60 -XX:+CMSParallelRemarkEnabled -XX:+CMSConcurrentMTEnabled
-XX:ParallelGCThreads=40 -XX:+DisableExplicitGC -XX:+PrintGCDetails -XX:+PrintGCDateStamps -verbose:gc
-XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=1 -XX:+CMSScavengeBeforeRemark -XX:+HeapDumpOnOutOfMemoryError

"
HBASE-15431	"I ran HBase with ""-XX:+PrintCompilation -XX:+UnlockDiagnosticVMOptions -XX:+PrintInlining"" and then looked for ""hot method too big"" log lines.

I'll attach a log of those messages.
I tried to increase -XX:FreqInlineSize to 1010 to inline all these methods (as long as they're hot, but actually didn't see any improvement).

In all cases I primed the JVM to make sure the JVM gets a chance to profile the methods and decide whether they're hot or not."
HBASE-17643	Looking at https://builds.apache.org/job/HBase-1.3-JDK8/ there's bunch of failing / flaky builds. Prior to releasing 1.3.1 we need to address those.
HBASE-17693	"testFailover is failing with Timeout Error.
{code}
Error Message

test timed out after 30000 milliseconds
Stacktrace

org.junit.runners.model.TestTimedOutException: test timed out after 30000 milliseconds
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hbase.util.Threads.sleep(Threads.java:146)
	at org.apache.hadoop.hbase.master.TestMasterNoCluster.testFailover(TestMasterNoCluster.java:246)
{code}"
HBASE-17695	"testMultiRegionTable UT fails as below.

{code}
Error Message

Failed after attempts=35, exceptions:
Thu Feb 23 21:07:26 UTC 2017, RpcRetryingCaller{globalStartTime=1487884046035, pause=100, retries=35}, org.apache.hadoop.hbase.MasterNotRunningException: java.io.IOException: Can't get master address from ZooKeeper; znode data == null
Thu Feb 23 21:07:26 UTC 2017, RpcRetryingCaller{globalStartTime=1487884046035, pause=100, retries=35}, org.apache.hadoop.hbase.MasterNotRunningException: java.io.IOException: Can't get master address from ZooKeeper; znode data == null
Thu Feb 23 21:07:26 UTC 2017, RpcRetryingCaller{globalStartTime=1487884046035, pause=100, retries=35}, org.apache.hadoop.hbase.MasterNotRunningException: java.io.IOException: Can't get master address from ZooKeeper; znode data == null
Thu Feb 23 21:07:27 UTC 2017, RpcRetryingCaller{globalStartTime=1487884046035, pause=100, retries=35}, org.apache.hadoop.hbase.MasterNotRunningException: java.io.IOException: Can't get master address from ZooKeeper; znode data == null{code}"
HBASE-17692	"testSameVersionUpdatesRecoveryWithCompaction test fails with NPE as below.
{code:title=cmd}mvn -B -nsu test -Dtest=TestOpenedRegionHandler,TestMasterFileSystemWithWALDir,TestDistributedLogSplitting,TestRegionPlacement,TestGetLastFlushedSequenceId,TestZKLessAMOnCluster,TestMasterRestartAfterDisablingTable,TestSplitLogManager,TestSimpleRegionNormalizerOnCluster,TestHFileLinkCleaner,TestSnapshotFromMaster,TestLogsCleaner,TestModifyColumnFamilyProcedure,TestCreateTableProcedure2,TestTruncateTableProcedure,TestWALProcedureStoreOnHDFS,TestProcedureAdmin,TestAddColumnFamilyProcedure,TestDisableTableProcedure,TestCreateTableProcedure,TestDeleteTableProcedure,TestEnableTableProcedure,TestMasterProcedureQueue,TestDeleteColumnFamilyProcedure,TestModifyTableProcedure,TestMasterFailoverWithProcedures,TestSnapshotFileCache,TestTableDeleteFamilyHandler,TestCreateTableHandler,TestTableDescriptorModification,TestEnableTableHandler,TestStochasticLoadBalancer,TestFavoredNodeAssignmentHelper,TestStochasticLoadBalancer2,TestConstraint,TestFilterList,TestParseFilter,TestSingleColumnValueFilter,TestMultipleColumnPrefixFilter,TestColumnPrefixFilter,TestColumnPaginationFilter,TestPrefixFilter,TestScanRowPrefix,TestNullComparator,TestRegexComparator,TestPageFilter,TestDependentColumnFilter,TestFuzzyRowAndColumnRangeFilter,TestFuzzyRowFilterEndToEnd,TestFilterSerialization,TestFilterWrapper,TestComparatorSerialization,TestRandomRowFilter,TestSingleColumnValueExcludeFilter,TestColumnRangeFilter,TestFilter,TestFuzzyRowFilter,TestFirstKeyValueMatchingQualifiersFilter,TestMultiRowRangeFilter,TestPutDeleteEtcCellIteration,TestResultSizeEstimation,TestTimestampsFilter,TestUpdateConfiguration,TestClientTimeouts,TestResult,TestClientPushback,TestConnectionUtils --projects :hbase-server{code}

{code}
Running org.apache.hadoop.hbase.filter.TestFirstKeyValueMatchingQualifiersFilter
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.4 sec - in org.apache.hadoop.hbase.filter.TestFirstKeyValueMatchingQualifiersFilter
Running org.apache.hadoop.hbase.filter.TestDependentColumnFilter
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.487 sec - in org.apache.hadoop.hbase.filter.TestDependentColumnFilter

Results :


Tests in error: 
  TestDistributedLogSplitting.testSameVersionUpdatesRecoveryWithCompaction:1377 禄 NullPointer


Tests run: 384, Failures: 0, Errors: 1, Skipped: 3

[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 59:21 min
[INFO] Finished at: 2017-02-23T21:57:52+00:00
[INFO] Final Memory: 51M/898M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18:test (default-test) on project hbase-server: There are test failures.
[ERROR] 
[ERROR] Please refer to /grid/0/nobody/workspace/build-support/SOURCES/hbase/hbase-server/target/surefire-reports for the individual test results.
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException{code}

{code}
Stacktrace

java.lang.NullPointerException: null
	at org.apache.hadoop.hbase.util.Bytes.toLong(Bytes.java:604)
	at org.apache.hadoop.hbase.util.Bytes.toLong(Bytes.java:578)
	at org.apache.hadoop.hbase.master.TestDistributedLogSplitting.testSameVersionUpdatesRecoveryWithCompaction(TestDistributedLogSplitting.java:1377){code}"
HBASE-17150	"This part of application is a legacy code from a first version. 
If backup destination is local cluster, then during restore we copy HFiles into local temp dir first. For remote cluster we do not do this. Seems should be other way around.
{quote}
What does this mean?
253 2016-11-17 14:13:39,782 DEBUG [main] util.RestoreServerUtil: File hdfs://ve0524.halxg.cloudera.com:8020/user/stack/backup/backup_1479419995738/default/x_1/archive/data/default/x_1 on local cluster, back it up before restore
Is this a full copy of the backup to elsewhere?
296 2016-11-17 14:13:47,907 DEBUG [main] util.RestoreServerUtil: Copied to temporary path on local cluster: /user/stack/hbase-staging/restore
{quote}"
HBASE-13942	"Performing any operataion using a single hconnection with client threads > hbase.hconnection.threads.max causing the client to stall indefinetly during first region split. All the hconnection threads in client side are waiting with the following stack. 

hconnection-0x648a83fd-shared--pool1-t8"" daemon prio=10 tid=0x00007f447c003800 nid=0x62ff waiting on condition [0x00007f44c72f0000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000007d768bdf0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
        at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:374)
        at org.apache.hadoop.hbase.util.BoundedCompletionService.take(BoundedCompletionService.java:74)
        at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:174)
        at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:56)
        at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:200)
        at org.apache.hadoop.hbase.client.ClientSmallReversedScanner.next(ClientSmallReversedScanner.java:145)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegionInMeta(ConnectionManager.java:1200)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1109)
        at org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.findAllLocationsOrFail(AsyncProcess.java:916)
        at org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.groupAndSendMultiAction(AsyncProcess.java:833)
        at org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.resubmit(AsyncProcess.java:1156)
        at org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.receiveMultiAction(AsyncProcess.java:1296)
        at org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.access$1200(AsyncProcess.java:574)
        at org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl$SingleServerRequestRunnable.run(AsyncProcess.java:716)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
 "
HBASE-17461	"On HBase shell, *major_compact* command simply passes the received *table_or_region_name* parameter straight to java *HBaseAdmin.majorCompact* method.

On some corner cases, HBase tables row keys may have special characters. Then, if a region is split in such a way that row keys with special characters are now part of the region name, calling *major_compact* on this regions will fail, if the special character ASCII code is higher than 127. This happens because Java byte type is signed, while ruby byte type isn't, causing the region name to be converted to a wrong string at Java side.

For example, considering a region named as below:

{noformat}
test,\xF8\xB9B2!$\x9C\x0A\xFEG\xC0\xE3\x8B\x1B\xFF\x15,1481745228583.b4bc69356d89018bfad3ee106b717285.
{noformat} 

Calling major_compat on it fails as follows:

{noformat}
hbase(main):008:0* major_compact ""test,\xF8\xB9B2!$\x9C\x0A\xFEG\xC0\xE3\x8B\x1B\xFF\x15,1484177359169.8128fa75ae0cd4eba38da2667ac8ec98.""

ERROR: Illegal character code:44, <,> at 4. User-space table qualifiers can only contain 'alphanumeric characters': i.e. [a-zA-Z_0-9-.]: test,锟紹2!$锟?锟紾锟斤拷锟?484177359169.8128fa75ae0cd4eba38da2667ac8ec98.
{noformat}

An easy solution is to convert *table_or_region_name* parameter properly, prior to calling *HBaseAdmin.majorCompact* in the same way as it's already done on some other shell commands, such as *get*:

{noformat}
admin.major_compact(table_or_region_name.to_s.to_java_bytes, family)
{noformat}"
HBASE-17378	"launch all mxosrvrs of a downed node, which will take a long time, about 4 mintues 45 seconds."
HBASE-17196	"In the following case, the deleted mob cell can come back.

{code}
1) hbase(main):001:0> create 't1', {NAME => 'f1', IS_MOB => true, MOB_THRESHOLD => 10}

2) hbase(main):002:0> put 't1', 'r1', 'f1:q1', 'aaaaaaaaaaaaaaaaaaaa'

3) hbase(main):003:0> flush 't1'

4) hbase(main):004:0> deleteall 't1', 'r1'

5) hbase(main):005:0> scan 't1'
ROW                                                 COLUMN+CELL                                                                                                                                           
0 row(s)

6) hbase(main):006:0> flush 't1'

7) hbase(main):007:0> major_compact 't1'

After that, go to mobdir, remove the _del file, this is to simulate the case that  mob minor compaction does not the _del file. Right now, the cell in normal region is gone after the major compaction.

8) hbase(main):008:0> put 't1', 'r2', 'f1:q1', 'bbbbbbbbbbbbbbbbbbbbbbbb'
                                                                                                                                                      
9) hbase(main):009:0> flush 't1'

10) hbase(main):010:0> scan 't1'
ROW                                                 COLUMN+CELL                                                                                                                                           
 r2                                                 column=f1:q1, timestamp=1480451201393, value=bbbbbbbbbbbbbbbbbbbbbbbb                                                                                 
1 row(s)

11) hbase(main):011:0> compact 't1', 'f1', 'MOB'

12) hbase(main):012:0> scan 't1'
ROW                                                 COLUMN+CELL                                                                                                                                           
 r1                                                 column=f1:q1, timestamp=1480450987725, value=aaaaaaaaaaaaaaaaaaaa                                                                                     
 r2                                                 column=f1:q1, timestamp=1480451201393, value=bbbbbbbbbbbbbbbbbbbbbbbb                                                                                 
2 row(s)

The deleted ""r1"" comes back. The reason is that mob minor compaction does not include _del files so it generates references for the deleted cell.
{code}"
HBASE-6338	"Every call in rpc handler a Method will be created, if we cache the method will improve a little.
I test with 0.90, Average Class.getMethod(String name, Class... parameterTypes) cost 4780 ns , if we cache it cost 2620 ns.
"
HBASE-6968	"Here are 2 hbase write performance improvements recently:

1) Avoid creating HBaseConfiguraiton object for each HLog. Every time when creating a HBaseConfiguraiton object, it would parse the xml configuration files from disk, which is not cheap operation.
In HLog.java:
orig:
{code:title=HLog.java}
  newWriter = createWriter(fs, newPath, HBaseConfiguration.create(conf));
{code}
new:
{code}
  newWriter = createWriter(fs, newPath, conf);
{code}


2) Change 2 hotspot synchronized functions into double locking pattern. So it shall remove the synchronization overhead in the normal case.
orig:
{code:title=HBaseRpcMetrics.java}
  public synchronized void inc(String name, int amt) {	
    MetricsTimeVaryingRate m = get(name);	
    if (m == null) {	
      m = create(name);	
    }	
    m.inc(amt);	
  }
{code}

new:
{code}
  public void inc(String name, int amt) {	
    MetricsTimeVaryingRate m = get(name);	
    if (m == null) {	
      synchronized (this) {	
        if ((m = get(name)) == null) {	
          m = create(name);	
        }	
      }	
    }	
    m.inc(amt);	
  }
{code}
=====================
orig:
{code:title=MemStoreFlusher.java}
  public synchronized void reclaimMemStoreMemory() {	
    if (this.server.getGlobalMemstoreSize().get() >= globalMemStoreLimit) {	
      flushSomeRegions();	
    }
  }	
{code}
new:
{code}
  public void reclaimMemStoreMemory() {	
    if (this.server.getGlobalMemstoreSize().get() >= globalMemStoreLimit) {	
      flushSomeRegions();	
    }
  }	
  private synchronized void flushSomeRegions() {	
    if (this.server.getGlobalMemstoreSize().get() < globalMemStoreLimit) {	
      return; // double check the global memstore size inside of the synchronized block.	
    }	
 ...   
 }
{code}

"
HBASE-6956	"Sometimes we see a lot of Exception about closed connections:
{code}
 org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@553fd068 closed
org.apache.hadoop.hbase.client.ClosedConnectionException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@553fd068 closed
{code}

After investigation we assumed that it occurs because closed connection returns back into HTablePool. 

For our opinion best solution is  check whether the table is closed in method HTablePool.putTable and if true don't add it into the queue and release such HTableInterface.

But unfortunatly right now there are no access to HTable#closed field through HTableInterface"
HBASE-6773	"Today, it's an application level configuration. So all the HFiles are replicated 3 times per default.

There are some reasons to make it per table:
- some tables are critical while some others are not. For example, meta would benefit from an higher level of replication, to ensure we continue working even when we lose 20% of the cluster.
- some tables are backuped somewhere else, used by non essential process, so the user may accept a lower level of replication for these ones.
- it should be a dynamic parameter. For example, during a bulk load we set a replication of 1 or 2, then we increase it. It's in the same space as disabling the WAL for some writes.


The case that seems important to me is meta. We can also handle this one by a specific parameter in the usual hbase-site.xml if we don't want a generic solution.
"
HBASE-6391	"The Scenario can be reproduce below.
Enabling an table, some region is online on regionserver,some are still being processed.
And restart the master.

when master failover:
        // Region is being served and on an active server
        // add only if region not in disabled and enabling table
        if (false == checkIfRegionBelongsToDisabled(regionInfo)
            && false == checkIfRegionsBelongsToEnabling(regionInfo)) {
          regions.put(regionInfo, regionLocation);
          addToServers(regionLocation, regionInfo);
        }

the opened region will not add to the Regions in master.
and in the following recoverTableInEnablingState,the region will be assigned again.
that will lead to the cluster inconsistent
"
HBASE-5686	Changes similar to HBASE-5209/HBASE-5596 need to be added for Avro.
HBASE-5148	"In case you do not override compaction parameter on the table/cf level, the values returned by the table descriptor will not reflect the value configured in the cluster.

For example - let assume you disabled major compaction by setting ""hbase.hregion.majorcompaction"" in the config to ""0"", prior starting the cluster. Let also assume that you have a table that in which you didn't set at all this parameter.
Then invoking 
HTableDescriptor hTableDescriptor = conn.getHTableDescriptor(Bytes.toBytes(""my table""));
hTableDescriptor.getValue(""hbase.hregion.majorcompaction"")
should return the cluster property (currently returns the default, ignoring the cluster prop.)"
HBASE-4134	"1. I found the problem(some regions were multiply assigned) while running hbck to check the cluster's health. Here's the result:
{noformat}
ERROR: Region test1,230778,1311216270050.fff783529fcd983043610eaa1cc5c2fe. is listed in META on region server 158-1-91-101:20020 but is multiply assigned to region servers 158-1-91-101:20020, 158-1-91-105:20020 
ERROR: Region test1,252103,1311216293671.fff9ed2cb69bdce535451a07686c0db5. is listed in META on region server 158-1-91-101:20020 but is multiply assigned to region servers 158-1-91-101:20020, 158-1-91-105:20020 
ERROR: Region test1,282187,1311216322104.ffff52782c0241a598b3e37ca8729da0. is listed in META on region server 158-1-91-103:20020 but is multiply assigned to region servers 158-1-91-103:20020, 158-1-91-105:20020 
Summary: 
  -ROOT- is okay. 
    Number of regions: 1 
    Deployed on: 158-1-91-105:20020 
  .META. is okay. 
    Number of regions: 1 
    Deployed on: 158-1-91-103:20020 
  test1 is okay. 
    Number of regions: 25297 
    Deployed on: 158-1-91-101:20020 158-1-91-103:20020 158-1-91-105:20020 
14829 inconsistencies detected. 
Status: INCONSISTENT 
{noformat}

2. Then I tried to use ""hbck -fix"" to fix the problem. Everything seemed ok. But I found that the total number of regions reported by load balancer (35029) was more than the actual region count(25299) after the fixing.
Here's the related logs snippet:
{noformat}
2011-07-22 02:19:02,866 INFO org.apache.hadoop.hbase.master.LoadBalancer: Skipping load balancing.  servers=3 regions=25299 average=8433.0 mostloaded=8433 
2011-07-22 03:06:11,832 INFO org.apache.hadoop.hbase.master.LoadBalancer: Skipping load balancing.  servers=3 regions=35029 average=11676.333 mostloaded=11677 leastloaded=11676
{noformat}

3. I tracked one region's behavior during the time. Taking the region of ""test1,282187,1311216322104.ffff52782c0241a598b3e37ca8729da0."" as example:
(1) It was assigned to ""158-1-91-101"" at first. 
(2) HBCK sent closing request to RegionServer. And RegionServer closed it silently without notice to HMaster.
(3) The region was still carried by RS ""158-1-91-103"" which was known to HMaster.
(4) HBCK will trigger a new assignment.

The fact is, the region was assigned again, but the old assignment information still remained in AM#regions,AM#servers.

That's why the problem of ""region count was larger than the actual number"" occurred.  

{noformat}
Line 178967: 2011-07-22 02:47:51,247 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling new unassigned node: /hbase/unassigned/ffff52782c0241a598b3e37ca8729da0 (region=test1,282187,1311216322104.ffff52782c0241a598b3e37ca8729da0., server=HBCKServerName, state=M_ZK_REGION_OFFLINE)
Line 178968: 2011-07-22 02:47:51,247 INFO org.apache.hadoop.hbase.master.AssignmentManager: Handling HBCK triggered transition=M_ZK_REGION_OFFLINE, server=HBCKServerName, region=ffff52782c0241a598b3e37ca8729da0
Line 178969: 2011-07-22 02:47:51,248 INFO org.apache.hadoop.hbase.master.AssignmentManager: HBCK repair is triggering assignment of region=test1,282187,1311216322104.ffff52782c0241a598b3e37ca8729da0.
Line 178970: 2011-07-22 02:47:51,248 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: No previous transition plan was found (or we are ignoring an existing plan) for test1,282187,1311216322104.ffff52782c0241a598b3e37ca8729da0. so generated a random one; hri=test1,282187,1311216322104.ffff52782c0241a598b3e37ca8729da0., src=, dest=158-1-91-101,20020,1311231878544; 3 (online=3, exclude=null) available servers
Line 178971: 2011-07-22 02:47:51,248 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region test1,282187,1311216322104.ffff52782c0241a598b3e37ca8729da0. to 158-1-91-101,20020,1311231878544
Line 178983: 2011-07-22 02:47:51,285 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=158-1-91-101,20020,1311231878544, region=ffff52782c0241a598b3e37ca8729da0
Line 179001: 2011-07-22 02:47:51,318 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENED, server=158-1-91-101,20020,1311231878544, region=ffff52782c0241a598b3e37ca8729da0
Line 179002: 2011-07-22 02:47:51,319 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Handling OPENED event for ffff52782c0241a598b3e37ca8729da0; deleting unassigned node
Line 179003: 2011-07-22 02:47:51,319 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:20000-0x1314ac5addb0042-0x1314ac5addb0042 Deleting existing unassigned node for ffff52782c0241a598b3e37ca8729da0 that is in expected state RS_ZK_REGION_OPENED
Line 179007: 2011-07-22 02:47:51,326 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:20000-0x1314ac5addb0042-0x1314ac5addb0042 Successfully deleted unassigned node for region ffff52782c0241a598b3e37ca8729da0 in expected state RS_ZK_REGION_OPENED
Line 179011: 2011-07-22 02:47:51,335 WARN org.apache.hadoop.hbase.master.AssignmentManager: Overwriting ffff52782c0241a598b3e37ca8729da0 on serverName=158-1-91-103,20020,1311232056655, load=(requests=0, regions=0, usedHeap=0, maxHeap=0)
Line 179012: 2011-07-22 02:47:51,335 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Opened region test1,282187,1311216322104.ffff52782c0241a598b3e37ca8729da0. on 158-1-91-101,20020,1311231878544
{noformat}"
HBASE-5583	"Create table using splitkeys
-> MAster goes down before all regions are added to meta
-> On master restart the table is again enabled but with less number of regions than specified in splitkeys

Anyway client will get an exception if i had called sync "
HBASE-17011	"As discussed in HBASE-16972, holding commits for now until 1.3.0 comes out."
HBASE-16685	"During code review of backup / restore, Matteo raised comment on the following code:
{code}
        res = snapshotCp.run(options);
{code}
I think this will be a first time where HBase server has a direct dependency on a MR job.

also we need to revisit this code to avoid having one handler blocked for N hours while we are doing a copy.

This issue is to address the above comment."
HBASE-16715	"I am trying to import the signing keys to verify downloaded hbase releases, but it appears to fail:

{code}
$ wget -O /tmp/KEYS https://www-us.apache.org/dist/hbase/KEYS
Connecting to www-us.apache.org (140.211.11.105:443)
KEYS                 100% |*******************************| 50537   0:00:00 ETA
$ gpg --import /tmp/KEYS
gpg: directory '/root/.gnupg' created
gpg: new configuration file '/root/.gnupg/dirmngr.conf' created
gpg: new configuration file '/root/.gnupg/gpg.conf' created
gpg: keybox '/root/.gnupg/pubring.kbx' created
gpg: /root/.gnupg/trustdb.gpg: trustdb created
gpg: key 945D66AF: public key ""Jean-Daniel Cryans (ASF key) <jdcryans@apache.org>"" imported
gpg: key D34B98D6: public key ""Michael Stack <stack@apache.org>"" imported
gpg: key 30CD0996: public key ""Michael Stack <stack@duboce.net>"" imported
gpg: key AEC77EAF: public key ""Todd Lipcon <tlipcon@mercea.net>"" imported
gpg: key F48B08A4: public key ""Ted Yu (Apache Public Key) <yuzhihong@gmail.com>"" imported
gpg: key 867B57B8: public key ""Ramkrishna S Vasudevan (for code checkin) <ram_krish_86@hotmail.com>"" imported
gpg: key 7CA45750: public key ""Lars Hofhansl (CODE SIGNING KEY) <larsh@apache.org>"" imported
gpg: key A1AC25A9: public key ""Lars Hofhansl (CODE SIGNING KEY) <larsh@apache.org>"" imported
gpg: key C7CFE328: public key ""Lars Hofhansl (CODE SIGNING KEY) <larsh@apache.org>"" imported
gpg: key E964B5FF: public key ""Enis Soztutar (CODE SIGNING KEY) <enis@apache.org>"" imported
gpg: key 0D80DB7C: public key ""Sean Busbey (CODE SIGNING KEY) <busbey@apache.org>"" imported
gpg: key 8644EEB6: public key ""Nick Dimiduk <ndimiduk@apache.org>"" imported
gpg: invalid radix64 character 3A skipped
gpg: CRC error; E1B6C3 - DFECFB
gpg: [don't know]: invalid packet (ctb=55)
gpg: read_block: read error: Invalid packet
gpg: import from '/tmp/KEYS' failed: Invalid keyring
gpg: Total number processed: 12
gpg:               imported: 12
gpg: no ultimately trusted keys found
{code}"
HBASE-16546	test
HBASE-16539	"A ITBLL 1B row run fails when table schema is changed to specify lz4 compression but then servers cannot deploy it. 1.1.6rc2 passed, 1.2.3rc0 does not. Same testing environment. Something has changed between 1.1 and 1.2. 

We end up in this state and jobs cannot progress:
{noformat}
Number of regions in transition: 7
  IntegrationTestBigLinkedList,\xA0\x00\x00\x00\x00\x00\x00\x00,1472681289247.d947338bc0a76a9bba5af4c515019bc1. state=FAILED_OPEN, ts=Wed Aug 31 22:58:47 UTC 2016 (419s ago), server=null
  IntegrationTestBigLinkedList,\x8F\xFD@wn\x90\x9F\x12\x0Cej\x1F$\xE1&{,1472684144278.57f4bf5d3c6d7dbd60aa54d7080d43b6. state=FAILED_OPEN, ts=Wed Aug 31 23:01:09 UTC 2016 (276s ago), server=null
  IntegrationTestBigLinkedList,\xE0\x00\x00\x00\x00\x00\x00\x00,1472681289247.d5d8ec90585f29cd729ea3f4bf0b791a. state=FAILED_OPEN, ts=Wed Aug 31 23:01:09 UTC 2016 (276s ago), server=null
  IntegrationTestBigLinkedList,,1472683517911.19f79f1865001d4ae1a261c77726fbd4. state=FAILED_OPEN, ts=Wed Aug 31 22:58:46 UTC 2016 (419s ago), server=null
  IntegrationTestBigLinkedList,\xC0\x00\x00\x00\x00\x00\x00\x00,1472681289247.9706c191970f442489033e8baa210bfb. state=FAILED_OPEN, ts=Wed Aug 31 22:55:59 UTC 2016 (586s ago), server=null
  IntegrationTestBigLinkedList,\x80\x00\x00\x00\x00\x00\x00\x00,1472684144278.c38d5068d2e52324ad398062fc2064a6. state=FAILED_OPEN, ts=Wed Aug 31 22:56:16 UTC 2016 (569s ago), server=null
  IntegrationTestBigLinkedList,`\x00\x00\x00\x00\x00\x00\x00,1472681289247.b04b1836a8c0efd5481ccd6f50817ddf. state=FAILED_OPEN, ts=Wed Aug 31 22:53:43 UTC 2016 (722s ago), server=null
{noformat}
"
HBASE-16514	"The regionserver process was shutdown by some reason.
I looked into regionserver's log , it looks like detected jvm pause about 9122ms (zk session timout is 40s), and after 10 seconds it start another process to delete  regionserver's ephemeral node in zk, and regionserver process was gone, here is the detail log:

{quote}
2016-08-14 22:16:53,139 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 9022ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=2 time=9122ms
2016-08-14 22:17:03,121 WARN  [main] util.HeapMemorySizeUtil: hbase.regionserver.global.memstore.upperLimit is deprecated by hbase.regionserver.global.memstore.size
2016-08-14 22:17:03,177 INFO  [main] zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2016-08-14 22:17:03,177 INFO  [main] zookeeper.ZooKeeper: Client environment:host.name=olap3.data.lq
2016-08-14 22:17:03,177 INFO  [main] zookeeper.ZooKeeper: Client environment:java.version=1.7.0_09-icedtea
2016-08-14 22:17:03,177 INFO  [main] zookeeper.ZooKeeper: Client environment:java.vendor=Oracle Corporation
2016-08-14 22:17:03,177 INFO  [main] zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre
2016-08-14 22:17:03,177 INFO  [main] zookeeper.ZooKeeper: Client environment:java.class.path=/data/sysdir/hbase-1.1.2/conf:/usr/lib/jvm/java-1.7.0-openjdk.x86_64/lib/tools.jar:/data/sysdir/hbase-1.1.2:/data/sysdir/hbase-1.1.2/lib/activation-1.1.jar:/data/sysdir/hbase-1.1.2/lib/aopalliance-1.0.jar:/data/sysdir/hbase-1.1.2/lib/apacheds-i18n-2.0.0-M15.jar:/data/sysdir/hbase-1.1.2/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/data/sysdir/hbase-1.1.2/lib/api-asn1-api-1.0.0-M20.jar:/data/sysdir/hbase-1.1.2/lib/api-util-1.0.0-M20.jar:/data/sysdir/hbase-1.1.2/lib/asm-3.1.jar:/data/sysdir/hbase-1.1.2/lib/avro-1.7.4.jar:/data/sysdir/hbase-1.1.2/lib/commons-beanutils-1.7.0.jar:/data/sysdir/hbase-1.1.2/lib/commons-beanutils-core-1.8.0.jar:/data/sysdir/hbase-1.1.2/lib/commons-cli-1.2.jar:/data/sysdir/hbase-1.1.2/lib/commons-codec-1.9.jar:/data/sysdir/hbase-1.1.2/lib/commons-collections-3.2.1.jar:/data/sysdir/hbase-1.1.2/lib/commons-compress-1.4.1.jar:/data/sysdir/hbase-1.1.2/lib/commons-configuration-1.6.jar:/data/sysdir/hbase-1.1.2/lib/commons-daemon-1.0.13.jar:/data/sysdir/hbase-1.1.2/lib/commons-digester-1.8.jar:/data/sysdir/hbase-1.1.2/lib/commons-el-1.0.jar:/data/sysdir/hbase-1.1.2/lib/commons-httpclient-3.1.jar:/data/sysdir/hbase-1.1.2/lib/commons-io-2.4.jar:/data/sysdir/hbase-1.1.2/lib/commons-lang-2.6.jar:/data/sysdir/hbase-1.1.2/lib/commons-logging-1.2.jar:/data/sysdir/hbase-1.1.2/lib/commons-math-2.2.jar:/data/sysdir/hbase-1.1.2/lib/commons-math3-3.1.1.jar:/data/sysdir/hbase-1.1.2/lib/commons-net-3.1.jar:/data/sysdir/hbase-1.1.2/lib/disruptor-3.3.0.jar:/data/sysdir/hbase-1.1.2/lib/findbugs-annotations-1.3.9-1.jar:/data/sysdir/hbase-1.1.2/lib/guava-12.0.1.jar:/data/sysdir/hbase-1.1.2/lib/guice-3.0.jar:/data/sysdir/hbase-1.1.2/lib/guice-servlet-3.0.jar:/data/sysdir/hbase-1.1.2/lib/hadoop-annotations-2.5.1.jar:/data/sysdir/hbase-1.1.2/lib/hadoop-auth-2.5.1.jar:/data/sysdir/hbase-1.1.2/lib/hadoop-client-2.5.1.jar:/data/sysdir/hbase-1.1.2/lib/hadoop-common-2.5.1.jar:/data/sysdir/hbase-1.1.2/lib/hadoop-hdfs-2.5.1.jar:/data/sysdir/hbase-1.1.2/lib/hadoop-mapreduce-client-app-2.5.1.jar:/data/sysdir/hbase-1.1.2/lib/hadoop-mapreduce-client-common-2.5.1.jar:/data/sysdir/hbase-1.1.2/lib/hadoop-mapreduce-client-core-2.5.1.jar:/data/sysdir/hbase-1.1.2/lib/hadoop-mapreduce-client-jobclient-2.5.1.jar:/data/sysdir/hbase-1.1.2/lib/hadoop-mapreduce-client-shuffle-2.5.1.jar:/data/sysdir/hbase-1.1.2/lib/hadoop-yarn-api-2.5.1.jar:/data/sysdir/hbase-1.1.2/lib/hadoop-yarn-client-2.5.1.jar:/data/sysdir/hbase-1.1.2/lib/hadoop-yarn-common-2.5.1.jar:/data/sysdir/hbase-1.1.2/lib/hadoop-yarn-server-common-2.5.1.jar:/data/sysdir/hbase-1.1.2/lib/hbase-annotations-1.1.2.jar:/data/sysdir/hbase-1.1.2/lib/hbase-annotations-1.1.2-tests.jar:/data/sysdir/hbase-1.1.2/lib/hbase-client-1.1.2.jar:/data/sysdir/hbase-1.1.2/lib/hbase-common-1.1.2.jar:/data/sysdir/hbase-1.1.2/lib/hbase-common-1.1.2-tests.jar:/data/sysdir/hbase-1.1.2/lib/hbase-examples-1.1.2.jar:/data/sysdir/hbase-1.1.2/lib/hbase-hadoop2-compat-1.1.2.jar:/data/sysdir/hbase-1.1.2/lib/hbase-hadoop-compat-1.1.2.jar:/data/sysdir/hbase-1.1.2/lib/hbase-it-1.1.2.jar:/data/sysdir/hbase-1.1.2/lib/hbase-it-1.1.2-tests.jar:/data/sysdir/hbase-1.1.2/lib/hbase-prefix-tree-1.1.2.jar:/data/sysdir/hbase-1.1.2/lib/hbase-procedure-1.1.2.jar:/data/sysdir/hbase-1.1.2/lib/hbase-protocol-1.1.2.jar:/data/sysdir/hbase-1.1.2/lib/hbase-resource-bundle-1.1.2.jar:/data/sysdir/hbase-1.1.2/lib/hbase-rest-1.1.2.jar:/data/sysdir/hbase-1.1.2/lib/hbase-server-1.1.2.jar:/data/sysdir/hbase-1.1.2/lib/hbase-server-1.1.2-tests.jar:/data/sysdir/hbase-1.1.2/lib/hbase-shell-1.1.2.jar:/data/sysdir/hbase-1.1.2/lib/hbase-thrift-1.1.2.jar:/data/sysdir/hbase-1.1.2/lib/htrace-core-3.1.0-incubating.jar:/data/sysdir/hbase-1.1.2/lib/httpclient-4.2.5.jar:/data/sysdir/hbase-1.1.2/lib/httpcore-4.1.3.jar:/data/sysdir/hbase-1.1.2/lib/jackson-core-asl-1.9.13.jar:/data/sysdir/hbase-1.1.2/lib/jackson-jaxrs-1.9.13.jar:/data/sysdir/hbase-1.1.2/lib/jackson-mapper-asl-1.9.13.jar:/data/sysdir/hbase-1.1.2/lib/jackson-xc-1.9.13.jar:/data/sysdir/hbase-1.1.2/lib/jamon-runtime-2.3.1.jar:/data/sysdir/hbase-1.1.2/lib/jasper-compiler-5.5.23.jar:/data/sysdir/hbase-1.1.2/lib/jasper-runtime-5.5.23.jar:/data/sysdir/hbase-1.1.2/lib/javax.inject-1.jar:/data/sysdir/hbase-1.1.2/lib/java-xmlbuilder-0.4.jar:/data/sysdir/hbase-1.1.2/lib/jaxb-api-2.2.2.jar:/data/sysdir/hbase-1.1.2/lib/jaxb-impl-2.2.3-1.jar:/data/sysdir/hbase-1.1.2/lib/jcodings-1.0.8.jar:/data/sysdir/hbase-1.1.2/lib/jersey-client-1.9.jar:/data/sysdir/hbase-1.1.2/lib/jersey-core-1.9.jar:/data/sysdir/hbase-1.1.2/lib/jersey-guice-1.9.jar:/data/sysdir/hbase-1.1.2/lib/jersey-json-1.9.jar:/data/sysdir/hbase-1.1.2/lib/jersey-server-1.9.jar:/data/sysdir/hbase-1.1.2/lib/jets3t-0.9.0.jar:/data/sysdir/hbase-1.1.2/lib/jettison-1.3.3.jar:/data/sysdir/hbase-1.1.2/lib/jetty-6.1.26.jar:/data/sysdir/hbase-1.1.2/lib/jetty-sslengine-6.1.26.jar:/data/sysdir/hbase-1.1.2/lib/jetty-util-6.1.26.jar:/data/sysdir/hbase-1.1.2/lib/joni-2.1.2.jar:/data/sysdir/hbase-1.1.2/lib/jruby-complete-1.6.8.jar:/data/sysdir/hbase-1.1.2/lib/jsch-0.1.42.jar:/data/sysdir/hbase-1.1.2/lib/jsp-2.1-6.1.14.jar:/data/sysdir/hbase-1.1.2/lib/jsp-api-2.1-6.1.14.jar:/data/sysdir/hbase-1.1.2/lib/jsr305-1.3.9.jar:/data/sysdir/hbase-1.1.2/lib/junit-4.11.jar:/data/sysdir/hbase-1.1.2/lib/leveldbjni-all-1.8.jar:/data/sysdir/hbase-1.1.2/lib/libthrift-0.9.0.jar:/data/sysdir/hbase-1.1.2/lib/log4j-1.2.17.jar:/data/sysdir/hbase-1.1.2/lib/metrics-core-2.2.0.jar:/data/sysdir/hbase-1.1.2/lib/netty-3.2.4.Final.jar:/data/sysdir/hbase-1.1.2/lib/netty-all-4.0.23.Final.jar:/data/sysdir/hbase-1.1.2/lib/paranamer-2.3.jar:/data/sysdir/hbase-1.1.2/lib/protobuf-java-2.5.0.jar:/data/sysdir/hbase-1.1.2/lib/servlet-api-2.5-6.1.14.jar:/data/sysdir/hbase-1.1.2/lib/servlet-api-2.5.jar:/data/sysdir/hbase-1.1.2/lib/slf4j-api-1.7.7.jar:/data/sysdir/hbase-1.1.2/lib/slf4j-log4j12-1.7.5.jar:/data/sysdir/hbase-1.1.2/lib/snappy-java-1.0.4.1.jar:/data/sysdir/hbase-1.1.2/lib/spymemcached-2.11.6.jar:/data/sysdir/hbase-1.1.2/lib/xmlenc-0.52.jar:/data/sysdir/hbase-1.1.2/lib/xz-1.0.jar:/data/sysdir/hbase-1.1.2/lib/zookeeper-3.4.6.jar:/data/sysdir/hadoop-2.4.1-hbase/etc/hadoop:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/commons-net-3.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/jets3t-0.9.0.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/commons-io-2.4.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/httpclient-4.2.5.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/commons-compress-1.4.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/asm-3.2.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/commons-collections-3.2.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/jettison-1.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/commons-logging-1.1.3.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/commons-codec-1.4.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/commons-digester-1.8.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/jsr305-1.3.9.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/activation-1.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/xmlenc-0.52.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/jackson-mapper-asl-1.8.8.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/jetty-6.1.26.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/log4j-1.2.17.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/netty-3.6.2.Final.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/stax-api-1.0-2.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/hadoop-auth-2.4.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/hadoop-annotations-2.4.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/jersey-core-1.9.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/avro-1.7.4.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/commons-cli-1.2.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/jackson-core-asl-1.8.8.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/jetty-util-6.1.26.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/jersey-server-1.9.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/httpcore-4.2.5.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/paranamer-2.3.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/jackson-jaxrs-1.8.8.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/jsch-0.1.42.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/guava-11.0.2.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/commons-configuration-1.6.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/mockito-all-1.8.5.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/jersey-json-1.9.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/jackson-xc-1.8.8.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/commons-el-1.0.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/jsp-api-2.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/servlet-api-2.5.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/commons-httpclient-3.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/zookeeper-3.4.5.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/commons-math3-3.1.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/commons-lang-2.6.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/junit-4.8.2.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/xz-1.0.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/hadoop-common-2.4.1-tests.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/hadoop-common-2.4.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/hadoop-common-2.4.1-path-1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/hadoop-lzo-0.4.20-SNAPSHOT.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/common/hadoop-nfs-2.4.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/hdfs:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/hdfs/lib/commons-io-2.4.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/hdfs/lib/asm-3.2.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/hdfs/lib/jackson-mapper-asl-1.8.8.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/hdfs/lib/jackson-core-asl-1.8.8.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/hdfs/lib/guava-11.0.2.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/hdfs/lib/commons-el-1.0.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/hdfs/hadoop-hdfs-2.4.1-patch-1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/hdfs/hadoop-hdfs-2.4.1-tests.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/hdfs/hadoop-hdfs-nfs-2.4.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/commons-io-2.4.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/javax.inject-1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/asm-3.2.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/jline-0.9.94.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/jettison-1.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/commons-codec-1.4.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/activation-1.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/jackson-mapper-asl-1.8.8.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/jetty-6.1.26.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/log4j-1.2.17.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/jersey-core-1.9.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/commons-cli-1.2.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/jackson-core-asl-1.8.8.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/jersey-server-1.9.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/aopalliance-1.0.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/jackson-jaxrs-1.8.8.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/guava-11.0.2.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/jersey-json-1.9.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/jackson-xc-1.8.8.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/servlet-api-2.5.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/zookeeper-3.4.5.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/commons-lang-2.6.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/guice-3.0.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/xz-1.0.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/lib/jersey-client-1.9.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.4.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.4.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.4.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.4.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/hadoop-yarn-common-2.4.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/hadoop-yarn-server-tests-2.4.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/hadoop-yarn-client-2.4.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.4.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.4.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/hadoop-yarn-server-common-2.4.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/yarn/hadoop-yarn-api-2.4.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/lib/hadoop-snappy-0.0.1-SNAPSHOT.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/lib/javax.inject-1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/lib/asm-3.2.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.8.8.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/lib/hadoop-annotations-2.4.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/lib/jackson-core-asl-1.8.8.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/lib/hamcrest-core-1.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/lib/junit-4.10.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/lib/guice-3.0.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/lib/xz-1.0.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.4.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.4.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.4.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.4.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.4.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.4.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.4.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.4.1.jar:/data/sysdir/hadoop-2.4.1-hbase/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.4.1-tests.jar:/data/sysdir/hadoop-2.4.1-hbase/contrib/capacity-scheduler/*.jar
2016-08-14 22:17:03,178 INFO  [main] zookeeper.ZooKeeper: Client environment:java.library.path=/data/sysdir/hadoop-2.4.1-hbase/lib/native
2016-08-14 22:17:03,178 INFO  [main] zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
2016-08-14 22:17:03,178 INFO  [main] zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
2016-08-14 22:17:03,178 INFO  [main] zookeeper.ZooKeeper: Client environment:os.name=Linux
2016-08-14 22:17:03,178 INFO  [main] zookeeper.ZooKeeper: Client environment:os.arch=amd64
2016-08-14 22:17:03,178 INFO  [main] zookeeper.ZooKeeper: Client environment:os.version=3.13.6
2016-08-14 22:17:03,178 INFO  [main] zookeeper.ZooKeeper: Client environment:user.name=hbase
2016-08-14 22:17:03,178 INFO  [main] zookeeper.ZooKeeper: Client environment:user.home=/data/home/hbase
2016-08-14 22:17:03,178 INFO  [main] zookeeper.ZooKeeper: Client environment:user.dir=/data/sysdir/hbase-1.1.2/conf
2016-08-14 22:17:03,180 INFO  [main] zookeeper.ZooKeeper: Initiating client connection, connectString=192.168.201.52:2181,192.168.201.53:2181,192.168.201.63:2181,192.168.201.64:2181,192.168.201.65:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@38987d26
2016-08-14 22:17:03,206 INFO  [main-SendThread(192.168.201.65:2181)] zookeeper.ClientCnxn: Opening socket connection to server 192.168.201.65/192.168.201.65:2181. Will not attempt to authenticate using SASL (unknown error)
2016-08-14 22:17:03,228 INFO  [main-SendThread(192.168.201.65:2181)] zookeeper.ClientCnxn: Socket connection established to 192.168.201.65/192.168.201.65:2181, initiating session
2016-08-14 22:17:03,234 INFO  [main-SendThread(192.168.201.65:2181)] zookeeper.ClientCnxn: Session establishment complete on server 192.168.201.65/192.168.201.65:2181, sessionid = 0x556683846220a79, negotiated timeout = 30000

{quote}

Then i looked into zk's log, i found these :
{quote}
8/14/16 10:17:03 PM CST session 0x556683846220a79 cxid 0x0 zxid 0xf73ca5859 createSession 30000
8/14/16 10:17:03 PM CST session 0x556683846220a79 cxid 0x1 zxid 0xf73ca585a delete '/olapHbase/rs/olap3.data.lq%2C16020%2C1470799848293
{quote}
And i looked master logs as below : 
{quote}
2016-08-14 22:16:43,942 INFO  [ProcedureExecutorThread-0] zookeeper.ZKTableStateManager: Moving table KYLIN_F5O605Z3QZ state from null to ENABLING
2016-08-14 22:16:43,952 INFO  [ProcedureExecutorThread-0] master.AssignmentManager: Bulk assigning 1 region(s) across 4 server(s), round-robin=true
2016-08-14 22:16:43,953 INFO  [olap1.data.lq,16000,1469605579716-GeneralBulkAssigner-2] master.AssignmentManager: Assigning 1 region(s) to olap3.data.lq,16020,1470799848293
2016-08-14 22:16:43,958 INFO  [olap1.data.lq,16000,1469605579716-GeneralBulkAssigner-2] master.RegionStates: Transition {6e207f0307d4213d6c0e195115df1b8d state=OFFLINE, ts=1471184203953, server=null} to {6e207f0307d4213d6c0e195115df1b8d state=PENDING_OPEN, ts=1471184203958, server=olap3.data.lq,16020,1470799848293}
2016-08-14 22:16:43,985 INFO  [AM.ZK.Worker-pool2-t4001] master.RegionStates: Transition {6e207f0307d4213d6c0e195115df1b8d state=PENDING_OPEN, ts=1471184203958, server=olap3.data.lq,16020,1470799848293} to {6e207f0307d4213d6c0e195115df1b8d state=OPENING, ts=1471184203985, server=olap3.data.lq,16020,1470799848293}
2016-08-14 22:17:03,238 INFO  [main-EventThread] zookeeper.RegionServerTracker: RegionServer ephemeral node deleted, processing expiration [olap3.data.lq,16020,1470799848293]
2016-08-14 22:17:03,240 WARN  [MASTER_SERVER_OPERATIONS-olap1:16000-2] hbase.HBaseConfiguration: Config option ""hbase.regionserver.lease.period"" is deprecated. Instead, use ""hbase.client.scanner.timeout.period""
2016-08-14 22:17:04,008 INFO  [ProcedureExecutorThread-0] master.AssignmentManager: Bulk assigning done
2016-08-14 22:17:04,011 INFO  [ProcedureExecutorThread-0] zookeeper.ZKTableStateManager: Moving table KYLIN_F5O605Z3QZ state from ENABLING to ENABLED
2016-08-14 22:17:04,251 INFO  [MASTER_SERVER_OPERATIONS-olap1:16000-2] handler.ServerShutdownHandler: Splitting logs for olap3.data.lq,16020,1470799848293 before assignment; region count=2033
2016-08-14 22:17:04,261 INFO  [MASTER_SERVER_OPERATIONS-olap1:16000-2] master.SplitLogManager: dead splitlog workers [olap3.data.lq,16020,1470799848293]
2016-08-14 22:17:04,263 INFO  [MASTER_SERVER_OPERATIONS-olap1:16000-2] master.SplitLogManager: started splitting 2 logs in [hdfs://olapHbaseCluster:8020/hbase/WALs/olap3.data.lq,16020,1470799848293-splitting] for [olap3.data.lq,16020,1470799848293]
2016-08-14 22:17:04,268 INFO  [main-EventThread] coordination.SplitLogManagerCoordination: task /olapHbase/splitWAL/WALs%2Folap3.data.lq%2C16020%2C1470799848293-splitting%2Folap3.data.lq%252C16020%252C1470799848293.default.1471182516751 acquired by olap1.data.lq,16020,1470799983323
2016-08-14 22:17:04,277 INFO  [main-EventThread] coordination.SplitLogManagerCoordination: task /olapHbase/splitWAL/WALs%2Folap3.data.lq%2C16020%2C1470799848293-splitting%2Folap3.data.lq%252C16020%252C1470799848293.default.1471180125256 acquired by olap2.data.lq,16020,1470800075996
2016-08-14 22:17:04,288 WARN  [olap1.data.lq,16000,1469605579716_ChoreService_1] hbase.HBaseConfiguration: Config option ""hbase.regionserver.lease.period"" is deprecated. Instead, use ""hbase.client.scanner.timeout.period""
 ....

{quote}

I'm not falimliar with hbase , may be it's not a bug , but i can not figure out why this happend. I will be grateful if somebody can help me.
"
HBASE-16470	Two background activities which changes table regions can interfere with HBase hbck repair activity (potentially).
HBASE-16468	"Little nit, but no reason to create the extra objects."
HBASE-16472	"When I created a table in HBase and then tried running create statement in phoenix. Phoenix returned me the following error. 

procedure.ModifyTableProcedure: Error trying to modify table=t21sample state=MODIFY_TABLE_PREPARE
org.apache.hadoop.hbase.TableNotDisabledException: t21sample
        at org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure.prepareModify(ModifyTableProcedure.java:298)
        at org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure.executeFromState(ModifyTableProcedure.java:98)
        at org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure.executeFromState(ModifyTableProcedure.java:54)
        at org.apache.hadoop.hbase.procedure2.StateMachineProcedure.execute(StateMachineProcedure.java:107)
        at org.apache.hadoop.hbase.procedure2.Procedure.doExecute(Procedure.java:400)
        at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.execProcedure(ProcedureExecutor.java:869)
        at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.execLoop(ProcedureExecutor.java:673)
        at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.execLoop(ProcedureExecutor.java:626)
        at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.access$200(ProcedureExecutor.java:70)
        at org.apache.hadoop.hbase.procedure2.ProcedureExecutor$1.run(ProcedureExecutor.java:413)

The same error occurred in the case of altering an existing table. 
I some how managed to create a table in Phoenix by the same sequence if I disable the table after creating it. The phoenix returned me table disabled error. But after enabling the table in hbase, the table was created in phoenix, same worked for alter statement as well."
HBASE-16457	"when i start the hbase cluster, the master node can't start with the error as follow:
2016-08-19 19:06:53,361 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_SERVER_SHUTDOWN
java.io.IOException: Server is stopped
	at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:193)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
2016-08-19 19:06:53,364 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_SERVER_SHUTDOWN
java.io.IOException: Server is stopped
	at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:193)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
2016-08-19 19:06:53,362 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_SERVER_SHUTDOWN
java.io.IOException: failed log splitting for vm-syscloud-s5,60020,1471599889568, will retry
	at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.resubmit(ServerShutdownHandler.java:346)
	at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:219)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: error or interrupted while splitting logs in [hdfs://nameservice1/hbase/WALs/vm-syscloud-s5,60020,1471599889568-splitting] Task = installed = 31 done = 0 error = 0
	at org.apache.hadoop.hbase.master.SplitLogManager.splitLogDistributed(SplitLogManager.java:291)
	at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:391)
	at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:364)
	at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:286)
	at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:212)
	... 4 more
2016-08-19 19:06:53,361 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_SERVER_SHUTDOWN
java.io.IOException: failed log splitting for vm-syscloud-s3,60020,1471599886484, will retry
	at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.resubmit(ServerShutdownHandler.java:346)
	at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:219)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: error or interrupted while splitting logs in [hdfs://nameservice1/hbase/WALs/vm-syscloud-s3,60020,1471599886484-splitting] Task = installed = 3 done = 0 error = 0
	at org.apache.hadoop.hbase.master.SplitLogManager.splitLogDistributed(SplitLogManager.java:291)
	at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:391)
	at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:364)
	at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:286)
	at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:212)
	... 4 more
2016-08-19 19:06:53,361 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_SERVER_SHUTDOWN
java.io.IOException: failed log splitting for vm-syscloud-s2,60020,1471599886831, will retry
	at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.resubmit(ServerShutdownHandler.java:346)
	at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:219)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: error or interrupted while splitting logs in [hdfs://nameservice1/hbase/WALs/vm-syscloud-s2,60020,1471599886831-splitting] Task = installed = 30 done = 0 error = 0
	at org.apache.hadoop.hbase.master.SplitLogManager.splitLogDistributed(SplitLogManager.java:291)
	at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:391)
	at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:364)
	at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:286)
	at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:212)
	... 4 more
2016-08-19 19:06:53,367 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_SERVER_SHUTDOWN
java.io.IOException: Server is stopped
	at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:193)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
2016-08-19 19:06:53,579 INFO org.apache.hadoop.hbase.master.SplitLogManager$TimeoutMonitor: Chore: SplitLogManager Timeout Monitor was stopped

i try to delete the log files in the HDFS,but i don't have permission because
the files owner user is hbase. i don't know how to solve this problem. "
HBASE-16424	"cygpath: can't convert empty path
cygpath: can't convert empty path
2016-08-16 13:25:30,999 ERROR [main] util.Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
        at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:355)
        at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:370)
        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:363)
        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
        at org.apache.hadoop.conf.Configuration.getStrings(Configuration.java:1699)
        at org.apache.hadoop.hbase.zookeeper.ZKConfig.makeZKPropsFromHbaseConfig(ZKConfig.java:151)
        at org.apache.hadoop.hbase.zookeeper.ZKConfig.makeZKProps(ZKConfig.java:67)
        at org.apache.hadoop.hbase.zookeeper.ZKServerTool.readZKNodes(ZKServerTool.java:47)
        at org.apache.hadoop.hbase.zookeeper.ZKServerTool.main(ZKServerTool.java:70)
"
HBASE-16136	Check components are alive when closing RegionServer .
HBASE-16104	Support ColumnFamily when use PerformanceEvaluation to testing reading and writing performance with a table existed . 
HBASE-5222	"After running ""stop_replication"" in the hbase shell on our slave cluster we saw replication continue for weeks. Turns out that the replication sink is missing a check to get the replication state and therefore continued to write."
HBASE-16406	"The code i wrote belows:
	public static void addRow(String tableName, String row,String columnFamily, String column, String value) throws Exception {
            HTable table = new HTable(conf,tableName);
            Put put = new Put(Bytes.toBytes(row));
            put.add(Bytes.toBytes(columnFamily), Bytes.toBytes(column),Bytes.toBytes(value));
            table.put(put);
            table.close();
    }
 public static void main(String[] args){
    	String tableName = ""test1"";
    	String[] columnFamilys = { ""info"", ""course"" };
    	//createTable(tableName,columnFamilys);
    	try {
			addRow(tableName, ""tht"", ""info"", ""age"", ""20"");
//			addRow(tableName, ""tht"", ""info"", ""sex"", ""boy"");
	        addRow(tableName, ""tht"", ""course"", ""china"", ""97"");
//	        addRow(tableName, ""tht"", ""course"", ""math"", ""128"");
//	        addRow(tableName, ""tht"", ""course"", ""english"", ""85"");
		} catch (Exception e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
And The error:
org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: test1: 1 time, 
	at org.apache.hadoop.hbase.client.AsyncProcess$BatchErrors.makeException(AsyncProcess.java:228)
	at org.apache.hadoop.hbase.client.AsyncProcess$BatchErrors.access$1700(AsyncProcess.java:208)
	at org.apache.hadoop.hbase.client.AsyncProcess.waitForAllPreviousOpsAndReset(AsyncProcess.java:1700)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.backgroundFlushCommits(BufferedMutatorImpl.java:208)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.flush(BufferedMutatorImpl.java:183)
	at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:1449)
	at org.apache.hadoop.hbase.client.HTable.put(HTable.java:1040)
	at hbase2.testCreate.addRow(testCreate.java:45)
	at hbase2.testCreate.main(testCreate.java:53)
How can i solve it?"
HBASE-16394	"My cluster dead one regionserver  because of ""Compaction is trying to add a bad range""
Here the log:
[2016-08-09T18:30:19.094+08:00] [INFO] regionserver.ReplicationSource : Log hdfs://athene/hbase/oldWALs/MJQ-HBASE-ATHENE-11139%2C16020%2C1470729882622.default.1470736608897 was moved to hdfs://athene/hbase/oldWA     Ls/MJQ-HBASE-ATHENE%2C16020%2C1470729882622.default.1470736608897
[2016-08-09T18:30:30.225+08:00] [INFO] regionserver.MemStoreFlusher : Waited 90070ms on a compaction to clean up 'TOO MANY STORE FILES'; waited long enough... proceeding with flush of tjs4:popt_info,160608008474430,147073716071     1.7900baab5204e4f36fa49379c30cd584.
[2016-08-09T18:30:30.226+08:00] [INFO] regionserver.HRegion : Started memstore flush for tjs4:popt_info,160608008474430,1470737160711.7900baab5204e4f36fa49379c30cd584., current region memstore size 769.41 MB, and 1/1 column fam     ilies' memstores are being flushed.
[2016-08-09T18:30:30.549+08:00] [INFO] regionserver.StripeStoreFileManager : 3 conflicting files (likely created by a flush)  of size 156153021 are moved to L0 due to concurrent stripe change
[2016-08-09T18:30:31.199+08:00] [INFO] regionserver.HStore : Completed compaction of 203 file(s) in c of tjs4:popt_info,160608008474430,1470737160711.7900baab5204e4f36fa49379c30cd584. into 20347d203d09442cac30c42b424adda6(size=     3.0 G), ded362eab9cf4a819675cd35992d4974(size=3.0 G), 281b1039ed2643679e5b0a3820f5059d(size=2.4 G), total size for store is 8.6 G. This selection was in queue for 0sec, and took 10mins, 16sec to execute.
[2016-08-09T18:30:31.200+08:00] [INFO] regionserver.CompactSplitThread : Completed compaction: Request = regionName=tjs4:popt_info,160608008474430,1470737160711.7900baab5204e4f36fa49379c30cd584., storeName=c, fileCount=203, fil     eSize=7.2 G, priority=-3, time=5388162916126535; duration=10mins, 16sec
[2016-08-09T18:30:31.201+08:00] [INFO] regionserver.HRegion : Starting compaction on ci in region ad_union:union_click,3487f383ad484bcbb5cef727b69cec2a,1466484980245.c5772fc60c54f64cc977ba9cc01d74ad.
[2016-08-09T18:30:31.201+08:00] [INFO] regionserver.HStore : Starting compaction of 14 file(s) in ci of ad_union:union_click,3487f383ad484bcbb5cef727b69cec2a,1466484980245.c5772fc60c54f64cc977ba9cc01d74ad. into tmpdir=hdfs://at     hene/hbase/data/ad_union/union_click/c5772fc60c54f64cc977ba9cc01d74ad/.tmp, totalSize=75.0 M
[2016-08-09T18:30:31.206+08:00] [INFO] hfile.CacheConfig : blockCache=org.apache.hadoop.hbase.io.hfile.CombinedBlockCache@52659482, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=fal     se, cacheEvictOnClose=false, cacheDataCompressed=false, prefetchOnOpen=false
[2016-08-09T18:30:32.893+08:00] [INFO] regionserver.ReplicationSource : Log hdfs://athene/hbase/oldWALs/MJQ-HBASE-ATHENE-11139l%2C16020%2C1470729882622.default.1470736612825 was moved to hdfs://athene/hbase/oldWA     Ls/MJQ-HBASE-ATHENE-11139.%2C16020%2C1470729882622.default.1470736612825
[2016-08-09T18:30:34.373+08:00] [INFO] regionserver.HStore : Added hdfs://athene/hbase/data/tjs4/popt_info/7900baab5204e4f36fa49379c30cd584/c/775e8956cd2a48aaae70b9eded4457e9, entries=4336457, sequenceid=582528, filesize=48.7 M
[2016-08-09T18:30:34.373+08:00] [FATAL] regionserver.HRegionServer : ABORTING region server MJQ-HBASE-ATHENE-11139.,16020,1470729882622: Replay of WAL required. Forcing server shutdown
org.apache.hadoop.hbase.DroppedSnapshotException: region: tjs4:popt_info,160608008474430,1470737160711.7900baab5204e4f36fa49379c30cd584.
at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushCacheAndCommit(HRegion.java:2354)
at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2057)
at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2019)
at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:1911)
at org.apache.hadoop.hbase.regionserver.HRegion.flush(HRegion.java:1837)
at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:510)
at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:471)
at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.access$800(MemStoreFlusher.java:75)
at org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushHandler.run(MemStoreFlusher.java:259)
at java.lang.Thread.run(Thread.java:745)
  Caused by: java.io.IOException: Compaction is trying to add a bad range.
at org.apache.hadoop.hbase.regionserver.StripeStoreFileManager$CompactionOrFlushMergeCopy.processNewCandidateStripes(StripeStoreFileManager.java:837)
at org.apache.hadoop.hbase.regionserver.StripeStoreFileManager$CompactionOrFlushMergeCopy.mergeResults(StripeStoreFileManager.java:672)
at org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.insertNewFiles(StripeStoreFileManager.java:144)
at org.apache.hadoop.hbase.regionserver.HStore.updateStorefiles(HStore.java:1052)
at org.apache.hadoop.hbase.regionserver.HStore.access$500(HStore.java:128)
at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.commit(HStore.java:2231)
at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushCacheAndCommit(HRegion.java:2315)
"
HBASE-16343	"When enabled replication,we found a large number of error logs.Is the cluster configuration incorrect?


2016-08-03 10:46:21,721 DEBUG org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Opening log for replication dn7%2C60020%2C1470136216957.1470192327030 at 16999670
2016-08-03 10:46:21,723 WARN org.apache.hadoop.hdfs.DFSClient: BlockReaderLocal requested with incorrect offset:  Offset 0 and length 17073479 don't match block blk_4137524355009640437_53760530 ( blockLen 16999670 )
2016-08-03 10:46:21,723 WARN org.apache.hadoop.hdfs.DFSClient: BlockReaderLocal: Removing blk_4137524355009640437_53760530 from cache because local file /sdd/hdfs/dfs/data/blocksBeingWritten/blk_4137524355009640437 could not be opened.
2016-08-03 10:46:21,724 INFO org.apache.hadoop.hdfs.DFSClient: Failed to read block blk_4137524355009640437_53760530 on local machinejava.io.IOException:  Offset 0 and length 17073479 don't match block blk_4137524355009640437_53760530 ( blockLen 16999670 )
        at org.apache.hadoop.hdfs.BlockReaderLocal.<init>(BlockReaderLocal.java:287)
        at org.apache.hadoop.hdfs.BlockReaderLocal.newBlockReader(BlockReaderLocal.java:171)
        at org.apache.hadoop.hdfs.DFSClient.getLocalBlockReader(DFSClient.java:358)
        at org.apache.hadoop.hdfs.DFSClient.access$800(DFSClient.java:74)
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:2073)
        at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:2224)
        at java.io.DataInputStream.read(DataInputStream.java:149)
        at java.io.DataInputStream.readFully(DataInputStream.java:195)
        at java.io.DataInputStream.readFully(DataInputStream.java:169)
        at org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:1508)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1486)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1475)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1470)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader$WALReader.<init>(SequenceFileLogReader.java:55)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.init(SequenceFileLogReader.java:178)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(HLog.java:734)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationHLogReaderManager.openReader(ReplicationHLogReaderManager.java:69)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.openReader(ReplicationSource.java:574)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:364)

2016-08-03 10:46:21,724 INFO org.apache.hadoop.hdfs.DFSClient: Try reading via the datanode on /192.168.7.139:50010"
HBASE-16203	"in hbase with kerbose and authorization on, I enter hbase shell with a hbase super user, and do the following steps:
{quote}
1. grant  ""newUser/slave2@HADOOP.COM""
""newUser/slave2@HADOOP.COM"" is one of the kerbose principles

2. exit hbase shell

3. enter hbase shell again with principle ""newUser/slave2@HADOOP.COM""

4. scan 't1'
t1 is one of the table in hbase
{quote}

the result is: AccessDeniedException 

after debug regionServer code, I find the problem is:
{quote}
1. when we grant the global admin to ""newUser/slave2@HADOOP.COM"", TableAuthManager store this info with the whole name, newUser/slave2@HADOOP.COM

2. when we enter hbase shell with principle ""newUser/slave2@HADOOP.COM"" and scan table, regionServer will do do authorization check, such as check if the user is superUser
when do this check, use the short name(newUser), not the whole name(newUser/slave2@HADOOP.COM)

{quote}

"
HBASE-16202	"HBASE-15353 added a separate metric for tracking the number of CallQueueTooBigExceptions, but only went in to 1.4+.  Since CQTBE is already in 1.2+, it would be nice to at least get this in the upcoming 1.3.0 release."
HBASE-16151	"here is the code and logs below:
HTable table = new HTable(tempConf,tableName);
LoadIncrementalHFiles loader = new LoadIncrementalHFiles(conf);
loader.doBulkLoad(dir, table);
13:49:15,806 DEBUG ProtobufRpcEngine:253 - Call: getBlockLocations took 0ms
13:49:15,821 DEBUG DFSClient:273 - newInfo = LocatedBlocks{
  fileLength=1051
  underConstruction=false
  blocks=[LocatedBlock{BP-644339120-172.30.115.58-1458531863647:blk_1075272092_1543930; getBlockSize()=1051; corrupt=false; offset=0; locs=[172.30.115.59:50010, 172.30.115.58:50010, 172.30.115.60:50010]; storageIDs=[DS-d09ae035-279c-408c-8272-d84eb29f3e3b, DS-30754a89-526a-40ca-b0e0-f4400b0527bb, DS-856412f0-e865-43c0-bd60-4650b7ff275d]; storageTypes=[DISK, DISK, DISK]}]
  lastLocatedBlock=LocatedBlock{BP-644339120-172.30.115.58-1458531863647:blk_1075272092_1543930; getBlockSize()=1051; corrupt=false; offset=0; locs=[172.30.115.59:50010, 172.30.115.58:50010, 172.30.115.60:50010]; storageIDs=[DS-d09ae035-279c-408c-8272-d84eb29f3e3b, DS-30754a89-526a-40ca-b0e0-f4400b0527bb, DS-856412f0-e865-43c0-bd60-4650b7ff275d]; storageTypes=[DISK, DISK, DISK]}
  isLastBlockComplete=true}
13:49:15,821 DEBUG Client:1025 - IPC Client (1094238221) connection to /172.30.115.58:8020 from uatxj990267 sending #4
13:49:15,821 DEBUG Client:1082 - IPC Client (1094238221) connection to /172.30.115.58:8020 from uatxj990267 got value #4
13:49:15,821 DEBUG ProtobufRpcEngine:253 - Call: getFileInfo took 0ms
13:49:15,821 DEBUG DFSClient:961 - Connecting to datanode 172.30.115.59:50010
13:49:15,821 DEBUG Client:1025 - IPC Client (1094238221) connection to /172.30.115.58:8020 from uatxj990267 sending #5
13:49:15,821 DEBUG Client:1082 - IPC Client (1094238221) connection to /172.30.115.58:8020 from uatxj990267 got value #5
13:49:15,821 DEBUG ProtobufRpcEngine:253 - Call: getServerDefaults took 0ms
13:49:15,837 DEBUG SaslDataTransferClient:244 - SASL client skipping handshake in unsecured configuration for addr = /172.30.115.59, datanodeId = 172.30.115.59:50010
13:49:15,899 DEBUG DFSClient:961 - Connecting to datanode 172.30.115.59:50010
13:49:15,915  INFO LoadIncrementalHFiles:517 - Trying to load hfile=hdfs://172.30.115.58:8020/apps/hbase/data/info/info/32d34121d62b4327a8303ee55bb98b68 first=14672656058391 last=14672656058393
13:49:15,946 DEBUG LoadIncrementalHFiles:618 - Going to connect to server region=emp,,1467010385314.eb65b8443faee7c72ee1f003f1b8e7ba., hostname=hadoop002.icccuat.com,60020,1467195299232, seqNum=335 for row  with hfile group [{[B@281d9e15,hdfs://172.30.115.58:8020/apps/hbase/data/info/info/32d34121d62b4327a8303ee55bb98b68}]
13:49:16,258 DEBUG ClientCnxn:818 - Reading reply sessionid:0x254ba19dd2d97eb, packet:: clientPath:null serverPath:null finished:false header:: 7,4  replyHeader:: 7,163216552663,0  request:: '/hbase-unsecure/table/emp,F  response:: #ffffffff000146d61737465723a363030303017ffffff951415ffffff9f372d375042554680,s{163215998528,163216219499,1467010385319,1467103321910,8,0,0,0,31,0,163215998528} 
13:49:16,258 DEBUG LoadIncrementalHFiles:618 - Going to connect to server region=emp,,1467010385314.eb65b8443faee7c72ee1f003f1b8e7ba., hostname=hadoop002.icccuat.com,60020,1467195299232, seqNum=335 for row  with hfile group [{[B@281d9e15,hdfs://172.30.115.58:8020/apps/hbase/data/info/info/32d34121d62b4327a8303ee55bb98b68}]
13:49:16,773 DEBUG ClientCnxn:818 - Reading reply sessionid:0x254ba19dd2d97eb, packet:: clientPath:null serverPath:null finished:false header:: 8,4  replyHeader:: 8,163216552663,0  request:: '/hbase-unsecure/table/emp,F  response:: #ffffffff000146d61737465723a363030303017ffffff951415ffffff9f372d375042554680,s{163215998528,163216219499,1467010385319,1467103321910,8,0,0,0,31,0,163215998528} 
13:49:16,773 DEBUG LoadIncrementalHFiles:618 - Going to connect to server region=emp,,1467010385314.eb65b8443faee7c72ee1f003f1b8e7ba., hostname=hadoop002.icccuat.com,60020,1467195299232, seqNum=335 for row  with hfile group [{[B@281d9e15,hdfs://172.30.115.58:8020/apps/hbase/data/info/info/32d34121d62b4327a8303ee55bb98b68}]
13:49:17,788 DEBUG ClientCnxn:818 - Reading reply sessionid:0x254ba19dd2d97eb, packet:: clientPath:null serverPath:null finished:false header:: 9,4  replyHeader:: 9,163216552666,0  request:: '/hbase-unsecure/table/emp,F  response:: #ffffffff000146d61737465723a363030303017ffffff951415ffffff9f372d375042554680,s{163215998528,163216219499,1467010385319,1467103321910,8,0,0,0,31,0,163215998528} 
13:49:17,788 DEBUG LoadIncrementalHFiles:618 - Going to connect to server region=emp,,1467010385314.eb65b8443faee7c72ee1f003f1b8e7ba., hostname=hadoop002.icccuat.com,60020,1467195299232, seqNum=335 for row  with hfile group [{[B@281d9e15,hdfs://172.30.115.58:8020/apps/hbase/data/info/info/32d34121d62b4327a8303ee55bb98b68}]
13:49:19,801 DEBUG ClientCnxn:818 - Reading reply sessionid:0x254ba19dd2d97eb, packet:: clientPath:null serverPath:null finished:false header:: 10,4  replyHeader:: 10,163216552668,0  request:: '/hbase-unsecure/table/emp,F  response:: #ffffffff000146d61737465723a363030303017ffffff951415ffffff9f372d375042554680,s{163215998528,163216219499,1467010385319,1467103321910,8,0,0,0,31,0,163215998528} 
13:49:19,801 DEBUG LoadIncrementalHFiles:618 - Going to connect to server region=emp,,1467010385314.eb65b8443faee7c72ee1f003f1b8e7ba., hostname=hadoop002.icccuat.com,60020,1467195299232, seqNum=335 for row  with hfile group [{[B@281d9e15,hdfs://172.30.115.58:8020/apps/hbase/data/info/info/32d34121d62b4327a8303ee55bb98b68}]
13:49:23,811 DEBUG ClientCnxn:818 - Reading reply sessionid:0x254ba19dd2d97eb, packet:: clientPath:null serverPath:null finished:false header:: 11,4  replyHeader:: 11,163216552675,0  request:: '/hbase-unsecure/table/emp,F  response:: #ffffffff000146d61737465723a363030303017ffffff951415ffffff9f372d375042554680,s{163215998528,163216219499,1467010385319,1467103321910,8,0,0,0,31,0,163215998528} 
13:49:23,811 DEBUG LoadIncrementalHFiles:618 - Going to connect to server region=emp,,1467010385314.eb65b8443faee7c72ee1f003f1b8e7ba., hostname=hadoop002.icccuat.com,60020,1467195299232, seqNum=335 for row  with hfile group [{[B@281d9e15,hdfs://172.30.115.58:8020/apps/hbase/data/info/info/32d34121d62b4327a8303ee55bb98b68}]
13:49:25,824 DEBUG Client:1185 - IPC Client (1094238221) connection to /172.30.115.58:8020 from uatxj990267: closed
13:49:25,824 DEBUG Client:980 - IPC Client (1094238221) connection to /172.30.115.58:8020 from uatxj990267: stopped, remaining connections 0
13:49:33,874 DEBUG ClientCnxn:818 - Reading reply sessionid:0x254ba19dd2d97eb, packet:: clientPath:null serverPath:null finished:false header:: 12,4  replyHeader:: 12,163216552690,0  request:: '/hbase-unsecure/table/emp,F  response:: #ffffffff000146d61737465723a363030303017ffffff951415ffffff9f372d375042554680,s{163215998528,163216219499,1467010385319,1467103321910,8,0,0,0,31,0,163215998528} 
13:49:33,874 DEBUG ClientCnxn:717 - Got ping response for sessionid: 0x254ba19dd2d97eb after 1ms
13:49:33,874 DEBUG LoadIncrementalHFiles:618 - Going to connect to server region=emp,,1467010385314.eb65b8443faee7c72ee1f003f1b8e7ba., hostname=hadoop002.icccuat.com,60020,1467195299232, seqNum=335 for row  with hfile group [{[B@281d9e15,hdfs://172.30.115.58:8020/apps/hbase/data/info/info/32d34121d62b4327a8303ee55bb98b68}]
13:49:43,935 DEBUG ClientCnxn:818 - Reading reply sessionid:0x254ba19dd2d97eb, packet:: clientPath:null serverPath:null finished:false header:: 13,4  replyHeader:: 13,163216552705,0  request:: '/hbase-unsecure/table/emp,F  response:: #ffffffff000146d61737465723a363030303017ffffff951415ffffff9f372d375042554680,s{163215998528,163216219499,1467010385319,1467103321910,8,0,0,0,31,0,163215998528} 
13:49:43,935 DEBUG ClientCnxn:717 - Got ping response for sessionid: 0x254ba19dd2d97eb after 1ms
13:49:43,935 DEBUG LoadIncrementalHFiles:618 - Going to connect to server region=emp,,1467010385314.eb65b8443faee7c72ee1f003f1b8e7ba., hostname=hadoop002.icccuat.com,60020,1467195299232, seqNum=335 for row  with hfile group [{[B@281d9e15,hdfs://172.30.115.58:8020/apps/hbase/data/info/info/32d34121d62b4327a8303ee55bb98b68}]
13:49:53,949 DEBUG ClientCnxn:818 - Reading reply sessionid:0x254ba19dd2d97eb, packet:: clientPath:null serverPath:null finished:false header:: 14,4  replyHeader:: 14,163216552723,0  request:: '/hbase-unsecure/table/emp,F  response:: #ffffffff000146d61737465723a363030303017ffffff951415ffffff9f372d375042554680,s{163215998528,163216219499,1467010385319,1467103321910,8,0,0,0,31,0,163215998528} 
13:49:53,949 DEBUG ClientCnxn:717 - Got ping response for sessionid: 0x254ba19dd2d97eb after 1ms
13:49:53,951 DEBUG LoadIncrementalHFiles:618 - Going to connect to server region=emp,,1467010385314.eb65b8443faee7c72ee1f003f1b8e7ba., hostname=hadoop002.icccuat.com,60020,1467195299232, seqNum=335 for row  with hfile group [{[B@281d9e15,hdfs://172.30.115.58:8020/apps/hbase/data/info/info/32d34121d62b4327a8303ee55bb98b68}]
13:50:03,994 DEBUG ClientCnxn:818 - Reading reply sessionid:0x254ba19dd2d97eb, packet:: clientPath:null serverPath:null finished:false header:: 15,4  replyHeader:: 15,163216552738,0  request:: '/hbase-unsecure/table/emp,F  response:: #ffffffff000146d61737465723a363030303017ffffff951415ffffff9f372d375042554680,s{163215998528,163216219499,1467010385319,1467103321910,8,0,0,0,31,0,163215998528} 
13:50:03,996 DEBUG LoadIncrementalHFiles:618 - Going to connect to server region=emp,,1467010385314.eb65b8443faee7c72ee1f003f1b8e7ba., hostname=hadoop002.icccuat.com,60020,1467195299232, seqNum=335 for row  with hfile group [{[B@281d9e15,hdfs://172.30.115.58:8020/apps/hbase/data/info/info/32d34121d62b4327a8303ee55bb98b68}]
13:50:04,193 DEBUG ClientCnxn:717 - Got ping response for sessionid: 0x254ba19dd2d97eb after 200ms
13:50:17,326 DEBUG ClientCnxn:717 - Got ping response for sessionid: 0x254ba19dd2d97eb after 0ms
13:50:24,172 DEBUG ClientCnxn:818 - Reading reply sessionid:0x254ba19dd2d97eb, packet:: clientPath:null serverPath:null finished:false header:: 16,4  replyHeader:: 16,163216552771,0  request:: '/hbase-unsecure/table/emp,F  response:: #ffffffff000146d61737465723a363030303017ffffff951415ffffff9f372d375042554680,s{163215998528,163216219499,1467010385319,1467103321910,8,0,0,0,31,0,163215998528} 
13:50:24,175 DEBUG LoadIncrementalHFiles:618 - Going to connect to server region=emp,,1467010385314.eb65b8443faee7c72ee1f003f1b8e7ba., hostname=hadoop002.icccuat.com,60020,1467195299232, seqNum=335 for row  with hfile group [{[B@281d9e15,hdfs://172.30.115.58:8020/apps/hbase/data/info/info/32d34121d62b4327a8303ee55bb98b68}]
13:50:24,182  INFO RpcRetryingCaller:129 - Call exception, tries=10, retries=35, started=68251 ms ago, cancelled=false, msg=row '' on table 'emp' at region=emp,,1467010385314.eb65b8443faee7c72ee1f003f1b8e7ba., hostname=hadoop002.icccuat.com,60020,1467195299232, seqNum=335
"
HBASE-14189	"The original description is ambiguous. I think i will rewrite it.
Let's see {{BlockCache}} constructor firstly
{code}
  public CacheConfig(Configuration conf, HColumnDescriptor family) {
    this(CacheConfig.instantiateBlockCache(conf),
        family.isBlockCacheEnabled(),
        family.isInMemory(),
        // For the following flags we enable them regardless of per-schema settings
        // if they are enabled in the global configuration.
        conf.getBoolean(CACHE_BLOCKS_ON_WRITE_KEY,
            DEFAULT_CACHE_DATA_ON_WRITE) || family.isCacheDataOnWrite(),
        conf.getBoolean(CACHE_INDEX_BLOCKS_ON_WRITE_KEY,
            DEFAULT_CACHE_INDEXES_ON_WRITE) || family.isCacheIndexesOnWrite(),
        conf.getBoolean(CACHE_BLOOM_BLOCKS_ON_WRITE_KEY,
            DEFAULT_CACHE_BLOOMS_ON_WRITE) || family.isCacheBloomsOnWrite(),
        conf.getBoolean(EVICT_BLOCKS_ON_CLOSE_KEY,
            DEFAULT_EVICT_ON_CLOSE) || family.isEvictBlocksOnClose(),
        conf.getBoolean(CACHE_DATA_BLOCKS_COMPRESSED_KEY, DEFAULT_CACHE_DATA_COMPRESSED),
        conf.getBoolean(PREFETCH_BLOCKS_ON_OPEN_KEY,
            DEFAULT_PREFETCH_ON_OPEN) || family.isPrefetchBlocksOnOpen(),
        conf.getBoolean(HColumnDescriptor.CACHE_DATA_IN_L1,
            HColumnDescriptor.DEFAULT_CACHE_DATA_IN_L1) || family.isCacheDataInL1(),
        conf.getBoolean(DROP_BEHIND_CACHE_COMPACTION_KEY,DROP_BEHIND_CACHE_COMPACTION_DEFAULT)
     );
  }
{code}

If we dig in it,  we will see {{CacheConfig.cacheDataOnRead}} is used to accept {{family.isBlockCacheEnabled()}}.  
I think it is confused as comments about {{cacheDataOnRead}}
{code}
  /**
   * Whether blocks should be cached on read (default is on if there is a
   * cache but this can be turned off on a per-family or per-request basis).
   * If off we will STILL cache meta blocks; i.e. INDEX and BLOOM types.
   * This cannot be disabled.
   */
  private boolean cacheDataOnRead;
{code}

So i think we should use another variable to represent for {{family.isBlockCacheEnabled()}}.

The secondary point is we use 'or' to decide {{cacheDataOnWrite}} is on/off when both CF and global has this setting.
{code}
conf.getBoolean(CACHE_BLOCKS_ON_WRITE_KEY,
            DEFAULT_CACHE_DATA_ON_WRITE) || family.isCacheDataOnWrite()
{code}

IMO we should use CF Level setting to override global setting. 


"
HBASE-15973	"In a production cluster, we have noticed a case where flushes were happening for 200-400KB in sizes. Turns out the periodic memstore flusher is force flushing because cells with older timestamps (in this case days old) were being inserted. 

We have periodic memstore flusher with 1 hour defaulted, so in a case where replication is lagging, or phoenix secondary index rebuild or the user doing back-in-time inserts with cell timestamps older than 1 hour, we will flush extremely frequently. "
HBASE-6103	"Currently HBaseServer is running with a single listener thread, which is responsible for accepting the connection, reading the data from network channel, deserializing the data into writable objects and handover to the IPC handler threads. 

When there are multiple hbase clients connecting to the region server (HBaseServer) and reading/writing a large set of data, the listener and the respond thread will be performance bottleneck. 

So the solution is to deserialize the data for each ipc connection in parallel for HBaseServer

BTW, it is also one of the reasons that the parallel scanning from multiple clients is far slower than single client case.







"
HBASE-15874	"regionserver can't restart anyway,
anytime I start regionserver,it crushes. the hbase version is 1.0.0,revision=6c98bff7b719efdb16f71606f3b7d8229445eb8
regionserver's log is :
ava.lang.NullPointerException
        at org.apache.hadoop.hbase.protobuf.generated.WALProtos$FamilyScope$Builder.setScopeType(WALProtos.java:3939)
        at org.apache.hadoop.hbase.wal.WALKey.getBuilder(WALKey.java:503)
        at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.append(ProtobufLogWriter.java:118)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1962)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:1843)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:1796)
        at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:128)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
2016-05-22 19:24:57,435 FATAL [regionserver/polaris-2/10.0.52.25:16020.logRoller] regionserver.HRegionServer: ABORTING region server polaris-2,16020,1463916286917: IOE in log roller
java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.ensureIOException(FSHLog.java:2002)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.blockOnSync(FSHLog.java:1437)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.replaceWriter(FSHLog.java:956)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.rollWriter(FSHLog.java:711)
        at org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:137)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hbase.protobuf.generated.WALProtos$FamilyScope$Builder.setScopeType(WALProtos.java:3939)
        at org.apache.hadoop.hbase.wal.WALKey.getBuilder(WALKey.java:503)
        at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.append(ProtobufLogWriter.java:118)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1962)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:1843)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:1796)
        at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:128)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        ... 1 more
2016-05-22 19:24:57,436 FATAL [regionserver/polaris-2/10.0.52.25:16020.logRoller] regionserver.HRegionServer: RegionServer abort: loaded coprocessors are: [org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint]
2016-05-22 19:24:57,452 INFO  [regionserver/polaris-2/10.0.52.25:16020.logRoller] regionserver.HRegionServer: Dump of metrics as JSON on abort: {"
HBASE-14363	"Currently HBCK just prints a vague ""Empty REGIONINFO_QUALIFIER found"" warning, and does not print the row it found that on. While fixing this is easy thanks to HBCK, some more detail (say the row/region ID) would be good to print, to avoid people manually scanning meta to obtain the very same info."
HBASE-15692	"we encounter this problem on our production cluster.   
This is exception in HMaster.log
{code}
2016-04-22 11:05:06,390 ERROR [f04,16000,1459941011479_ChoreService_3] snapshot.SnapshotHFileCleaner: Exception while checking if files were valid, keeping them just in case.
org.apache.hadoop.hbase.snapshot.CorruptedSnapshotException: Couldn't read snapshot info from:hdfs://f04/hbase/.hbase-snapshot/.tmp/frog_stastic_2016-04-07/.snapshotinfo
        at org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils.readSnapshotInfo(SnapshotDescriptionUtils.java:295)
        at org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil.getHFileNames(SnapshotReferenceUtil.java:328)
        at org.apache.hadoop.hbase.master.snapshot.SnapshotHFileCleaner$1.filesUnderSnapshot(SnapshotHFileCleaner.java:85)
        at org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache.getSnapshotsInProgress(SnapshotFileCache.java:303)
        at org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache.getUnreferencedFiles(SnapshotFileCache.java:194)
        at org.apache.hadoop.hbase.master.snapshot.SnapshotHFileCleaner.getDeletableFiles(SnapshotHFileCleaner.java:62)
        at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteFiles(CleanerChore.java:233)
        at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteEntries(CleanerChore.java:157)
        at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteDirectory(CleanerChore.java:180)
        at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteEntries(CleanerChore.java:149)
        at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteDirectory(CleanerChore.java:180)
        at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteEntries(CleanerChore.java:149)
        at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteDirectory(CleanerChore.java:180)
        at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteEntries(CleanerChore.java:149)
        at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteDirectory(CleanerChore.java:180)
        at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteEntries(CleanerChore.java:149)
        at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteDirectory(CleanerChore.java:180)
        at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteEntries(CleanerChore.java:149)
        at org.apache.hadoop.hbase.master.cleaner.CleanerChore.chore(CleanerChore.java:124)
        at org.apache.hadoop.hbase.ScheduledChore.run(ScheduledChore.java:185)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.FileNotFoundException: File does not exist: /hbase/.hbase-snapshot/.tmp/frog_stastic_2016-04-07/.snapshotinfo
        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:64)
        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:54)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsUpdateTimes(FSNamesystem.java:1795)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1738)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1718)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1690)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:519)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:337)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

        at sun.reflect.GeneratedConstructorAccessor17.newInstance(Unknown Source)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
        at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
        at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
        at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1167)
        at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1155)
        at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1145)
        at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:268)
        at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:235)
        at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:228)
        at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1318)
        at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:293)
        at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:289)
{code}

I notice the code
{code: title=SnapshotDescriptionUtils#writeSnapshotInfo}
      // if we get an exception, try to remove the snapshot info
      if (!fs.delete(snapshotInfo, false)) {
        String msg = ""Couldn't delete snapshot info file: "" + snapshotInfo;
        LOG.error(msg);
        throw new IOException(msg);
      }
{code}

IMO we should delete the entire relates snapshot dir when we write failed. 

"
HBASE-15642	"This happened after we upgrade our cluster from 0.98.6 to 0.98.17. 
The number should be reduced,   but it always increases from original 20+ to 49 now. (yesterday it was 48)
Need to dig."
HBASE-15404	"On running hbase pe --nomapred increment/append 10,  i see the following output where it seems like threads are executing operations serially. In the UI too, only one RS is getting requests at a time.
{noformat}
6/03/05 22:26:15 INFO hbase.PerformanceEvaluation: Timed test starting in thread TestClient-1
16/03/05 22:26:15 INFO hbase.PerformanceEvaluation: Timed test starting in thread TestClient-2
16/03/05 22:26:15 INFO hbase.PerformanceEvaluation: Timed test starting in thread TestClient-8
16/03/05 22:26:15 INFO hbase.PerformanceEvaluation: Timed test starting in thread TestClient-0
16/03/05 22:26:15 INFO hbase.PerformanceEvaluation: Timed test starting in thread TestClient-4
16/03/05 22:26:15 INFO hbase.PerformanceEvaluation: Timed test starting in thread TestClient-6
16/03/05 22:26:15 INFO hbase.PerformanceEvaluation: Timed test starting in thread TestClient-7
16/03/05 22:26:15 INFO hbase.PerformanceEvaluation: Timed test starting in thread TestClient-5
16/03/05 22:26:15 INFO hbase.PerformanceEvaluation: Timed test starting in thread TestClient-9
16/03/05 22:26:15 INFO hbase.PerformanceEvaluation: Timed test starting in thread TestClient-3
16/03/05 22:27:54 INFO hbase.PerformanceEvaluation: 0/104857/1048576, latency mean=942.48, min=390.00, max=163444.00, stdDev=892.64, 95th=1361.00, 99th=1605.00
16/03/05 22:27:54 INFO hbase.PerformanceEvaluation: 0/104857/1048576, latency mean=942.53, min=366.00, max=163400.00, stdDev=885.49, 95th=1361.00, 99th=1605.00
16/03/05 22:27:54 INFO hbase.PerformanceEvaluation: 0/104857/1048576, latency mean=942.41, min=402.00, max=163436.00, stdDev=891.54, 95th=1359.00, 99th=1602.41
16/03/05 22:27:54 INFO hbase.PerformanceEvaluation: 0/104857/1048576, latency mean=942.51, min=399.00, max=163610.00, stdDev=892.40, 95th=1360.00, 99th=1600.00
16/03/05 22:27:54 INFO hbase.PerformanceEvaluation: 0/104857/1048576, latency mean=942.59, min=393.00, max=162932.00, stdDev=887.65, 95th=1361.00, 99th=1604.00
16/03/05 22:27:54 INFO hbase.PerformanceEvaluation: 0/104857/1048576, latency mean=942.26, min=385.00, max=163482.00, stdDev=891.71, 95th=1358.00, 99th=1599.00
16/03/05 22:27:54 INFO hbase.PerformanceEvaluation: 0/104857/1048576, latency mean=942.51, min=383.00, max=163246.00, stdDev=888.07, 95th=1360.00, 99th=1605.00
16/03/05 22:27:54 INFO hbase.PerformanceEvaluation: 0/104857/1048576, latency mean=942.45, min=385.00, max=163405.00, stdDev=886.65, 95th=1359.00, 99th=1604.00
16/03/05 22:27:54 INFO hbase.PerformanceEvaluation: 0/104857/1048576, latency mean=942.38, min=400.00, max=163580.00, stdDev=887.28, 95th=1359.00, 99th=1602.00
16/03/05 22:27:54 INFO hbase.PerformanceEvaluation: 0/104857/1048576, latency mean=942.29, min=407.00, max=163403.00, stdDev=889.77, 95th=1357.00, 99th=1597.00
16/03/05 22:29:35 INFO hbase.PerformanceEvaluation: 0/209714/1048576, latency mean=950.75, min=366.00, max=163400.00, stdDev=817.84, 95th=1363.00, 99th=1605.00
16/03/05 22:29:35 INFO hbase.PerformanceEvaluation: 0/209714/1048576, latency mean=950.75, min=383.00, max=163246.00, stdDev=821.95, 95th=1363.00, 99th=1604.00
16/03/05 22:29:35 INFO hbase.PerformanceEvaluation: 0/209714/1048576, latency mean=950.75, min=389.00, max=163444.00, stdDev=824.03, 95th=1364.00, 99th=1603.00
16/03/05 22:29:35 INFO hbase.PerformanceEvaluation: 0/209714/1048576, latency mean=950.56, min=382.00, max=163403.00, stdDev=822.44, 95th=1363.00, 99th=1603.00
16/03/05 22:29:35 INFO hbase.PerformanceEvaluation: 0/209714/1048576, latency mean=950.79, min=393.00, max=162932.00, stdDev=818.75, 95th=1365.00, 99th=1601.84
16/03/05 22:29:35 INFO hbase.PerformanceEvaluation: 0/209714/1048576, latency mean=950.70, min=388.00, max=163436.00, stdDev=823.52, 95th=1364.00, 99th=1606.00
16/03/05 22:29:35 INFO hbase.PerformanceEvaluation: 0/209714/1048576, latency mean=950.72, min=376.00, max=163405.00, stdDev=820.65, 95th=1364.00, 99th=1605.00
16/03/05 22:29:35 INFO hbase.PerformanceEvaluation: 0/209714/1048576, latency mean=950.56, min=382.00, max=163482.00, stdDev=823.43, 95th=1363.00, 99th=1599.00
16/03/05 22:29:35 INFO hbase.PerformanceEvaluation: 0/209714/1048576, latency mean=950.67, min=376.00, max=163580.00, stdDev=821.59, 95th=1364.00, 99th=1602.00
16/03/05 22:29:35 INFO hbase.PerformanceEvaluation: 0/209714/1048576, latency mean=950.77, min=390.00, max=163610.00, stdDev=823.88, 95th=1363.00, 99th=1600.00
16/03/05 22:31:16 INFO hbase.PerformanceEvaluation: 0/314571/1048576, latency mean=952.21, min=369.00, max=162932.00, stdDev=787.36, 95th=1361.00, 99th=1595.00
16/03/05 22:31:16 INFO hbase.PerformanceEvaluation: 0/314571/1048576, latency mean=952.18, min=374.00, max=163610.00, stdDev=791.11, 95th=1359.00, 99th=1594.00
16/03/05 22:31:16 INFO hbase.PerformanceEvaluation: 0/314571/1048576, latency mean=952.30, min=367.00, max=163444.00, stdDev=802.21, 95th=1362.00, 99th=1597.00
16/03/05 22:31:16 INFO hbase.PerformanceEvaluation: 0/314571/1048576, latency mean=952.38, min=366.00, max=163400.00, stdDev=799.61, 95th=1360.00, 99th=1596.00
16/03/05 22:31:16 INFO hbase.PerformanceEvaluation: 0/314571/1048576, latency mean=952.31, min=375.00, max=163580.00, stdDev=802.77, 95th=1359.00, 99th=1596.00
16/03/05 22:31:16 INFO hbase.PerformanceEvaluation: 0/314571/1048576, latency mean=952.12, min=388.00, max=163436.00, stdDev=791.34, 95th=1361.00, 99th=1598.00
16/03/05 22:31:16 INFO hbase.PerformanceEvaluation: 0/314571/1048576, latency mean=952.13, min=368.00, max=163405.00, stdDev=788.29, 95th=1360.00, 99th=1598.00
16/03/05 22:31:16 INFO hbase.PerformanceEvaluation: 0/314571/1048576, latency mean=952.35, min=383.00, max=163246.00, stdDev=801.26, 95th=1362.00, 99th=1599.00
16/03/05 22:31:16 INFO hbase.PerformanceEvaluation: 0/314571/1048576, latency mean=952.21, min=382.00, max=163403.00, stdDev=801.88, 95th=1359.00, 99th=1598.00
{noformat}"
HBASE-15564	"I'm using org.apache.hadoop.hbase.mapreduce.HashTable to create hashes for use in SyncTable.  Occasionally, the job page in jobhistory will say the job succeeded, but in my filesystem, I see ""manifest.tmp"" instead of the expected ""manifest"".  According to the code[1], the job must have failed, but I don't see failure anywhere.  

[1]https://github.com/apache/hbase/blob/ad3feaa44800f10d102255a240c38ccf23a82d49/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/HashTable.java#L739-L741"
HBASE-13245	"Counts of deletes and puts ( called mutations in metrics ) are totally off.

The time for delete or mutate is really the time that a batch of mutations containing a put or delete. The same is true for count.

We should have:
* number of puts
* number of deletes
* time for batch mutations
* number of batch mutations
* histogram of number of mutations in each batch

That way it's less reading tea leaves and more explicit that things are batched and we don't have more fine grained info."
HBASE-15546	Improvements needed to consistency logic.
HBASE-15452	"In looking why we spent so much time in StoreScanner.next when doing a simple Phoenix count\(*) query I came across checkScanOrder. Not only is this a function dispatch (that the JIT would eventually inline), it also requires setting the prevKV member for every Cell encountered.

Removing that logic a yields measurable end-to-end improvement of 5-20% (in 0.98).
I will repeat this test on my work machine tomorrow.

I think we're stable enough to remove that check anyway.
"
HBASE-15372	"Currently, we keep WAL file per table in backup site, this creates significant data duplication in case of many tables in a backup set. We have to keep all WAL files in a single place and keep track of WAL files involved per table/backup id as links (references) 

This is not only the data duplication issue, but a performance issue as well (we copy the same file over and over again for every table in a backup set). "
HBASE-11352	"We are exporting a very large table.  The export snapshot job takes 7+ days to complete.  During that time we had to bounce HMaster.  When HMaster initializes, it initializes the SnapshotManager which subsequently deletes the .tmp directory.

If this happens while the ExportSnapshot job is running the reference files get removed and the job fails.

Maybe we could put some sort of token such that when this job is running HMaster wont reset the tmp directory."
HBASE-15303	"After [HBASE-8521|https://issues.apache.org/jira/browse/HBASE-8521], the LoadIncrementalHFiles could ask server to assign sequence id for bulk load hfiles by invoking SecureBulkLoadClient#bulkLoadHFiles:
{code}
  public boolean bulkLoadHFiles(List<Pair<byte[], String>> familyPaths, Token<?> userToken,
      String bulkToken, boolean assignSeqNum) throws IOException {
    try {
      return (Boolean) Methods.call(protocolClazz, proxy, ""bulkLoadHFiles"", new Class[] {
          List.class, Token.class, String.class, Boolean.class },
        new Object[] { familyPaths, userToken, bulkToken, assignSeqNum });
    } catch (Exception e) {
      throw new IOException(""Failed to bulkLoadHFiles"", e);
    }
  }
{code}
However, SecureBulkLoadProtocol does not define such interface(with assignSeqNum as last parameter), so that the client will encounter NoSuchMethodException when using secure. "
HBASE-15268	"<Pre-condition>

1. Enable Ranger for HBASE

2. Ranger has account: tester锛孯ole: admin

3. Disable Kerberos

<Reproduce steps>

1. use admin account to login Ranger Web

2. go to HBase repository

3. Create HBase policy

Policy Name: 111

HBase Table: * (exclude)

HBase Column-family: * (include)

HBase Column: * (include)

Audit Logging: Yes

Select Group: qateam

Select User: tester

Permissions: Read

Delegate Admin: Yes

Policy Name: 222

HBase Table: demoACL (include)

HBase Column-family: * (include)

HBase Column: * (include)

Audit Logging: Yes

Select Group: No

Group Select User: tester

Permissions: Read

Delegate Admin: No

4. logout admin

5. login tester

6. see HBase repository

<Actual Result>

tester can not see any policy (why I can not see 111 Policy?)"
HBASE-15237	"Just ran into this while testing with a custom build with made up version {{1.1.1-dal}} 
{code}
2016-02-08 20:10:38,494 ERROR [B.defaultRpcServer.handler=1,queue=1,port=52622] ipc.RpcServer: Unexpected throwable object路
java.lang.NumberFormatException: For input string: ""1-dal""
  at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
  at java.lang.Integer.parseInt(Integer.java:492)
  at java.lang.Integer.parseInt(Integer.java:527)
  at org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch.currentClientHasMinimumVersion(ProcedurePrepareLatch.java:61)
  at org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch.hasProcedureSupport(ProcedurePrepareLatch.java:47)
  at org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch.createLatch(ProcedurePrepareLatch.java:43)
  at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.java:1530)
  at org.apache.hadoop.hbase.master.MasterRpcServices.createTable(MasterRpcServices.java:449)
  at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java:51097)
  at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2114)
  at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:101)
  at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:130)
  at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:107)
  at java.lang.Thread.run(Thread.java:745)
{code}"
HBASE-15193	"master has ByteBuffInputStream while branch-1 has ByteBufferInputStream.

cc. [~ram_krish], [~anoopsharma]. "
HBASE-14916	"Given test-patch.sh is always run from master, and that it now uses checkstyle_report.py, we should pull back the script to other branches too.
Otherwise we see error like: /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/jenkins.build/dev-support/test-patch.sh: line 662: /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/hbase/dev-support/checkstyle_report.py: No such file or directory

[reference|https://builds.apache.org/job/PreCommit-HBASE-Build/16734//consoleFull]"
HBASE-15049	"I set {{hive.server2.authentication}} to be {{NONE}}

After HS2 start, i see exception in log below:
{code}
2015-12-29 16:58:42,339 ERROR [HiveServer2-Handler-Pool: Thread-31]: server.TThreadPoolServer (TThreadPoolServer.java:run(296)) - Error occurred during processing of message.
java.lang.RuntimeException: org.apache.thrift.transport.TSaslTransportException: No data or no sasl data in the stream
        at org.apache.thrift.transport.TSaslServerTransport$Factory.getTransport(TSaslServerTransport.java:219)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:268)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.thrift.transport.TSaslTransportException: No data or no sasl data in the stream
        at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:328)
        at org.apache.thrift.transport.TSaslServerTransport.open(TSaslServerTransport.java:41)
        at org.apache.thrift.transport.TSaslServerTransport$Factory.getTransport(TSaslServerTransport.java:216)
        ... 4 more
{code}

IMO the problem is we use Sasl transport when authType is NONE, 
{code:title=HiveAuthFactory.java}
  public TTransportFactory getAuthTransFactory() throws LoginException {
    TTransportFactory transportFactory;
    if (authTypeStr.equalsIgnoreCase(AuthTypes.KERBEROS.getAuthName())) {
      try {
        transportFactory = saslServer.createTransportFactory(getSaslProperties());
      } catch (TTransportException e) {
        throw new LoginException(e.getMessage());
      }
    } else if (authTypeStr.equalsIgnoreCase(AuthTypes.NONE.getAuthName())) {
      transportFactory = PlainSaslHelper.getPlainTransportFactory(authTypeStr);
    } else if (authTypeStr.equalsIgnoreCase(AuthTypes.LDAP.getAuthName())) {
      transportFactory = PlainSaslHelper.getPlainTransportFactory(authTypeStr);
    } else if (authTypeStr.equalsIgnoreCase(AuthTypes.PAM.getAuthName())) {
      transportFactory = PlainSaslHelper.getPlainTransportFactory(authTypeStr);
    } else if (authTypeStr.equalsIgnoreCase(AuthTypes.NOSASL.getAuthName())) {
      transportFactory = new TTransportFactory();
    } else if (authTypeStr.equalsIgnoreCase(AuthTypes.CUSTOM.getAuthName())) {
      transportFactory = PlainSaslHelper.getPlainTransportFactory(authTypeStr);
    } else {
      throw new LoginException(""Unsupported authentication type "" + authTypeStr);
    }
    return transportFactory;
  }
{code}
"
HBASE-4845	"When we bulkLoadHFile, we should check the hfile num and compact it."
HBASE-14956	"To perform the zkcli operations using hbase, 
jline is disabled,

{code}
[ERROR] Terminal initialization failed; falling back to unsupported
java.lang.IncompatibleClassChangeError: Found class jline.Terminal, but interface was expected
        at jline.TerminalFactory.create(TerminalFactory.java:101)
        at jline.TerminalFactory.get(TerminalFactory.java:158)
        at jline.console.ConsoleReader.<init>(ConsoleReader.java:229)
        at jline.console.ConsoleReader.<init>(ConsoleReader.java:221)
        at jline.console.ConsoleReader.<init>(ConsoleReader.java:209)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
        at org.apache.zookeeper.ZooKeeperMain.run(ZooKeeperMain.java:335)
        at org.apache.zookeeper.ZooKeeperMain.main(ZooKeeperMain.java:303)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperMainServer.main(ZooKeeperMainServer.java:108)

JLine support is disabled
{code}

To enable this jline-<version>.jar is needed in hbase libraries.
eg: jline-2.11.jar should be exist in hbase/lib directory.

"
HBASE-14195	"There is the issue with ConnectionCache (used to cache connections in Thrift and REST servers) chore implementation when closed connection is not removed from a cache.

Thrift server is affected most because it has local thread cache of Table instances and it does not check if Table instance is invalid (due to closed underlying connection) and it can't do it - no API.  "
HBASE-13161	"Making a note of my recent experience with ITBLL; here is stuff that needs fixing.

+ I was able to run generate for about 24 hours with monkey going and loaded 10B rows. When I ran the verify (a few times), it got stuck at about 90odd percent and never made progress beyond that (this was about 20 hours in IIRC). I spent no time trying to figure why (ran out of cluster and time).
+ Verify at least takes too long. Need to make it run faster."
HBASE-14981	"TestShell seems to be replaced by an AbstractTestShell class. Thus it does not seem that any of the shell unit tests are running? If so, is the intent to create a TestShell which subclasses the abstract class? 

On a related note I've written shell unit tests for HBASE-6721 and it requires a different Shell subclass as I need to have a different balancer and coprocessor installed. I'm hoping we can address that use case in this jira as well."
HBASE-14820	"After the region server rolls back a timed out attempt of  region split, the region becomes unavailable. 

Symptoms:
The RS displays the region open in the web UI.
The meta table still points to the RS
Requests for the regions receive a NotServingRegionException. 
hbck reports 0 inconsistencies. 
Moving the region fails. 

Restarting the region server fixes the problem.

We have see multiple occurrences which require operation intervention."
HBASE-1782	"When a region has too many store files and that that region server holds .META., it is easy to get a 90s deadlock. I will paste the stack traces in a moment."
HBASE-1785	"the zk_dump command depends on zoo.cfg existing, which means it doesnt work when there is no zoo.cfg (a valid cluster config) and the quorum is in hbase-site.xml"
HBASE-1171	"We decided to have the force split, compact, or major compact actions on tables or regions go through the master, in part because then it is easy to have pushbuttons in the master UI for table actions. Requests are relayed by the master via the instruction stream returned when HRS call in. As Zookeeper integration progresses, and the master shrinks accordingly, the code in HBaseAdmin for requesting force splits etc. should move away from the HMasterInterface.modifyTable and instead talk directly to the regionserver(s) as desired. "
HBASE-1419	"Assigner needs to take into account current region balance.  I'm seeing on recovery from a crash on a big cluster that we'll give near ten regions to first regionserver that reports in.  If that regionserver successfully opens the assigned ten, then that'll put it over the balancer slop differential and it'll then be asked to close its regions by the balancer.  This adds to the regionserver crash churn.  Dumb."
HBASE-1983	"I was wondering why I was only flushing every 1k edits though I'd set hbase.regionserver.flushlogentries to 100.  Couldn't figure why.  J-D set me straight.  Flush is done up in HRS now at end of a put.  If the put is a big batch put, then 1k edits will go in before I sync.  In our descriptions in hbase-default.xml, need to bring this out.  I'm sure this file could do with a good clean up by now too... Let this issue cover that too.  For 0.21."
HBASE-1976	"I was browsing though the HBase 0.20.1 code in order to learn about the way HBase deals with Configuration and I noticed that HBaseConfiguration overrides hashCode() without implementing equals(). This can cause some tricky, hard to debug problems whenever instances of this class are added to Maps or HashSets.
"
HBASE-1729	"We log this every minute in RS logs:

{code}
2009-07-30 16:24:54,479 DEBUG org.apache.hadoop.hbase.io.hfile.LruBlockCache: Cache Stats: Sizes: Total=414.8218MB (434972192), Free=252.50949MB (264775392), Max=667.3313MB (699747584), Counts: Blocks=1106, Access=273562, Hit=271671, Miss=1891, Evictions=0, Evicted=0, Ratios: Hit Ratio=99.30874705314636%, Miss Ratio=0.691250991076231%, Evicted/Run=NaN
{code}

Thats a metric.  Should be over in metrics.  Metrics does some of above but not all.  Extend it so just as comprehensive.  Then we can see it ganglia, jmx, etc."
HBASE-1753	"There is a deadlock somewhere in HTable or HConnectionManager when using the deprecated API in a multi-threaded application.

I haven't had a chance to look at the thread dump in detail yet, but will attach it to this issue"
HBASE-1817	Add to package documentation the need of zk to be in classpath as of 0.20.0
HBASE-2162	"15:54 < dj_ryan> - next:
15:54 < dj_ryan> do next on Tree in memstore get rowid
15:55 < dj_ryan> acquire read lock on rowid (wait if necessary)
15:55 < dj_ryan> snapshot the entire row worth of data from memstore into scanner
15:55 < dj_ryan> release read lock on rowid
15:56 < dj_ryan> so if we get an insert on a row after the 'do next' which would call 'next' on the 
                 previous last value grabbed from the tree
15:56 < dj_ryan> we'd skip that one
15:56 < dj_ryan> which is fine
15:57 < dj_ryan> i guess the issue comes in when you have to next a bunch to get to the next row
15:57 < dj_ryan> but that is doable with a simple little while loop
15:57 < dj_ryan> this only applies to memstore
15:57 < dj_ryan> snapshot = hfile 
15:57 < dj_ryan> since its immutable"
HBASE-2552	Assigning Lars F to check we don't have this issue in the rehashed thrift code (thanks Lars).
HBASE-2289	"I tried a few things but don't seem to be able to make it build.   Maybe someone has an idea?  (I'm trying to add in the site/javadoc to hudson build):

{code}
pynchon-2:trunk stack$ MAVEN_OPTS=-Xmx1024m
pynchon-2:trunk stack$ mvn site
[INFO] Scanning for projects...
[INFO] Reactor build order: 
[INFO]   HBase
[INFO]   HBase Core
[INFO]   HBase Contrib
[INFO]   HBase Contrib - Multi Datacenter Replication
[INFO]   HBase Contrib - Stargate
[INFO]   HBase Contrib - Transactional
[INFO] ------------------------------------------------------------------------
[INFO] Building HBase
[INFO]    task-segment: [site]
[INFO] ------------------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[ERROR] FATAL ERROR
[INFO] ------------------------------------------------------------------------
[INFO] Java heap space
[INFO] ------------------------------------------------------------------------
[INFO] Trace
java.lang.OutOfMemoryError: Java heap space
        at java.util.HashMap.<init>(HashMap.java:209)
        at org.codehaus.plexus.util.xml.Xpp3Dom.<init>(Xpp3Dom.java:81)
        at hidden.org.codehaus.plexus.util.xml.Xpp3DomBuilder.build(Xpp3DomBuilder.java:104)
        at hidden.org.codehaus.plexus.util.xml.Xpp3DomBuilder.build(Xpp3DomBuilder.java:75)
        at hidden.org.codehaus.plexus.util.xml.Xpp3DomBuilder.build(Xpp3DomBuilder.java:40)
        at org.apache.maven.plugin.descriptor.PluginDescriptorBuilder.buildConfiguration(PluginDescriptorBuilder.java:330)
        at org.apache.maven.plugin.descriptor.PluginDescriptorBuilder.build(PluginDescriptorBuilder.java:50)
        at org.apache.maven.plugin.MavenPluginDiscoverer.createComponentDescriptors(MavenPluginDiscoverer.java:52)
        at org.codehaus.plexus.component.discovery.AbstractComponentDiscoverer.findComponents(AbstractComponentDiscoverer.java:78)
        at org.codehaus.plexus.DefaultPlexusContainer.discoverComponents(DefaultPlexusContainer.java:717)
        at org.codehaus.plexus.DefaultPlexusContainer.addJarResource(DefaultPlexusContainer.java:1396)
        at org.apache.maven.plugin.DefaultPluginManager.ensurePluginContainerIsComplete(DefaultPluginManager.java:853)
        at org.apache.maven.plugin.DefaultPluginManager.getConfiguredMojo(DefaultPluginManager.java:647)
        at org.apache.maven.plugin.DefaultPluginManager.getReport(DefaultPluginManager.java:583)
        at org.apache.maven.lifecycle.DefaultLifecycleExecutor.getReports(DefaultLifecycleExecutor.java:982)
        at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoals(DefaultLifecycleExecutor.java:650)
        at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoalWithLifecycle(DefaultLifecycleExecutor.java:556)
        at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoal(DefaultLifecycleExecutor.java:535)
        at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoalAndHandleFailures(DefaultLifecycleExecutor.java:387)
        at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeTaskSegments(DefaultLifecycleExecutor.java:348)
        at org.apache.maven.lifecycle.DefaultLifecycleExecutor.execute(DefaultLifecycleExecutor.java:180)
        at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:328)
        at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:138)
        at org.apache.maven.cli.MavenCli.main(MavenCli.java:362)
        at org.apache.maven.cli.compat.CompatibleMain.main(CompatibleMain.java:60)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.codehaus.classworlds.Launcher.launchEnhanced(Launcher.java:315)
        at org.codehaus.classworlds.Launcher.launch(Launcher.java:255)
        at org.codehaus.classworlds.Launcher.mainWithExitCode(Launcher.java:430)
{code}"
HBASE-2860	"Here is current help:

{code}
hbase(main):008:0> help 'drop'

COMMAND: drop
          Drop the named table. Table must first be disabled. If table has
          more than one region, run a major compaction on .META.:

          hbase> major_compact "".META.""
{code}

Here is the exception it throws:

{code}
ERROR: java.io.IOException: java.io.IOException: java.lang.NoSuchMethodError: org.apache.hadoop.hbase.master.HMaster.access$1(Lorg/apache/hadoop/hbase/master/HMaster;Lorg/apache/hadoop/hbase/client/Result;)Lorg/apache/hadoop/hbase/util/Pair;
        at org.apache.hadoop.hbase.master.HMaster$1.processRow(HMaster.java:890)
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:156)
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:68)
        at org.apache.hadoop.hbase.master.HMaster.getTableRegions(HMaster.java:901)
        at org.apache.hadoop.hbase.master.HMaster.modifyTable(HMaster.java:1036)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:576)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:919)
{code}"
HBASE-3049	"Recently I've been seeing some strange behavior around HLogs, HBASE-3038 is an example. Looking in my oldlogs folder, I see:

{noformat}
-rw-r--r--   3 hadoop supergroup  716716459 2010-09-29 01:08 /hbase/.oldlogs/10.20.20.176%3A60020.1285722528252
-rw-r--r--   3 hadoop supergroup   64841781 2010-09-29 01:10 /hbase/.oldlogs/10.20.20.176%3A60020.1285722620286
bunch of normally sized HLogs... then
-rw-r--r--   3 hadoop supergroup  769729956 2010-09-29 01:13 /hbase/.oldlogs/10.20.20.176%3A60020.1285722785347
{noformat}

680MB is way off the 64MB limit. My feeling is that the optimizations I did in the scope of HBASE-2922 that remove some blocking behavior have the side effect of getting a lot more data in a lot faster in some edge cases. Need to investigate more."
HBASE-2872	"This is related to HBASE-2866 Regions going permanently offline. The fix prevented multiple duplicate updates from going to ZK. But the master still tries to update these regions.
"
HBASE-2329	"In current trunk, mvn install is broke.  Test compilation fails:

{code}
[INFO] [resources:testResources {execution: default-testResources}]
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 4 resources
[INFO] [compiler:testCompile {execution: default-testCompile}]
[INFO] Compiling 101 source files to /Users/Stack/checkouts/trunk/core/target/test-classes
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR :
[INFO] -------------------------------------------------------------
[ERROR] /Users/Stack/checkouts/trunk/core/src/test/java/org/apache/hadoop/hbase/io/hfile/TestSeekTo.java:[26,30] [deprecation] org.apache.hadoop.hbase.HBaseTestCase in org.apache.hadoop.hbase has been deprecated

[ERROR] /Users/Stack/checkouts/trunk/core/src/test/java/org/apache/hadoop/hbase/io/hfile/TestSeekTo.java:[27,35] cannot find symbol
symbol  : class Bytes
location: package org.apache.hadoop.hbase.util

[ERROR] /Users/Stack/checkouts/trunk/core/src/test/java/org/apache/hadoop/hbase/HBaseTestCase.java:[36,37] cannot find symbol
symbol  : class Delete
location: package org.apache.hadoop.hbase.client
...
{code}

It does not seem to be able to find hbase classes."
HBASE-2384	"Currently, HTable#flushCommits()  ( i.e. HTableInterface) returns void , but the failure of the flush is not obvious to the user (although, taken care internally by appending to writeBuffer ). 

Changing return type to boolean , to help handle on the client side ( logging / retry etc. ) . 


Propagate the change to put(Put), put(List<Put) . 

Add a warning to close() as well. 
"
HBASE-2791	"I think this is part of the Master/ZooKeeper refactoring project but I'm putting it up here to be sure we cover it. Currently in ZKW (and other places around the code base) we do ZK operations and we don't really handle the exceptions, for example in ZKW.setClusterState:

{code}
    } catch (InterruptedException e) {
      LOG.warn(""<"" + instanceName + "">"" + ""Failed to set state node in ZooKeeper"", e);
    } catch (KeeperException e) {
      if(e.code() == KeeperException.Code.NODEEXISTS) {
        LOG.debug(""<"" + instanceName + "">"" + ""State node exists."");
      } else {
        LOG.warn(""<"" + instanceName + "">"" + ""Failed to set state node in ZooKeeper"", e);
      }
{code}

This has been always like that since we started using ZK.

What if the session was expired? What if it was only the connection that had a blip? Do we handle it correctly? We need to have this discussion."
HBASE-3193	"From Charles Thayer up on the list:

{code}
I haven't seen any replies, which is probably because the master seems to
be changing rapidly at the moment.  However, if anyone needs this for
hbase 0.89.20100726, here's a patch to work around the issue temporarily
until 0.90.0 (which will probably fix the problem).

/charles thayer

--- src/main/java/org/apache/hadoop/hbase/master/HMaster.java   2010-07-30 21:09:11.000000000 +0000
+++ src/main/java/org/apache/hadoop/hbase/master/HMaster.java   2010-10-11 20:51:30.821519000 +0000
@@ -1297,11 +1297,18 @@

              runtime.getVmVendor() + "", vmVersion="" + runtime.getVmVersion());
            LOG.info(""vmInputArguments="" + runtime.getInputArguments());
          }
+
+         boolean hbase_manages_zk = true;
+         if (System.getenv(""HBASE_MANAGES_ZK"") != null
+             && System.getenv(""HBASE_MANAGES_ZK"").equals(""false""))
+           hbase_manages_zk = false;

+
          // If 'local', defer to LocalHBaseCluster instance.  Starts master
          // and regionserver both in the one JVM.
          if (LocalHBaseCluster.isLocal(conf)) {
            final MiniZooKeeperCluster zooKeeperCluster =
              new MiniZooKeeperCluster();
+           if (hbase_manages_zk) {  // thayer

            File zkDataPath = new File(conf.get(""hbase.zookeeper.property.dataDir""));
            int zkClientPort = conf.getInt(""hbase.zookeeper.property.clientPort"", 0);
            if (zkClientPort == 0) {
@@ -1319,11 +1326,15 @@

            }
            conf.set(""hbase.zookeeper.property.clientPort"",
              Integer.toString(clientPort));
+           } // thayer

+
            // Need to have the zk cluster shutdown when master is shutdown.
            // Run a subclass that does the zk cluster shutdown on its way out.
            LocalHBaseCluster cluster = new LocalHBaseCluster(conf, 1,
              LocalHMaster.class, HRegionServer.class);
+           if (hbase_manages_zk) {

            ((LocalHMaster)cluster.getMaster()).setZKCluster(zooKeeperCluster);
+           }
            cluster.startup();
          } else {
            HMaster master = constructMaster(masterClass, conf);
{code}"
HBASE-2235	"Here is the short story:

Scenario is a cluster of 3 servers.  Server 1. crashed.  It was carrying the .META.   We split the logs.  .META. is put on the head of the assignment queue.  Server 2. happens to be in a state where it wants to report a split.  The master fails the report because there is no .META. (It fails it ugly with a NPE).  Server 3. checks in and falls into the assignment code (RegionManager#regionsAwaitingAssignment).  In here we have this bit of code around line #412:

{code}
    if (reassigningMetas && isMetaOrRoot && !isSingleServer) {
      return regionsToAssign; // dont assign anything to this server.
    }
{code}

Because we think this not a single server cluster -- we think there are two 'live' nodes -- we won't assign meta."
HBASE-2409	"Ryan apparently said this 6 months ago, that on split, the regions are assigned back to the parent hosting regionserver almost always.  Then, to keep up some kinda balance, the load balancer kicks and closes the just opened daughters -- which have just been opened a moment ago -- to deploy them elsewhere in the name of keeping good balance.

This issue is to confirm the above behavior indeed happens and then to take action to make it so at least one of the daughters is held up so it doesn't go back to the current heartbeating host, the parent hosting server."
HBASE-3187	"Saw this testing.... All opener handlers stuck waiting on meta... doesn't recover... This RS had also been given a bunch of regions to open.

{code}
""RS_OPEN_REGION-sv2borg188,60020,1288644070661-2"" daemon prio=10 tid=0x0000000042c65000 nid=0x4e61 in Object.wait() [0x00007f9709e5b000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
    at java.lang.Object.wait(Native Method)
    - waiting on <0x00007f972268ad00> (a java.util.concurrent.atomic.AtomicBoolean)
    at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMeta(CatalogTracker.java:327)
    - locked <0x00007f972268ad00> (a java.util.concurrent.atomic.AtomicBoolean)
    at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMetaServerConnectionDefault(CatalogTracker.java:362)
    at org.apache.hadoop.hbase.catalog.MetaEditor.updateRegionLocation(MetaEditor.java:146)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.postOpenDeployTasks(HRegionServer.java:1280)
    at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:156)
    at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:151)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)

""RS_OPEN_REGION-sv2borg188,60020,1288644070661-1"" daemon prio=10 tid=0x0000000042ce7800 nid=0x4e5c in Object.wait() [0x00007f9709f5c000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
    at java.lang.Object.wait(Native Method)
    - waiting on <0x00007f972268ad00> (a java.util.concurrent.atomic.AtomicBoolean)
    at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMeta(CatalogTracker.java:327)
    - locked <0x00007f972268ad00> (a java.util.concurrent.atomic.AtomicBoolean)
    at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMetaServerConnectionDefault(CatalogTracker.java:362)
    at org.apache.hadoop.hbase.catalog.MetaEditor.updateRegionLocation(MetaEditor.java:146)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.postOpenDeployTasks(HRegionServer.java:1280)
    at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:156)
    at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:151)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
{code}
"
HBASE-3245	"In getMap() method the comparator passed to versionMap does:

      public int compare(Long l1, Long l2) {
            return l2.compareTo(l1);
       }

which inverts the result of the comparison. "
HBASE-10453	"Right now we will encode tags always with prefix tree encoding. With other encoders it checks HCD#shouldCompressTags().  Suggest we can do the same fro PrefixTree also.

I can see some places PrefixTree impl passes booleans like isIncludeMvcc isIncludeTags etc.  We have encapsulated all such info into a HFileContext and the code path uses that now.  We can do the same with PrefixTree code path also."
HBASE-4130	"Using a HTable instance with auto commit disabled in multiple thread may raise IndexOutOfBoundsException , as the processBatchOfPuts remove the commited results by their index which may be wrong as the list size shortened by other thread.

the following is the stack trace.

java.lang.IndexOutOfBoundsException: Index: 3781, Size: 2222
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.remove(ArrayList.java:387)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.processBatchOfPuts(HConnectionManager.java:1252)
	at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:826)
	at org.apache.hadoop.hbase.client.HTable.doPut(HTable.java:682)
	at org.apache.hadoop.hbase.client.HTable.put(HTable.java:667)"
HBASE-4182	"1. Start 2 RS.  Create some regions so that is is balanced.
2. Stop RS2.  Now all the Regions from RS2 are assigned to RS1.
3. Again start RS2.
4. Load Balancing is calculated and few regions from RS1 are assigned to RS2.
As part of this step Master tries to unassign the regions from RS1.
{noformat}
          RegionTransitionData data = ZKAssign.getDataNoWatch(zkw, ZKAssign
              .getNodeName(zkw, region.getEncodedName()), null);
          if (data.equals(EventType.RS_ZK_REGION_CLOSING)) {
            ZKAssign.createNodeClosing(zkw, region, master.getServerName());
          }
{noformat}

Now there is no data present in the unassigned node.  We are directly comparing the data.
Here data is null. Hence nullpointer exception is thrown.
Hence load balancing fails."
HBASE-10261	I can start hbase in my hadoop-2.2.0 cluster
HBASE-9245	"This is an umbrella issue that will cover the removal or refactoring of dangling dead code and cruft.  Some can make it into 0.96, some may have to wait for an 0.98.  The ""great culling"" of code will be grouped patches that are logically related."
HBASE-10937	"There is known issue to run MR job for hbase0.96+ version. Details at section 鈥淣otice to Mapreduce users of HBase 0.96.1 and above鈥?https://hbase.apache.org/book.html

Basically we need to put hbase-protocol*.jar before hadoop loads protobuf-java jar.  I updated our documentation on http://phoenix.incubator.apache.org/bulk_dataload.html on how to use CsvBulkLoadTool for Phoenix 4.0 as following:

{noformat}
HADOOP_CLASSPATH=$(hbase mapredcp)::/path/to/hbase/conf hadoop jar phoenix-4.0.0-incubating-client.jar org.apache.phoenix.mapreduce.CsvBulkLoadTool --table EXAMPLE --input /data/example.csv
{noformat}
OR
{noformat}
HADOOP_CLASSPATH=/path/to/hbase-protocol.jar:/path/to/hbase/conf hadoop jar phoenix-4.0.0-incubating-client.jar org.apache.phoenix.mapreduce.CsvBulkLoadTool --table EXAMPLE --input /data/example.csv
{noformat} 

Thanks."
HBASE-14744	"Hi Anoop/Ted,
I need to delete bulk records in Hbase table based on certain criteria.
I copied one piece of code from similar thread and it throws ""can not find  symbol"" error for most of the code.

Can you please provide me right code which would be used to delete bulk records

Here are the errors
DeleteRowsTest.java:75: error: cannot find symbol
   HTable tableName = new HTable(conf, ""salestoolsdata:account"");
                                 ^
  symbol:   variable conf
  location: class DeleteRowsTest
DeleteRowsTest.java:79: error: no suitable constructor found for SingleColumnValueFilter(char,String,CompareOp,byte[])
    SingleColumnValueFilter scvf = new SingleColumnValueFilter('d',""ISDELETED"",CompareOp.EQUAL, Bytes.toBytes(""true""));
                                   ^
    constructor SingleColumnValueFilter.SingleColumnValueFilter(byte[],byte[],CompareOp,byte[]) is not applicable
      (argument mismatch; char cannot be converted to byte[])
    constructor SingleColumnValueFilter.SingleColumnValueFilter(byte[],byte[],CompareOp,ByteArrayComparable) is not applicable
      (argument mismatch; char cannot be converted to byte[])
DeleteRowsTest.java:85: error: cannot find symbol
    long noOfRowsDeleted = invokeBulkDeleteProtocol(tableName, scan, 500, DeleteType.ROW, null);
                                                                          ^
  symbol:   variable DeleteType
  location: class DeleteRowsTest
DeleteRowsTest.java:89: error: cannot find symbol
    for (Result result : ht.getScanner(new Scan())) {
                         ^
  symbol:   variable ht
  location: class DeleteRowsTest
DeleteRowsTest.java:98: error: cannot find symbol
    HTable ht = new HTable(conf, tableName);
                           ^
  symbol:   variable conf
  location: class DeleteRowsTest
DeleteRowsTest.java:100: error: cannot find symbol
    Batch.Call<BulkDeleteProtocol, BulkDeleteResponse> callable = 
               ^
  symbol:   class BulkDeleteProtocol
  location: class DeleteRowsTest
DeleteRowsTest.java:100: error: cannot find symbol
    Batch.Call<BulkDeleteProtocol, BulkDeleteResponse> callable = 
                                   ^
  symbol:   class BulkDeleteResponse
  location: class DeleteRowsTest
DeleteRowsTest.java:101: error: cannot find symbol
        new Batch.Call<BulkDeleteProtocol, BulkDeleteResponse>() {
                       ^
  symbol:   class BulkDeleteProtocol
  location: class DeleteRowsTest
DeleteRowsTest.java:101: error: cannot find symbol
        new Batch.Call<BulkDeleteProtocol, BulkDeleteResponse>() {
                                           ^
  symbol:   class BulkDeleteResponse
  location: class DeleteRowsTest
DeleteRowsTest.java:102: error: cannot find symbol
      public BulkDeleteResponse call(BulkDeleteProtocol instance) throws IOException {
                                     ^
  symbol: class BulkDeleteProtocol
DeleteRowsTest.java:102: error: cannot find symbol
      public BulkDeleteResponse call(BulkDeleteProtocol instance) throws IOException {
             ^
  symbol: class BulkDeleteResponse
DeleteRowsTest.java:106: error: cannot find symbol
    Map<byte[], BulkDeleteResponse> result = ht.coprocessorExec(BulkDeleteProtocol.class,
                ^
  symbol:   class BulkDeleteResponse
  location: class DeleteRowsTest
DeleteRowsTest.java:106: error: cannot find symbol
    Map<byte[], BulkDeleteResponse> result = ht.coprocessorExec(BulkDeleteProtocol.class,
                                                                ^
  symbol:   class BulkDeleteProtocol
  location: class DeleteRowsTest
DeleteRowsTest.java:108: error: cannot find symbol
    for (BulkDeleteResponse response : result.values()) {
         ^
  symbol:   class BulkDeleteResponse
  location: class DeleteRowsTest
Note: Some messages have been simplified; recompile with -Xdiags:verbose to get full output
18 errors



Thanks
Marimuthu

"
HBASE-14265	"Now, there is no limit for users who can create table under 'hbase' NameSpace. I think it has some risk.

Because we use {{TableName.systemTable}} to decide whether this table is System or not.

But as code,  {{TableName.systemTable}} will be true, if NS equals ""hbase'
{code}
 if (Bytes.equals(NamespaceDescriptor.SYSTEM_NAMESPACE_NAME, namespace)) {
        this.namespace = NamespaceDescriptor.SYSTEM_NAMESPACE_NAME;
        this.namespaceAsString = NamespaceDescriptor.SYSTEM_NAMESPACE_NAME_STR;
        this.systemTable = true;
      } 
{code}
 
And we treat system table and normal table differently. 
For example,  https://issues.apache.org/jira/browse/HBASE-14257 will flush fast if table belong to system table.



"
HBASE-14410	"Replication hangs until target cluster is restarted. 
IPC queue was at max bytes on a single region server on target cluster. Master appeared OK. Region server serving hbase:meta appeared OK. Have seen this several times since upgrade from .98.6 to 1.0.0.

Observed this in the stack trace in single region server on target cluster:
""hconnection-0x59e10d51-shared--pool8-t97669"" daemon prio=10 tid=0x0000000001235000 nid=0xa47 in Object.wait() [0x00007ff5186fb000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.hbase.ipc.RpcClientImpl.call(RpcClientImpl.java:1189)
        - locked <0x00000004147a0000> (a org.apache.hadoop.hbase.ipc.Call)
        at org.apache.hadoop.hbase.ipc.AbstractRpcClient.callBlockingMethod(AbstractRpcClient.java:216)
        at org.apache.hadoop.hbase.ipc.AbstractRpcClient$BlockingRpcChannelImplementation.callBlockingMethod(AbstractRpcClient.java:300)
        at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.get(ClientProtos.java:31865)
        at org.apache.hadoop.hbase.protobuf.ProtobufUtil.getRowOrBefore(ProtobufUtil.java:1580)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegionInMeta(ConnectionManager.java:1294)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1126)
        at org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.findAllLocationsOrFail(AsyncProcess.java:916)
        at org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.groupAndSendMultiAction(AsyncProcess.java:833)
        at org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.resubmit(AsyncProcess.java:1156)
        at org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.receiveGlobalFailure(AsyncProcess.java:1123)
        at org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.access$1100(AsyncProcess.java:574)
        at org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl$SingleServerRequestRunnable.run(AsyncProcess.java:705)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)"
HBASE-8810	"After the defaults were changed in the xml some constants were left the same.

DEFAULT_HBASE_CLIENT_PAUSE for example."
HBASE-14396	"Currently the hbase audit log only records the user and scope,we can't know which table the user is operating on unless the scope is table.

It would be better to know what's going on if we record the exact table and column family we are operating on besides the scope.

  String logMessage =
        ""Access "" + (result.isAllowed() ? ""allowed"" : ""denied"") + "" for user ""
            + (result.getUser() != null ? result.getUser().getShortName() : ""UNKNOWN"")
            + ""; reason: "" + result.getReason() + ""; remote address: ""
            + (remoteAddr != null ? remoteAddr : """") + ""; request: "" + result.getRequest()
            + ""; context: "" + result.toContextString();"
HBASE-14390	"mvn clean package -DskipTests

{code}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.2:compile (default-compile) on project hbase-rest: Compilation failure: Compilation failure:
[ERROR] /Users/chenheng/apache/hbase/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RootResource.java:[108,10] 鎵句笉鍒扮鍙?[ERROR] 绗﹀彿:   绫?NamespacesResource
[ERROR] 浣嶇疆: 绫?org.apache.hadoop.hbase.rest.RootResource
[ERROR] /Users/chenheng/apache/hbase/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/provider/JAXBContextResolver.java:[34,42] 鎵句笉鍒扮鍙?[ERROR] 绗﹀彿:   绫?NamespacesInstanceModel
[ERROR] 浣嶇疆: 绋嬪簭鍖?org.apache.hadoop.hbase.rest.model
[ERROR] /Users/chenheng/apache/hbase/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/provider/JAXBContextResolver.java:[35,42] 鎵句笉鍒扮鍙?[ERROR] 绗﹀彿:   绫?NamespacesModel
[ERROR] 浣嶇疆: 绋嬪簭鍖?org.apache.hadoop.hbase.rest.model
[ERROR] /Users/chenheng/apache/hbase/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/RootResource.java:[109,16] 鎵句笉鍒扮鍙?[ERROR] 绗﹀彿:   绫?NamespacesResource
[ERROR] 浣嶇疆: 绫?org.apache.hadoop.hbase.rest.RootResource
[ERROR] /Users/chenheng/apache/hbase/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/provider/JAXBContextResolver.java:[68,5] 鎵句笉鍒扮鍙?[ERROR] 绗﹀彿:   绫?NamespacesModel
[ERROR] 浣嶇疆: 绫?org.apache.hadoop.hbase.rest.provider.JAXBContextResolver
[ERROR] /Users/chenheng/apache/hbase/hbase-rest/src/main/java/org/apache/hadoop/hbase/rest/provider/JAXBContextResolver.java:[69,5] 鎵句笉鍒扮鍙?[ERROR] 绗﹀彿:   绫?NamespacesInstanceModel
[ERROR] 浣嶇疆: 绫?org.apache.hadoop.hbase.rest.provider.JAXBContextResolver
[ERROR] -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR]
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hbase-rest
{code}

[~stack]"
HBASE-5789	"The HConnectionManager offers a few methods to manage deletion of connections. The public deleteConnection method does not allow the caller to specify that the connection is stale. The deleteStaleConnection takes an HConnection instance. In order to get the HConnection instance a caller need to call getConnection with the connection's Configuration instance to retrieve the connection. But, getConnection will attempt to connect if the connection has already been deleted, which sort of defeat the purpose of deleting the connection. It also complicates caller as they need to handle ZooKeeperConnectionException. 

To simplify clients it would be nice to be able to specify ""staleConnection"" on the deleteConnection method or offer a deleteStaleConnection method that takes a Configuration parameter. Or something completely different? "
HBASE-5362	"I have an application like a scheduler that starts periodically some MR jobs that reads from one HBase table and write in more tables, changing the row key and the columns list. 
I have encounted a problem with this: after each MR job, I have +1 connections open to ZK and after a period of time, I have received IOException. I saw that I am not the first that had had this problem, but I didn't see any fix. No in HBase 0.90.4, nor in HBase 0.92.
To fix this issue I wrote a custom TableInputFormat that forces to close the connection to ZK after each getSplits call. 
Also, the initTableMapperJob was overwritten.
I found an open JIRA issue here: https://issues.apache.org/jira/browse/HBASE-3792

Regards,
Ionut I. "
HBASE-4762	"Patch in HBASE-3914 fixed root assigned in two regionservers. But it seemed like root region will never be assigned if verifyRootRegionLocation throws IOE.
Like following master logs:
{noformat}
2011-10-19 19:13:34,873 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_META_SERVER_S
HUTDOWN
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hbase.ipc.ServerNotRunningException: Server is not running yet
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1090)

        at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:771)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:256)
        at $Proxy7.getRegionInfo(Unknown Source)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.verifyRegionLocation(CatalogTracker.java:424)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.verifyRootRegionLocation(CatalogTracker.java:471)
        at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.verifyAndAssignRoot(ServerShutdownHandler.java:90)
        at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:126)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:151)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}
After this, -ROOT-'s region won't be assigned, like this:
{noformat}
2011-10-19 19:18:40,000 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation: locateRegionInMeta parent
Table=-ROOT-, metaLocation=address: dw79.kgb.sqa.cm4:60020, regioninfo: -ROOT-,,0.70236052, attempt=0 of 10 failed; retrying after s
leep of 1000 because: org.apache.hadoop.hbase.NotServingRegionException: Region is not online: -ROOT-,,0
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:2771)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getClosestRowBefore(HRegionServer.java:1802)
        at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:569)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1091)
{noformat}
So we should rewrite the verifyRootRegionLocation method."
HBASE-6198	"We synchronize on region state doing single assign of a region but then over in the handleRegion zk callback, we don't synchronize on the regionstate instance.  Makes no sense.  Either get rid of all synchronization or put synchronization everywhere (we should probably do the latter since it makes things easier to reason about, and these states are already complicated.  There could be a performance issue though)."
HBASE-14263	"It seems that logic around selection of store file candidates is broken:
{code}
        // Compute the total size of files that will
        // have to be read if this set of files is compacted.
        long size = getTotalStoreSize(potentialMatchFiles);
        // Store the smallest set of files.  This stored set of files will be used
        // if it looks like the algorithm is stuck.
        if (mightBeStuck && size < smallestSize) {
          smallest = potentialMatchFiles;
          smallestSize = size;
        }
        if (size > comConf.getMaxCompactSize()) {
          continue;
        }

        ++opts;
        if (size >= comConf.getMinCompactSize()
            && !filesInRatio(potentialMatchFiles, currentRatio)) {
          continue;
        }
{code}
This is from applyCompactionPolicy method. As you can see, both min compaction size and max compaction size are applied to a *selection* of files and not to individual files. It mostly works as expected only because nobody seems using non-default hbase.hstore.compaction.max.size, which is  Long.MAX_VALUE  and  it  is not  that  easy  to  figure out  what  is  going  on  on an opposite side (why small files do not get included?)"
HBASE-14316	"On truncate table command, the hbase doesn't maintain the pre-defined splits. It simply drops and re creates table. It should have some mechanism to maintain the predefined splits."
HBASE-14281	"Follow-on issue for HBASE-13329: CellComparator#getMinimumMidpointArray seems to have had a necessary change omitted and the patch only covered one of the two places diffIdx could overflow the short.

For some background, we ran into the HBASE-13329 issue where a flush would cause a regionserver abort. After abort, the region in question would almost indefinitely sit in the FAILED_OPEN state. Applying the patch from HBASE-13329 didn't solve the issue, but I noticed a comment in that issue which applied the same change in CellComparator#getMinimumMidpointArray, but the change was omitted from the attached patch.

RS abort for reference:

slave3.xxx.xxx.xxx,60020,1440131603772: Replay of WAL required. Forcing server shutdown
org.apache.hadoop.hbase.DroppedSnapshotException: region: deduplication,P\xDFt\x10\x053e73ceff5a2717d2ba76887ea21a2a8e353d1372\xFE,1438362391124.2bb6a602be6b1bfcea0508af4ba42235.
at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushCacheAndCommit(HRegion.java:2243)
at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1972)
at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1935)
at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:1833)
at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:452)
at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:413)
at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.access$800(MemStoreFlusher.java:70)
at org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushHandler.run(MemStoreFlusher.java:229)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NegativeArraySizeException
at org.apache.hadoop.hbase.CellComparator.getMinimumMidpointArray(CellComparator.java:494)
at org.apache.hadoop.hbase.CellComparator.getMidpoint(CellComparator.java:448)
at org.apache.hadoop.hbase.io.hfile.HFileWriterV2.finishBlock(HFileWriterV2.java:165)
at org.apache.hadoop.hbase.io.hfile.HFileWriterV2.checkBlockBoundary(HFileWriterV2.java:146)
at org.apache.hadoop.hbase.io.hfile.HFileWriterV2.append(HFileWriterV2.java:263)
at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.append(StoreFile.java:949)
"
HBASE-14299	for modularization purpose code refactor should be applied to storage module
HBASE-14284	"TestDistributedLogReplay puts up regionservers with *40* priority handlers each. This makes for TDLR running with many hundreds of threads. Trying to figure why 40, I see the test can hang if less with all client use stuck never timing out:

{code}
""RS:2;localhost:58498"" prio=5 tid=0x00007fd284d4e800 nid=0x416af in Object.wait() [0x000000012952e000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	at java.lang.Object.wait(Object.java:461)
	at io.netty.util.concurrent.DefaultPromise.await0(DefaultPromise.java:355)
	- locked <0x00000007dff93ea0> (a org.apache.hadoop.hbase.ipc.AsyncCall)
	at io.netty.util.concurrent.DefaultPromise.await(DefaultPromise.java:266)
	at io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:42)
	at org.apache.hadoop.hbase.ipc.AsyncRpcClient.call(AsyncRpcClient.java:231)
	at org.apache.hadoop.hbase.ipc.AbstractRpcClient.callBlockingMethod(AbstractRpcClient.java:214)
	at org.apache.hadoop.hbase.ipc.AbstractRpcClient$BlockingRpcChannelImplementation.callBlockingMethod(AbstractRpcClient.java:288)
	at org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$RegionServerStatusService$BlockingStub.regionServerReport(RegionServerStatusProtos.java:8994)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.tryRegionServerReport(HRegionServer.java:1148)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:957)
	at org.apache.hadoop.hbase.MiniHBaseCluster$MiniHBaseClusterRegionServer.runRegionServer(MiniHBaseCluster.java:156)
	at org.apache.hadoop.hbase.MiniHBaseCluster$MiniHBaseClusterRegionServer.access$000(MiniHBaseCluster.java:108)
	at org.apache.hadoop.hbase.MiniHBaseCluster$MiniHBaseClusterRegionServer$1.run(MiniHBaseCluster.java:140)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:356)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1594)
	at org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:279)
	at org.apache.hadoop.hbase.MiniHBaseCluster$MiniHBaseClusterRegionServer.run(MiniHBaseCluster.java:138)
	at java.lang.Thread.run(Thread.java:744)

{code}

We  never recover."
HBASE-13391	"TestRegionObserverInterface is frequently failing on branch-1 .

Example:

{noformat}
java.lang.AssertionError: Result of org.apache.hadoop.hbase.coprocessor.SimpleRegionObserver$Legacy.getCtPreWALRestore is expected to be 1, while we get 0
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.apache.hadoop.hbase.coprocessor.TestRegionObserverInterface.verifyMethodResult(TestRegionObserverInterface.java:751)
	at org.apache.hadoop.hbase.coprocessor.TestRegionObserverInterface.testLegacyRecovery(TestRegionObserverInterface.java:685)
{noformat}"
HBASE-14204	"After upgrade hbase-0.98.3-hadoop2 to hbase-1.0.11, everything works fine, HMaster and RegionServers all started OK, hbase shell works OK, table scan works OK. Except pig script failed to store data to Hbase using org.apache.pig.backend.hadoop.hbase.HBaseStorage.


Detailed exception from pig.
{quote}
Pig Stack Trace
---------------
ERROR 1200: Pig script failed to parse:
<line 13, column 0> pig script failed to validate: java.lang.RuntimeException: could not instantiate 'org.apache.pig.backend.hadoop.hbase.HBaseStorage' with arguments '[cf:*]'

Failed to parse: Pig script failed to parse:
<line 13, column 0> pig script failed to validate: java.lang.RuntimeException: could not instantiate 'org.apache.pig.backend.hadoop.hbase.HBaseStorage' with arguments '[cf:*]'
        at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:199)
        at org.apache.pig.PigServer$Graph.validateQuery(PigServer.java:1707)
        at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1680)
        at org.apache.pig.PigServer.registerQuery(PigServer.java:623)
        at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:1082)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:505)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:230)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:205)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:66)
        at org.apache.pig.Main.run(Main.java:565)
        at org.apache.pig.Main.main(Main.java:177)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:601)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
Caused by:
<line 13, column 0> pig script failed to validate: java.lang.RuntimeException: could not instantiate 'org.apache.pig.backend.hadoop.hbase.HBaseStorage' with arguments '[cf:*]'
        at org.apache.pig.parser.LogicalPlanBuilder.buildStoreOp(LogicalPlanBuilder.java:1009)
        at org.apache.pig.parser.LogicalPlanGenerator.store_clause(LogicalPlanGenerator.java:7806)
        at org.apache.pig.parser.LogicalPlanGenerator.op_clause(LogicalPlanGenerator.java:1669)
        at org.apache.pig.parser.LogicalPlanGenerator.general_statement(LogicalPlanGenerator.java:1102)
        at org.apache.pig.parser.LogicalPlanGenerator.statement(LogicalPlanGenerator.java:560)
        at org.apache.pig.parser.LogicalPlanGenerator.query(LogicalPlanGenerator.java:421)
        at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:191)
        ... 15 more
Caused by: java.lang.RuntimeException: could not instantiate 'org.apache.pig.backend.hadoop.hbase.HBaseStorage' with arguments '[cf:*]'
        at org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:772)
        at org.apache.pig.parser.LogicalPlanBuilder.buildStoreOp(LogicalPlanBuilder.java:988)
        ... 21 more
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
        at org.apache.pig.impl.PigContext.instantiateFuncFromSpec(PigContext.java:740)
        ... 22 more
Caused by: java.lang.NoSuchMethodError: org.apache.hadoop.hbase.client.Scan.setCacheBlocks(Z)V
        at org.apache.pig.backend.hadoop.hbase.HBaseStorage.initScan(HBaseStorage.java:427)
        at org.apache.pig.backend.hadoop.hbase.HBaseStorage.<init>(HBaseStorage.java:368)
        at org.apache.pig.backend.hadoop.hbase.HBaseStorage.<init>(HBaseStorage.java:239)
        ... 27 more
================================================================================

{quote}


Here is the classpath for running pig, as you can see, I am using the hbase-client.1.0.1.1 version. 
{quote}
[hadoop@hadoop-master-1 ~]$ pig -useHCatalog
ls: cannot access /opt/apache-hive-0.14.0-bin/lib/slf4j-api-*.jar: No such file or directory
ls: cannot access /opt/apache-hive-0.14.0-bin/hcatalog/lib/*hbase-storage-handler-*.jar: No such file or directory
Find hadoop at /opt/hadoop-2.4.1/bin/hadoop
dry run:
HADOOP_CLASSPATH: /opt/hbase-1.0.1.1/conf:/opt/pig-0.15.0/conf:/usr/java/latest/lib/tools.jar:/opt/apache-hive-0.14.0-bin/lib/hive-metastore-0.14.0.jar:/opt/apache-hive-0.14.0-bin/lib/libthrift-0.9.0.jar:/opt/apache-hive-0.14.0-bin/lib/hive-exec-0.14.0.jar:/opt/apache-hive-0.14.0-bin/lib/libfb303-0.9.0.jar:/opt/apache-hive-0.14.0-bin/lib/jdo-api-3.0.1.jar::/opt/apache-hive-0.14.0-bin/lib/hive-hbase-handler-0.14.0.jar:/opt/apache-hive-0.14.0-bin/hcatalog/share/hcatalog/hive-hcatalog-core-0.14.0.jar::/opt/apache-hive-0.14.0-bin/hcatalog/share/hcatalog/hive-hcatalog-pig-adapter-0.14.0.jar:/opt/apache-hive-0.14.0-bin/conf:/opt/hadoop-2.4.1/etc/hadoop/:/opt/pig-0.15.0/lib/accumulo-core-1.5.0.jar:/opt/pig-0.15.0/lib/accumulo-fate-1.5.0.jar:/opt/pig-0.15.0/lib/accumulo-server-1.5.0.jar:/opt/pig-0.15.0/lib/accumulo-start-1.5.0.jar:/opt/pig-0.15.0/lib/accumulo-trace-1.5.0.jar:/opt/pig-0.15.0/lib/antlr-runtime-3.4.jar:/opt/pig-0.15.0/lib/asm-3.3.1.jar:/opt/pig-0.15.0/lib/automaton-1.11-8.jar:/opt/pig-0.15.0/lib/avro-1.7.5.jar:/opt/pig-0.15.0/lib/avro-tools-1.7.5-nodeps.jar:/opt/pig-0.15.0/lib/groovy-all-1.8.6.jar:/opt/pig-0.15.0/lib/guava-11.0.jar:/opt/pig-0.15.0/lib/hive-common-0.14.0.jar:/opt/pig-0.15.0/lib/hive-exec-0.14.0-core.jar:/opt/pig-0.15.0/lib/hive-serde-0.14.0.jar:/opt/pig-0.15.0/lib/hive-shims-common-0.14.0.jar:/opt/pig-0.15.0/lib/hive-shims-common-secure-0.14.0.jar:/opt/pig-0.15.0/lib/jackson-core-asl-1.8.8.jar:/opt/pig-0.15.0/lib/jackson-mapper-asl-1.8.8.jar:/opt/pig-0.15.0/lib/jansi-1.9.jar:/opt/pig-0.15.0/lib/jline-1.0.jar:/opt/pig-0.15.0/lib/joda-time-2.5.jar:/opt/pig-0.15.0/lib/jruby-complete-1.6.7.jar:/opt/pig-0.15.0/lib/js-1.7R2.jar:/opt/pig-0.15.0/lib/json-simple-1.1.jar:/opt/pig-0.15.0/lib/jython-standalone-2.5.3.jar:/opt/pig-0.15.0/lib/kryo-2.22.jar:/opt/pig-0.15.0/lib/piggybank.jar:/opt/pig-0.15.0/lib/protobuf-java-2.5.0.jar:/opt/pig-0.15.0/lib/snappy-java-1.1.0.1.jar:/opt/pig-0.15.0/lib/ST4-4.0.4.jar:/opt/pig-0.15.0/lib/trevni-avro-1.7.5.jar:/opt/pig-0.15.0/lib/trevni-core-1.7.5.jar:/opt/pig-0.15.0/lib/zookeeper-3.4.5.jar:/opt/hbase-1.0.1.1/lib/activation-1.1.jar:/opt/hbase-1.0.1.1/lib/aopalliance-1.0.jar:/opt/hbase-1.0.1.1/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hbase-1.0.1.1/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hbase-1.0.1.1/lib/api-asn1-api-1.0.0-M20.jar:/opt/hbase-1.0.1.1/lib/api-util-1.0.0-M20.jar:/opt/hbase-1.0.1.1/lib/asm-3.1.jar:/opt/hbase-1.0.1.1/lib/avro-1.7.4.jar:/opt/hbase-1.0.1.1/lib/commons-beanutils-1.7.0.jar:/opt/hbase-1.0.1.1/lib/commons-beanutils-core-1.8.0.jar:/opt/hbase-1.0.1.1/lib/commons-cli-1.2.jar:/opt/hbase-1.0.1.1/lib/commons-codec-1.9.jar:/opt/hbase-1.0.1.1/lib/commons-collections-3.2.1.jar:/opt/hbase-1.0.1.1/lib/commons-compress-1.4.1.jar:/opt/hbase-1.0.1.1/lib/commons-configuration-1.6.jar:/opt/hbase-1.0.1.1/lib/commons-daemon-1.0.13.jar:/opt/hbase-1.0.1.1/lib/commons-digester-1.8.jar:/opt/hbase-1.0.1.1/lib/commons-el-1.0.jar:/opt/hbase-1.0.1.1/lib/commons-httpclient-3.1.jar:/opt/hbase-1.0.1.1/lib/commons-io-2.4.jar:/opt/hbase-1.0.1.1/lib/commons-lang-2.6.jar:/opt/hbase-1.0.1.1/lib/commons-logging-1.2.jar:/opt/hbase-1.0.1.1/lib/commons-math-2.2.jar:/opt/hbase-1.0.1.1/lib/commons-math3-3.1.1.jar:/opt/hbase-1.0.1.1/lib/commons-net-3.1.jar:/opt/hbase-1.0.1.1/lib/disruptor-3.3.0.jar:/opt/hbase-1.0.1.1/lib/findbugs-annotations-1.3.9-1.jar:/opt/hbase-1.0.1.1/lib/guava-12.0.1.jar:/opt/hbase-1.0.1.1/lib/guice-3.0.jar:/opt/hbase-1.0.1.1/lib/guice-servlet-3.0.jar:/opt/hbase-1.0.1.1/lib/hadoop-annotations-2.5.1.jar:/opt/hbase-1.0.1.1/lib/hadoop-auth-2.5.1.jar:/opt/hbase-1.0.1.1/lib/hadoop-client-2.5.1.jar:/opt/hbase-1.0.1.1/lib/hadoop-common-2.5.1.jar:/opt/hbase-1.0.1.1/lib/hadoop-hdfs-2.5.1.jar:/opt/hbase-1.0.1.1/lib/hadoop-mapreduce-client-app-2.5.1.jar:/opt/hbase-1.0.1.1/lib/hadoop-mapreduce-client-common-2.5.1.jar:/opt/hbase-1.0.1.1/lib/hadoop-mapreduce-client-core-2.5.1.jar:/opt/hbase-1.0.1.1/lib/hadoop-mapreduce-client-jobclient-2.5.1.jar:/opt/hbase-1.0.1.1/lib/hadoop-mapreduce-client-shuffle-2.5.1.jar:/opt/hbase-1.0.1.1/lib/hadoop-yarn-api-2.5.1.jar:/opt/hbase-1.0.1.1/lib/hadoop-yarn-client-2.5.1.jar:/opt/hbase-1.0.1.1/lib/hadoop-yarn-common-2.5.1.jar:/opt/hbase-1.0.1.1/lib/hadoop-yarn-server-common-2.5.1.jar:/opt/hbase-1.0.1.1/lib/hadoop-yarn-server-nodemanager-2.5.1.jar:/opt/hbase-1.0.1.1/lib/hbase-annotations-1.0.1.1.jar:/opt/hbase-1.0.1.1/lib/hbase-annotations-1.0.1.1-tests.jar:/opt/hbase-1.0.1.1/lib/hbase-checkstyle-1.0.1.1.jar:/opt/hbase-1.0.1.1/lib/hbase-client-1.0.1.1.jar:/opt/hbase-1.0.1.1/lib/hbase-common-1.0.1.1.jar:/opt/hbase-1.0.1.1/lib/hbase-common-1.0.1.1-tests.jar:/opt/hbase-1.0.1.1/lib/hbase-examples-1.0.1.1.jar:/opt/hbase-1.0.1.1/lib/hbase-hadoop2-compat-1.0.1.1.jar:/opt/hbase-1.0.1.1/lib/hbase-hadoop-compat-1.0.1.1.jar:/opt/hbase-1.0.1.1/lib/hbase-it-1.0.1.1.jar:/opt/hbase-1.0.1.1/lib/hbase-it-1.0.1.1-tests.jar:/opt/hbase-1.0.1.1/lib/hbase-prefix-tree-1.0.1.1.jar:/opt/hbase-1.0.1.1/lib/hbase-protocol-1.0.1.1.jar:/opt/hbase-1.0.1.1/lib/hbase-rest-1.0.1.1.jar:/opt/hbase-1.0.1.1/lib/hbase-server-1.0.1.1.jar:/opt/hbase-1.0.1.1/lib/hbase-server-1.0.1.1-tests.jar:/opt/hbase-1.0.1.1/lib/hbase-shell-1.0.1.1.jar:/opt/hbase-1.0.1.1/lib/hbase-testing-util-1.0.1.1.jar:/opt/hbase-1.0.1.1/lib/hbase-thrift-1.0.1.1.jar:/opt/hbase-1.0.1.1/lib/htrace-core-3.1.0-incubating.jar:/opt/hbase-1.0.1.1/lib/httpclient-4.2.5.jar:/opt/hbase-1.0.1.1/lib/httpcore-4.1.3.jar:/opt/hbase-1.0.1.1/lib/jackson-core-asl-1.8.8.jar:/opt/hbase-1.0.1.1/lib/jackson-jaxrs-1.8.8.jar:/opt/hbase-1.0.1.1/lib/jackson-mapper-asl-1.8.8.jar:/opt/hbase-1.0.1.1/lib/jackson-xc-1.8.8.jar:/opt/hbase-1.0.1.1/lib/jamon-runtime-2.3.1.jar:/opt/hbase-1.0.1.1/lib/jasper-compiler-5.5.23.jar:/opt/hbase-1.0.1.1/lib/jasper-runtime-5.5.23.jar:/opt/hbase-1.0.1.1/lib/javax.inject-1.jar:/opt/hbase-1.0.1.1/lib/java-xmlbuilder-0.4.jar:/opt/hbase-1.0.1.1/lib/jaxb-api-2.2.2.jar:/opt/hbase-1.0.1.1/lib/jaxb-impl-2.2.3-1.jar:/opt/hbase-1.0.1.1/lib/jcodings-1.0.8.jar:/opt/hbase-1.0.1.1/lib/jersey-client-1.9.jar:/opt/hbase-1.0.1.1/lib/jersey-core-1.9.jar:/opt/hbase-1.0.1.1/lib/jersey-guice-1.9.jar:/opt/hbase-1.0.1.1/lib/jersey-json-1.9.jar:/opt/hbase-1.0.1.1/lib/jersey-server-1.9.jar:/opt/hbase-1.0.1.1/lib/jets3t-0.9.0.jar:/opt/hbase-1.0.1.1/lib/jettison-1.3.3.jar:/opt/hbase-1.0.1.1/lib/jetty-6.1.26.jar:/opt/hbase-1.0.1.1/lib/jetty-sslengine-6.1.26.jar:/opt/hbase-1.0.1.1/lib/jetty-util-6.1.26.jar:/opt/hbase-1.0.1.1/lib/joni-2.1.2.jar:/opt/hbase-1.0.1.1/lib/jruby-complete-1.6.8.jar:/opt/hbase-1.0.1.1/lib/jsch-0.1.42.jar:/opt/hbase-1.0.1.1/lib/jsp-2.1-6.1.14.jar:/opt/hbase-1.0.1.1/lib/jsp-api-2.1-6.1.14.jar:/opt/hbase-1.0.1.1/lib/jsr305-1.3.9.jar:/opt/hbase-1.0.1.1/lib/junit-4.11.jar:/opt/hbase-1.0.1.1/lib/leveldbjni-all-1.8.jar:/opt/hbase-1.0.1.1/lib/libthrift-0.9.0.jar:/opt/hbase-1.0.1.1/lib/log4j-1.2.17.jar:/opt/hbase-1.0.1.1/lib/metrics-core-2.2.0.jar:/opt/hbase-1.0.1.1/lib/netty-3.2.4.Final.jar:/opt/hbase-1.0.1.1/lib/netty-all-4.0.23.Final.jar:/opt/hbase-1.0.1.1/lib/paranamer-2.3.jar:/opt/hbase-1.0.1.1/lib/protobuf-java-2.5.0.jar:/opt/hbase-1.0.1.1/lib/servlet-api-2.5-6.1.14.jar:/opt/hbase-1.0.1.1/lib/servlet-api-2.5.jar:/opt/hbase-1.0.1.1/lib/slf4j-api-1.7.7.jar:/opt/hbase-1.0.1.1/lib/slf4j-log4j12-1.7.7.jar:/opt/hbase-1.0.1.1/lib/snappy-java-1.0.4.1.jar:/opt/hbase-1.0.1.1/lib/xmlenc-0.52.jar:/opt/hbase-1.0.1.1/lib/xz-1.0.jar:/opt/hbase-1.0.1.1/lib/zookeeper-3.4.6.jar:/opt/zookeeper-3.4.6/zookeeper-3.4.6.jar:/opt/pig-0.15.0/pig-0.15.0-core-h2.jar:/opt/pig-0.15.0/lib/h2/avro-mapred-1.7.5-hadoop2.jar:/opt/pig-0.15.0/lib/h2/commons-collections4-4.0.jar:/opt/pig-0.15.0/lib/h2/hbase-client-0.98.12-hadoop2.jar:/opt/pig-0.15.0/lib/h2/hbase-common-0.98.12-hadoop2.jar:/opt/pig-0.15.0/lib/h2/hbase-hadoop2-compat-0.98.12-hadoop2.jar:/opt/pig-0.15.0/lib/h2/hbase-hadoop-compat-0.98.12-hadoop2.jar:/opt/pig-0.15.0/lib/h2/hbase-protocol-0.98.12-hadoop2.jar:/opt/pig-0.15.0/lib/h2/hbase-server-0.98.12-hadoop2.jar:/opt/pig-0.15.0/lib/h2/hive-shims-0.23-0.14.0.jar:/opt/pig-0.15.0/lib/h2/tez-api-0.7.0.jar:/opt/pig-0.15.0/lib/h2/tez-common-0.7.0.jar:/opt/pig-0.15.0/lib/h2/tez-dag-0.7.0.jar:/opt/pig-0.15.0/lib/h2/tez-mapreduce-0.7.0.jar:/opt/pig-0.15.0/lib/h2/tez-runtime-internals-0.7.0.jar:/opt/pig-0.15.0/lib/h2/tez-runtime-library-0.7.0.jar:/opt/pig-0.15.0/lib/h2/tez-yarn-timeline-history-with-acls-0.7.0.jar:
HADOOP_OPTS:
HADOOP_CLIENT_OPTS: -Xmx1000m  -Dpig.log.dir=/opt/pig-0.15.0/logs -Dpig.log.file=pig.log -Dpig.home.dir=/opt/pig-0.15.0 -Dpig.additional.jars.uris=file:///opt/apache-hive-0.14.0-bin/lib/hive-metastore-0.14.0.jar,file:///opt/apache-hive-0.14.0-bin/lib/libthrift-0.9.0.jar,file:///opt/apache-hive-0.14.0-bin/lib/hive-exec-0.14.0.jar,file:///opt/apache-hive-0.14.0-bin/lib/libfb303-0.9.0.jar,file:///opt/apache-hive-0.14.0-bin/lib/jdo-api-3.0.1.jar,file://,file:///opt/apache-hive-0.14.0-bin/lib/hive-hbase-handler-0.14.0.jar,file:///opt/apache-hive-0.14.0-bin/hcatalog/share/hcatalog/hive-hcatalog-core-0.14.0.jar,file://,file:///opt/apache-hive-0.14.0-bin/hcatalog/share/hcatalog/hive-hcatalog-pig-adapter-0.14.0.jar
/opt/hadoop-2.4.1/bin/hadoop jar /opt/pig-0.15.0/pig-0.15.0-core-h2.jar


{quote}

Also I try to find this class org.apache.hadoop.hbase.client.Scan, there is no other jar contains it, So I am not sure why there is such incompatible. From the source code, setCacheBlocks, this method should exist.  

{quote}
[hadoop@hadoop-master-1 opt]$ find . -name ""*.jar"" -exec grep -l org.apache.hadoop.hbase.client.Scan {} \;

./hbase-1.0.1.1/lib/hbase-client-1.0.1.1.jar
./pig-0.15.0/lib/h2/hbase-client-0.98.12-hadoop2.jar
./pig-0.15.0/lib/h1/hbase-client-0.98.12-hadoop1.jar
{quote}

"
HBASE-14182	"I use docker to deploy my hbase cluster, and the RS ip changed. When restart this RS,  hmaster webUI shows it connect to hmaster, but regions num. is zero after a long time. I check the hmaster log and found that master still use old ip to connect this rs.

This is hmaster's log below:
PS: 10.11.21.140 is old ip of  rs dx-ape-regionserver1-online
{code}
2015-08-04 17:24:00,081 INFO  [AM.ZK.Worker-pool2-t14141] master.AssignmentManager: Assigning solar_image,\x01Y\x8E\xA3y,1434968237206.4a1bdeec85b9f55b962596f9fb2cd07f. to dx-ape-regionserver1-online,60020,1438679950072
2015-08-04 17:24:06,800 WARN  [AM.ZK.Worker-pool2-t14133] master.AssignmentManager: Failed assignment of solar_image,\x00\x94\x09\x8D\x95,1430991781025.b0f5b755f443d41cf306026a60675020. to dx-ape-regionserver1-online,60020,1438679950072, trying to assign elsewhere instead; try=3 of 10
java.net.ConnectException: Connection timed out
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
        at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupConnection(RpcClient.java:578)
        at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:868)
        at org.apache.hadoop.hbase.ipc.RpcClient.getConnection(RpcClient.java:1543)
        at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1442)
        at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1661)
        at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1719)
        at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.openRegion(AdminProtos.java:20964)
        at org.apache.hadoop.hbase.master.ServerManager.sendRegionOpen(ServerManager.java:671)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:2097)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1577)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1550)
        at org.apache.hadoop.hbase.master.handler.ClosedRegionHandler.process(ClosedRegionHandler.java:104)
        at org.apache.hadoop.hbase.master.AssignmentManager.handleRegion(AssignmentManager.java:999)
        at org.apache.hadoop.hbase.master.AssignmentManager$6.run(AssignmentManager.java:1447)
        at org.apache.hadoop.hbase.master.AssignmentManager$3.run(AssignmentManager.java:1260)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
2015-08-04 17:24:06,801 WARN  [AM.ZK.Worker-pool2-t14140] master.AssignmentManager: Failed assignment of solar_image,\x00(.\xE7\xB1L,1430024620929.534025fcf4cae5516513b9c9a4cf73dc. to dx-ape-regionserver1-online,60020,1438679950072, trying to assign elsewhere instead; try=2 of 10
java.net.ConnectException: Call to dx-ape-regionserver1-online/10.11.21.140:60020 failed on connection exception: java.net.ConnectException: Connection timed out
        at org.apache.hadoop.hbase.ipc.RpcClient.wrapException(RpcClient.java:1483)
        at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1461)
        at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1661)
        at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1719)
        at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.openRegion(AdminProtos.java:20964)
        at org.apache.hadoop.hbase.master.ServerManager.sendRegionOpen(ServerManager.java:671)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:2097)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1577)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1550)
        at org.apache.hadoop.hbase.master.handler.ClosedRegionHandler.process(ClosedRegionHandler.java:104)
        at org.apache.hadoop.hbase.master.AssignmentManager.handleRegion(AssignmentManager.java:999)
        at org.apache.hadoop.hbase.master.AssignmentManager$6.run(AssignmentManager.java:1447)
        at org.apache.hadoop.hbase.master.AssignmentManager$3.run(AssignmentManager.java:1260)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection timed out
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
        at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupConnection(RpcClient.java:578)
        at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:868)
        at org.apache.hadoop.hbase.ipc.RpcClient.getConnection(RpcClient.java:1543)
        at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1442)
        ... 16 more
{code}
"
HBASE-14074	"I found hbase clutser crashed on-the-hour
HBase master running log as follows

""2015-07-14 14:41:49,832 DEBUG [master:10.240.131.18:60000.oldLogCleaner] master.ReplicationLogCleaner: Didn't find this log in ZK, deleting: 10-241-125-46%2C60020%2C1436841063572.1436851865226
2015-07-14 14:45:49,822 DEBUG [master:10.240.131.18:60000.oldLogCleaner] master.ReplicationLogCleaner: Didn't find this log in ZK, deleting: 10-241-85-137%2C60020%2C1436841341086.1436852143141
2015-07-14 15:00:03,481 INFO  [main] util.VersionInfo: HBase 0.96.2-hadoop2
2015-07-14 15:00:03,481 INFO  [main] util.VersionInfo: Subversion https://svn.apache.org/repos/asf/hbase/tags/0.96.2RC2 -r 1581096
2015-07-14 15:00:03,481 INFO  [main] util.VersionInfo: Compiled by stack on Mon Mar 24 16:03:18 PDT 2014
2015-07-14 15:00:03,729 INFO  [main] zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.5-1392090, built on 09/30/2012 17:52 GMT
2015-07-14 15:00:03,730 INFO  [main] zookeeper.ZooKeeper: Client environment:host.name=10-240-131-18
2015-07-14 15:00:03,730 INFO  [main] zookeeper.ZooKeeper: Client environment:java.version=1.7.0_72

...

2015-07-14 15:00:03,749 INFO  [main] zookeeper.RecoverableZooKeeper: Process identifier=clean znode for master connecting to ZooKeeper ensemble=10.240.131.17:2200,10.240.131.16:2200,10.240.131.15:2200,10.240.131.14:2200,10.240.131.18:2200
2015-07-14 15:00:03,751 INFO  [main-SendThread(10-240-131-18:2200)] zookeeper.ClientCnxn: Opening socket connection to server 10-240-131-18/10.240.131.18:2200. Will not attempt to authenticate using SASL (unknown error)
2015-07-14 15:00:03,757 INFO  [main-SendThread(10-240-131-18:2200)] zookeeper.ClientCnxn: Socket connection established to 10-240-131-18/10.240.131.18:2200, initiating session
2015-07-14 15:00:03,764 INFO  [main-SendThread(10-240-131-18:2200)] zookeeper.ClientCnxn: Session establishment complete on server 10-240-131-18/10.240.131.18:2200, sessionid = 0x34e8a64b453024a, negotiated timeout = 40000
2015-07-14 15:00:04,835 INFO  [main] zookeeper.ZooKeeper: Session: 0x34e8a64b453024a closed
2015-07-14 15:00:04,835 INFO  [main-EventThread] zookeeper.ClientCnxn: EventThread shut down""


After print "" Didn't find this log in ZK..."" every hour at a time
The master dead


Zookeeper  running log as follows

""2015-07-14 15:00:03,756 [myid:3] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2200:NIOServerCnxnFactory@197] - Accepted socket connection from /10.240.131.18:52733
2015-07-14 15:00:03,761 [myid:3] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2200:ZooKeeperServer@868] - Client attempting to establish new session at /10.240.131.18:52733
2015-07-14 15:00:03,762 [myid:3] - INFO  [CommitProcessor:3:ZooKeeperServer@617] - Established session 0x34e8a64b453024a with negotiated timeout 40000 for client /10.240.131.18:52733
2015-07-14 15:00:04,836 [myid:3] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2200:NIOServerCnxn@1007] - Closed socket connection for client /10.240.131.18:52733 which had sessionid 0x34e8a64b453024a""
"
HBASE-13724	"A little background, 

We run our server in -ea mode and have seen quite a few replication sources silently die over the past few months.

Note: the stacktrace I posted below comes from a regionserver running 0.94 but quickly looking at this issue, I believe this will happen in 98 too.  

Should we harden replication source to deal with these types of assertion errors by catching throwables, should we be dealing with this at the sequence file reader level?  Still looking into the root cause of this issue but when manually shutdown our regionservers the regionserver that recovered its queue replicated that log just fine.  So in our case a simple retry would've worked just fine.  

{code}
2015-05-08 11:04:23,348 ERROR org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Unexpected exception in ReplicationSource, currentPath=hdfs://hm6.xxx.flurry.com:9000/hbase/.logs/xxxxx.yy.flurry.com,60020,1426792702998/xxxxx.atl.flurry.com%2C60020%2C1426792702998.1431107922449
java.lang.AssertionError
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader$WALReader$WALReaderFSDataInputStream.getPos(SequenceFileLogReader.java:121)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1489)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1479)
        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1474)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader$WALReader.<init>(SequenceFileLogReader.java:55)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.init(SequenceFileLogReader.java:178)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.getReader(HLog.java:734)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationHLogReaderManager.openReader(ReplicationHLogReaderManager.java:69)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.openReader(ReplicationSource.java:583)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:373)
{code}"
HBASE-14056	"This afternoon,HBase access failure锛寀se command  'hbase hbck' Check data inconsistencies銆?Hmaster on the log is as follows锛?2015-07-10 16:24:41,285 INFO  [FifoRpcScheduler.handler1-thread-43] master.HMaster: Client=hadoop//192.168.28.12 assign dama:lowprice_intl_roundtrip_20150311,002687951001426005490000,1426090250918.01b4e069174f6bb786b4e0632f4e4740.
2015-07-10 16:24:41,285 DEBUG [FifoRpcScheduler.handler1-thread-43] master.AssignmentManager: Force region state offline {01b4e069174f6bb786b4e0632f4e4740 state=OPEN, ts=1436433547853, server=l-hbase1.dba.cn1.qunar.com,60020,1436433434134}
2015-07-10 16:24:56,285 INFO  [FifoRpcScheduler.handler1-thread-43] master.AssignmentManager: Server l-hbase1.dba.cn1.qunar.com,60020,1436433434134 returned java.net.ConnectException: Connection timed out for dama:lowprice_intl_roundtrip_20150311,002687951001426005490000,1426090250918.01b4e069174f6bb786b4e0632f4e4740., try=1 of 10
java.net.ConnectException: Connection timed out
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)
        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
        at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupConnection(RpcClient.java:578)
        at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:868)
        at org.apache.hadoop.hbase.ipc.RpcClient.getConnection(RpcClient.java:1543)
        at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1442)
        at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1661)
        at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1719)
        at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
        at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1717)
        at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:730)
        at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1705)
        at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1822)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1453)
        at org.apache.hadoop.hbase.master.HMaster.assignRegion(HMaster.java:2500)
        at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java:42259)
        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2031)
        at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:108)
        at org.apache.hadoop.hbase.ipc.FifoRpcScheduler$1.run(FifoRpcScheduler.java:74)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)

Regionserver on the log is as follows锛?2015-07-10 16:34:35,672 WARN  [B.defaultRpcServer.handler=18,queue=0,port=60020] regionserver.HRegion: Failed getting lock in batch put, row=LEDHKTABPNCB4RU004001150711999999
org.apache.hadoop.hbase.regionserver.WrongRegionException: Requested row out of range for row lock on HRegion atpco:atp_fare,M,1435898852855.e2f9f303a434f3f27dfc839c01c3d9eb., startKey='M', getEndKey()='N', row='LEDHKTABPNCB4RU004001150711999999'
        at org.apache.hadoop.hbase.regionserver.HRegion.checkRow(HRegion.java:3456)
        at org.apache.hadoop.hbase.regionserver.HRegion.getRowLock(HRegion.java:3474)
        at org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchMutation(HRegion.java:2394)
        at org.apache.hadoop.hbase.regionserver.HRegion.batchMutate(HRegion.java:2261)
        at org.apache.hadoop.hbase.regionserver.HRegion.batchMutate(HRegion.java:2213)
        at org.apache.hadoop.hbase.regionserver.HRegion.batchMutate(HRegion.java:2217)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.doBatchOp(HRegionServer.java:4386)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.doNonAtomicRegionMutation(HRegionServer.java:3588)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.java:3477)
        at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:29593)
        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2031)
        at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:108)
        at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:114)
        at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:94)
        at java.lang.Thread.run(Thread.java:744)

What's the problem锛?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2015-07-10 16:56:20.457,,false,,,,,,,,,,,,,9223372036854775807,,,,,Fri Jul 10 16:56:20 UTC 2015,,,,,,0|i2h3cf:,9223372036854775807,,,,,,,,10/Jul/15 16:56;stack;Please do not file an issue and ask a question on the mailing list. Resolving as invalid.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBase Srores NULL Value from delimited File Input,HBASE-14023,12842653,Bug,Resolved,HBASE,HBase,software,stack,Apache HBase is an open-source"
HBASE-6578	"Right now, because sync-on-block-close is enabled, HLog causes the disk to stall out on large writes (esp when we cross block boundary).

We currently use 256MB blocks. The idea is that if we use smaller block sizes, we should be able to spray the data across more disks (because of round robin scheduling) and this would cause more uniform disk usage."
HBASE-13962	"hi every body
my table has some cell that load with bulk load scenario and some cells for increment.
we use 2 job to load data into table, first job use increment in reduce site and second job use bulk load.
first we run increment job, next run bulk job and run completebulkload job, after that we got this exception:
2015-06-24 17:40:01,557 INFO  [regionserver60020-smallCompactions-1434448531302] regionserver.HRegion: Starting compaction on c2 in region table1,\x04C#P1""\x07\x94 ,1435065082383.0fe38a6c782600e4d46f1f148144b489.
2015-06-24 17:40:01,558 INFO  [regionserver60020-smallCompactions-1434448531302] regionserver.HStore: Starting compaction of 3 file(s) in c2 of table1,\x04C#P1""\x07\x94 ,1435065082383.0fe38a6c782600e4d46f1f148144b489. into tmpdir=hdfs://m2/hbase2/data/default/table1/0fe38a6c782600e4d46f1f148144b489/.tmp, totalSize=43.1m
2015-06-24 17:40:01,558 DEBUG [regionserver60020-smallCompactions-1434448531302] regionserver.StoreFileInfo: reference 'hdfs://m2/hbase2/data/default/table1/0fe38a6c782600e4d46f1f148144b489/c2/6b1249a3b474474db5cf6c664f2d98dc.d21f8ee8b3c915fd9e1c143a0f1892e5' to region=d21f8ee8b3c915fd9e1c143a0f1892e5 hfile=6b1249a3b474474db5cf6c664f2d98dc
2015-06-24 17:40:01,558 DEBUG [regionserver60020-smallCompactions-1434448531302] compactions.Compactor: Compacting hdfs://m2/hbase2/data/default/table1/0fe38a6c782600e4d46f1f148144b489/c2/6b1249a3b474474db5cf6c664f2d98dc.d21f8ee8b3c915fd9e1c143a0f1892e5-hdfs://m2/hbase2/data/default/table1/d21f8ee8b3c915fd9e1c143a0f1892e5/c2/6b1249a3b474474db5cf6c664f2d98dc-top, keycount=575485, bloomtype=ROW, size=20.8m, encoding=NONE, seqNum=9, earliestPutTs=1434875448405
2015-06-24 17:40:01,558 DEBUG [regionserver60020-smallCompactions-1434448531302] compactions.Compactor: Compacting hdfs://m2/hbase2/data/default/table1/0fe38a6c782600e4d46f1f148144b489/c2/41e13b20ee79435ebc260d11d3bf9920_SeqId_11_, keycount=562988, bloomtype=ROW, size=10.1m, encoding=NONE, seqNum=11, earliestPutTs=1435076732205
2015-06-24 17:40:01,558 DEBUG [regionserver60020-smallCompactions-1434448531302] compactions.Compactor: Compacting hdfs://m2/hbase2/data/default/table1/0fe38a6c782600e4d46f1f148144b489/c2/565c45ff05b14a419978834c86defa1a_SeqId_12_, keycount=554577, bloomtype=ROW, size=12.2m, encoding=NONE, seqNum=12, earliestPutTs=1435136926850
2015-06-24 17:40:01,560 ERROR [regionserver60020-smallCompactions-1434448531302] regionserver.CompactSplitThread: Compaction failed Request = regionName=table1,\x04C#P1""\x07\x94 ,1435065082383.0fe38a6c782600e4d46f1f148144b489., storeName=c2, fileCount=3, fileSize=43.1m (20.8m, 10.1m, 12.2m), priority=1, time=6077271921381072
java.io.IOException: Could not seek StoreFileScanner[org.apache.hadoop.hbase.io.HalfStoreFileReader$1@1d1eb574, cur=null] to key /c2:/LATEST_TIMESTAMP/DeleteFamily/vlen=0/mvcc=0
        at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seek(StoreFileScanner.java:164)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.seekScanners(StoreScanner.java:329)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.<init>(StoreScanner.java:252)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.<init>(StoreScanner.java:214)
        at org.apache.hadoop.hbase.regionserver.compactions.Compactor.createScanner(Compactor.java:299)
        at org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.compact(DefaultCompactor.java:87)
        at org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$DefaultCompactionContext.compact(DefaultStoreEngine.java:112)
        at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:1113)
        at org.apache.hadoop.hbase.regionserver.HRegion.compact(HRegion.java:1519)
        at org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner.run(CompactSplitThread.java:498)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to read compressed block at 10930320, onDiskSizeWithoutHeader=22342, preReadHeaderSize=33, header.length=33, header bytes: \x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00
        at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderV2.readBlockDataInternal(HFileBlock.java:1549)
        at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderV2.readBlockData(HFileBlock.java:1413)
        at org.apache.hadoop.hbase.io.hfile.HFileReaderV2.readBlock(HFileReaderV2.java:394)
        at org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader.loadDataBlockWithScanInfo(HFileBlockIndex.java:253)
        at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.seekTo(HFileReaderV2.java:539)
        at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.seekTo(HFileReaderV2.java:560)
        at org.apache.hadoop.hbase.io.hfile.AbstractHFileReader$Scanner.seekTo(AbstractHFileReader.java:308)
        at org.apache.hadoop.hbase.io.HalfStoreFileReader$1.seekTo(HalfStoreFileReader.java:205)
        at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seekAtOrAfter(StoreFileScanner.java:244)
        at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seek(StoreFileScanner.java:152)
        ... 12 more
Caused by: java.io.IOException: Invalid HFile block magic: \x00\x00\x00\x00\x00\x00\x00\x00
        at org.apache.hadoop.hbase.io.hfile.BlockType.parse(BlockType.java:154)
        at org.apache.hadoop.hbase.io.hfile.BlockType.read(BlockType.java:165)
        at org.apache.hadoop.hbase.io.hfile.HFileBlock.<init>(HFileBlock.java:252)
        at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderV2.readBlockDataInternal(HFileBlock.java:1546)
        ... 21 more"
HBASE-13818	"manual region split from HBase shell, I found that split command acts incorrectly with hex split keys

hbase(main):001:0> split 'sdb,\x00\x00+Ug\xD60\x00\x00\x01\x00\x10\xC0,1432909366893.6b601fa4eb9e1244d049bde93e340736.'
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/data/xiaoju/hbase-0.96.2-hadoop2/lib/phoenix-4.1.0-client-hadoop2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/data/xiaoju/hbase-0.96.2-hadoop2/lib/slf4j-log4j12-1.6.4.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/data/xiaoju/hadoop-2.2.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2015-06-01 11:40:46,986 WARN  [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable

ERROR: Illegal character code:44, <,> at 3. User-space table qualifiers can only contain 'alphanumeric characters': i.e. [a-zA-Z_0-9-.]: sdb,""\x00\x00+Ug\xD60\x00\x00\x01\x00\x10\xC0"",1432909366893.6b601fa4eb9e1244d049bde93e340736.

Here is some help for this command:
Split entire table or pass a region to split individual region.  With the 
second parameter, you can specify an explicit split key for the region.  
Examples:
    split 'tableName'
    split 'namespace:tableName'
    split 'regionName' # format: 'tableName,startKey,id'
    split 'tableName', 'splitKey'
    split 'regionName', 'splitKey'"
HBASE-13808	"Found a few NPEs in unit tests.
org.apache.phoenix.end2end.RegexpSplitFunctionIT Time elapsed: 0.002 sec <<< ERROR!
java.lang.NullPointerException: null
at org.apache.phoenix.query.BaseTest.disableAndDropNonSystemTables(BaseTest.java:1629)
at org.apache.phoenix.query.BaseTest.dropNonSystemTables(BaseTest.java:519)
at org.apache.phoenix.end2end.BaseHBaseManagedTimeIT.doTeardown(BaseHBaseManagedTimeIT.java:59)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:601)
at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
at org.junit.runners.Suite.runChild(Suite.java:128)
at org.junit.runners.Suite.runChild(Suite.java:27)
at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:107)
at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeLazy(JUnitCoreWrapper.java:88)
at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:57)
at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144)
at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203)
at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155)
at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)"
HBASE-13705	"I've found the issue during code review, so I don't have tests and I even didn't test this case manualy. So I'll try to describe it in words.
Pre-condition: we're using scan with MultiRowRangeFilter with some RowRange's with startRowInclusive = false. This means that we want to include all rows that are strictly greater than startRow (and less then stopRow, but it doesn't matter for now). 
What happens in MultiRowRangeFilter.filterRowKey (worth case is described):
1. Line 91: Check if current range contains a row. Lets follow the case when it doesn't.
2. Line 94: Search for the next RowRange in method getNextRangeIndex.
3. Line 238: We've found a RowRange, check if startRowInclusive == false and set EXCLUSIVE = true. This variable indicates if next row should be excluded.
4. Line 105: Check if EXCLUSIVE == true, if so skip this row.
The problem: we've skipped first row we got in this range, but we never checked if this row is a RowRange.startRow . In distributed system may not get RowRange.startRow on current instance, so we may exclude some another row. Moreover, we may not have RowRange.startRow at all in the DB, we will exclude some rows that are (possible) close to RowRange.startRow, but not equals to it."
HBASE-13678	"When batch get some data from cluster, the client will block some threads base on region server amount

In code client use Future.get block a thread for each related region servers and wait them finish one by one.

In our case we have 12 region servers, so most of time the client will cost 12 threads for a batch get, if we have many concurrent request it will OOM by can not create native threads.

I suggest we use some kind async or call back to process region server's response."
HBASE-13189	PrefixTreeCell should implement HeapSize. The CellUtil.estimatedHeapSize getting used in scanner estimates.  With PrefixTreecell not implementing HeapSize we may get an incorrect result.  
HBASE-6148	"Recently RegionServer allocates very large objects when reading some corrupted RPC calls, which may caused by client-server version incompatibility. We need to add a protection before allocating the objects. 

Apache trunk won't suffer from this problem since it had moved to the versioned invocation."
HBASE-9683	Port HTrace (HBASE-6524) to 0.94. This patch includes a wire-format change on writable RPC. It serializes the traceID and spanID in class Invocation. This should be compatible with old versions.
HBASE-8720	"{code}
    SnapshotSubprocedurePool(String name, Configuration conf) {
      // configure the executor service
      long keepAlive = conf.getLong(
        RegionServerSnapshotManager.SNAPSHOT_TIMEOUT_MILLIS_KEY,
        RegionServerSnapshotManager.SNAPSHOT_TIMEOUT_MILLIS_DEFAULT);
      int threads = conf.getInt(CONCURENT_SNAPSHOT_TASKS_KEY, DEFAULT_CONCURRENT_SNAPSHOT_TASKS);
      this.name = name;
      executor = new ThreadPoolExecutor(1, threads, keepAlive, TimeUnit.MILLISECONDS,
          new LinkedBlockingQueue<Runnable>(), new DaemonThreadFactory(""rs(""
              + name + "")-snapshot-pool""));
      taskPool = new ExecutorCompletionService<Void>(executor);
    }
{code}
ThreadPoolExecutor锛?
corePoolSize锛?
maximumPoolSize锛?
workQueue锛歀inkedBlockingQueue锛寀nlimited
so when a new task submit to the ThreadPoolExecutor, if there is a task is running, the new task is queued in the queue, so all snapshot region tasks execute one by one."
HBASE-3577	"The current thrift interface has the getTableRegions() interface like below.

{code}
  list<TRegionInfo> getTableRegions(
    /** table name */
    1:Text tableName)
    throws (1:IOError io)
{code}
{code}
struct TRegionInfo {
  1:Text startKey,
  2:Text endKey,
  3:i64 id,
  4:Text name,
  5:byte version
}
{code}

But the method don't have the region location information (where the region is located).

I want to add the Thrift interfaces like below in HTable.java.

{code}
public Map<HRegionInfo, HServerAddress> getRegionsInfo() throws IOException
{code}
{code}
public HRegionLocation getRegionLocation(final String row)
{code}"
HBASE-4337	"In HADOOP-6255, a proposal was made for common directory layout for Hadoop ecosystem.  This jira is to track the necessary work for making HBase directory structure aligned with Hadoop for better integration."
HBASE-4635	"Comment from HBASE-3606:

Eric, it looks like hbase rpm spec file sets dependency on jdk. Can we remove the jdk dependency ? As everyone will not be installing jdk through rpm.

There are multiple ways to install Java on Linux.  It would be better to remove Java dependency declaration for packaging.
"
HBASE-6480	"Current if the callQueueSize exceed maxQueueSize, all call will be rejected, Should we let the priority Call pass through?

Current:
{code}
if ((callSize + callQueueSize.get()) > maxQueueSize) {
  Call callTooBig = xxx
  return ;
}
if (priorityCallQueue != null && getQosLevel(param) > highPriorityLevel) {
  priorityCallQueue.put(call);
  updateCallQueueLenMetrics(priorityCallQueue);
} else {
  callQueue.put(call);              // queue the call; maybe blocked here
  updateCallQueueLenMetrics(callQueue);
}
{code}
Should we change it to :
{code}
if (priorityCallQueue != null && getQosLevel(param) > highPriorityLevel) {
  priorityCallQueue.put(call);
  updateCallQueueLenMetrics(priorityCallQueue);
} else {
  if ((callSize + callQueueSize.get()) > maxQueueSize) {
   Call callTooBig = xxx
   return ;
  }
  callQueue.put(call);              // queue the call; maybe blocked here
  updateCallQueueLenMetrics(callQueue);
}
{code}"
HBASE-13474	"during selecting Hfiles for the Stripe Compaction,
If includeL0==true, int minFiles set to the number of allFiles in the stripe.
It make compaction for All of files in the stripe 
or No compaction at all (which is the problem). 

the Stripe compaction uses exploring compaction inside.
some of HFiles in the stripe is too big, these all files are gonna never pass the ratio check
and compaction will be cancelled 
next time the compaction is occurred, includeL0 will be true again. 
so compactions (even minor compactions) will not happen almost forever.
Flushing makes more small HFiles and no compaction happening,
so numerous tiny HFiles are gonna file up in the stripe, and it鈥檚 going to be a problem.

there is no such thing as major compaction in the stripe compaction.
But we need to compact every file of the stripe to drop deletes at some point.

IMHO  there is not one stripe in the region, there are many.
when includeL0==true, compact all HFiles in the stripe without selecting is reasonable."
HBASE-13461	"I try to dump  thread stack below:

""RpcServer.handler=63,port=60020"" daemon prio=10 tid=0x00007fdcddc5d000 nid=0x5f9 waiting for monitor entry [0x00007fd289194000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:98)
        - waiting to lock <0x00007fd36c023728> (a org.apache.hadoop.hdfs.DFSOutputStream)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:59)
        at java.io.DataOutputStream.write(DataOutputStream.java:90)
        - locked <0x00007fd510cfdc28> (a org.apache.hadoop.hdfs.client.HdfsDataOutputStream)
        at com.google.protobuf.CodedOutputStream.refreshBuffer(CodedOutputStream.java:833)
        at com.google.protobuf.CodedOutputStream.flush(CodedOutputStream.java:843)
        at com.google.protobuf.AbstractMessageLite.writeDelimitedTo(AbstractMessageLite.java:91)
        at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.append(ProtobufLogWriter.java:87)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog$LogSyncer.hlogFlush(FSHLog.java:1026)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.syncer(FSHLog.java:1075)
        - locked <0x00007fd2d9bbfad0> (a java.lang.Object)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.sync(FSHLog.java:1240)
        at org.apache.hadoop.hbase.regionserver.HRegion.syncOrDefer(HRegion.java:5593)
        at org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchMutation(HRegion.java:2315)
        at org.apache.hadoop.hbase.regionserver.HRegion.batchMutate(HRegion.java:2028)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.doBatchOp(HRegionServer.java:4094)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.doNonAtomicRegionMutation(HRegionServer.java:3380)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.java:3284)
        at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:26935)
        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2185)
        at org.apache.hadoop.hbase.ipc.RpcServer$Handler.run(RpcServer.java:1889)

""RpcServer.handler=12,port=60020"" daemon prio=10 tid=0x00007fdcddf2c800 nid=0x5c6 in Object.wait() [0x00007fd28c4c7000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.hdfs.DFSOutputStream.waitForAckedSeqno(DFSOutputStream.java:1803)
        - locked <0x00007fd45857c540> (a java.util.LinkedList)
        at org.apache.hadoop.hdfs.DFSOutputStream.flushOrSync(DFSOutputStream.java:1697)
        at org.apache.hadoop.hdfs.DFSOutputStream.hflush(DFSOutputStream.java:1590)
        at org.apache.hadoop.hdfs.DFSOutputStream.sync(DFSOutputStream.java:1575)
        at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:121)
        at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:135)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.syncer(FSHLog.java:1098)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.sync(FSHLog.java:1240)
        at org.apache.hadoop.hbase.regionserver.HRegion.syncOrDefer(HRegion.java:5593)
        at org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchMutation(HRegion.java:2315)
        at org.apache.hadoop.hbase.regionserver.HRegion.batchMutate(HRegion.java:2028)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.doBatchOp(HRegionServer.java:4094)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.doNonAtomicRegionMutation(HRegionServer.java:3380)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.java:3284)
        at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:26935)
        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2185)
        at org.apache.hadoop.hbase.ipc.RpcServer$Handler.run(RpcServer.java:1889)

""RpcServer.handler=11,port=60020"" daemon prio=10 tid=0x00007fdcdd9e1000 nid=0x5c5 in Object.wait() [0x00007fd28c5c8000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.hdfs.DFSOutputStream.waitForAckedSeqno(DFSOutputStream.java:1803)
        - locked <0x00007fd45857c540> (a java.util.LinkedList)
        at org.apache.hadoop.hdfs.DFSOutputStream.flushOrSync(DFSOutputStream.java:1697)
        at org.apache.hadoop.hdfs.DFSOutputStream.hflush(DFSOutputStream.java:1590)
        at org.apache.hadoop.hdfs.DFSOutputStream.sync(DFSOutputStream.java:1575)
        at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:121)
        at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:135)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.syncer(FSHLog.java:1098)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.sync(FSHLog.java:1240)
        at org.apache.hadoop.hbase.regionserver.HRegion.syncOrDefer(HRegion.java:5593)
        at org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchMutation(HRegion.java:2315)
        at org.apache.hadoop.hbase.regionserver.HRegion.batchMutate(HRegion.java:2028)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.doBatchOp(HRegionServer.java:4094)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.doNonAtomicRegionMutation(HRegionServer.java:3380)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.java:3284)
        at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:26935)
        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2185)
        at org.apache.hadoop.hbase.ipc.RpcServer$Handler.run(RpcServer.java:1889)


""RpcServer.handler=95,port=60020"" daemon prio=10 tid=0x00007fdcdc50b800 nid=0x619 in Object.wait() [0x00007fd287174000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:485)
        at org.apache.hadoop.hdfs.DFSOutputStream.waitAndQueueCurrentPacket(DFSOutputStream.java:1475)
        - locked <0x00007fd45857c540> (a java.util.LinkedList)
        at org.apache.hadoop.hdfs.DFSOutputStream.flushOrSync(DFSOutputStream.java:1688)
        - locked <0x00007fd36c023728> (a org.apache.hadoop.hdfs.DFSOutputStream)
        at org.apache.hadoop.hdfs.DFSOutputStream.hflush(DFSOutputStream.java:1590)
        at org.apache.hadoop.hdfs.DFSOutputStream.sync(DFSOutputStream.java:1575)
        at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:121)
        at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:135)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.syncer(FSHLog.java:1098)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.sync(FSHLog.java:1240)
        at org.apache.hadoop.hbase.regionserver.HRegion.syncOrDefer(HRegion.java:5593)
        at org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchMutation(HRegion.java:2315)
        at org.apache.hadoop.hbase.regionserver.HRegion.batchMutate(HRegion.java:2028)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.doBatchOp(HRegionServer.java:4094)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.doNonAtomicRegionMutation(HRegionServer.java:3380)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.java:3284)
        at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:26935)
        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2185)
        at org.apache.hadoop.hbase.ipc.RpcServer$Handler.run(RpcServer.java:1889)




""DataStreamer for file /hbase/WALs/hdp910.qq.diditaxi.com,60020,1428079391214/hdp910.qq.diditaxi.com%2C60020%2C1428079391214.1428919313998 block BP-1892361854-10.231.149.77-1397112594861:blk_1320569732_10997627
53479"" daemon prio=10 tid=0x00007fdccda8f800 nid=0x4c34 in Object.wait() [0x00007fd23646c000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:491)
        - locked <0x00007fd45857c540> (a java.util.LinkedList)"
HBASE-4630	"I've been doing some isolated benchmarking of a single RS and can repeatedly trigger some craziness in the master if I shutdown the RS.  It is never able to recover after bringing RSs back online.  I seem to see different behavior across different branches / revisions of the 92 branch, but there does seem to be an issue in several of them.

Putting against 0.92.1 so we don't hold up the release of 0.92.  Should not be a blocker.

Working on a unit test now."
HBASE-5265	"The 'revoke' shell command needs to be reworked for the AccessControlProtocol implementation that was finalized for 0.92. The permissions being removed must exactly match what was previously granted. No wildcard matching is done server side.

Allow two forms of the command in the shell for convenience:

Revocation of a specific grant:
{code}
revoke <user>, <table>, <column family> [ , <column_qualifier> ]
{code}

Have the shell automatically do so for all permissions on a table for a given user:
{code}
revoke <user>, <table>
{code}
"
HBASE-5905	"After a short discussion with Stack, I create a jira.
--
I'am a little bit confused by the protobuf interface for closeRegion.

We have two types of closeRegion today:
1) the external ones; available in client.HBaseAdmin. They take the server and the region identifier as a parameter and nothing else.
2) The internal ones, called for example by the master. They have more parameters (like versionOfClosingNode or transitionInZK).

When I look at protobuf.ProtobufUtil, I see:

  public static void closeRegion(final AdminProtocol admin,
      final byte[] regionName, final boolean transitionInZK) throws IOException {
    CloseRegionRequest closeRegionRequest =
      RequestConverter.buildCloseRegionRequest(regionName, transitionInZK);
    try {
      admin.closeRegion(null, closeRegionRequest);
    } catch (ServiceException se) {
      throw getRemoteException(se);
    }
  }


In other words, it seems that we merged the two interfaces into a single one. Is that the intend?
I checked, the internal fields in closeRegionRequest are all optional (that's good). Still, it means that the end user could use them or at least would need to distinguish between the ""optional for functional reasons"" and the ""optional - do not use"".
"
HBASE-5846	"Here is how I executed rpm build: 
{noformat}
MAVEN_OPTS=""-Xmx2g"" mvn clean package assembly:single -Prpm -DskipTests
{noformat}

The issues with the rpm build are: 
* There is no clean (%clean) section in the hbase.spec file . Last run can leave stuff in RPM_BUILD_ROOT which in turn will fail build. As a fix I added 'rm -rf $RPM_BUILD_ROOT' to %clean section

* The Buildroot is set to _build_dir . The build fails with this error. 
{noformat}
cp: cannot copy a directory, `/data/9adda425-1f1e-4fe5-8a53-83bd2ce5ad45/app/jenkins/workspace/hbase.92/target/rpm/hbase/BUILD', into itself, `/data/9adda425-1f1e-4fe5-8a53-83bd2ce5ad45/app/jenkins/workspace/hbase.92/target/rpm/hbase/BUILD/BUILD'
{noformat}
If we set it to ' %{_tmppath}/%{name}-%{version}-root' build passes

* The src/packages/update-hbase-env.sh script will leave inconsistent state if 'yum update hbase' is executed. It deletes data from /etc/init.d/hbase* and does not put scripts back during update. 


"
HBASE-5832	"From IRC this morning:

{code}
07:26 < ntelford> is there a way to monitor the number and size of
store files *per region*?
07:26 < ntelford> I know region servers expose a metric on the total
across all regions, but that's fairly unhelpful to us
...
08:11 < St^Ack> ntelford: no. number is easy but when you say size,
you mean size of all the storefiles in the region, not the size per
storefile (asking because one of the lads is exposing per region
metrics at mo and those would be easy to add)
08:12 < ntelford> St^Ack, for size we're actually interested in the
individual store file size
08:13 < St^Ack> ntelford: how would we do that in metric?  metric
would be dynamic
08:13 < ntelford> specifically, we want to monitor: the maximum,
minimum, mean (and some percentiles) number of store files within a
region
...
08:13 < ntelford> and the maximum, minimum, mean (+ percentiles) size
of individual store files
08:13 < ntelford> :)
08:13 < St^Ack> now you are verging on abuse!
...
{code}"
HBASE-4835	"Mikhail reported this from a five-node, three-RS cluster test:

{code}
2011-11-21 01:30:15,188 FATAL org.apache.hadoop.hbase.regionserver.HRegionServer: ABORTING region server <machine_name>,60020,1321867814890: Initialization of RS failed. Hence aborting RS.
java.util.ConcurrentModificationException
at java.util.Hashtable$Enumerator.next(Hashtable.java:1031)
at org.apache.hadoop.conf.Configuration.iterator(Configuration.java:1042)
at org.apache.hadoop.hbase.zookeeper.ZKConfig.makeZKProps(ZKConfig.java:75)
at org.apache.hadoop.hbase.zookeeper.ZKConfig.getZKQuorumServersString(ZKConfig.java:245)
at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.<init>(ZooKeeperWatcher.java:144)
at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.<init>(ZooKeeperWatcher.java:124)
at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getZooKeeperWatcher(HConnectionManager.java:1262)
at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.setupZookeeperTrackers(HConnectionManager.java:568)
at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.<init>(HConnectionManager.java:559)
at org.apache.hadoop.hbase.client.HConnectionManager.getConnection(HConnectionManager.java:183)
at org.apache.hadoop.hbase.catalog.CatalogTracker.<init>(CatalogTracker.java:177)
at org.apache.hadoop.hbase.regionserver.HRegionServer.initializeZooKeeper(HRegionServer.java:575)
at org.apache.hadoop.hbase.regionserver.HRegionServer.preRegistrationInitialization(HRegionServer.java:534)
at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:642)
at java.lang.Thread.run(Thread.java:619)
{code}"
HBASE-5543	"On the user list someone wrote in with a connection failure due to a long running coprocessor:
{quote}
On Wed, Mar 7, 2012 at 10:59 PM, raghavendhra rahul wrote:
2012-03-08 12:03:09,475 WARN org.apache.hadoop.ipc.HBaseServer: IPC Server Responder, call execCoprocessor([B@50cb21, getProjection(), rpc version=1, client version=0, methodsFingerPrint=0), rpc version=1, client version=29, methodsFingerPrint=54742778 from 10.184.17.26:46472: output error
2012-03-08 12:03:09,476 WARN org.apache.hadoop.ipc.HBaseServer: IPC Server handler 7 on 60020 caught: java.nio.channels.ClosedChannelException
{quote}

I suggested in response we might consider give our RPC a keepalive option for calls that may run for a long time (like execCoprocessor).

LarsH +1ed the idea:
{quote}
+1 on ""keepalive"". It's a shame (especially for long running server code) to do all the work, just to find out at the end that the client has given up.

Or maybe there should be a way to cancel an operation if the clients decides it does not want to wait any longer (PostgreSQL does that for example). Here that would mean the server would need to check periodically and coprocessors would need to be written to support that - so maybe that's no-starter.
{quote}"
HBASE-5530	We currently have major versions 0 and 1. The HBase checksum patch introduces minor version 0 and 1. Then the columnar HFileBlock might introduce yet another disk format version. We need a simple but elegant framework to test the compatibility of code with all these disk format versions. We also want to do this without much code duplication in TestHFileBlockCompatibility.
HBASE-5262	"Creating this JIRA to open a discussion about a structured (machine-readable) log that will record events such as compaction start/end times, compaction input/output files, their sizes, the same for flushes, etc. This can be stored e.g. in a new system table in HBase itself. The data from this log can then be analyzed and used to optimize compactions at run time, or otherwise auto-tune HBase configuration to reduce the number of knobs the user has to configure."
HBASE-5462	"We have a big table which have 30k regions but the request is not very high (about 50K per day).
We use the hbase.master.cluster_request metrics to monitor the cluster request but find that lots of requests is generated by master, which scan the meta table at regular intervals.
It is hard for us to monitor the real request from the client, it is possible to filter the scanning meta table or create a new metric which could show the real request from client.

Thank you.


"
HBASE-5374	"SchemaMetrics.useTableNameGlobally is a Boolean object that is not initialized. It depends on public static method configureGlobally() to initialize it based on the configuration file. But this is only done for writer, not for reader. So when invoking hfile tool,
{code}
hbase/bin/hbase org.apache.hadoop.hbase.io.hfile.HFile -v -f YourFile
{code}
where HFileReaderV2 is invoked, it throws exception complaining the flag is null. "
HBASE-5301	"Mikael Sitruk commented on this back in Nov 2011 and after looking at this I completely agree with him.  For example, ""flushSize_avg_time"" makes no sense.  ""flushSize"" is in bytes, so is this the average flush size?  Or the average time per flush?  In which case, why not call the measure ""flush_avg_time"".  But to add to the confusion there is already a ""flushTime_avg_time"" metric.  There is also ""flushTime_num_ops"" and ""flushSize_num_ops"" that are confusing.  Is the former the number of flushes?  In which case, why have ""time"" in the metric name?  


On 11/22/11 5:23 PM, ""Mikael Sitruk"" <mikael.sitruk@gmail.com> wrote:

Hi

I have enabled metrics on Hbase cluster (0.90.1), and mapped the metrics
to
3 categories (missing, Present but not documented/Incomplete
documentation,Ok) according to their status in the book (
http://hbase.apache.org/book.html#hbase_metrics). Is it possible to udpate
the book accordingly?
It seems also that rpc metrics are not documented at all.

And now some questions on the metrics:
I can see some metrics present a num_ops and avg_time suffix (like rpc)
but
it seems that for certain metrics is it totally unclear (to me at least)
or
their name is missleading - for example what
means compactionTime_avg_time/compactionTime_num_ops? or
flushSize_avg_time
and flushSize_num_ops? I mean I would have understood compaction_avg_time
and flushSize or flush_avg_time.
"
HBASE-5316	"Currently HTable and HBaseAdmin read configurations based on the same config key, such as hbase.client.retries.number and hbase.client.retries.number
Actually in some cases, the client needs different settings for HTable operations and HBaseAdmin operations.
One way is to pass different HBaseConfiguration objects to HTable and HBaseAdmin.
Another much clearer way is to separate the configurations for HTable and HBaseAdmin by using different config keys.
"
HBASE-5272	"Currently the body of DefaultLoadBalancer.balanceCluster() is 250 lines long.
It involves multiple iterations which deal with various kinds of corner cases.

We should simplify this part of code."
HBASE-5135	"The jar adding methods of TableMapReduceUtil seem to be bypassing the DistributedCache API by plugging in the jar lists themselves to the actual property. This is not a good practice and must be avoided if possible.

_Observed during HBASE-3274_."
HBASE-5132	Consider exposing master event handler state as monitored tasks.
HBASE-1254	"Chatting up on IRC, 2G seems like way outer reaches of what we could ever handle.  Add checks to client."
HBASE-5067	"In STANDALONE mode:
When setting the configuration option ""hbase.master.dns.interface"" (and optional ""hbase.master.dns.nameserver"") to non-default values,

it is EXPECTED that the master node would report its fully qualified dns name when registering in ZooKeeper,
BUT INSTEAD, the machines hostname is taken instead.

For example, my machine is called (aka ""its hostname is..."") ""machine1"" but it's name in the network is ""machine1.our-dev-network.my-corp.com"", so to find this machine's IP anywhere on the network i would need to query for the whole name (because trying to find ""machine1"" is ambiguous on a network).

Why is this a bug, because when trying to connect to this stand-alone hbase installation from outside the machine it is running on, when querying ZK for /hbase/master we get only the ""machine1"" part, and then fail with an unresolvable address for the master (which later even gives a null pointer because of a missing null check).

This is the stack trace when calling HTable's c'tor:
java.lang.IllegalArgumentException: hostname can't be null
	at java.net.InetSocketAddress.<init>(InetSocketAddress.java:139) ~[na:1.7.0_02]
	at org.apache.hadoop.hbase.HServerAddress.getResolvedAddress(HServerAddress.java:108) ~[hbase-0.90.4.jar:0.90.4]
	at org.apache.hadoop.hbase.HServerAddress.<init>(HServerAddress.java:64) ~[hbase-0.90.4.jar:0.90.4]
	at org.apache.hadoop.hbase.zookeeper.RootRegionTracker.dataToHServerAddress(RootRegionTracker.java:82) ~[hbase-0.90.4.jar:0.90.4]
	at org.apache.hadoop.hbase.zookeeper.RootRegionTracker.waitRootRegionLocation(RootRegionTracker.java:73) ~[hbase-0.90.4.jar:0.90.4]
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:579) ~[hbase-0.90.4.jar:0.90.4]
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:559) ~[hbase-0.90.4.jar:0.90.4]
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:688) ~[hbase-0.90.4.jar:0.90.4]
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:590) ~[hbase-0.90.4.jar:0.90.4]
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:559) ~[hbase-0.90.4.jar:0.90.4]
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:688) ~[hbase-0.90.4.jar:0.90.4]
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:594) ~[hbase-0.90.4.jar:0.90.4]
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:559) ~[hbase-0.90.4.jar:0.90.4]
	at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:173) ~[hbase-0.90.4.jar:0.90.4]
	at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:147) ~[hbase-0.90.4.jar:0.90.4]

==============

Why this happens?
1. When building the HMaster object we correctly use the static 'getMyAddress(conf)' to read the configuration options, and then to try and resolve the machine's ip. This method returns the full qualified name correctly, and this is then used to construct an 'HServerAddress' object which is locally stored as 'a'.
2. So far so good, but now, instead of using this object as the value for the master's 'address' field the code goes on to initialize the 'rpcServer' field. As part of this calls the static 'HBaseRPC.getServer' method is called with, among others, the HServerAddress's BIND ADDRESS (aka the IP) that we have just built.
3. But now, when we finally get to setting the value for HMaster's 'address' field, we initialize a NEW HServerAddress initialized with rpcServer.getListenerAddress() (which is basically the IP we just gave it, with a new listening port.
4. HServerAddress calls 'getAddress().getHostName()' on this address object, which would return the local hostname of the machine, because the IP would be resolved locally by the machine, and not using a nameserver.

So eventually, the fully qualified name computed in step 1 is NOT USED in any way, instead, all further processing is done on the IP address of the host (and its local resolving to the hostname).

=======

What should happen?
The 'HMaster.address' field should be set to an address which is made of the fully qualified name retrieved in step 1, combined with the port retrieved from the rpcServer computed at step 2.

====

Notes:

1. It seems that the 'HBaseServer' c'tor (which is called when 'HBaseRPC.getServer()' static method is called) is faulty as it doesn't use the port number sent to it in effect (it sets the local 'port' field to it, but then overrides it without ever reading it later on, with the port returned from the new 'Listener' object. This might be a bug, but i have not checked it enough.

2. The same bug with the master node could repeat itself in the region server code, but i haven't checked that at all."
HBASE-4961	"I was logged into the ASF build machines and saw about 10-15 HBase precommit builds that have been hung for weeks. I took a jstack of each, which I'll attach here. I then kill -9ed them to free up the resources."
HBASE-4902	"i have 13 millions keys, I use presplit of 1000 regions.
while looking at the regions created, i see 

Region Name : TC,sub_10386999,1322603111143.5b36001298f3dab177edf3a7265c628a.
Start Key: sub_10386999
End Key:   sub_103999 

That is instead sub_10386999 + 13000 = sub_10399999 the ui will only show sub_1039999 (missing last 9 digit)
It occurs in several place in different region
for another key also:
Start: sub_4406999
End:   sub_441999
instead of sub_4419999

"
HBASE-4887	"Currently the ordinal of compression algorithms is used.
This places unnecessary constraint when new compress algorithm is added.

We should write full enum name of Compression.Algorithm into HFile"
HBASE-4873	"HBASE-4863 introduced bounded thread pool for Thrift server.
thrift2/ThriftServer should have this enhancement as well."
HBASE-4846	"the shell script:${HBASE_HOME}/bin/hbase load the hadoop native lib like this:

if [ -d ""/usr/lib/hadoop-0.20/lib/native/${JAVA_PLATFORM}"" ] ; then
  JAVA_LIBRARY_PATH=$(append_path ""${JAVA_LIBRARY_PATH}"" /usr/lib/hadoop-0.20/lib/native/${JAVA_PLATFORM})
fi

It should work like this:

if [ -d ""${HADOOP_HOME}/lib/native/${JAVA_PLATFORM}"" ] ; then
  JAVA_LIBRARY_PATH=$(append_path ""${JAVA_LIBRARY_PATH}"" ${HADOOP_HOME}/lib/native/${JAVA_PLATFORM})
fi"
HBASE-4431	Do the rebalance_based_off_region_hits_but_native_balancer_must_be_disabled part of hbase-3507.
HBASE-4586	"For write-heavy/bulk-load environments, two common recommendations are to disable autoFlush and increase the size of the write buffer:

HTable.setAutoFlush(false)
HTable.setWriteBufferSize(long writeBufferSize)

Neither of exposed via the Thrift API, which seems to preclude using the Thrift interface for write-heavy environments.

As a workaround, we could specify hbase.client.write.buffer in hbase-site.xml, but there is unfortunately not an equivalent configuration setting for autoflush."
HBASE-4569	"I spent a little bit of time last night hacking on the slab cache implementation to make it a little simpler. The change is:
- no longer has the composition of SlabCache containing a SingleSizeCache per slab size. SlabCache holds its own slabs
- no longer use guava's map implementations to handle a size-bounded cache. Instead, manages its own LRU linked list
- significantly less clever about synchronization. since this is an L2 cache, it should be less contended than the L1 cache, and I think we can afford to be dumb.
- should have less memory usage since there's only one map entry per key instead of several.
"
HBASE-4514	"The current coprocessor RegionObserver API takes in a bunch of arguments into each of the method calls. Instead, it will be better if we encapsulate these arguments into a single object."
HBASE-4067	It would be a good idea to make the compaction intervals of regions into a table-level property instead of an instance-wide one.
HBASE-3284	"In AssignmentManager.handleSplitReport() we do:

{noformat}
    regionOffline(parent);
    regionOnline(a, hsi);
    regionOnline(b, hsi);
{noformat}

Within each of these, there is locking, but there is no locking around this entire operation.  There should be.

This might be the cause of HBASE-3278"
HBASE-3067	"On Fri, Oct 1, 2010 at 8:31 AM, Stack <stack@duboce.net> wrote:
Blockcache is made of blocks pulled from HDFS.  It'd be a little awkward inserting hot records into the block cache w/o going via HDFS.

But, yes, you have a good point that flush is disruptive of hot records.

In the past we talked of a keyvalue cache on top of the block cache but it fell out of favor because block cache seemed to be good enough but sounds like we need to revive it or do some fancy dancing if column family is marked in-memory, we keep around the snapshot of memstore until we know the block cache has been populated?

Any other suggestions?

On Fri, Oct 1, 2010 at 1:26 AM, Abhijit Pol <apol@rocketfuel.com> wrote:
> we are trying to read efficiently a hot column family (in_memory=true, blockcaching=true) that get writes at say 500 qps and reads at 10,000 qps.
> - as long as writes are in memstore we get them from memstore and its fast
> - if we have read it once it will be at least in block cache (gets priority due to in_memory=true) and subsequent reads are faster
> - however memstore flush puts records on disk which demands for disk IO to get them back in block cache
>
> is there a way for memstore flush to go to blockcache?
"
HBASE-4133	"One desirable feature for hbck is snapshot.
hbck found 650 inconsistencies for 0.90.3 in our staging cluster. I upgraded to 0.90.4
It would be nice if I can take snapshot of the inconsistencies so that I can detect new problems (delta) after we run 0.90.4 for some time using the diff capability.
"
HBASE-4066	"Right now, major compaction interval settings affects ALL the tables including .META. and -ROOT- as well.

It would be a good addition to let .META. compaction intervals be managed separately with its own configuration so it isn't a hassle having to do it manually like the rest of the tables."
HBASE-1530	"As discussed in HBASE-1207 and to build on what was implemented in HBASE-1503, we should modify the existing KeyValueHeap in place by reusing any open scanners possible."
HBASE-3479	improve our javadoc!
HBASE-1764	"Working on new getClosestAtOrBefore in HBASE-1761, the way hfile scanner works where it gets the asked for key or the one just before makes for our doing more work than we should.   See the code in Store where we want to get to first element in row.  We have to go to the row before and then walk forward skipping the item that is in row before.

Consider flipping catalog tables to be regions by end-key rather than start-key at same time (RE: conversation had on the saturday night at the SUSF hackathon)."
HBASE-2809	"A lot of data is going through the ReplicationSources, we need to take it into our general accounting."
HBASE-1667	"0.20 supports multi masters. However,

bin/hbase-daemon.sh stop master

on backup masters will bring the whole cluster down.

Per rolling upgrade wiki that stack pointed out, kill -9 for backup master is the only way to go currently.

I think it's better to make some sort of magic that we can use something like

bin/hbase-daemon.sh stop master

to properly stop either the backup master or the whole cluster.
"
HBASE-3526	"If you have a table with TTL, the memstore will grow until it hits flush size, at which point the flush code will prune the KVs going to hfile. If you have a small TTL, it may not be necessary to flush, since pruning data in memory would ensure that we never grow too big.

"
HBASE-4550	"when master passed regionserver different address, regionserver didn't create new zookeeper znode, master store new address in ServerManager, when call stop-hbase.sh , RegionServerTracker.nodeDeleted received path is old address, serverManager.expireServer is not be called. so stop-hbase.sh is hang."
HBASE-5165	"Concurrent processing of DeleteTableHandler and ServerShutdownHandler may cause following situation
1.Table has already be disabled.
2.ServerShutdownHandler is doing MetaReader.getServerUserRegions.
3.When step2 is processing or is completed just now, DeleteTableHandler starts to delete region(Remove region from META and Delete region from FS)
4.DeleteTableHandler set table enabled.
4.ServerShutdownHandler is starting to assign region which is alread deleted by DeleteTableHandler.

The result of above operations is producing an invalid record in .META.  and can't be fixed by hbck "
HBASE-11035	We can use online configuration change to specify the active list of coprocessors in CoprocessorHost (RegionCoprocessorHost). That way we don't need to close regions when we want to add/remove new coprocessors (regionobservers)
HBASE-9972	"I was debugging something in the swift branch, and found that HBase doesn't export to JMX by default. The JMX server is being spun-up anyways in single node setup, we might as well export the metrics to it."
HBASE-9891	"Add counters for retried puts, and succeeded puts in HTableMultiplexer."
HBASE-10945	"In case of DNS errors on the machine, locating a ZK quorum might throw UnknownHostException-s, but this might be a temporary issue in most cases. Retrying a couple of times, might resolve this problem. We already retry while getting ZK node data, it only makes sense to do the same while creating the connection."
HBASE-10901	"createMultiRegions and its friends have a problem that leads a bug getting null HRegionInfo from the .META. table in metaScan.

The reason is : After createMultiRegions creates new regions, it deletes old region  info from the META table. But some region server may have been openning the regions (though with a very low probability). After the rows of the openning region is completely removed, the region is openned. Then the region server tells the master its server and startCode. The master then write a row of only server address and startCode back to the META. Then a row without HRegionInfo in META table is generated.

Suggestion of fix is not to use this method. Create tables directly with specified start/stop keys."
HBASE-10177	"The netty developers changed their group id from org.jboss.netty to io.netty. As a result, the zookeeper and hadoop dependencies pull in the older netty (3.2.2) and swift related dependencies pull in the newer netty (3.7.0). As a result we get ClassNotFoundExceptions, when the older 3.2.2 jar is picked up in place of 3.7.0. "
HBASE-11254	"The failed puts in HTableMultiplexer.HTableFlushWorker are immediately resubmitted in the original queue. This queue is being flushed every 100ms (default configuration) and the maximal retry of a failed put is by default 10 times (client configurable). That leaves us with ~1 second to complete a Put (with default configuration). We can improve this by gradually increasing the time for retrying, each time when the put fails. (example: first time retry after 100ms, then after 200ms, 400ms, etc). "
HBASE-8227	Umbrella task to investigate outliers in the get and put path.
HBASE-11592	We would like to make the number of SplitLogWorkers online configurable with this JIRA.
HBASE-10910	"batchGet(List<Get>) is more performant since it splits the list of Gets on regionserver level, and get(List<Get>) does that on region level. 
If we have a list of gets for regions on a same regionserver, get(List<Get>) will do #regions rpc calls and batchGet(List<Get>) will do just one rpc call. 

Changing HTable.get(List<Get>) to internally call HTable.batchGet(List<Get>)"
HBASE-11015	"Use `ScheduledThreadPoolExecutor`
Change some logic
Add testcase
"
HBASE-10042	"Updating the libthrift, zookeeper, mockito, log4j and slf4j jars, since one of the customers is running into problems with the old jars that HBase pulls in. 

Also added the requireUpperBoundDeps as a rule during the building process. This will force us to specify the correct version of a dependency, if it is required transitively by multiple dependencies."
HBASE-12244	"In our testing we keep running into https://jira.codehaus.org/browse/SUREFIRE-1091

My guess is that bug is also somewhat responsible fore our zombie tests."
HBASE-8713	"Testing 0.95.1 RC1, I see these 2 lines every time the WAL is rolled:

{noformat}
2013-06-07 17:19:33,182 INFO  [RS_CLOSE_REGION-ip-10-20-46-44:50653-1] util.FSUtils: FileSystem doesn't support getDefaultReplication
2013-06-07 17:19:33,182 INFO  [RS_CLOSE_REGION-ip-10-20-46-44:50653-1] util.FSUtils: FileSystem doesn't support getDefaultBlockSize
{noformat}

It only happens on hadoop1."
HBASE-12566	"I've discovered after HBASE-12550 that Phoenix has a class that was broken by a change to a package scoped method in HRegion:
{code}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/reg
index 39a9fdc..3377e6b 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
@@ -4628,11 +4628,12 @@ public class HRegion implements HeapSize { // , Writable{
   /**
    * Create a daughter region from given a temp directory with the region data.
    * @param hri Spec. for daughter region to open.
+   * @param expectedReferenceFileCount
    * @throws IOException
    */
-  HRegion createDaughterRegionFromSplits(final HRegionInfo hri) throws IOException {
+  HRegion createDaughterRegionFromSplits(final HRegionInfo hri, int expectedReferenceFileCount) throws IOException {
     // Move the files from the temporary .splits to the final /table/region directory
-    fs.commitDaughterRegion(hri);
+    fs.commitDaughterRegion(hri, expectedReferenceFileCount);
{code}

We should change the HRegion InterfaceAudience to LimitedPrivate(COPROC, PHOENIX)."
HBASE-12937	"See HTRACE-92 for more detail. I'm hoping that adding calls to Tracer.getInstance().continueSpan(null) in the correct place(s) will do the trick. This is rendering tracing unusable by Phoenix. Not sure if it impact general HTrace use in HBase or not.

FYI, [~samarthjain], [~jesse_yates]."
HBASE-7631	"Of a sudden on a prod cluster, a table's rows gained girth... hundreds of thousands of rows... and the application was pulling them all back every time but only once a second or so.  Regionserver was carrying hundreds of regions.  Was plain that there was lots of network out traffic.  It was tough figuring which region was the culprit (JD's trick was moving the regions off one at a time while watching network out traffic on cluster to see whose spiked next -- it worked but just some time).

If we had per region read/write sizes in metrics, that would have saved a bunch of diagnostic time."
HBASE-6362	"When user uploads logs / images / non-trunk patches, Hadoop QA would complain that the file couldn't be applied as a patch (for trunk).

We should make this script smarter by recognizing image files and non-trunk patches."
HBASE-7734	"In RegionScanner.nextRows, checking filter's done value to ensure that null, which was the expected value by the client, was returned. This check was removed when ScanPrefetcher was introduced."
HBASE-6486	Idea is to know how many MB/sec of throughput we are able to get by writing into HBase using a simple tool.
HBASE-6235	"The max and min values for time varying metrics are not reset to 0 correctly. 
The fix is to reset them during the JMX refresh time."
HBASE-6943	"When getting a regionserver connection in 0.89-fb in HBaseClient, we catch all types of Throwable. I have observed a real case when the client looked stuck. On debugging it turned out that a NoSuchMethodError was thrown and caught, leaving the connection in an inconsistent state (initialized socket but null streams). All following attempts resulted in NPEs that were also caught, and no errors were logged. From the user's perspective the client was just stuck. The root cause was the absence of a required jar (hence the NoSuchMethodError) but it was not reported properly."
HBASE-4963	"We need to make sure we get per-(table, CF) get size metrics in 0.89-fb, similarly to what was done in https://reviews.facebook.net/D483 for the trunk. Currently we only get metrics such as 

  hadoop.regionserver_cf.<cf>.getsize

even with per-table metrics turned on."
HBASE-2161	"the client side code has this in ClientScanner.next():

            if (e instanceof UnknownScannerException &&
                lastNext + scannerTimeout < System.currentTimeMillis()) {
              ScannerTimeoutException ex = new ScannerTimeoutException();
              ex.initCause(e);
              throw ex;
            }

This will cause the client-side to timeout after the 'scannerTimeout'. There doesn't seem to be any good reason for this - we can just restart the scanner at the last row.  Leave the server-side scanner as is, but clients should be able to have a 5 day scan that is continuously being restarted."
HBASE-5703	"We need to bound the number of threads spawned in HRegionThriftServer, similarly to what was done in HBASE-4863 to the standalone Thrift gateway."
HBASE-7112	"the ZK_CLIENT_PORT_KEY is declared as followed:
private static final String ZK_CFG_PROPERTY = ""hbase.zookeeper.property."";
  private static final String ZK_CLIENT_PORT_KEY = ZK_CFG_PROPERTY
      + ""clientPort"";

i think it is not correct exactly, it should not contain the prefix  ZK_CFG_PROPERTY.

IMO, need to change it to private static final String ZK_CLIENT_PORT_KEY = ""clientPort"";"
HBASE-7267	"In HBASE-6059, it introduced a new behavior that the compaction would create the HFileWriter no mater whether there is any key/value as the output or not. This new behavior actually is conflicts with HBASE-5199 (Delete out of TTL store files before compaction selection) so that compacting the expired hfiles would generate one more expired hfiles.

Actually we only needs to create the dummy hfile IFF the maxSequenceID among the compaction candidates is equal to the maxSequenceID among all the on-disk hfiles. "
HBASE-3663	"This is an interesting starvation case. There are 2 conditions to trigger this problem.
Condition1: r/s - r/(s+1) << 1 
Let r: the number of regions
Let s: the number of servers

Condition2: for each server, the load of each server is less or equal the ceil of avg load.

Here is the unit test to verify this problem: 
For example, there are 16 servers and 62 regions. The avg load is 
3.875. And setting the slot to 0 to keep the load of each server either 3 or 4. 
When a new server is coming,  no server needs to assign regions to this new server, since no one is larger the ceil of the avg.
(Setting slot to 0 is to easily trigger this situation, otherwise it needs much larger numbers)

Solutions is pretty straightforward. Just compare the floor of the avg instead of the ceil. This solution will evenly balance the load from the servers which is little more loaded than others. 

I also attached the comparison result  for the case mentioned above between the old balance algorithm and new balance algorithm. (I set the slot = 0 when testing)
"
HBASE-6279	I don't see a reason for having the RPC methods (like HMasterInterface) define methods with the RpcController argument (which is always passed as null during invocation). 
HBASE-6425	Currently it needs to be specified in millisecs when it usually is a big duration. It would make more sense to specify it in hours.
HBASE-4525	"When we build the hbase, we can say which version of hadoop we want to build with.
For example:
 mvn  -DskipTests=true package
 mvn -Dhadoop.profile=22 -DskipTests=true package

However during the runtime, bin/hbase scripts will call add_maven_deps_to_classpath, which will always add the default hadoop version(0.20-append) to classpath (targets/cached_classpath.txt).

"
HBASE-6006	I noticed that TestDelayedRpc is flaky - testTooManyDelayedRpcs fails quite often in my setup. Seems to me that the test needs to be fixed for it to not depend on the (unpredictable) thread scheduling order in the RPC server/clients that the test creates.
HBASE-5079	"The DLS interrupt can kill the regionserver if happens while conversation w/ namenode is going on.

The interrupt is used to end a task on regionserver when done whether successful or to interrupt an ongoing split since assumed by another server.

I saw this issue testing because I was killing servers.  I also was suffering ""HBASE-5078 DistributedLogSplitter failing to split file because it has edits for lots of regions"" which made it more likely to happen.

Here is what it looks like on the regionserver that died:

{code}
2011-12-20 17:54:58,009 INFO org.apache.hadoop.hbase.regionserver.SplitLogWorker: task /hbase/splitlog/hdfs%3A%2F%2Fsv4r11s38%3A7000%2Fhbase%2F.logs%2Fsv4r31s44%2C7003%2C1324365396770-splitting%2Fsv4r31s44%252C7003%252C1324365396770.1324403495463 preempted from sv4r13s38,7003,1324365396583, current task state and owner=owned sv4r27s44,7003,1324365396664
2011-12-20 17:54:58,009 INFO org.apache.hadoop.hbase.regionserver.SplitLogWorker: Sending interrupt to stop the worker thread
2011-12-20 17:54:59,133 INFO org.apache.hadoop.hbase.regionserver.SplitLogWorker: task /hbase/splitlog/hdfs%3A%2F%2Fsv4r11s38%3A7000%2Fhbase%2F.logs%2Fsv4r31s44%2C7003%2C1324365396770-splitting%2Fsv4r31s44%252C7003%252C1324365396770.1324403495463 preempted from sv4r13s38,7003,1324365396583, current task state and owner=owned sv4r27s44,7003,1324365396664
2011-12-20 17:54:59,134 INFO org.apache.hadoop.hbase.regionserver.SplitLogWorker: Sending interrupt to stop the worker thread
...
2011-12-20 17:55:25,505 INFO org.apache.hadoop.hbase.regionserver.SplitLogWorker: task /hbase/splitlog/hdfs%3A%2F%2Fsv4r11s38%3A7000%2Fhbase%2F.logs%2Fsv4r31s44%2C7003%2C1324365396770-splitting%2Fsv4r31s44%252C7003%252C1324365396770.1324403495463 preempted from sv4r13s38,7003,1324365396583, current task state and owner=unassigned sv4r11s38,7001,1324365395047
2011-12-20 17:55:25,505 INFO org.apache.hadoop.hbase.regionserver.SplitLogWorker: Sending interrupt to stop the worker thread
{code}

Three interrupts are sent over period of 31 seconds or so.

Eventually the interrupt has an effect and I get:

{code}
2011-12-20 17:55:25,505 INFO org.apache.hadoop.hbase.regionserver.SplitLogWorker: Sending interrupt to stop the worker thread
2011-12-20 17:55:48,022 DEBUG org.apache.hadoop.hbase.regionserver.LogRoller: HLog roll requested
2011-12-20 17:55:58,070 WARN org.apache.hadoop.hdfs.DFSClient: DataStreamer Exception: java.io.IOException: Call to sv4r11s38/10.4.11.38:7000 failed on local exception: java.nio.channels.ClosedByInterruptException
        at org.apache.hadoop.ipc.Client.wrapException(Client.java:1103)
        at org.apache.hadoop.ipc.Client.call(Client.java:1071)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)
        at $Proxy9.addBlock(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor37.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
        at $Proxy9.addBlock(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.locateFollowingBlock(DFSClient.java:3507)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:3370)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2700(DFSClient.java:2586)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2826)
Caused by: java.nio.channels.ClosedByInterruptException
        at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:184)
        at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:341)
        at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:55)
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:146)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:107)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
        at java.io.DataOutputStream.flush(DataOutputStream.java:106)
        at org.apache.hadoop.ipc.Client$Connection.sendParam(Client.java:779)
        at org.apache.hadoop.ipc.Client.call(Client.java:1047)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)
        at $Proxy9.getFileInfo(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
        at $Proxy9.getFileInfo(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:875)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:513)
        at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:768)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.convertRegionEditsToTemp(HLogSplitter.java:1097)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.createWAP(HLogSplitter.java:1066)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.convertRegionEditsToTemp(HLogSplitter.java:1097)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.createWAP(HLogSplitter.java:1066)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFileToTemp(HLogSplitter.java:410)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFileToTemp(HLogSplitter.java:351)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker$1.exec(SplitLogWorker.java:113)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.grabTask(SplitLogWorker.java:266)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.taskLoop(SplitLogWorker.java:197)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.run(SplitLogWorker.java:165)
        at java.lang.Thread.run(Thread.java:662)
2011-12-20 17:55:58,070 WARN org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Could not prepare temp staging area
java.io.IOException: Call to sv4r11s38/10.4.11.38:7000 failed on local exception: java.nio.channels.ClosedByInterruptException
        at org.apache.hadoop.ipc.Client.wrapException(Client.java:1103)
        at org.apache.hadoop.ipc.Client.call(Client.java:1071)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)
        at $Proxy9.getFileInfo(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
        at $Proxy9.getFileInfo(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:875)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:513)
        at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:768)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.convertRegionEditsToTemp(HLogSplitter.java:1097)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.createWAP(HLogSplitter.java:1066)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFileToTemp(HLogSplitter.java:410)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFileToTemp(HLogSplitter.java:351)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker$1.exec(SplitLogWorker.java:113)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.grabTask(SplitLogWorker.java:266)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.taskLoop(SplitLogWorker.java:197)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.run(SplitLogWorker.java:165)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.nio.channels.ClosedByInterruptException
        at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:184)
        at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:341)
        at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:55)
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:146)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:107)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
        at java.io.DataOutputStream.flush(DataOutputStream.java:106)
        at org.apache.hadoop.ipc.Client$Connection.sendParam(Client.java:779)
        at org.apache.hadoop.ipc.Client.call(Client.java:1047)
        ... 20 more
{code}

Now here is the wacky part.  Above we are trying to go to the namenode it looks like and the interrupt is closing socket.  At same, time, I'm trying to flush and it fails with same stack trace ... and because we failed a flush, regionserver goes down:

{code}
2011-12-20 17:55:58,071 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block blk_-6239131583587622790_187561 bad datanode[0] nodes == null
2011-12-20 17:55:58,073 WARN org.apache.hadoop.hdfs.DFSClient: Could not get block locations. Source file ""/hbase/TestTable/9f98e6764d322832c845b740336e5750/.tmp/0df1673c96274028bda24c9cb49e9c3e"" - Aborting...
2011-12-20 17:55:58,074 FATAL org.apache.hadoop.hbase.regionserver.HRegionServer: ABORTING region server sv4r13s38,7003,1324365396583: Replay of HLog required. Forcing server shutdown
org.apache.hadoop.hbase.DroppedSnapshotException: region: TestTable,0134394898,1323822783216.9f98e6764d322832c845b740336e5750.
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1276)
        at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1160)
        at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:1102)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:400) 
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:374)
        at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.run(MemStoreFlusher.java:243)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Call to sv4r11s38/10.4.11.38:7000 failed on local exception: java.nio.channels.ClosedByInterruptException
        at org.apache.hadoop.ipc.Client.wrapException(Client.java:1103)
        at org.apache.hadoop.ipc.Client.call(Client.java:1071)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)
        at $Proxy9.addBlock(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor37.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
        at $Proxy9.addBlock(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.locateFollowingBlock(DFSClient.java:3507)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:3370)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2700(DFSClient.java:2586)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2826)
Caused by: java.nio.channels.ClosedByInterruptException
        at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:184)
        at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:341)
        at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:55)
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:146)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:107)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
        at java.io.DataOutputStream.flush(DataOutputStream.java:106)
        at org.apache.hadoop.ipc.Client$Connection.sendParam(Client.java:779)
        at org.apache.hadoop.ipc.Client.call(Client.java:1047)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)
        at $Proxy9.getFileInfo(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
        at $Proxy9.getFileInfo(Unknown Source)
...
{code}

"
HBASE-5250	"From Benoit... 
{code}2012-01-20 17:13:20,634 INFO org.apache.hadoop.ipc.HBaseServer: IPC
Server Responder: doAsyncWrite threw exception java.io.IOException:
Broken pipe
2012-01-20 17:13:20,635 WARN org.apache.hadoop.ipc.HBaseServer: IPC
Server listener on 52146: readAndProcess threw exception
java.lang.NullPointerException. Count of bytes read: 0
java.lang.NullPointerException
       at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.readAndProcess(HBaseServer.java:1119)
       at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.doRead(HBaseServer.java:703)
       at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.doRunLoop(HBaseServer.java:495)
       at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:470)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:680)

This is when I unexpectedly close a connection to a RS.  When the
channel gets disconnected, some buffer is nulled out, but then if
readAndProcess() is called again, it triggers the NPE above.  Not a
huge deal, but also indicates that maybe some logic in the RS's error
handling is incorrect.
{code}"
HBASE-5109	"TestAvroServer has the following issue
 
    impl.modifyTable(tableAname, tableA);
    // It can take a while for the change to take effect. Wait here a while.
    while(impl.describeTable(tableAname) == null ) {
      Threads.sleep(100);
    }
    assertTrue(impl.describeTable(tableAname).maxFileSize == 123456L);
 
impl.describeTable(tableAname) returns the default maxSize 256M right away as modifyTable is async. Before HBASE-4328 is fixed, we can fix the test code to wait for say max of 5 seconds to check if impl.describeTable(tableAname).maxFileSize is uploaded to 123456L. "
HBASE-4619	"This appears in TableOutputFormat.java: 

{code}
@Override
    public void close(TaskAttemptContext context)
    throws IOException {
      table.flushCommits();
      // The following call will shutdown all connections to the cluster from
      // this JVM.  It will close out our zk session otherwise zk wil log
      // expired sessions rather than closed ones.  If any other HTable instance
      // running in this JVM, this next call will cause it damage.  Presumption
      // is that the above this.table is only instance.
      HConnectionManager.deleteAllConnections(true);
    }
{code}

It's not a safe assumption that a single TableOutputFormat is the only HBase client in a JVM."
HBASE-4264	"The TestMergeTool.testMergeTool failure in build 2154 was due to the following check:
{noformat}
      HBaseAdmin.checkHBaseAvailable(getConf());
      LOG.fatal(""HBase cluster must be off-line."");
{noformat}
HBase cluster from some other test(s) was hanging since ""merging regions 0 and 1"" is the first merge.
The test can be made more robust by calling HBaseAdmin.shutdown() before making the first merge.

In Merge.java, -1 is returned in 5 places. I think we should return different values so that it is more obvious what the cause was."
HBASE-12800	"Hi:
I am getting constant stability problems with the HBase Regionserver, it dies randomly everyday or every other day. It normally dies shortly after printing the following:

2014-12-30 23:06:17,091 ERROR [regionserver60020.logRoller] wal.ProtobufLogWriter: Got IOException while writing trailer
org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /hbase/WALs/zjdx107,60020,1418269148759/zjdx107%2C60020%2C1418269148759.1419977176935 could only be replicated to 0 nodes instead of minReplication (=1).  There are 12 datanode(s) running and no node(s) are excluded in this operation.
at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1430)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2659)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)

        at org.apache.hadoop.ipc.Client.call(Client.java:1409)
        at org.apache.hadoop.ipc.Client.call(Client.java:1362)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
        at com.sun.proxy.$Proxy13.addBlock(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
at com.sun.proxy.$Proxy13.addBlock(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:361)
        at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:266)
        at com.sun.proxy.$Proxy14.addBlock(Unknown Source)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1437)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1260)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:525)
2014-12-30 23:06:17,092 ERROR [regionserver60020.logRoller] wal.FSHLog: Failed close of HLog writer
org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /hbase/WALs/zjdx107,60020,1418269148759/zjdx107%2C60020%2C1418269148759.1419977176935 could only be replicated to 0 nodes instead of minReplication (=1).  There are 12 datanode(s) running and no node(s) are excluded in this operation.
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1430)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2659)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:569)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:440)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)"
HBASE-7962	"I was trying to install Snappy.  Found this link.
http://www.spaggiari.org/index.php/hbase/how-to-install-snappy-with#.US7JNzDX_D4.

As per this link there is a native folder under lib directory in the HBase installation path and it is available in 0.94.

But when i install HBase trunk i could not see one

If am not finding this in the correct place, pls invalidate the issue. Will try figuring out what could be the problem"
HBASE-12780	"Invalid issue, please ignore it."
HBASE-1900	Add back native lib when hadoop 0.21 is built.  Temporarily removed by hbase-1893.
HBASE-5284	"Here's how to reproduce:

{noformat}
$ mvn clean -DskipTests -Dhadoop.profile=23 -Dinstall site assembly:assembly -Dmaven.repo.local=/home/rvs/.m2/repository
........
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.0.2:testCompile (default-testCompile) on project hbase: Compilation failure
[ERROR] /home/rvs/src/bigtop/output/hbase/hbase-0.92.0/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java:[341,33] cannot find symbol
[ERROR] symbol  : variable dnRegistration
[ERROR] location: class org.apache.hadoop.hdfs.server.datanode.DataNode
[ERROR] -> [Help 1]
{noformat}"
HBASE-3815	"the loadbalancer should remember which region server is constantly having trouble opening regions and it should take that rs out of the equation ... otherwise the lb goes into an unproductive loop ... 

I don't have logs handy for this one."
HBASE-5149	"The following code will not return the cluster configuration but the local one which is somewhat misleading.

{code}
conn = (HConnection) HConnectionManager.getConnection(m_hbConfig); //here the configuration is local
Configuration conf = conn.getConfiguration()
conf.getString(""hbase.hregion.majorcompaction""); // will return the parameter from the local config instead of the cluster the connection is connected to.
{code}

It is suggested that once a connection has been acquired the configuration object should be the one of the cluster.

As a general observation it is not possible to retrieve the used configuration on the cluster
It is suggested to add API at {{HRegionServerInterface}}, {{HMasterInterface}} to get the configuration used by the component appropriately (note in 0.90.4 the getConfiguration exist on the Server interface implemented by HRegionServer and HMaster classes) but this interface is not visible/extended by HRegionServerInterface/HMasterInterface, therefore not accessible from client code.

Also an API like {{HashMap<HserverInfo,Configuration> getClusterConfigurations()}} can be added on the HConnection object.


Additional notes:
Since servers can have different properties values (like disk, tmp dir,...) it can be acceptable that the configuration object returned by the connection returns special value to indicate - conflict between value or that multiple values exist.

"
HBASE-4475	"As discussed over in HBASE-4460, the current approach to ThriftServer authentication (provided in HBASE-4099) will not work in an embedded context, since the region server will already does a login for the process.

We could make the embedded thrift server still run as a separate user, though, by doing something like the following:

* add a {{User.loginAndReturnUser()}} variant that delegates to {{UserGroupInformation.loginUserFromKeytabAndReturnUGI()}}, then returns a wrapping {{User}} instance
* call this method on startup for the embedded thrift server to get the thrift user instance
* use {{User.runAs()}} to execute the body of {{HRegionThriftServer.run()}} as the logged in thrift user
"
HBASE-3651	"A lot of systems management is using ""role"" packages to indicate what services should run on which node.  These role packages usually contain only /etc/init.d scripts and depend on the 'core' package (see HBASE-3606) to provide the binaries.
"
HBASE-12310	"hbase-annotations includes a custom doclet used for filtering APIs out of the user API javadoc given our project specific interface annotations. However, this is problematic for a few reasons:
- To build the doclet we include a system scope dependency to tools.jar. Default Nexus rules disallow that. Staging downstream HBase artifacts will be problematic. I don't know how we were able to release 0.98.7 with this in place. I think someone will be looking into the Apache Nexus configuration.
- As I understand it, system scope dependencies will not be supported by Maven 4 because they've been determined to be generally problematic. 
- As [~busbey] mentioned on HBASE-12299, the root pom specifies the javadoc dependency for all modules and it creates a circular need with the hbase-annotations module.

Do we really need a custom doclet? Can we simply remove all of this?"
HBASE-12598	"HBASE-12490 changed the 2.0.0 branch.  This is a similar change to branch-1.

The various uses of setAutoFlush() seem to need some tlc. There's a note in HTableInterface: ""@deprecated in 0.99 since setting clearBufferOnFail is deprecated. Use setAutoFlushTo(boolean) instead."" It would be ideal to change all internal uses of setAutoFlush(boolean, boolean) to use setAutoFlushTo, if possible.
HTable.setAutoFlush(boolean, boolean) is used in a handful of places. setAutoFlush(false, false) has the same results as HTable.setAutoFlush(false). Calling HTable.setAutoFlush(false, true) has the same affect as Table.setAutoFlushTo(false), assuming HTable.setAutoFlush(false) was not called previously (by default, the second parameter, clearBufferOnFail, is true and should remain true according to the comments)."
HBASE-12588	"Currently we don't fail write operations when can't acquiring row locks as shown below in HRegion#doMiniBatchMutation. 
{code}
...
        RowLock rowLock = null;
        try {
          rowLock = getRowLock(mutation.getRow(), shouldBlock);
        } catch (IOException ioe) {
          LOG.warn(""Failed getting lock in batch put, row=""
            + Bytes.toStringBinary(mutation.getRow()), ioe);
        }
        if (rowLock == null) {
          // We failed to grab another lock
          assert !shouldBlock : ""Should never fail to get lock when blocking"";
          break; // stop acquiring more rows for this batch
        } else {
          acquiredRowLocks.add(rowLock);
        }
...
{code}

We saw this issue when there is meta corruption problem and checkRow fails with error:
{noformat}
org.apache.hadoop.hbase.regionserver.WrongRegionException: Requested row out of range for row lock on HRegion
{noformat}

While current code still continues with writes. In all cases, this is so dangerous because row locks have to be acquired before update operations to guarantee row update atomicity.

"
HBASE-12571	"you can follow these steps to repeat the bug:

hbase shell
create 'test', 'cf'  
disable 'test'

reboot hbase


hbase shell
enable 'test'"
HBASE-12453	"Currently (in trunk, with zk-less assignment), a region is available to serving requests only after RS notifies the master the region is open, and the meta is updated with the new location. We may be able to do better than this."
HBASE-12303	"Currently we seek to the next column when we encounter a family delete marker.
I think we safely seek the current store to next row.

We ran into a scenario with very slow scans after a lot of rows have been deleted with family delete markers. Some profiling revealed that we seek for once for each row and column.

This won't make this go away entirely, but at least we can seek once per row rather than once per column."
HBASE-12175	"Trying to create a table from hbase shell and couldn't get region assigned:

{noformat}
^Gdefault^R^Dtest
        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2213)
        at org.apache.hadoop.hbase.ipc.RpcServer$Handler.run(RpcServer.java:1879)
Caused by: java.lang.IllegalArgumentException: Illegal character <10> at 0. Namespaces can only contain 'alphanumeric characters': i.e. [a-zA-Z_0-9]:
^Gdefault^R^Dtest
        at org.apache.hadoop.hbase.TableName.isLegalNamespaceName(TableName.java:215)
        at org.apache.hadoop.hbase.TableName.isLegalNamespaceName(TableName.java:204)
        at org.apache.hadoop.hbase.TableName.<init>(TableName.java:302)
        at org.apache.hadoop.hbase.TableName.createTableNameIfNecessary(TableName.java:339)
        at org.apache.hadoop.hbase.TableName.valueOf(TableName.java:460)
{noformat}"
HBASE-12097	"Hbase shows an increasing number of IPC Threads in BLOCKED state

Hundreds of these,more and more appearing over hours, performance degrading, requiring regionserver restart to restore performance.

Thread:

Thread 421 (IPC Server handler 368 on 60201):
  State: BLOCKED
  Blocked count: 19314
  Waited count: 322565
  Blocked on org.apache.hadoop.metrics.util.MetricsIntValue@1ec5ca55
  Blocked by 236 (IPC Server handler 183 on 60201)
  Stack:
    org.apache.hadoop.metrics.util.MetricsIntValue.set(MetricsIntValue.java:73)
org.apache.hadoop.hbase.ipc.HBaseServer.updateCallQueueLenMetrics(HBaseServer.java:1360)   org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1399)

i dont actually know how to troubleshoot this much further... Happy to take suggestions..."
HBASE-12063	"2014-09-23 14:57:02,161 DEBUG [RS_OPEN_META-logs05:60020-0-WAL.AsyncWriter] wal.FSHLog: RS_OPEN_META-logs05:60020-0-WAL.AsyncWriter interrupted while waiting for newer writes added to local buffer
2014-09-23 14:57:02,161 INFO  [RS_OPEN_META-logs05:60020-0-WAL.AsyncWriter] wal.FSHLog: RS_OPEN_META-logs05:60020-0-WAL.AsyncWriter exiting
2014-09-23 14:57:02,161 DEBUG [regionserver60020] wal.FSHLog: Closing WAL writer in hdfs://logs01.yz.bj.uar.nsn:8020/apps/hbase/data/WALs/logs05.yz.bj.uar.nsn,60020,1411454591351
2014-09-23 14:57:02,161 ERROR [regionserver60020] wal.ProtobufLogWriter: Got IOException while writing trailer
java.io.IOException: All datanodes 10.136.172.105:50010 are bad. Aborting...
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1128)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:924)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:486)
2014-09-23 14:57:02,163 ERROR [regionserver60020] regionserver.HRegionServer: Metalog close and delete failed
java.io.IOException: All datanodes 10.136.172.105:50010 are bad. Aborting...
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1128)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:924)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:486)
2014-09-23 14:57:02,163 DEBUG [regionserver60020-WAL.AsyncNotifier] wal.FSHLog: regionserver60020-WAL.AsyncNotifier interrupted while waiting for  notification from AsyncSyncer thread
2014-09-23 14:57:02,163 INFO  [regionserver60020-WAL.AsyncNotifier] wal.FSHLog: regionserver60020-WAL.AsyncNotifier exiting
2014-09-23 14:57:02,163 DEBUG [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: regionserver60020-WAL.AsyncSyncer0 interrupted while waiting for notification from AsyncWriter thread
2014-09-23 14:57:02,163 INFO  [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: regionserver60020-WAL.AsyncSyncer0 exiting
2014-09-23 14:57:02,163 DEBUG [regionserver60020-WAL.AsyncSyncer1] wal.FSHLog: regionserver60020-WAL.AsyncSyncer1 interrupted while waiting for notification from AsyncWriter thread
2014-09-23 14:57:02,163 INFO  [regionserver60020-WAL.AsyncSyncer1] wal.FSHLog: regionserver60020-WAL.AsyncSyncer1 exiting
2014-09-23 14:57:02,164 DEBUG [regionserver60020-WAL.AsyncSyncer2] wal.FSHLog: regionserver60020-WAL.AsyncSyncer2 interrupted while waiting for notification from AsyncWriter thread
2014-09-23 14:57:02,164 INFO  [regionserver60020-WAL.AsyncSyncer2] wal.FSHLog: regionserver60020-WAL.AsyncSyncer2 exiting
2014-09-23 14:57:02,164 DEBUG [regionserver60020-WAL.AsyncSyncer3] wal.FSHLog: regionserver60020-WAL.AsyncSyncer3 interrupted while waiting for notification from AsyncWriter thread
2014-09-23 14:57:02,164 INFO  [regionserver60020-WAL.AsyncSyncer3] wal.FSHLog: regionserver60020-WAL.AsyncSyncer3 exiting
2014-09-23 14:57:02,164 DEBUG [regionserver60020-WAL.AsyncSyncer4] wal.FSHLog: regionserver60020-WAL.AsyncSyncer4 interrupted while waiting for notification from AsyncWriter thread
2014-09-23 14:57:02,164 INFO  [regionserver60020-WAL.AsyncSyncer4] wal.FSHLog: regionserver60020-WAL.AsyncSyncer4 exiting
2014-09-23 14:57:02,164 DEBUG [regionserver60020-WAL.AsyncWriter] wal.FSHLog: regionserver60020-WAL.AsyncWriter interrupted while waiting for newer writes added to local buffer
2014-09-23 14:57:02,164 INFO  [regionserver60020-WAL.AsyncWriter] wal.FSHLog: regionserver60020-WAL.AsyncWriter exiting
2014-09-23 14:57:02,165 DEBUG [regionserver60020] wal.FSHLog: Closing WAL writer in hdfs://logs01.yz.bj.uar.nsn:8020/apps/hbase/data/WALs/logs05.yz.bj.uar.nsn,60020,1411454591351
2014-09-23 14:57:02,591 ERROR [regionserver60020] regionserver.HRegionServer: Close and delete failed
org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /apps/hbase/data/WALs/logs05.yz.bj.uar.nsn,60020,1411454591351/logs05.yz.bj.uar.nsn%2C60020%2C1411454591351.1411454597278: File does not exist. [Lease.  Holder: DFSClient_hb_rs_logs05.yz.bj.uar.nsn,60020,1411454591351_1603749335_33, pendingcreates: 1]
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2946)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3010)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2990)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at org.apache.hadoop.hbase.RemoteExceptionHandler.decodeRemoteException(RemoteExceptionHandler.java:97)
        at org.apache.hadoop.hbase.RemoteExceptionHandler.checkThrowable(RemoteExceptionHandler.java:49)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.closeWAL(HRegionServer.java:1188)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:998)
        at java.lang.Thread.run(Thread.java:662)
2014-09-23 14:57:02,692 INFO  [regionserver60020] regionserver.Leases: regionserver60020 closing leases
2014-09-23 14:57:02,692 INFO  [regionserver60020] regionserver.Leases: regionserver60020 closed leases
2014-09-23 14:57:08,460 INFO  [regionserver60020.leaseChecker] regionserver.Leases: regionserver60020.leaseChecker closing leases
2014-09-23 14:57:08,461 INFO  [regionserver60020.leaseChecker] regionserver.Leases: regionserver60020.leaseChecker closed leases
2014-09-23 14:57:08,466 INFO  [regionserver60020.periodicFlusher] regionserver.HRegionServer$PeriodicMemstoreFlusher: regionserver60020.periodicFlusher exiting
2014-09-23 14:57:08,467 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=logs02.yz.bj.uar.nsn:2181,logs01.yz.bj.uar.nsn:2181,logs03.yz.bj.uar.nsn:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase-unsecure/replication/rs/logs05.yz.bj.uar.nsn,60020,1411454591351
2014-09-23 14:57:08,467 INFO  [regionserver60020] util.RetryCounter: Sleeping 1000ms before retry #0...
2014-09-23 14:57:09,468 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=logs02.yz.bj.uar.nsn:2181,logs01.yz.bj.uar.nsn:2181,logs03.yz.bj.uar.nsn:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase-unsecure/replication/rs/logs05.yz.bj.uar.nsn,60020,1411454591351
2014-09-23 14:57:09,468 INFO  [regionserver60020] util.RetryCounter: Sleeping 2000ms before retry #1...
2014-09-23 14:57:11,468 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=logs02.yz.bj.uar.nsn:2181,logs01.yz.bj.uar.nsn:2181,logs03.yz.bj.uar.nsn:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase-unsecure/replication/rs/logs05.yz.bj.uar.nsn,60020,1411454591351
2014-09-23 14:57:11,468 INFO  [regionserver60020] util.RetryCounter: Sleeping 4000ms before retry #2...
2014-09-23 14:57:15,469 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=logs02.yz.bj.uar.nsn:2181,logs01.yz.bj.uar.nsn:2181,logs03.yz.bj.uar.nsn:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase-unsecure/replication/rs/logs05.yz.bj.uar.nsn,60020,1411454591351
2014-09-23 14:57:15,469 INFO  [regionserver60020] util.RetryCounter: Sleeping 8000ms before retry #3...
2014-09-23 14:57:23,469 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=logs02.yz.bj.uar.nsn:2181,logs01.yz.bj.uar.nsn:2181,logs03.yz.bj.uar.nsn:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase-unsecure/replication/rs/logs05.yz.bj.uar.nsn,60020,1411454591351
2014-09-23 14:57:23,469 ERROR [regionserver60020] zookeeper.RecoverableZooKeeper: ZooKeeper getChildren failed after 4 attempts
2014-09-23 14:57:23,490 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=logs02.yz.bj.uar.nsn:2181,logs01.yz.bj.uar.nsn:2181,logs03.yz.bj.uar.nsn:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase-unsecure/rs/logs05.yz.bj.uar.nsn,60020,1411454591351
2014-09-23 14:57:23,490 INFO  [regionserver60020] util.RetryCounter: Sleeping 1000ms before retry #0...
2014-09-23 14:57:24,490 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=logs02.yz.bj.uar.nsn:2181,logs01.yz.bj.uar.nsn:2181,logs03.yz.bj.uar.nsn:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase-unsecure/rs/logs05.yz.bj.uar.nsn,60020,1411454591351
2014-09-23 14:57:24,490 INFO  [regionserver60020] util.RetryCounter: Sleeping 2000ms before retry #1...
2014-09-23 14:57:26,491 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=logs02.yz.bj.uar.nsn:2181,logs01.yz.bj.uar.nsn:2181,logs03.yz.bj.uar.nsn:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase-unsecure/rs/logs05.yz.bj.uar.nsn,60020,1411454591351
2014-09-23 14:57:26,491 INFO  [regionserver60020] util.RetryCounter: Sleeping 4000ms before retry #2...
2014-09-23 14:57:30,492 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=logs02.yz.bj.uar.nsn:2181,logs01.yz.bj.uar.nsn:2181,logs03.yz.bj.uar.nsn:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase-unsecure/rs/logs05.yz.bj.uar.nsn,60020,1411454591351
2014-09-23 14:57:30,492 INFO  [regionserver60020] util.RetryCounter: Sleeping 8000ms before retry #3...
2014-09-23 14:57:38,492 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=logs02.yz.bj.uar.nsn:2181,logs01.yz.bj.uar.nsn:2181,logs03.yz.bj.uar.nsn:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase-unsecure/rs/logs05.yz.bj.uar.nsn,60020,1411454591351
2014-09-23 14:57:38,492 ERROR [regionserver60020] zookeeper.RecoverableZooKeeper: ZooKeeper delete failed after 4 attempts
2014-09-23 14:57:38,492 WARN  [regionserver60020] regionserver.HRegionServer: Failed deleting my ephemeral node
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase-unsecure/rs/logs05.yz.bj.uar.nsn,60020,1411454591351
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:127)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:873)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.delete(RecoverableZooKeeper.java:156)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.deleteNode(ZKUtil.java:1270)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.deleteNode(ZKUtil.java:1259)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.deleteMyEphemeralNode(HRegionServer.java:1286)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:1016)
        at java.lang.Thread.run(Thread.java:662)
2014-09-23 14:57:38,502 INFO  [regionserver60020] regionserver.HRegionServer: stopping server logs05.yz.bj.uar.nsn,60020,1411454591351; zookeeper connection closed.
2014-09-23 14:57:38,502 INFO  [regionserver60020] regionserver.HRegionServer: regionserver60020 exiting
2014-09-23 14:57:38,503 ERROR [main] regionserver.HRegionServerCommandLine: Region server exiting
java.lang.RuntimeException: HRegionServer Aborted
        at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:66)
        at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:85)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:2403)
2014-09-23 14:57:38,506 INFO  [Thread-11] regionserver.ShutdownHook: Shutdown hook starting; hbase.shutdown.hook=true; fsShutdownHook=org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer@6e75d758
2014-09-23 14:57:38,506 INFO  [Thread-11] regionserver.HRegionServer: STOPPED: Shutdown hook
2014-09-23 14:57:38,507 INFO  [Thread-11] regionserver.ShutdownHook: Starting fs shutdown hook thread.
2014-09-23 14:57:38,508 INFO  [Thread-11] regionserver.ShutdownHook: Shutdown hook finished."
HBASE-11954	"When I want to create snapshot of a table, I get some exception like this:
{code}
hbase(main):004:0> snapshot 'booking', 'booking-snapshot-20140912'

ERROR: org.apache.hadoop.hbase.snapshot.HBaseSnapshotException: Snapshot { ss=booking-snapshot-20140912 table=booking type=FLUSH } had an error.  Procedure booking-snapshot-20140912 { waiting=[hbase1.data.cn,60020,1407930968832, hbase45.data.cn,60020,1408609189376, hbase23.data.cn,60020,1407930978740, hbase37.data.cn,60020,1408608587411, hbase46.data.cn,60020,1408609190515, hbase6.data.cn,60020,1407930958926, hbase44.data.cn,60020,1408609188252, hbase7.data.cn,60020,1407930960021, hbase49.data.cn,60020,1408609193897, hbase47.data.cn,60020,1408609191647, hbase21.data.cn,60020,1407930976874, hbase39.data.cn,60020,1408608669063, hbase13.data.cn,60020,1407930966976, hbase15.data.cn,60020,1407930969235, hbase19.data.cn,60020,1407930973863, hbase16.data.cn,60020,1407930971152, hbase18.data.cn,60020,1407930972762, hbase43.data.cn,60020,1408609187126, hbase12.data.cn,60020,1407930966365, hbase10.data.cn,60020,1407930963512, hbase3.data.cn,60020,1407930955378, hbase11.data.cn,60020,1407930965112, hbase24.data.cn,60020,1407930979654, hbase2.data.cn,60020,1407930954308, hbase9.data.cn,60020,1407930962354, hbase38.data.cn,60020,1408608663894, hbase40.data.cn,60020,1408608674240, hbase41.data.cn,60020,1408609184867, hbase4.data.cn,60020,1407930956670, hbase36.data.cn,60020,1408608406292, hbase17.data.cn,60020,1407930972505, hbase35.data.cn,60020,1408607982898, hbase20.data.cn,60020,1407930974993, hbase48.data.cn,60020,1408609192763, hbase22.data.cn,60020,1407930978159, hbase8.data.cn,60020,1407930961333] done=[] }
	at org.apache.hadoop.hbase.master.snapshot.SnapshotManager.isSnapshotDone(SnapshotManager.java:342)
	at org.apache.hadoop.hbase.master.HMaster.isSnapshotDone(HMaster.java:2905)
	at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java:40494)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2012)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:98)
	at org.apache.hadoop.hbase.ipc.FifoRpcScheduler$1.run(FifoRpcScheduler.java:73)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
Caused by: org.apache.hadoop.hbase.errorhandling.ForeignException$ProxyThrowable via timer-java.util.Timer@69db0cb4:org.apache.hadoop.hbase.errorhandling.ForeignException$ProxyThrowable: org.apache.hadoop.hbase.errorhandling.TimeoutException: Timeout elapsed! Source:Timeout caused Foreign Exception Start:1410453067992, End:1410453127992, diff:60000, max:60000 ms
	at org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher.rethrowException(ForeignExceptionDispatcher.java:83)
	at org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler.rethrowExceptionIfFailed(TakeSnapshotHandler.java:320)
	at org.apache.hadoop.hbase.master.snapshot.SnapshotManager.isSnapshotDone(SnapshotManager.java:332)
	... 10 more
Caused by: org.apache.hadoop.hbase.errorhandling.ForeignException$ProxyThrowable: org.apache.hadoop.hbase.errorhandling.TimeoutException: Timeout elapsed! Source:Timeout caused Foreign Exception Start:1410453067992, End:1410453127992, diff:60000, max:60000 ms
	at org.apache.hadoop.hbase.errorhandling.TimeoutExceptionInjector$1.run(TimeoutExceptionInjector.java:70)
	at java.util.TimerThread.mainLoop(Timer.java:555)
	at java.util.TimerThread.run(Timer.java:505)
{code}

I find the solution by google, and somebody say it maybe caused by the flush snapshot attempting to take a region lock. See [HBASE-7703|https://issues.apache.org/jira/browse/HBASE-7703]. But this exception  has different features.

After I flush the table, it success to create snapshot. 
{code}
hbase(main):005:0> flush 'booking'
0 row(s) in 4.5220 seconds

hbase(main):006:0> snapshot 'booking', 'booking-snapshot-20140912'
0 row(s) in 4.1270 seconds
{code}"
HBASE-11944	"Just ran an upgrade that finished but it wasn't able to move the descriptor.

The cluster had a bad /tmp permissions, but the command should have failed out."
HBASE-11818	"2014-08-25 20:55:55,899 DEBUG org.apache.hadoop.hbase.io.hfile.LruBlockCache: Stats: total=118.29 MB, free=11.87 GB, max=11.99 GB, blocks=2046, accesses=1173812, hits=96283, hitRatio=8.20%, , cachingAccesses=98331, cachingHits=96254, cachingHitsRatio=97.88%, , evictions=0, evicted=31, evictedPerRun=Infinity
2014-08-25 20:58:00,058 INFO org.apache.hadoop.hdfs.DFSClient: Could not complete file /hbase/month_keyword/f8362a663438796ee0396aba456a53e3/.tmp/1c24f7879da241d38d699f37601ce8ce retrying...
2014-08-25 20:58:00,098 INFO org.apache.hadoop.hdfs.DFSClient: Could not complete file /hbase/month_keyword/17fd4fd7b516ae6fdabd6b769db04177/.tmp/ce608124f41f49c4811edcb31b681394 retrying...
2014-08-25 20:58:00,098 INFO org.apache.hadoop.hdfs.DFSClient: Could not complete file /hbase/month_keyword/b10d7cd47416d11e62e66c76a9167183/.tmp/6f6853a11ea74df8ae03a04c49607cda retrying...

Environment锛欻adoop 1.1.2 and HBase 0.94.13
For me, this sounds like a HDFS issue, but I cannot find the root cause.
any idea is welcome.
I checked OS file descriptors, HDFS file xcievers, anything looks ok.

Thanks in advance"
HBASE-3480	"When faced with a gigabit ethernet network connection, things are pretty slow actually.  For example, let's take a 2 MB reply, using a 120MB/sec line rate, we are talking about about 16ms to transfer that data across a gige line.  This is a pretty significant amount of time.

So this JIRA is about reducing the size of the Result[] serialization.  By exploiting family and qualifier and rowkey duplication, I created a simple encoding scheme to use a dictionary instead of literal strings.  

in my testing, I am seeing some success with the sizes.  Average serialized size is about 1/2 of previous, but time to serialize on the regionserver side is way up, by a factor of 10x.  This might be due to the simplistic first implementation however.

Here is the post change size:
grep 'Serialized size' * | perl -ne '/Serialized size: (\d+?) in (\d+?) ns/ ; print $1, "" "", $2, ""\n"" if $1 > 10000;' | cut -f1 -d' ' | perl -ne '$sum += $_; $count++; END {print $sum/$count, ""\n""}'
377047.1125

Here is the pre change size:
grep 'Serialized size' * | perl -ne '/Serialized size: (\d+?) in (\d+?) ns/ ; print $1, "" "", $2, ""\n"" if $1 > 10000;' | cut -f1 -d' ' | perl -ne '$sum += $_; $count++; END {print $sum/$count, ""\n""}'
601078.505882353

That is about a 60% improvement in size.

But times are not so good, here are some samples of the old, in (size) (time in ns)
3874599 10685836
5582725 11525888

so that is about 11ms to serialize 3-5mb of data.

In the new implementation:
1898788 118504672
1630058 91133003

this is 118-91ms for serialized sizes of 1.6-1.8 MB.

"
HBASE-3206	"Something that has been bothering me for a while was to understand when a region server was being slow because of frequent and small GC pauses. I usually go into that RS's GC output, watch it going for a while then decide if it's under some kind of memory pressure. Here's an example (grepped ""Full"" from the GC log):

{noformat}
12:03:42.460-0800: [Full GC [CMS2010-11-08T12:03:43.081-0800: [CMS-concurrent-mark: 4.381/5.819 secs] [Times: user=60.51 sys=2.54, real=5.82 secs] 
12:04:06.916-0800: [Full GC [CMS2010-11-08T12:04:07.316-0800: [CMS-concurrent-mark: 4.006/5.080 secs] [Times: user=55.16 sys=2.13, real=5.08 secs] 
12:04:32.559-0800: [Full GC [CMS2010-11-08T12:04:33.286-0800: [CMS-concurrent-mark: 4.133/5.303 secs] [Times: user=53.61 sys=2.40, real=5.30 secs] 
12:05:24.299-0800: [Full GC [CMS2010-11-08T12:05:25.397-0800: [CMS-concurrent-sweep: 1.325/1.388 secs] [Times: user=4.66 sys=0.15, real=1.38 secs] 
12:05:50.069-0800: [Full GC [CMS2010-11-08T12:05:50.240-0800: [CMS-concurrent-mark: 4.831/6.346 secs] [Times: user=69.43 sys=2.76, real=6.35 secs] 
12:06:16.146-0800: [Full GC [CMS2010-11-08T12:06:16.631-0800: [CMS-concurrent-mark: 4.942/7.010 secs] [Times: user=69.25 sys=2.69, real=7.01 secs] 
12:07:08.899-0800: [Full GC [CMS2010-11-08T12:07:10.033-0800: [CMS-concurrent-sweep: 1.197/1.202 secs] [Times: user=1.96 sys=0.04, real=1.20 secs] 
12:08:01.871-0800: [Full GC [CMS2010-11-08T12:08:01.949-0800: [CMS-concurrent-mark: 4.154/5.443 secs] [Times: user=61.11 sys=2.29, real=5.44 secs] 
12:08:53.343-0800: [Full GC [CMS2010-11-08T12:08:53.549-0800: [CMS-concurrent-mark: 4.447/5.713 secs] [Times: user=65.19 sys=2.42, real=5.72 secs] 
12:09:42.841-0800: [Full GC [CMS2010-11-08T12:09:43.664-0800: [CMS-concurrent-mark: 4.025/5.053 secs] [Times: user=51.40 sys=2.02, real=5.06 secs]
{noformat}

In this case, that RS's TT was down so it was getting all the non-local maps at the end of the job at the same time... generating a >1000% CPU usage. With scanner caching set to 10k, it's easy to understand that there's memory pressure since we have all those objects in flight that we don't account for.

One solution I was thinking of was to have a sleeper thread that sleeps for 1 sec all the time and outputs when it sees that it slept for a bit more than 1 sec. Then let's say the region server records that it saw a few of those under x minutes and decides to somehow throttle the traffic.

What I often saw is that if this situation is kept unnoticed, we end up GCing more and more and in some cases I saw a region server going almost zombie for 2 hours before finally getting it's lease expired."
HBASE-1177	"During testing of HBASE-80, we uncovered a strange 40ms delay for random reads.  We ran a series of tests and found that it only happens when the client is on the same node as the RS and for a certain range of payloads (not specifically related to number of columns or size of them, only total payload).  It appears to be precisely 40ms every time.

Unsure if this is particular to our architecture, but it does happen on all nodes we've tried.  Issue completely goes away with very large payloads or moving the client.

Will post a test program tomorrow if anyone can test on a different architecture.

Making a blocker for 0.20.  Since this happens when you have an MR task running local to the RS, and this is what we try to do, might also consider making this a blocker for 0.19.1."
HBASE-3475	MetaScanner in client package is a little more involved but MetaReader does similar.   Both allow you specify a Visitor on .META. and -ROOT-.  We should dump one of them.
HBASE-11675	"The bug mentioned in http://hbase.apache.org/book.html

5.9.2.2. Major compactions change query results
鈥?..create three cell versions at t1, t2 and t3, with a maximum-versions setting of 2. So when getting all versions, only the values at t2 and t3 will be returned. But if you delete the version at t2 or t3, the one at t1 will appear again. Obviously, once a major compaction has run, such behavior will not be the case anymore...鈥?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,2014-08-05 12:03:36.518,,false,,,,,,,,,,,,,410007,,,,,Tue Aug 05 15:42:40 UTC 2014,,,,,,0|i1yjiv:,410000,,,,,,,,05/Aug/14 12:03;jmspaggi;Hi [~tobe]"
HBASE-11618	"After much digging, I finally figured out how to get the classpaths just right to run the ExportSnapshot command: 
{code}
/usr/bin/hbase -classpath '/usr/lib/hbase/lib/*:/usr/lib/hadoop/client/*' org.apache.hadoop.hbase.snapshot.ExportSnapshot -snapshot 'myTable_hbaseSnapshot_20140730' -copy-to hdfs://10.0.1.21:8020/hbase -mappers 4
{code}

only to run into the following error: 

{code}
Exception in thread ""main"" org.apache.hadoop.hbase.snapshot.CorruptedSnapshotException: Couldn't read snapshot info from:hdfs://ip-10-0-1-31.ec2.internal:8020/tmp/hbase-hbase/hbase/.hbase-snapshot/myTable_hbaseSnapshot_20140730/.snapshotinfo
{code}"
HBASE-7372	"January 1st is deadline for changing how we publish our website.  We may no longer rsync out to people.apache.org.  Apache infrastructure supplies two options here: http://www.apache.org/dev/project-site.html  We could redo our site in apache cms format.  Or we could just use svnpubsub and keep on w/ how the site is currently generated and on checkin, have it autopublished.  I'll go the latter route unless I hear otherwise.

For svnpubsub, we need to point apache infrastructure at a directory that has our checkedin site in it.  I was thinking ${hbasedir}/hbase.apache.org

Let me raise this on the dev list too."
HBASE-11563	"STEPS:
-------
1. start HBase cluster.
2. Navigate to the RegionServer UI.
3. Instance not found exception is throwing.
{code}
2014-07-22 12:16:41,504 DEBUG [257789301@qtp-605399375-0] util.DirectMemoryUtils: Failed to retrieve nio.BufferPool direct MemoryUsed attribute.
javax.management.InstanceNotFoundException: java.nio:type=BufferPool,name=direct
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1094)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:662)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:638)
	at org.apache.hadoop.hbase.util.DirectMemoryUtils.<clinit>(DirectMemoryUtils.java:72)
	at org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmplImpl.__jamon_innerUnit__memoryStats(ServerMetricsTmplImpl.java:217)
	at org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmplImpl.renderNoFlush(ServerMetricsTmplImpl.java:68)
	at org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl.renderNoFlush(ServerMetricsTmpl.java:133)
	at org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmplImpl.renderNoFlush(RSStatusTmplImpl.java:104)
	at org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.renderNoFlush(RSStatusTmpl.java:170)
	at org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.render(RSStatusTmpl.java:161)
	at org.apache.hadoop.hbase.regionserver.RSStatusServlet.doGet(RSStatusServlet.java:56)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
	at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:1089)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)

{code}"
HBASE-3588	"The ReadWriteConsistencyControl (RWCC) mechanism facilitates making a set of memstore updates atomically visible to readers. Also, the rwcc.completeMemstoreInsert() blocks till the memstore read point advances to the current writeNumber. This is done to ensure that if an application that does a put immediately issues a new get call for the same key, then the get should see the values inserted by the previous call to put. The current implementation assumes this worst-case and penalizes the put rpc to not return to the client until the read point advances to this transaction's write number.

In many use-cases, the application never actually issues a get for the most recent put that it inserted. In this case, it would be nice if we can transfer the penalty (of blocking) to the get call that follows the initial put."
HBASE-3567	"There was [this Bug-Report (HBASE-2791)|https://issues.apache.org/jira/browse/HBASE-2791], which was dismissed as 'redone in the master rewrite'. But for the InterruptedException nothing has changed.
InterruptedException is swallowed or wrapped in other Exceptions at many places, which is just not right. Please read [this|http://www.ibm.com/developerworks/java/library/j-jtp05236.html]."
HBASE-3467	"Timestamps are allowed to be negative values, since they're stored as type long.  However, using the Get object to search for versions by setting setMaxVersions() misses them.

I'm not sure where this code lives, but my suspicion is that the code which scans for matching value timestamps has a min value of 0 when it should be using {{java.lang.Long.MIN_VALUE}}."
HBASE-3491	"Hello.
I'm working in progress hbase-jdo-util
You can download here. 
http://code.google.com/p/hbase-jdo/downloads/list



- connect table
- table scan
- setting(configuration)
- hadoop explorer

I'll update hbase wiki page also.
Thanks."
HBASE-3486	"we have unit tests that cover durability, but they did not detect the problem outlined in HBASE-3481.  We should fix those tests."
HBASE-3463	"It would be great if the HBase management user interface provided more bullet-proof reporting in terms of region counts and status. The regions counts go up and down and different views show different counts and they don't always add up. It makes one less than confident of what is going on. 

Cluster wide And per table it would be great to get summary counts of all regions in the various states. It would also be useful to see this state in the description of the region. There is a compaction count, but what about in split, memstore flush, read/write lock, disabled, etc. Fundamentally this is the health monitor of the system and as a dba one really needs to know the 100% count of regions and where they are all at in terms of availability. Are they disabled, blocked for writes, blocked for reads, in compaction, etc. etc.

The first step is to define and document exactly what states a region could be in and what impact that state could have on performance, write blocking, read blocking, compaction, disabling, etc. etc. This needs to be defined and well documented. Once these states are defined and well understood (and explain every problem one could have reading/writing/pausing/disabling) then adding to the UI view to show a count of all regions within each state (with a percentage) and also report this state in the detailed list of regions it would give one an holistic view of exactly what is going on. "
HBASE-3461	"Hello.
first of all, thanks to hbase members so much ,
you helped me a lot.

I wrote simple jdo module for hbase newbie. I wish this module can help them.

Some features
- simple jdo(used reflection)
- HTable pool ( I already know HTablePool in habse )
- simple query classes ( insert,delete,update,select)
- some convinent mehtods.
- table sequence generator

I want to contribute this module to hbase
absolutely, I'll do upgrade that more than more.

again, thanks for help.

[http://code.google.com/p/hbase-jdo/]
---------------------------------------------
{code} 

AbstractHBaseDBO dbo = new HBaseDBOImpl();
		
		//drop if table is already exist.
		if(dbo.isTableExist(""user"")){
			dbo.deleteTable(""user"");
		}
		
		//create table
		dbo.createTableIfNotExist(""user"",HBaseOrder.DESC,""account"");
		//dbo.createTableIfNotExist(""user"",HBaseOrder.ASC,""account"");
		
		//create index.
		String[] cols={""id"",""name""};
		dbo.addIndexExistingTable(""user"",""account"",cols);
		
		//insert
		InsertQuery insert = dbo.createInsertQuery(""user"");
		UserBean bean = new UserBean();
		bean.setFamily(""account"");
		bean.setAge(20);
		bean.setEmail(""ncanis@gmail.com"");
		bean.setId(""ncanis"");
		bean.setName(""ncanis"");
		bean.setPassword(""1111"");
		insert.insert(bean);
		
		//select 1 row
		SelectQuery select = dbo.createSelectQuery(""user"");
		UserBean resultBean = (UserBean)select.select(bean.getRow(),UserBean.class);
		
		// select column value.
		String value = (String)select.selectColumn(bean.getRow(),""account"",""id"",String.class);
		
		// search with option (QSearch has EQUAL, NOT_EQUAL, LIKE)
		// select id,password,name,email from account where id='ncanis' limit startRow,20
		HBaseParam param = new HBaseParam();
		param.setPage(bean.getRow(),20);
		param.addColumn(""id"",""password"",""name"",""email"");
		param.addSearchOption(""id"",""ncanis"",QSearch.EQUAL);
		select.search(""account"", param, UserBean.class);
		
		// search column value is existing.
		boolean isExist = select.existColumnValue(""account"",""id"",""ncanis"".getBytes());
		
		// update password.
		UpdateQuery update = dbo.createUpdateQuery(""user"");
		Hashtable<String, byte[]> colsTable = new Hashtable<String, byte[]>();
		colsTable.put(""password"",""2222"".getBytes());
		update.update(bean.getRow(),""account"",colsTable);
		
		//delete
		DeleteQuery delete = dbo.createDeleteQuery(""user"");
		delete.deleteRow(resultBean.getRow());
	
		////////////////////////////////////
		// etc
		
		// HTable pool with apache commons pool
		// borrow and release. HBasePoolManager(maxActive, minIdle etc..)
		IndexedTable table = dbo.getPool().borrow(""user"");
		dbo.getPool().release(table);
		
		// upload bigFile by hadoop directly.
		HBaseBigFile bigFile = new HBaseBigFile();
		File file = new File(""doc/movie.avi"");
		FileInputStream fis = new FileInputStream(file);
		Path rootPath = new Path(""/files/"");
		String filename = ""movie.avi"";
		bigFile.uploadFile(rootPath,filename,fis,true);
		
		// receive file stream from hadoop.
		Path p = new Path(rootPath,filename);
		InputStream is = bigFile.path2Stream(p,4096);

{code} "
HBASE-3459	"usually, we need to find column's value is existing.

in db) select count(*)  from user where id='ncanis'

if we use EqualsColumnValueGetFilter filter

we can find out column value is existing.
{code}
----------------------
	public EqualsColumnValueGetFilter(final byte[] colName, final byte[] value, final int limit) {
		this.colName = colName;
		this.value = value;
		this.limit = limit;
	}

------------------------
{code}


"
HBASE-2796	"Backport the hbase-2707 fix to the 0.20 branch.  If 2707 happens, it hoses cluster..."
HBASE-3165	"in an attempt to improve the profile of the serialization of results in the regionserver side I did a large number of things to reduce buffer copies, improve the API usage efficiency (using the BB API directly) and so on.

Using a YCSB config like so:
recordcount=10000
#recordcount=5
operationcount=1000
workload=com.yahoo.ycsb.workloads.CoreWorkload

readallfields=true


readproportion=0
updateproportion=0
scanproportion=1
insertproportion=0

fieldlength=10
fieldcount=100

requestdistribution=zipfian


scanlength=300
scanlengthdistribution=zipfian

threadcount=1

columnfamily=data

Doing a medium sized scan of 1-300 rows.

Top line performance was at about 67ms, but these micro improvements didnt budge that needle, and it didnt change the scale of the CPU profiler - ie: cpu time spent in serialization was the same.

Since then I also made an improvement to HBase-YCSB which may have been masking the performance gains.  I have suspended this work in favor of 0.90 pre-release work for now."
HBASE-2237	"From the list from Erik Rozendaal:

{code}

We're getting about 2500 gets/sec max per regionserver when it serves data from the block cache using HBase 0.20.3. All data is in the same region and the region has just one store file for the family we're querying. HBASE-2180 didn't seem to help here. At this point the region server is using about 1.5 cores. Since we have 8 cores available the bottleneck does not seem to be CPU.

The number of clients does not seem to matter (we tested with 1-4 clients).

The only other bottleneck that I can think of is network, but we test on a local 1 Gbit LAN. Our web server can handle 15,000 requests/sec, so that doesn't seem to be the case.

Maybe there is some synchronization going limiting the CPU scalability of a single region server?
{code}

If same speed whether 1 or 4 clients, it would seem to indicate a synchronization issue."
HBASE-2478	"HDFS keeps a separate ""checksum"" file for every  block. By default, io.bytes.per.checksum is set at 512, and the checksums are 4 bytes... i.e. for every 512 bytes of data in the block we maintain a 4 byte checksum. For 4TB of data, for instance, that's about 31GB of checksum data.

A read that needs to read a small section (such as a 64k HFile block) from a HDFS block, especially on a cold access, is likely to end up doing two random disk reads--- one from the data file for the block and one from the checksum file.

A though was that instead of keeping a checksum for every 512 bytes, given  that HBase will interact with HDFS on reads at the granularity of HBase block size (typically 64k, but smaller if compressed), should we consider keeping checksums at a coarser granularity (e.g, for every 8k bytes) for HFiles?  The advantage
with this would be that the checksum files would be much smaller (in proportion to the data) and the hot working set for ""checksum data""  should fit better in the OS buffer cache (thus eliminating a good majority of the disk seeks for checksum data).

The intent of the JIRA is to experiment with different settings for ""io.bytes.per.checksum"" for HFiles. 

Note: For the previous example, of 4TB of data, with an io.bytes.per.checksum setting of 8k, the size of the checksum data would drop to about 2Gig.

Making the io.bytes.per.checksum too big might reduce the effectiveness of the checksum. So that needs to be taken into account as well in terms of determining a good value.

[For HLogs files, on the other hand, I suspect we would want to leave the checksum at finer granularity because my understanding is that if we are doing lots of small writes/syncs (as we do to HLogs), finer grained checksums are better (because the code currently doesn't do a rolling checksum, and needs to rewind to the nearest checksum block boundary and recomputed the checksum on every edit).]


"
HBASE-3428	"I know I am new Hbase user.
When I learn Hbase, I was to hard about learning that.
I couldn't find well made examples

now, I wrote somd examples for Hbase starter.

I wish this examples can help you.

few more days later, I'll write more good examples.

sources.

AbstractHBase.java
HBaseClient.java
StressTest.java

Thanks."
HBASE-3325	"Currently when the master splits logs, it outputs all edits it finds, even those that have already been obsoleted by flushes. At replay time on the RS we discard the edits that have already been flushed.

We could do a pretty simple optimization here - basically the RS should replicate a map ""region id -> last flushed seq id"" into ZooKeeper (this can be asynchronous by some seconds without any problems). Then when doing log splitting, if we have this map available, we can discard any edits found in the logs that were already flushed, and thus output a much smaller amount of data."
HBASE-3397	The 'getRow' method can be misleading and should be renamed to 'getRowKey' to better describe its functionality.
HBASE-3385	"table.jsp should allow user to initiate reassignment of region(s) whose row in .META. doesn't have serverinfo
There should be two options for such request:
1. regions without serverinfo are assigned
2. all regions for the table are reassigned (round-robin)

Option 2 is especially useful when table's regions don't have even distribution among region servers."
HBASE-3329	"After a RS dies or the cluster goes down and we are recovering, we first split HLogs into the logs for the regions. Then the region servers that host the regions replay the logs and open the regions.

This can be made more efficient by directly creating HFiles from the HLogs (instead of producing a split HLogs file)."
HBASE-3242	"Currently, our memstore flush algorithm is pretty trivial.  We let it grow to a flushsize and flush a region or grow to a certain log count and then flush everything below a seqid.  In certain situations, we can get big wins from being more intelligent with our memstore flush algorithm.  I suggest we look into algorithms to intelligently handle HLog compactions.  By compaction, I mean replacing existing HLogs with new HLogs created using the contents of a memstore snapshot.  Situations where we can get huge wins:

1. In the incrementColumnValue case,  N HLog entries often correspond to a single memstore entry.  Although we may have large HLog files, our memstore could be relatively small.
2. If we have a hot region, the majority of the HLog consists of that one region and other region edits would be minuscule.

In both cases, we are forced to flush a bunch of very small stores.  Its really hard for a compaction algorithm to be efficient when it has no guarantees of the approximate size of a new StoreFile, so it currently does unconditional, inefficient compactions.  Additionally, compactions & flushes suck because they invalidate cache entries: be it memstore or LRUcache.  If we can limit flushes to cases where we will have significant HFile output on a per-Store basis, we can get improved performance, stability, and reduced failover time."
HBASE-2584	"Currently HBase clients can only acquire row locks via the blocking lockRow() method in HTable. As ryan described on the mailing list, relying on this method in rare highly contended situations can lead to (temporary) deadlock. This deadlock occurs if a client acquires the lock, and a large number of other clients attempt to acquire the lock and block. Each blocked client awaiting lock acquisition consumes one of the limited I/O handler threads on the regionserver. When lock holder wishes to release the lock, he will be unable to as all I/O threads are currently serving clients that are blocking on lock acquisition -- and thus no I/O threads are open to process the unlock request.

To avoid deadlock situations such as the one described above, I have added support for 'tryLock' in HTable (and on the regionservers). The 'tryLock' method will attempt to acquire a row lock. In the event that a lock is already held, tryLock immediately returns null rather than blocking and waiting for the lock to be acquire. Clients can then implement their own backoff/retry policy to re-acquire the lock, determine their own timeout values, etc based on their application performance characteristics rather than block on the regionserver and tie up precious I/O regionserver handler threads for an indefinite amount of time. "
HBASE-3300	"Background: For use cases that explicitly set versions, after a delete, it is not possible to insert values with older versions. So for example if you did a puts (v1) and delete (@ v2), subsequent puts with an older version (e.g., v1) will not take effect since the delete has a higher version.

{code}
#1. PUT(row, col, version1,  old-value)
#2. DELETE(row, col, version2)
#3. PUT(row, col, version1, new-value)
{code}

The row/col stays deleted, and this is expected behavior since the delete has a higher timestamp.

Feature Request: It would be good to provide a ""force"" delete mechanism -- something that allows the row or a specific column to be started with a clean slate. i.e. forget about everything that happened to this item earlier, and lets you start afresh. Without this there is no good cleanup mechanism for use cases that set versions explicitly.

[Note: The only workaround for this depends on a subtle implementation detail that major compactions discard delete markers. So if a major compaction happened between steps #2 & #3, then you would in fact be able to put a value with an older version.]

Thoughts?


"
HBASE-3246	"In the new Increment class, the API to add columns is {{addColumn()}}.  If you do this multiple times for an individual column, the amount to increment by is replaced.  I think this is the right way for this method to work and it is javadoc'd with the behavior.

We should add a new method, {{incrementColumn()}} which will increment any existing amount for the specified column rather than replacing it."
HBASE-3254	"We are running HBase on EC2 and I'm trying to get a client external from EC2 to connect to the cluster.  But, each of the nodes appears to be publishing its IP address into zookeeper.  The problem is that the nodes on EC2 see a 10. IP address that is only resolvable inside of EC2.

Specifically for EC2, there is a DNS name that will resolve properly both externally and internally, so it would be nice if I could tell each of the processes what host to publish into zookeeper via a property.  As it stands, I have to do ssh tunnelling/muck with the hosts file in order to get my client to connect.
 
This problem could occur anywhere that you have a different DNS entry for public vs. private access.  That might only ever happen on EC2, but it might happen elsewhere.  I don't really know :)."
HBASE-3251	"We saw this in our integration test log - packageindex table was 'broekn':
{code}
2010-11-19 05:12:42,216 Thread-20 ERROR [StripedHBaseTable] Could not create packageindex
org.apache.hadoop.hbase.TableExistsException: org.apache.hadoop.hbase.TableExistsException: packageindex
	at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.java:799)
	at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.java:763)
	at sun.reflect.GeneratedMethodAccessor25.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:657)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:998)
...
2010-11-19 05:12:42,218 Thread-20 INFO  [HBasePackageIndexTableMapperNew] Creating table packageindex - Done
2010-11-19 05:12:42,235 Thread-20 INFO  [CodecPool] Got brand-new decompressor
2010-11-19 05:12:42,262 Thread-20 INFO  [HBasePackageIndexTableMapperNew] OnClose called
2010-11-19 05:12:42,263 Thread-20 WARN  [LocalJobRunner] job_local_0001
org.apache.hadoop.hbase.TableNotFoundException: packageindex
	at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegionInMeta(HConnectionManager.java:698)
	at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:634)
	at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:601)
	at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:134)
	at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:112)
{code}

In HConnectionManager.listTables():
{code}
           byte[] value = result.getValue(CATALOG_FAMILY,
REGIONINFO_QUALIFIER);
           HRegionInfo info = null;
           if (value != null) {
             info = Writables.getHRegionInfo(value);
           }
           // Only examine the rows where the startKey is zero length
           if (info != null && info.getStartKey().length == 0) {
             uniqueTables.add(info.getTableDesc());
           }
{code}
For a broken table, there would be a row in .META (see below). but the table wouldn't be included in uniqueTables.

We need a way for listTables() to mark the broken table and return it so that master.jsp can show the table in prominent way.

{code}
 packageindex,E70888DD48276D column=info:regioninfo, timestamp=1290188566363, value=REGION => {NAME => 'packag
 FAD4D26FEB08DC7045,12901630 eindex,E70888DD48276DFAD4D26FEB08DC7045,1290163034864', STARTKEY => 'E70888DD4827
 34864                       6DFAD4D26FEB08DC7045', ENDKEY => 'E83A8362462AF0D097810F96ED7103C2', ENCODED => 2
                             080544777, OFFLINE => true, TABLE => {{NAME => 'packageindex', FAMILIES => [{NAME
                              => 'i', COMPRESSION => 'GZ', VERSIONS => '1', TTL => '31536000', BLOCKSIZE => '6
                             5536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'u', COMPRESSION =>
                             'GZ', VERSIONS => '1', TTL => '31536000', BLOCKSIZE => '65536', IN_MEMORY => 'fal
                             se', BLOCKCACHE => 'true'}]}}
{code}

Here is what led to broken table in our cluster.

2010-11-19 12:49:23,067 main INFO  [PackageIndexTableTest]
[10:57am] tyu: Deleting packageindex content ...

From hbase-hadoop-regionserver-us01-ciqps1-grid05.ciq.com.log:
{code}
2010-11-19 12:49:41,119 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Caches flushed, doing commit now (which includes update scanners)
2010-11-19 12:49:41,121 INFO org.apache.hadoop.hbase.regionserver.HRegion: Finished memstore flush of ~16.0k for region .META.,,1 in 83ms, sequence id=48465684, compaction requested=true
2010-11-19 12:49:41,121 DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitThread: Compaction requested for region .META.,,1/1028785192 because: regionserver/10.202.50.105:60020.cacheFlusher
2010-11-19 12:54:11,353 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Scanner 6416258050001207387 lease expired
2010-11-19 12:54:11,353 DEBUG org.apache.hadoop.hbase.io.hfile.LruBlockCache: Cache Stats: Sizes: Total=633.5422MB (664317096), Free=161.05789MB (168881432), Max=794.60004MB (833198528), Counts: Blocks=75276, Access=90464772, Hit=86854034, Miss=3610738, Evictions=222, Evicted=2121113, Ratios: Hit Ratio=96.00868225097656%, Miss Ratio=3.9913196116685867%, Evicted/Run=9554.5634765625
2010-11-19 12:54:11,353 WARN org.apache.hadoop.hbase.util.Sleeper: We slept 269945ms, ten times longer than scheduled: 10000
2010-11-19 12:54:11,353 DEBUG org.apache.hadoop.hbase.io.hfile.LruBlockCache: Cache Stats: Sizes: Total=633.5422MB (664317096), Free=161.05789MB (168881432), Max=794.60004MB (833198528), Counts: Blocks=75276, Access=90464772, Hit=86854034, Miss=3610738, Evictions=222, Evicted=2121113, Ratios: Hit Ratio=96.00868225097656%, Miss Ratio=3.9913196116685867%, Evicted/Run=9554.5634765625
2010-11-19 12:54:11,353 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Scanner -1270857692790249130 lease expired
2010-11-19 12:54:11,354 DEBUG org.apache.hadoop.hbase.io.hfile.LruBlockCache: Cache Stats: Sizes: Total=633.5422MB (664317096), Free=161.05789MB (168881432), Max=794.60004MB (833198528), Counts: Blocks=75276, Access=90464772, Hit=86854034, Miss=3610738, Evictions=222, Evicted=2121113, Ratios: Hit Ratio=96.00868225097656%, Miss Ratio=3.9913196116685867%, Evicted/Run=9554.5634765625
...
2010-11-19 12:54:11,354 WARN org.apache.zookeeper.ClientCnxn: Exception closing session 0x12c520945433100 to sun.nio.ch.SelectionKeyImpl@78317d11
java.io.IOException: TIMED OUT
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:906)
2010-11-19 12:54:11,391 WARN org.apache.zookeeper.ClientCnxn: Exception closing session 0x12c520945432f46 to sun.nio.ch.SelectionKeyImpl@727d3468
java.io.IOException: TIMED OUT
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:906)
2010-11-19 12:54:11,354 ERROR org.apache.hadoop.hbase.regionserver.HRegionServer:
org.apache.hadoop.hbase.UnknownScannerException: Name: 6416258050001207387 on us01-ciqps1-grid05.carrieriq.com,60020,1290003836762
        at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1885)
        at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:657)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:998)
2010-11-19 12:54:11,354 ERROR org.apache.hadoop.hbase.regionserver.HRegionServer:
org.apache.hadoop.hbase.UnknownScannerException: Name: -1270857692790249130 on us01-ciqps1-grid05.carrieriq.com,60020,1290003836762
        at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1885)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1873)
        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:657)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:998)
2010-11-19 12:54:11,354 WARN org.apache.hadoop.hbase.regionserver.HRegionServer: unable to report to master for 270306 milliseconds - retrying
2010-11-19 12:54:11,415 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 16 on 60020, call next(6416258050001207387, 100) from 10.202.36.42:37477: error: org.apache.hadoop.hbase.UnknownScannerException: Name: 6416258050001207387 on us01-ciqps1-grid05.carrieriq.com,60020,1290003836762
org.apache.hadoop.hbase.UnknownScannerException: Name: 6416258050001207387 on us01-ciqps1-grid05.carrieriq.com,60020,1290003836762
        at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1885)
        at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:657)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:998)
...
{code}"
HBASE-3226	"It exists in HBase 0.20.5 , and also exist in TRUNK
org.apache.hadoop.hbase.filter.CompareFilter.doCompare(CompareOp, WritableByteArrayComparable, byte[], int, int),
  ---------------------------------------------
  switch (compareOp) {
      case LESS:
        return compareResult <= 0;   
      case LESS_OR_EQUAL:     
        return compareResult < 0;
      case EQUAL:
        return compareResult != 0;
      case NOT_EQUAL:
        return compareResult == 0;
      case GREATER_OR_EQUAL:
        return compareResult > 0;
      case GREATER:
        return compareResult >= 0;
      default:
        throw new RuntimeException(""Unknown Compare op "" +
          compareOp.name());
    }
-----------------------------------------------------
!!! modified code:
    switch (compareOp) {
      case LESS:
        return compareResult < 0;
      case LESS_OR_EQUAL:
        return compareResult <= 0;
      case EQUAL:
        return compareResult == 0;
      case NOT_EQUAL:
        return compareResult != 0;
      case GREATER_OR_EQUAL:
        return compareResult >= 0;
      case GREATER:
        return compareResult >0;
      default:
        throw new RuntimeException(""Unknown Compare op "" +
          compareOp.name());
    }"
HBASE-3106	"Each region's memtable size is 64M, global memtable limit is 4G on one region server
In heavy write scenario, especially disable WAL, sequential put  fills memtable to full very quickly, and will generates too many storefiles in a short time.
Then there are more and more compaction tasks put into compactionQueue, single thread excutes compaction task too slowly, finally forms a vicious cycle.
This will block some region's data importing for a long time

We simplely using ThreadPoolExecutor as multi-thread compaction sulution
(As we know, 0.90 or 0.92 will optimize memflush, compaction, open, close...etc using multi-thread, but this magnificent version hasn't bee released yet)"
HBASE-3099	"Right now log splitting is slower than we'd like.  The slow pace of log splitting is one of the reasons why we have to keep a short, bounded, limit of the outstanding log files.  It would be nice to up that limit, to allow perhaps hundreds of logs.  It would increase efficiency because we would not be force-flushing regions at non-ideal sizes.

But more data means more to process.  Except that not all of the logs for a regionserver are actually useful.  This is because some regions got flushed before the oldest log was trimmed.  So during log recovery if we read the most recent sequenceid, we could skip, during log splitting (in the master), those entries and avoid writing them to the per-region log recovery.  It would reduce the IO by part, and if our serialization/deser code was clever we might be able to avoid deserializing much.  

It's not clear how effective or worthwhile this might be."
HBASE-3084	"Nicolas reviewed HBASE-2437 after it had been committed and raised some issues that need addressing.  The main issue is that splitLog will ignore user interrupts (aka: InterruptException).  Currently, data loss can occur (with 'skip.errors' == true) if a user calls 'kill <master-pid>"" during splits because the code will consider the interrupted file completely split.  If the master receives an interrupt during splitLog, it needs to abort splitting, delete all partially-complete split files, and exit gracefully.  "
HBASE-3075	"New master makes it so enable/disable show as near instantaneous but behind the scenes we still have to do the region close/opens.  These can take time.  HBASE-3063 adds into the disable table handler a wait on regions leaving regions in transition but, what if the user did enable/disable/enable/disable then delete.  Turns out we don't do this well.  It confuses.  See the TestThriftServer where we've disabled such a sequence.  This issue is about working on making delete work better; e.g. why bother closing stuff if we're going to delete it --- just abort the tables regions and save all the flushing, etc., that slows the close-before-delete."
HBASE-2492	"See tail of hbase-2180 for detail on this issue.  If a lot of random reading, we start to us up descriptors because sockets stuck in TIME_WAIT.  Workaround described over in hbase-2180.  This issue covers fix for hdfs so we don't open a socket each pread."
HBASE-2789	"If HBase config is modified when HBase cluster is running, the changes wouldn't propagate to region servers after restarting cluster.

This is different from hadoop behavior where changes get automatically copied to data nodes.

This feature is desirable when enabling JMX, e.g."
HBASE-2957	"Is there a reason to hold on to the row-lock while waiting for the WAL-sync to be completed by the logSyncer thread?

I think data consistency will be guaranteed even if the following happens (a) the row lock is held while the row is updated in memory (b) the row lock is released after queuing the KV record for WAL-syncing (c) the log-sync system guarantees that the log records for any given row are synced in order (d) the HBase client only receives a success notification after the sync completes (no change from the current state)

I think this should be a huge win. For my use case, and I am sure for others,  the handler thread spends the bulk of its row-lock critical section  time waiting for sync to complete.

Even if the log-sync system cannot guarantee the orderly completion of sync records, the ""Don't hold row lock while waiting for sync"" option should be available to HBase clients on a per request basis."
HBASE-2891	"Currently rowcounter accepts one table for each run.
It is desirable to add capability to process multiple tables in one run.
"
HBASE-2693	"Need to also be clear that if need patched hadoop, then the patched hadoop needs to be put under hbase/lib."
HBASE-2686	"J-D just pointed out the fact that LogRollListener Interface cannot listen.  It has a single method named logRollRequested.   Lets fix.  Rename the interface LogRoll (and the method name while we're at it... its past tense but returns a void?)

Lets fix this stuff.  It makes the code base hard to grok."
HBASE-2659	
HBASE-1969	"The issue that HBASE-1626 tried to fix is that we can hand in Put or Delete instances to the TableOutputFormat. So the explicit Put reference was changed to Writable in the process. But that does not work as expected:

{code}09/11/04 13:35:56 INFO mapred.JobClient: Task Id : attempt_200911031030_0004_m_000013_2, Status : FAILED
java.io.IOException: Type mismatch in value from map: expected org.apache.hadoop.io.Writable, recieved org.apache.hadoop.hbase.client.Put
        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:812)
        at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:504)
        at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
        at com.worldlingo.hadoop.mapred.RestoreTable$RestoreMapper.map(RestoreTable.java:140)
        at com.worldlingo.hadoop.mapred.RestoreTable$RestoreMapper.map(RestoreTable.java:69)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:583)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305){code}

The issue is that the MapReduce framework checks not polymorphic for the type using ""instanceof"" but with a direct class comparison. In MapTask.java you find this code

{code}
    public synchronized void collect(K key, V value, int partition
                                     ) throws IOException {
      reporter.progress();
      if (key.getClass() != keyClass) {
        throw new IOException(""Type mismatch in key from map: expected ""
                              + keyClass.getName() + "", recieved ""
                              + key.getClass().getName());
      }
      if (value.getClass() != valClass) {
        throw new IOException(""Type mismatch in value from map: expected ""
                              + valClass.getName() + "", recieved ""
                              + value.getClass().getName());
      }
      ... {code}

So it does not work using a Writable as the MapOutputValueClass for the job and then hand in a Put or Delete! The test case TestMapReduce did not pick this up as it has this line in it

{code}
      TableMapReduceUtil.initTableMapperJob(
        Bytes.toString(table.getTableName()), scan,
        ProcessContentsMapper.class, ImmutableBytesWritable.class, 
        Put.class, job);{code}

which sets the value class to Put

{code}if (outputValueClass != null) job.setMapOutputValueClass(outputValueClass);{code}

To fix this (for now) one can set the class to Put the same way or explicitly in their code 

{code}job.setMapOutputValueClass(Put.class);{code}
 
But the whole idea only seems feasable if a) the Hadoop class is amended to use ""instanceof"" instead (lodge Hadoop MapRed JIRA issue?) or b) we have a combined class that represent a Put *and* a Delete - which seems somewhat wrong, but doable. It would only really find use in that context and would require the user to make use of it when calling context.write(). This is making things not easier to learn.

Suggestions?"
HBASE-2634	"HRegion now has four different lock objects:
- splitsAndClosesLock (RWLock)
- newScannerLock (RWLock)
- updatesLock (RWLock)
- splitLock (Object)

It's not well documented what the purpose of each is, and I imagine we may be able to get rid of at least one."
HBASE-2554	"At http://hadoop.apache.org/hbase/docs/r0.20.4/api/org/apache/hadoop/hbase/HTableDescriptor.html, we currently have both HColumnDescriptor[] getColumnFamilies() and Collection<HColumnDescriptor> getFamilies(); perhaps only one is needed?"
HBASE-2362	"Right now, we get one KeyValue at a time and run it through the matcher even if the query is to fetch the entire row (or even an entire column family for a key). This causes the blocks to be fetched serially from HDFS. We can optimize this by looking at the block map, and fetching all the blocks spanned by the key in one request to HDFS."
HBASE-2159	Its an error if HFOF is passed a KV with a timestamp of LATEST_TIMESTAMP (see HBASE-2157).  Add a check for it to HFOF and throw an exception if we see it.
HBASE-2015	"HBase does not get privileges to run access controlled code, such as when using Sockets.

------------------------------------------
A DataNucleus, HBase user posted this:
http://www.jpox.org/servlet/forum/viewthread_thread,5869

He is having trouble to run DataNucleus HBase in a JVM running with
security manager activated.

The permission required for the codebase is a SocketPermission,
however the HBase client api does not run in a privileged block.

To workaround we've added the doPrivileged block in DataNucleus, and
the user grants datanucleus-hbase jar the SocketPermission.

However, I think you should add these doPrivileged blocks to HBase
code. Could you please look at these, and let me know when it's
solved, so we can remove the doPrivileged blocks from DataNucleus
code?"
HBASE-1353	The regex filters are odd in a world of byte arrays.  This is brought home in the world of KeyValue -- hbase-1234 -- where we are going out of our way to avoid Object creations.  The regex fitlers are extremely expensive to run.   The regex filters should be redone otherwise or at least get some attention to clean them up.
HBASE-1510	"instead of configuring IPs in zoo.conf, allow retrieving the IPs from DNS.  This way we don't have to supply zoo.conf or any hbase-site.xml to clients.  

eg, one quorum might ahve a dns name of 'quorum1', the hbase path, you could specify the cluster in code like:
HTable table = new HTable(""/quorum1/hbase/table_name"");

and avoid having to distribute hbase-site.xml or zoo.cfg."
HBASE-1248	"I have an HBase table where items are keyed by a sequential row id. When doing bulk appends to this table, the last region of the table gets pretty large until enough compactions can be done to sort everything out.

In this type of case, it'd be better if when the region serving the largest keys doesn't split at the midkey, but on the last key. One way to implement this would be by saying that when the top region reaches MAX_REGION_SIZE/2, create a new region, with the lower half getting all the data and the top half empty. For bulk sequential inserts, this should avoid the need for any compactions."
HBASE-893	Client-side logging shows on console if you are using shell but otherwise goes to /dev/null.  Set up logging so it goes somewhere -- into a file under logs or some such.
HBASE-2585	"While inserting a large number of rows into a HBase table (approx 100M) we got this error many times (usually do to multiple attempts) on one of our region servers:

2010-05-19 16:51:30,745 WARN org.apache.hadoop.hbase.regionserver.HRegionServer: Attempt=1
java.io.IOException: java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.hbase.master.ServerManager.assignSplitDaughter(ServerManager.java:535)
        at org.apache.hadoop.hbase.master.ServerManager.processSplitRegion(ServerManager.java:512)
        at org.apache.hadoop.hbase.master.ServerManager.processMsgs(ServerManager.java:463)
        at org.apache.hadoop.hbase.master.ServerManager.processRegionServerAllsWell(ServerManager.java:414)
        at org.apache.hadoop.hbase.master.ServerManager.regionServerReport(ServerManager.java:323)
        at org.apache.hadoop.hbase.master.HMaster.regionServerReport(HMaster.java:724)
        at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:657)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)

        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at org.apache.hadoop.hbase.RemoteExceptionHandler.decodeRemoteException(RemoteExceptionHandler.java:94)
        at org.apache.hadoop.hbase.RemoteExceptionHandler.checkThrowable(RemoteExceptionHandler.java:48)
        at org.apache.hadoop.hbase.RemoteExceptionHandler.checkIOException(RemoteExceptionHandler.java:66)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:577)
        at java.lang.Thread.run(Thread.java:619)
"
HBASE-2464	"When the master is told to shut down, it calls stopScanners() before setting master.closed. If root isn't assigned, the root scanner gets stuck in the loop in RegionManager.waitForRootRegionLocation. "
HBASE-1120	"{code}
hbase(main):003:0> close_region '-ROOT-,,0'
NativeException: java.io.IOException: java.io.IOException: java.lang.NullPointerException
 at org.apache.hadoop.hbase.master.HMaster.modifyTable(HMaster.java:830)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
 at java.lang.reflect.Method.invoke(Method.java:597)
...
{code}

Presumption is that its a region that can be found in .META. whereas, I have a cluster where master and regionserver don't agree on -ROOT- and want to close '-ROOT-,,0' but get the NPE when I try."
HBASE-11494	"Table鈥檚 cell having a large number of versions, it works fine when I read 100K VERSIONS from the cell, but regionServer gives exception when I attempted to read upto 1M VERSIONS. error happens with both hbase shell , and happy base client

2014-07-10 10:45:30,098 WARN [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooLarge): {鈥減rocessingtimems鈥?104,鈥漜all鈥?鈥滸et(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$GetRequest)鈥?鈥漜lient鈥?鈥?7.203.54.172:56290鈥?鈥漵tarttimems鈥?1405014329979,鈥漲ueuetimems鈥?0,鈥漜lass鈥?鈥滺RegionServer鈥?鈥漴esponsesize鈥?157334052,鈥漨ethod鈥?鈥滸et鈥潁
2014-07-10 10:45:30,923 INFO [RpcServer.reader=6,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: count of bytes read: 0
java.io.IOException: Connection reset by peer
at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
at sun.nio.ch.IOUtil.read(IOUtil.java:197)
at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
at org.apache.hadoop.hbase.ipc.RpcServer.channelRead(RpcServer.java:2224)
at org.apache.hadoop.hbase.ipc.RpcServer$Connection.readAndProcess(RpcServer.java:1415)
at org.apache.hadoop.hbase.ipc.RpcServer$Listener.doRead(RpcServer.java:790)
at org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.doRunLoop(RpcServer.java:581)
at org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.run(RpcServer.java:556)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:744)
2014-07-10 10:45:30,923 WARN [RpcServer.responder] ipc.RpcServer: RpcServer.respondercallId: 93 service: ClientService methodName: Get size: 225 connection: 17.203.54.172:56290: output error
2014-07-10 10:45:30,924 INFO [RpcServer.responder] ipc.RpcServer: RpcServer.responder: asyncWrite
java.io.IOException: Broken pipe
at sun.nio.ch.FileDispatcherImpl.writev0(Native Method)
at sun.nio.ch.SocketDispatcher.writev(SocketDispatcher.java:51)
at sun.nio.ch.IOUtil.write(IOUtil.java:148)
at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:524)
at org.apache.hadoop.hbase.ipc.BufferChain.write(BufferChain.java:106)
at org.apache.hadoop.hbase.ipc.RpcServer.channelWrite(RpcServer.java:2204)
at org.apache.hadoop.hbase.ipc.RpcServer$Responder.processResponse(RpcServer.java:1004)
at org.apache.hadoop.hbase.ipc.RpcServer$Responder.doAsyncWrite(RpcServer.java:944)
at org.apache.hadoop.hbase.ipc.RpcServer$Responder.doRunLoop(RpcServer.java:873)
at org.apache.hadoop.hbase.ipc.RpcServer$Responder.run(RpcServer.java:849)"
HBASE-11495	"Now in HBase the REST server is started as a non daemon process.
It will be better it is supported as process from hbase-daemon.sh

Also can support stop and autorestart commands for REST server."
HBASE-11498	ipc.server.callqueue.read.share and ipc.server.callqueue.handler.factor are missing docs to help elucidate how they work for mere mortals. 
HBASE-11478	"In a Java Pig UDF including an Hbase Client, which fetches the desired record from Hbase, 'hbase-site.xml' is not read from the Classpath.

// Set Zookeeper Quorum used by HBASE
            // Read hbase-site.xml from CLASSPATH
            Configuration config = HBaseConfiguration.addHbaseResources(UDFContext.getUDFContext().getJobConf());

We have verified that the directory 'etc/hbase/conf' where 'hbase-site.xml' is in, is on the HADOOP_CLASSPATH . I have tried everything, including the 'etc/hbase/conf' or '/etc/hbase/conf/hbase-site.xml' to PIG_CLASSPATH, CLASSPATH environment variables. We also tried adding the hbase-site.xml to our HADOOP_CONF_DIR and HBASE_CONF_DIR is set.

I have also tried REGISTER '/etc/hbase/conf/hbase-site.xml' in Pig. Finally the following did not work either:

conf.addResource(""/etc/hbase/conf/hbase-site.xml"");

I hope this helps."
HBASE-11365	"Maybe this is a misconception on my side, but I assumed that the 'cells' returned by a reverse scanner (HBASE-4811) would be in reverse order, not just the rows.

Was I wrong in my assumption?"
HBASE-10882	"I came across the problem in the early morning several days ago. It happened when I used hadoop completebulkload command to bulk load some hdfs files into hbase table. Several regions hung and after retried three times they all threw RegionTooBusyExceptions. Fortunately, I caught one of the exceptional region鈥檚 HRegionServer process鈥檚 jstack info just in time.
I found that the bulkload process was waiting for a write lock:
at java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.tryLock(ReentrantReadWriteLock.java:1115)
The lock id is 0x00000004054ecbf0.
In the meantime, many other Get/Scan operations were also waiting for the same lock id. And, of course, they were waiting for the read lock:
at java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock.tryLock(ReentrantReadWriteLock.java:873)
The most ridiculous thing is NO ONE OWNED THE LOCK! I searched the jstack output carefully, but cannot find any process who claimed to own the lock.
When I restart the bulk load process, it failed at different regions but with the same RegionTooBusyExceptions. 
I guess maybe the region was doing some compactions at that time and owned the lock, but I couldn鈥檛 find compaction info in the hbase-logs.
Finally, after several days鈥?hard work, the only temporary solution to this problem was found, that is TRIGGERING A MAJOR COMPACTION BEFORE THE BULKLOAD, 
So which process owned the lock? Has anyone came across the same problem before?"
HBASE-11271	The [ref guide chapter on testing discusses|http://hbase.apache.org/book/hbase.tests.html#integration.tests] Chaos Monkey and speaks extensively of running it standalone on a deployed cluster. This functionality hasn't been in HBase since 0.94 and so should probably be taken out of the documentation or updated with some sort deprecation notice.
HBASE-9965	"Having installed the latest stable hbase-0.94.13 version, and following the instructions at    hbase-0.94.13/docs/book/quickstart.html  ,  
the hbase master fails to start and hbase is unusable, owing to this Java RuntimeException occurring, as shown in the log file :

2013-11-13 13:52:06,316 INFO org.apache.hadoop.hbase.master.ActiveMasterManager: Deleting ZNode for /hbase/backup-masters/jvds,52926,1384350725521 from backup master directory
2013-11-13 13:52:06,318 INFO org.apache.zookeeper.server.PrepRequestProcessor: Got user-level KeeperException when processing sessionid:0x14251bbb3d40000 type:delete cxid:0x13 zxid:0xb txntype:-1 reqpath:n/a Error Path:/hbase/backup-masters/jvds,52926,1384350725521 Error:KeeperErrorCode = NoNode for /hbase/backup-masters/jvds,52926,1384350725521
2013-11-13 13:52:06,320 WARN org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper: Node /hbase/backup-masters/jvds,52926,1384350725521 already deleted, and this is not a retry
2013-11-13 13:52:06,320 INFO org.apache.hadoop.hbase.master.ActiveMasterManager: Master=jvds,52926,1384350725521
2013-11-13 13:52:06,348 INFO org.apache.hadoop.hbase.master.SplitLogManager: timeout = 300000
2013-11-13 13:52:06,348 INFO org.apache.hadoop.hbase.master.SplitLogManager: unassigned timeout = 180000
2013-11-13 13:52:06,348 INFO org.apache.hadoop.hbase.master.SplitLogManager: resubmit threshold = 3
2013-11-13 13:52:06,352 INFO org.apache.hadoop.hbase.master.SplitLogManager: found 0 orphan tasks and 0 rescan nodes
2013-11-13 13:52:06,385 INFO org.apache.hadoop.util.NativeCodeLoader: Loaded the native-hadoop library
2013-11-13 13:52:06,385 ERROR org.apache.hadoop.hbase.master.HMasterCommandLine: Failed to start master
java.lang.RuntimeException: Failed suppression of fs shutdown hook: Thread[Thread-27,5,main]
	at org.apache.hadoop.hbase.regionserver.ShutdownHook.suppressHdfsShutdownHook(ShutdownHook.java:196)
	at org.apache.hadoop.hbase.regionserver.ShutdownHook.install(ShutdownHook.java:83)
	at org.apache.hadoop.hbase.util.JVMClusterUtil.startup(JVMClusterUtil.java:191)
	at org.apache.hadoop.hbase.LocalHBaseCluster.startup(LocalHBaseCluster.java:420)
	at org.apache.hadoop.hbase.master.HMasterCommandLine.startMaster(HMasterCommandLine.java:149)
	at org.apache.hadoop.hbase.master.HMasterCommandLine.run(HMasterCommandLine.java:104)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:76)
	at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:2120)
2013-11-13 13:52:06,386 ERROR org.apache.hadoop.io.nativeio.NativeIO: Unable to initialize NativeIO libraries
java.lang.NoSuchFieldError: workaroundNonThreadSafePasswdCalls
	at org.apache.hadoop.io.nativeio.NativeIO.initNative(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO.<clinit>(NativeIO.java:58)
	at org.apache.hadoop.fs.FileUtil.setPermission(FileUtil.java:653)
	at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:509)
	at org.apache.hadoop.fs.FilterFileSystem.setPermission(FilterFileSystem.java:286)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:385)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:364)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:555)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:536)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:443)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:435)
	at org.apache.hadoop.hbase.util.FSUtils.setVersion(FSUtils.java:475)
	at org.apache.hadoop.hbase.util.FSUtils.setVersion(FSUtils.java:375)
	at org.apache.hadoop.hbase.master.MasterFileSystem.checkRootDir(MasterFileSystem.java:436)
	at org.apache.hadoop.hbase.master.MasterFileSystem.createInitialFileSystemLayout(MasterFileSystem.java:148)
	at org.apache.hadoop.hbase.master.MasterFileSystem.<init>(MasterFileSystem.java:133)
	at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:573)
	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:432)
	at org.apache.hadoop.hbase.master.HMasterCommandLine$LocalHMaster.run(HMasterCommandLine.java:226)
	at java.lang.Thread.run(Thread.java:744)
2013-11-13 13:52:06,388 DEBUG org.apache.hadoop.hbase.util.FSUtils: Created version file at file:/home/jason/3P/hbase/data set its version at:7

I would expect after downloading the latest stable version and following the instructions closely that the hbase master should start and hbase should be usable.  

As specified in quickstart.html, the only file I edited was conf/hbase-site.xml, which is :
 <configuration>

  <property>
    <name>hbase.rootdir</name>
    <value>file:///home/jason/3P/hbase/data</value>
  </property>

  <property>
    <name>hbase.zookeeper.property.dataDir</name>
    <value>/home/jason/3P/hbase/zookeeper-data</value>
  </property>

</configuration>
"
HBASE-11197	"When looking into test failure: testVisibilityLabelsOnKillingOfRSContainingLabelsTable

and find this is what has happened:

1. try to assign a region a region server;
2. master creates a znode, and send an openRegion request to the rs;
3. rs gets the request and sends back a response, then crashed;
4. try to assign the region again with forceNewPlan = true;
5. since the region is in transition, master tries to close it and get region server stopped exception;
6. master offlines the region and removes it from transition; but can't assign the region since the dead server is not processed;
7. now SSH finally kicks in, tries to assign this region again;
8. SSH will fail to assign it since the znode is there already.

We should clean up the znode in force offline a region.
"
HBASE-10625	"In reseekTo we find this
{code}
...
        compared = compareKey(reader.getComparator(), key, offset, length);
        if (compared < 1) {
          // If the required key is less than or equal to current key, then
          // don't do anything.
          return compared;
        } else {
           ...
           return loadBlockAndSeekToKey(this.block, this.nextIndexedKey,
              false, key, offset, length, false);
...
{code}
loadBlockAndSeekToKey already does the right thing when a we pass a key that sorts before the current key. It's less efficient than this early check, but in the vast (all?) cases we pass forward keys (as required by the reseek contract). We're optimizing the wrong thing.

Scanning with the ExplicitColumnTracker is 20-30% faster.
(I tested with rows of 5 short KVs selected the 2nd and or 4th column)

I propose simply removing that check."
HBASE-1197	Several instances of OOME when trying to serve up large cells to clients have been observed. IPC should send large cell content in chunks instead of as one large naive copy. 
HBASE-9240	"Over on HADOOP-9876, Hadoop enumerates the benefits of protobuf 2.5.0. We have the same considerations for our RPC and elsewhere after HBASE-8165"
HBASE-7030	"The coprocessor class loader should allow the user to supply a list of packages or classes to resolve directly with the parent (RegionServer) classloader.

See HBASE-6843 for some background."
HBASE-3707	Memstore upsert performance may be impacted by having a large number of values in the map. Consider flushing the store after a configurable number of inserts.
HBASE-1477	"From  Jeremy Pinkham up on hbase-users@: 

bq. A typical mapper in the job takes several minutes, how many minutes depends on whether I use the the region partitioner and how many I let run concurrently... it's been anywhere from 2 minutes with no partitioner and small concurrency (5 mappers) to 8 minutes with the region partitioner and high concurrency (150 mappers).  This seems to directly correlate with how long it takes to do a simple count of .META. while each job is running (2 seconds to 1 minute)

bq. I was able to get past this issue affecting my data load by reorganizing some of my workflow and data structures to force the ordering of keys without the region partitioner.  Those changes appear to have side stepped the problem for me as I can now load from 100+ mappers without seeing the degradation that I was seeing with 40 when using the partitioner (and getting some sweet numbers in the requests column of the UI).  It's still an interesting scaling situation with the region partitioner, but I'm good to go without it.

I have seen this also in the form of freezing of master UI during high load, where the UI comes back as soon as load is reduced. When I thread dump it looks like all IPC handlers on the region server hosting .META. are busy. "
HBASE-5343	"To use the access control mechanism added in HBASE-3025, users should either use the shell interface, or use the coprocessor API directly, which is not very user friendly. We can add grant/revoke/user_permission commands similar to the shell interface to HBaseAdmin assuming HBASE-5341 is in. "
HBASE-11091	"Running  Hbase utilities from 0.98.1-hadoop2 with Hadoop 2.3.0 hihgligh incompatibility of client versions. For example ""hbase org.apache.hadoop.hbase.PerformanceEvaluation  randomWrite 5"" fails at the end with ""No enum constant org.apache.hadoop.mapreduce.JobCounter.MB_MILLIS_REDUCES""

This seem to be related with https://issues.apache.org/jira/browse/HBASE-10601 and https://issues.apache.org/jira/browse/MAPREDUCE-5831
"
HBASE-11066	"Configuration conf = HBaseConfiguration.create();
conf.addResource(""hbase-site.xml"");

this code could not work find, all configuration in hbase-site.xml could not be loaded, and log will show as below:
14/04/24 12:06:13 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=hconnection-0x26f997aa, quorum=localhost:2181, baseZNode=/hbase
14/04/24 12:06:13 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x26f997aa connecting to ZooKeeper ensemble=localhost:2181
14/04/24 12:06:13 INFO zookeeper.ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
14/04/24 12:06:13 INFO zookeeper.ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
14/04/24 12:06:13 INFO zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x14572d37f6a00c1, negotiated timeout = 40000
14/04/24 12:06:13 INFO client.ZooKeeperRegistry: ClusterId read in ZooKeeper is null

but if I set properties like this:
conf.set(""hbase.zookeeper.quorum"", ""mater"");
conf.set(""hbase.zookeeper.property.clientPort"", ""8181"");
conf.set(""hbase.master.port"", ""60000"");
it will work fine."
HBASE-9933	"See HRegionServer::mutate switch statement. For puts/deletes it checks condition, for i/a it just does the operation. Discovered while doing stuff for HBASE-3787"
HBASE-10920	"          Configuration conf = HBaseConfiguration.create();
          conf.set(""hbase.zookeeper.quorum"",""znode-1"");
          conf.set(""hbase.zookeeper.property.clientPort"",""2181"");
          conf.set(""hbase.master"",master:16000"");
          hconnection = HConnectionManager.createConnection(conf); //(Parser.java:40)
=====================
Results In
======================
Error: java.io.IOException: java.lang.reflect.InvocationTargetException at org.apache.hadoop.hbase.client.ConnectionManager.createConnection(ConnectionManager.java:341) at org.apache.hadoop.hbase.client.ConnectionManager.createConnectionInternal(ConnectionManager.java:234) at org.apache.hadoop.hbase.client.HConnectionManager.createConnection(HConnectionManager.java:215) at com.example.job.Mapper.setup(Parser.java:40) at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:142) at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1597) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:169) Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:526) at org.apache.hadoop.hbase.client.ConnectionManager.createConnection(ConnectionManager.java:339) ... 11 more Caused by: java.lang.NoClassDefFoundError: org/htrace/Trace at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.exists(RecoverableZooKeeper.java:195) at org.apache.hadoop.hbase.zookeeper.ZKUtil.checkExists(ZKUtil.java:480) at org.apache.hadoop.hbase.zookeeper.ZKClusterId.readClusterIdZNode(ZKClusterId.java:65) at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getClusterId(ZooKeeperRegistry.java:84) at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.retrieveClusterId(ConnectionManager.java:779) at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.<init>(ConnectionManager.java:588) ... 16 more Caused by: java.lang.ClassNotFoundException: org.htrace.Trace at java.net.URLClassLoader$1.run(URLClassLoader.java:366) at java.net.URLClassLoader$1.run(URLClassLoader.java:355) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:354) at java.lang.ClassLoader.loadClass(ClassLoader.java:425) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308) at java.lang.ClassLoader.loadClass(ClassLoader.java:358) ... 22 more"
HBASE-10878	"I used setup similar to that from HBASE-10863, with fix for HBASE-10863 :
{code}
hbase(main):003:0> scan 'hbase:labels'
ROW                                          COLUMN+CELL
 \x00\x00\x00\x01                            column=f:\x00, timestamp=1395944796030, value=system
 \x00\x00\x00\x01                            column=f:hbase, timestamp=1395944796030, value=
 \x00\x00\x00\x02                            column=f:\x00, timestamp=1395951045442, value=TOP_SECRET
 \x00\x00\x00\x02                            column=f:hrt_qa, timestamp=1395951229682, value=
 \x00\x00\x00\x02                            column=f:hrt_qa1, timestamp=1395951270297, value=
 \x00\x00\x00\x02                            column=f:mapred, timestamp=1395958442326, value=
 \x00\x00\x00\x03                            column=f:\x00, timestamp=1395952069731, value=TOP_TOP_SECRET
 \x00\x00\x00\x03                            column=f:mapred, timestamp=1395956032141, value=
 \x00\x00\x00\x04                            column=f:\x00, timestamp=1395971516605, value=A
 \x00\x00\x00\x04                            column=f:oozie, timestamp=1395971647859, value=
 \x00\x00\x00\x05                            column=f:\x00, timestamp=1395971520327, value=B
5 row(s) in 0.0580 seconds
{code}
I did the following as user oozie using hbase shell:
{code}
hbase(main):001:0> scan 'tb', { AUTHORIZATIONS => ['A']}
ROW                                          COLUMN+CELL
 row                                         column=f1:q, timestamp=1395971660859, value=v1
 row2                                        column=f1:q, timestamp=1395972271343, value=v2
 row3                                        column=f1:q, timestamp=1396067477702, value=v3
3 row(s) in 0.2050 seconds

hbase(main):002:0> scan 'tb', { AUTHORIZATIONS => ['A|B']}
ROW                                          COLUMN+CELL
 row2                                        column=f1:q, timestamp=1395972271343, value=v2
1 row(s) in 0.0150 seconds

hbase(main):003:0> scan 'tb', { AUTHORIZATIONS => ['B|A']}
ROW                                          COLUMN+CELL
 row2                                        column=f1:q, timestamp=1395972271343, value=v2
1 row(s) in 0.0260 seconds
{code}
Rows 'row' and 'row3' were inserted with label 'A'.
Row 'row2' was inserted without label.
Row 'row1' was inserted with label 'B'.

I would expect row1 to also be returned."
HBASE-7118	"org.apache.hadoop.hbase.replication.TestReplicationPeer

Running org.apache.hadoop.hbase.replication.TestReplicationPeer
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 25.89 sec <<< FAILURE!
	--- stable failures, new for hbase 0.92.0, need to be fixed firstly.

--------------------------------	
 target/surefire-reports/org.apache.hadoop.hbase.replication.TestReplicationPeer.txt output:

Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 28.245 sec <<< FAILURE!
testResetZooKeeperSession(org.apache.hadoop.hbase.replication.TestReplicationPeer)  Time elapsed: 25.247 sec  <<< FAILURE!
junit.framework.AssertionFailedError: ReplicationPeer ZooKeeper session was not properly expired.
        at junit.framework.Assert.fail(Assert.java:50)
        at org.apache.hadoop.hbase.replication.TestReplicationPeer.testResetZooKeeperSession(TestReplicationPeer.java:73)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:60)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:37)
        at java.lang.reflect.Method.invoke(Method.java:611)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
        at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:62)
        

target/surefire-reports/org.apache.hadoop.hbase.replication.TestReplicationPeer-output.txt content:
       
2012-03-25 20:52:42,979 INFO  [main] zookeeper.MiniZooKeeperCluster(174): Started MiniZK Cluster and connect 1 ZK server on client port: 21818

2012-03-25 20:52:43,023 DEBUG [main] zookeeper.ZKUtil(96): connection to cluster: clusterId opening connection to ZooKeeper with ensemble (localhost:21818)

2012-03-25 20:52:43,082 INFO  [main] zookeeper.RecoverableZooKeeper(89): The identifier of this process is 4095@svltest116.svl.ibm.com

2012-03-25 20:52:43,166 DEBUG [main-EventThread] zookeeper.ZooKeeperWatcher(257): connection to cluster: clusterId Received ZooKeeper Event, type=None, state=SyncConnected, path=null

2012-03-25 20:52:43,175 INFO  [Thread-9] replication.TestReplicationPeer(53): Expiring ReplicationPeer ZooKeeper session.

2012-03-25 20:52:43,196 DEBUG [main-EventThread] zookeeper.ZooKeeperWatcher(334): connection to cluster: clusterId-0x1364d226a3d0000 connected

2012-03-25 20:52:43,308 INFO  [Thread-9] hbase.HBaseTestingUtility(1234): ZK Closed Session 0x1364d226a3d0000; sleeping=25000

2012-03-25 20:53:08,323 INFO  [Thread-9] replication.TestReplicationPeer(57): Attempting to use expired ReplicationPeer ZooKeeper session."
HBASE-10872	This issue is a regression caused by v94 patch of HBASE-10648
HBASE-9368	"Follow on from hbase-8567.  See https://builds.apache.org/view/H-L/view/HBase/job/hbase-0.95-on-hadoop2/275/testReport/junit/org.apache.hadoop.hbase.master/TestDistributedLogSplitting/testDelayedDeleteOnFailure/
{code}
org.apache.hadoop.hbase.master.TestDistributedLogSplitting.testDelayedDeleteOnFailure

Failing for the past 1 build (Since Failed#275 )
Took 7 ms.
add description
Error Message

test timed out after 30000 milliseconds
Stacktrace

java.lang.Exception: test timed out after 30000 milliseconds
	at java.lang.Thread.start0(Native Method)
	at java.lang.Thread.start(Thread.java:640)
	at org.apache.zookeeper.ClientCnxn.start(ClientCnxn.java:403)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:450)
	at org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:380)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.<init>(RecoverableZooKeeper.java:114)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.connect(ZKUtil.java:135)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.<init>(ZooKeeperWatcher.java:167)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.<init>(ZooKeeperWatcher.java:136)
	at org.apache.hadoop.hbase.client.ZooKeeperKeepAliveConnection.<init>(ZooKeeperKeepAliveConnection.java:43)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getKeepAliveZooKeeperWatcher(HConnectionManager.java:1788)
	at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getClusterId(ZooKeeperRegistry.java:82)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.retrieveClusterId(HConnectionManager.java:778)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.<init>(HConnectionManager.java:618)
	at sun.reflect.GeneratedConstructorAccessor38.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.hadoop.hbase.client.HConnectionManager.createConnection(HConnectionManager.java:377)
	at org.apache.hadoop.hbase.client.HConnectionManager.createConnection(HConnectionManager.java:358)
	at org.apache.hadoop.hbase.client.HConnectionManager.getConnection(HConnectionManager.java:293)
	at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:191)
	at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniHBaseCluster(HBaseTestingUtility.java:887)
	at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniHBaseCluster(HBaseTestingUtility.java:853)
	at org.apache.hadoop.hbase.master.TestDistributedLogSplitting.startCluster(TestDistributedLogSplitting.java:156)
	at org.apache.hadoop.hbase.master.TestDistributedLogSplitting.startCluster(TestDistributedLogSplitting.java:141)
	at org.apache.hadoop.hbase.master.TestDistributedLogSplitting.testDelayedDeleteOnFailure(TestDistributedLogSplitting.java:970)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)

{code}

We seem stuck.   Nothing happens after master initializes... we just wait:

{code}
2013-08-28 00:42:53,492 INFO  [M:0;quirinus:56658] master.HMaster(901): Master has completed initialization
2013-08-28 00:42:53,496 DEBUG [NamespaceJanitor-quirinus:56658] client.ClientScanner(218): Finished {ENCODED => 5fb8721e18f122319e47336ad5221a58, NAME => 'hbase:namespace,,1377650558736.5fb8721e18f122319e47336ad5221a58.', STARTKEY => '', ENDKEY => ''}
2013-08-28 00:42:53,534 DEBUG [CatalogJanitor-quirinus:56658] client.ClientScanner(218): Finished {ENCODED => 1588230740, NAME => 'hbase:meta,,1', STARTKEY => '', ENDKEY => ''}
2013-08-28 00:43:34,443 INFO  [Thread-2407] zookeeper.RecoverableZooKeeper(122): Process identifier=hconnection-0x958cd8 connecting to ZooKeeper ensemble=localhost:60031
2013-08-28 00:43:34,443 FATAL [pool-1-thread-1] master.HMaster(2240): Master server abort: loaded coprocessors are: [org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint]
2013-08-28 00:43:34,443 FATAL [pool-1-thread-1] master.HMaster(2245): closing...
java.lang.Exception: Trace info
	at org.apache.hadoop.hbase.master.TestDistributedLogSplitting.after(TestDistributedLogSplitting.java:169)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
2013-08-28 00:43:34,444 INFO  [pool-1-thread-1] master.HMaster(2434): Aborting
2013-08-28 00:43:34,443 WARN  [Thread-2407] zookeeper.ZKUtil(489): hconnection-0x958cd8 Unable to set watcher on znode (/hbase/hbaseid)
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Object.wait(Object.java:485)
	at org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1309)
	at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1036)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.exists(RecoverableZooKeeper.java:202)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.checkExists(ZKUtil.java:482)
	at org.apache.hadoop.hbase.zookeeper.ZKClusterId.readClusterIdZNode(ZKClusterId.java:65)
	at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getClusterId(ZooKeeperRegistry.java:83)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.retrieveClusterId(HConnectionManager.java:778)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.<init>(HConnectionManager.java:618)
	at sun.reflect.GeneratedConstructorAccessor38.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.hadoop.hbase.client.HConnectionManager.createConnection(HConnectionManager.java:377)
	at org.apache.hadoop.hbase.client.HConnectionManager.createConnection(HConnectionManager.java:358)
	at org.apache.hadoop.hbase.client.HConnectionManager.getConnection(HConnectionManager.java:293)
	at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:191)
	at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniHBaseCluster(HBaseTestingUtility.java:887)
	at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniHBaseCluster(HBaseTestingUtility.java:853)
	at org.apache.hadoop.hbase.master.TestDistributedLogSplitting.startCluster(TestDistributedLogSplitting.java:156)
	at org.apache.hadoop.hbase.master.TestDistributedLogSplitting.startCluster(TestDistributedLogSplitting.java:141)
	at org.apache.hadoop.hbase.master.TestDistributedLogSplitting.testDelayedDeleteOnFailure(TestDistributedLogSplitting.java:970)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
...
{code}

What you think [~jeffreyz]?  Thanks boss."
HBASE-9636	"{noformat}

Problem:
HBase shell/client 'scan table' operation is getting failed inbetween the when the regions are shifted from one Region Server to another Region Server

When the table regions data moved from one Region Server to another Region Server then the client/shell should be able to handle the data from the 
new Region server automatically (because when we have huge data in terms of GB/TB at that time one of the Region Server going down in the cluster is frequent)


Procedure:
1. Setup Non HA Hadoop Cluster with two nodes (Node1-XX.XX.XX.XX,  Node2-YY.YY.YY.YY)
2. Install Zookeeper, HMaster & HRegionServer in Node-1
3. Install HRegionServer in Node-2
4. From Node2 create HBase Table ( table name 't1' with one column family 'cf1' )
5. add around 367120 rows to the table
6. scan the table 't1' using hbase shell & at the same time switch the region server 1 & 2 (so that the table 't1' regions data are moved from Region Server 1 to 1 & vice versa)
7. During this time hbase shell is getting failed in between of the scan operation as below

...................................................................                                
 row172266                        column=cf1:a, timestamp=1379680737307, value=100                                              
 row172267                        column=cf1:a, timestamp=1379680737311, value=100                                              
 row172268                        column=cf1:a, timestamp=1379680737314, value=100                                              
 row172269                        column=cf1:a, timestamp=1379680737317, value=100                                              
 row17227                         column=cf1:a, timestamp=1379679668631, value=100                                              
 row17227                         column=cf1:b, timestamp=1379681090560, value=200                                             

ERROR: java.lang.RuntimeException: org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=7, exceptions:
Fri Sep 20 18:20:58 IST 2013, org.apache.hadoop.hbase.client.ScannerCallable@1999dc4f, java.net.ConnectException: Connection refused
Fri Sep 20 18:20:59 IST 2013, org.apache.hadoop.hbase.client.ScannerCallable@1999dc4f, org.apache.hadoop.hbase.ipc.HBaseClient$FailedServerException: This server is in the failed servers list: HOST-YY.YY.YY.YY/YY.YY.YY.YY:61020
Fri Sep 20 18:21:00 IST 2013, org.apache.hadoop.hbase.client.ScannerCallable@1999dc4f, java.net.ConnectException: Connection refused
Fri Sep 20 18:21:01 IST 2013, org.apache.hadoop.hbase.client.ScannerCallable@1999dc4f, org.apache.hadoop.hbase.ipc.HBaseClient$FailedServerException: This server is in the failed servers list: HOST-YY.YY.YY.YY/YY.YY.YY.YY:61020
Fri Sep 20 18:21:07 IST 2013, org.apache.hadoop.hbase.client.ScannerCallable@1999dc4f, java.net.ConnectException: Connection refused
Fri Sep 20 18:21:09 IST 2013, org.apache.hadoop.hbase.client.ScannerCallable@1999dc4f, java.net.ConnectException: Connection refused
Fri Sep 20 18:21:17 IST 2013, org.apache.hadoop.hbase.client.ScannerCallable@1999dc4f, java.net.ConnectException: Connection refused

hbase(main):014:0> 


{noformat}"
HBASE-10421	"Testing the ""internal friction"" of scanning in HBase (all data in the block cache, and all rows filtered at the server by a ValueFilter, so that one really measures the work HBase does internally), I found that 0.98 is almost 35% slower than 0.94.

Scanning 50m rows (one col each, 8 byte keys, 8 byte values) takes 13.2 in 0.94.17-SNAPSHOT and 18.5s in 0.98.

This probably came about with all the protobuf changes in 0.96.
It should be possible to bring back to par with 0.94.
"
HBASE-4464	"balancer.balanceCluster() generates RegionPlans for HMaster.balance() to execute.
We don't retract any RegionPlan in balancer.balanceCluster().
In the near future, more complex algorithm would be introduced to try achieving maximum block location affinity for the regions to be moved. This means balancer.balanceCluster() would take longer to return.

This JIRA makes region balancing parallel with balancer.balanceCluster()
Meaning region balancing would be performed when balancer.balanceCluster() is still running."
HBASE-4136	"I observed in our staging cluster that load balancer didn't run for a long period of time.
I saw the following in master log:
{code}
2011-07-24 15:56:32,333 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because 2 region(s) in transition: {4e7416833e3bbd6a8ade26f6986529bf=TABLE-1311419946465,E'\xFD\xDDu\xC3\x894\xF4$\xC0K\xA3!\x82\xB9\xD0\x7F|>\xAC\xDA81\xB6\x92\xED\xA9\x9C\xA6^\xF4,1311419961631.4e7416833e3bbd6a8ade26f6986529bf. state=PENDING_CLOSE, ts=...
{code}
This means we need to find a better way of permitting one balance run at a time. In HMaster.balance():
{code}
      if (this.assignmentManager.isRegionsInTransition()) {
{code}"
HBASE-3644	"See HBASE-3387 or http://www.ibm.com/developerworks/java/library/j-jtp05273.html#N10184 .  Basically, 'a' denotes HServerAddress(DNS) & 'b' denotes HServerAddress(nslookup(DNS)).  This is extremely common within HBase when 'conf/regionserver' contains DNS entries because ClusterStatus.getServers() is IP-based. You have a.address.equals(b.address) && !a.stringValue.equals(b.stringValue).  In this case, a.equals(b) while a.hashCode() != b.hashCode().  "
HBASE-3611	hbase-default.xml isnt getting updated when the version number is changed.  mvn clean is required all the time now.  The result is a jar that is unrunnable. 
HBASE-3556	"{code}
2011-02-18 10:08:14,873 ERROR org.apache.hadoop.hbase.HServerAddress: Could not resolve the DNS name of zcl.local:60020
2011-02-18 10:08:14,874 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_SERVER_SHUTDOWN
java.lang.IllegalArgumentException: Could not resolve the DNS name of zcl.local:60020
        at org.apache.hadoop.hbase.HServerAddress.checkBindAddressCanBeResolved(HServerAddress.java:105)
        at org.apache.hadoop.hbase.HServerAddress.<init>(HServerAddress.java:66)
        at org.apache.hadoop.hbase.catalog.MetaReader.metaRowToRegionPairWithInfo(MetaReader.java:407)
        at org.apache.hadoop.hbase.catalog.MetaReader.getServerUserRegions(MetaReader.java:594)
        at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:124)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:151)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}

We never go back and reprocess.  Meantime, balancer never runs because server stuck in dead list."
HBASE-3549	"To minimize cost of migration, it is desirable to pull new APIs into 0.20.6 (0.89)
This way client code compiles against both hbase jars.

Our changes include:
making MetaScanner public class
adding the following ctor for Put
{code}
  public Put(byte[] row, long ts) {
    this(row, null);
    setTimeStamp(ts);
  }
{code}
adding the following ctor for HColumnDescriptor:
{code}
  public HColumnDescriptor(final byte [] familyName, final int maxVersions,
	      final String compression, final boolean inMemory,
	      final boolean blockCacheEnabled,
	      final int timeToLive, final String bloomFilter) {
	  this(familyName, maxVersions,
	       compression, inMemory,
	       blockCacheEnabled,
	       timeToLive,!bloomFilter.equals(""NONE""));
  }
{code}
"
HBASE-3469	Andrew suggests we add 611 to append branch: See http://mail-archives.apache.org/mod_mbox/hbase-user/201101.mbox/%3C4D3D6EBF.8010302@gmail.com%3E
HBASE-3429	"Presently, an exception is actually thrown when deserializing the output of HBaseObjectWritable.writeObject(SomeSubclassOfWritable[]).

The issue is pretty difficult to debug.

This patch adds support for arrays whose contents are subtypes of both Serializable and Writable."
HBASE-3236	"jdcryans suggests a page in book where we list major changes in API -- the deprecated stuff removed in 0.90.

Ted Yu started a list up in mailing list:

I am trying to compile our code against 0.90
Result.getCellValue() is no longer available.

Can you tell me the alternative ?
..


HConstants is final class now instead of interface
RowFilterInterface is gone
org.apache.hadoop.hbase.io.Cell is gone
org.apache.hadoop.hbase.io.RowResult is gone
constructor
HColumnDescriptor(byte[],int,java.lang.String,boolean,boolean,int,boolean)
is gone
Put.setTimeStamp() is gone
org.apache.hadoop.hbase.filter.Filter has added
getNextKeyHint(org.apache.hadoop.hbase.KeyValue)

If you know the alternative to some of the old classes, please share."
HBASE-3177	"Add a search into the hbase book as per the example pointed to here: https://issues.apache.org/jira/browse/HBASE-2650?focusedCommentId=12885581&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#action_12885581

Also add some of the hbase navbar if possible to make navigation back to the cite from within the manual easier?"
HBASE-2416	"It is desirable to provide HBase table export/import capability through table.jsp, similar to compaction/split buttons.

For import/export button, an input field allows user to specify source/destination hdfs directory."
HBASE-10410	"This is frustrating. On some JREs (Java 7) I am seeing hbase-server fail due to timeout, without any test actually reporting a timeout.  This StackOverflow answer talks about how JUnit < 4.12-SNAPSHOT may report a timeout if the test throws an InterruptedException: http://stackoverflow.com/questions/17016011/junit-test-times-out-despite-executing-quickly  Those happen at various points when shutting down the minicluster. We may be letting one escape. "
HBASE-10387	"I was running Hoya load test which uses LoadTestTool.
The load was successful. However Hoya test failed with:
{code}
[ERROR] Command was/bin/sh -c cd /home/yarn/hoya/hoya-funtest && /usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre/bin/java -Xmx1024m -XX:+HeapDumpOnOutOfMemoryError -jar /home/yarn/hoya/hoya-funtest/target/surefire/surefirebooter274017860635722411.jar /home/yarn/hoya/hoya-funtest/target/surefire/surefire6543037778494048137tmp /home/yarn/hoya/hoya-funtest/target/surefire/surefire_07364766695548839031tmp
[ERROR] -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.16:test (default-test) on project hoya-funtest: Execution default-test of goal org.apache.maven.plugins:maven-surefire-plugin:2.16:test failed: The forked VM terminated without saying properly goodbye. VM crash or System.exit called ?
Command was/bin/sh -c cd /home/yarn/hoya/hoya-funtest && /usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre/bin/java -Xmx1024m -XX:+HeapDumpOnOutOfMemoryError -jar /home/yarn/hoya/hoya-funtest/target/surefire/surefirebooter274017860635722411.jar /home/yarn/hoya/hoya-funtest/target/surefire/surefire6543037778494048137tmp /home/yarn/hoya/hoya-funtest/target/surefire/surefire_07364766695548839031tmp
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:224)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(LifecycleStarter.java:183)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:161)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:317)
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)
        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)
        at org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(LifecycleStarter.java:183)
        at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:161)
        at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:317)
        at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:152)
        at org.apache.maven.cli.MavenCli.execute(MavenCli.java:555)
        at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:214)
        at org.apache.maven.cli.MavenCli.main(MavenCli.java:158)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:601)
        at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
        at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
        at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
        at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: org.apache.maven.plugin.PluginExecutionException: Execution default-test of goal org.apache.maven.plugins:maven-surefire-plugin:2.16:test failed: The forked VM terminated without saying properly goodbye. VM crash or System.exit called ?
Command was/bin/sh -c cd /home/yarn/hoya/hoya-funtest && /usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre/bin/java -Xmx1024m -XX:+HeapDumpOnOutOfMemoryError -jar /home/yarn/hoya/hoya-funtest/target/surefire/surefirebooter274017860635722411.jar /home/yarn/hoya/hoya-funtest/target/surefire/surefire6543037778494048137tmp /home/yarn/hoya/hoya-funtest/target/surefire/surefire_07364766695548839031tmp
        at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:115)
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
        ... 19 more
Caused by: java.lang.RuntimeException: The forked VM terminated without saying properly goodbye. VM crash or System.exit called ?
Command was/bin/sh -c cd /home/yarn/hoya/hoya-funtest && /usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre/bin/java -Xmx1024m -XX:+HeapDumpOnOutOfMemoryError -jar /home/yarn/hoya/hoya-funtest/target/surefire/surefirebooter274017860635722411.jar /home/yarn/hoya/hoya-funtest/target/surefire/surefire6543037778494048137tmp /home/yarn/hoya/hoya-funtest/target/surefire/surefire_07364766695548839031tmp
        at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:485)
        at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:352)
        at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:158)
        at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:958)
        at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:822)
        at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:720)
        at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:106)
        ... 20 more
{code}
This was due to the following call in doStaticMain():
{code}
    System.exit(ret);
{code}
A config option can be added so that the above call is skipped in the case where LoadTestTool is activated from maven and load test succeeds."
HBASE-10325	"Unknown option or illegal argument: -XX:OnOutOfMemoryError=kill -9 %p. 
Please check for incorrect spelling or review documentation of startup options.

Could not create the Java virtual machine.
starting master, logging to /home/hadoop/hbase-0.96.1.1-hadoop2/logs/hbase-hadoop-master-namenode0.hadoop.out
Unknown option or illegal argument: -XX:OnOutOfMemoryError=kill -9 %p. 
Please check for incorrect spelling or review documentation of startup options"
HBASE-10305	"In our use case, we use a small number (~5) of proxy programs that read from a queue and batch update to HBase. Our program is multi-threaded and HBase client will batch mutations to each RS.

We found we're getting lower TPS when there are more regions. I think the reason is RS syncs HLog for each region. Suppose there is a single region, the batch update will only touch one region and therefore syncs HLog once. And suppose there are 10 regions per server, in RS#multi() it have to process update for each individual region and sync HLog 10 times.

Please note that in our scenario, batched mutations usually are independent with each other and need to touch a various number of regions.

We are using the 0.94 series, but I think the trunk should have the same problem after a quick look into the code."
HBASE-10293	"My issue manifests when I uncomment the line {{export SERVER_GC_OPTS=...}} in hbase-env.sh and start HBase. It's a single node in distributed mode, so both a Master and RegionServer are started on that host. Both start commands are run in the same minute, so only one gc.log-`date` file is created. `lsof` indicates two processes are writing to that file and the output of `ps` confirms they both received the same {{-Xloggc:/grid/0/var/log/hbase/gc.log-201401071515}} argument.

Presumably, the same will happen for folks running the thrift and rest gateways on the same box (any java process itemized in the server_cmds array in bin/hbase).

Related (the reason I discovered this issue in the first place), stopping the master process results in its gc.log being truncated."
HBASE-10300	"As discussed on HBASE-10292, we may not be throwing DoNotRetryIOExceptions back to the client when aborting the server, especially when handling fatal coprocessor exceptions."
HBASE-10290	"I suffering an issue is, I only get one _KeyValue_ object (five expected) from one row if I use _FilterList_ with _Operator.MUST_PASS_ALL_ to add both _QualifierFiter_ and _Operator.MUST_PASS_ALL_, the details as follows

h4.Test for Operator.MUST_PASS_ALL
1. I generate 10 rows of test data, two column familys and five column qualifiers for each row.
{code:title=Test Data}
r=keyvalues={row001/cf1:dummy/1389104461530/Put/vlen=5/mvcc=0, row001/cf1:ip/1389104461530/Put/vlen=10/mvcc=0, row001/cf1:port-10/1389104461530/Put/vlen=5/mvcc=0, row001/cf2:dummy1/1389104461530/Put/vlen=6/mvcc=0, row001/cf2:dummy2/1389104461530/Put/vlen=6/mvcc=0}
r=keyvalues={row002/cf1:dummy/1389104461530/Put/vlen=5/mvcc=0, row002/cf1:ip/1389104461530/Put/vlen=10/mvcc=0, row002/cf1:port-20/1389104461530/Put/vlen=5/mvcc=0, row002/cf2:dummy1/1389104461530/Put/vlen=6/mvcc=0, row002/cf2:dummy2/1389104461530/Put/vlen=6/mvcc=0}
r=keyvalues={row003/cf1:dummy/1389104461530/Put/vlen=5/mvcc=0, row003/cf1:ip/1389104461530/Put/vlen=10/mvcc=0, row003/cf1:port-30/1389104461530/Put/vlen=5/mvcc=0, row003/cf2:dummy1/1389104461530/Put/vlen=6/mvcc=0, row003/cf2:dummy2/1389104461530/Put/vlen=6/mvcc=0}
r=keyvalues={row004/cf1:dummy/1389104461530/Put/vlen=5/mvcc=0, row004/cf1:ip/1389104461530/Put/vlen=10/mvcc=0, row004/cf1:port-40/1389104461530/Put/vlen=5/mvcc=0, row004/cf2:dummy1/1389104461530/Put/vlen=6/mvcc=0, row004/cf2:dummy2/1389104461530/Put/vlen=6/mvcc=0}
r=keyvalues={row005/cf1:dummy/1389104461530/Put/vlen=5/mvcc=0, row005/cf1:ip/1389104461530/Put/vlen=10/mvcc=0, row005/cf1:port-50/1389104461530/Put/vlen=5/mvcc=0, row005/cf2:dummy1/1389104461530/Put/vlen=6/mvcc=0, row005/cf2:dummy2/1389104461530/Put/vlen=6/mvcc=0}
r=keyvalues={row006/cf1:dummy/1389104461530/Put/vlen=5/mvcc=0, row006/cf1:ip/1389104461530/Put/vlen=10/mvcc=0, row006/cf1:port-60/1389104461530/Put/vlen=5/mvcc=0, row006/cf2:dummy1/1389104461530/Put/vlen=6/mvcc=0, row006/cf2:dummy2/1389104461530/Put/vlen=6/mvcc=0}
r=keyvalues={row007/cf1:dummy/1389104461530/Put/vlen=5/mvcc=0, row007/cf1:ip/1389104461530/Put/vlen=10/mvcc=0, row007/cf1:port-70/1389104461530/Put/vlen=5/mvcc=0, row007/cf2:dummy1/1389104461530/Put/vlen=6/mvcc=0, row007/cf2:dummy2/1389104461530/Put/vlen=6/mvcc=0}
r=keyvalues={row008/cf1:dummy/1389104461530/Put/vlen=5/mvcc=0, row008/cf1:ip/1389104461530/Put/vlen=10/mvcc=0, row008/cf1:port-80/1389104461530/Put/vlen=5/mvcc=0, row008/cf2:dummy1/1389104461530/Put/vlen=6/mvcc=0, row008/cf2:dummy2/1389104461530/Put/vlen=6/mvcc=0}
r=keyvalues={row009/cf1:dummy/1389104461530/Put/vlen=5/mvcc=0, row009/cf1:ip/1389104461530/Put/vlen=10/mvcc=0, row009/cf1:port-80/1389104461530/Put/vlen=5/mvcc=0, row009/cf2:dummy1/1389104461530/Put/vlen=6/mvcc=0, row009/cf2:dummy2/1389104461530/Put/vlen=6/mvcc=0}
r=keyvalues={row010/cf1:dummy/1389104461530/Put/vlen=5/mvcc=0, row010/cf1:ip/1389104461530/Put/vlen=11/mvcc=0, row010/cf1:port-80/1389104461530/Put/vlen=5/mvcc=0, row010/cf2:dummy1/1389104461530/Put/vlen=6/mvcc=0, row010/cf2:dummy2/1389104461530/Put/vlen=6/mvcc=0}
{code}

2. Then I use following code to try to filter out only one row I want
{code:title=FilterList codes}
//...
fl = new FilterList(Operator.MUST_PASS_ALL);

filter =
    new SingleColumnValueFilter(CF_1_NAME_B, Bytes.toBytes(""ip""), CompareOp.EQUAL,
        Bytes.toBytes(""127.0.0.80""));
fl.addFilter(filter);

filter = new QualifierFilter(CompareOp.EQUAL, new BinaryComparator(Bytes.toBytes(""port-80"")));
fl.addFilter(filter);

scan.setFilter(fl);
rs = table.getScanner(scan);

for (Result r : rs) {
  //...
}
{code}
3. I get the right and only one row returned, but not get whole data (_KeyValue_s) of this row.
{code:title=Results not expected}
r=keyvalues={row008/cf1:port-80/1389104629712/Put/vlen=5/mvcc=0}
{code}
Actually, I expect following data would return, whole record data, two column families and five column qualifiers.
{code:tittle=Results I expected}
r=keyvalues={row008/cf1:dummy/1389104461530/Put/vlen=5/mvcc=0, row008/cf1:ip/1389104461530/Put/vlen=10/mvcc=0, row008/cf1:port-80/1389104461530/Put/vlen=5/mvcc=0, row008/cf2:dummy1/1389104461530/Put/vlen=6/mvcc=0, row008/cf2:dummy2/1389104461530/Put/vlen=6/mvcc=0}
{code}

h4.Test for Operator.MUST_PASS_ONE
then I test the same code except to change the _Operator.MUST_PASS_ALL_ to _Operator.MUST_PASS_ONE_, and things getting more worse...
{code:title=Result not expected}
r=keyvalues={row001/cf1:dummy/1389113376745/Put/vlen=5/mvcc=0}
r=keyvalues={row002/cf1:dummy/1389113376745/Put/vlen=5/mvcc=0}
r=keyvalues={row003/cf1:dummy/1389113376745/Put/vlen=5/mvcc=0}
r=keyvalues={row004/cf1:dummy/1389113376745/Put/vlen=5/mvcc=0}
r=keyvalues={row005/cf1:dummy/1389113376745/Put/vlen=5/mvcc=0}
r=keyvalues={row006/cf1:dummy/1389113376745/Put/vlen=5/mvcc=0}
r=keyvalues={row007/cf1:dummy/1389113376745/Put/vlen=5/mvcc=0, row007/cf1:ip/1389113376745/Put/vlen=10/mvcc=0, row007/cf1:port-70/1389113376745/Put/vlen=5/mvcc=0, row007/cf2:dummy1/1389113376745/Put/vlen=6/mvcc=0, row007/cf2:dummy2/1389113376745/Put/vlen=6/mvcc=0}
r=keyvalues={row008/cf1:dummy/1389113376745/Put/vlen=5/mvcc=0, row008/cf1:ip/1389113376745/Put/vlen=10/mvcc=0, row008/cf1:port-80/1389113376745/Put/vlen=5/mvcc=0, row008/cf2:dummy1/1389113376745/Put/vlen=6/mvcc=0, row008/cf2:dummy2/1389113376745/Put/vlen=6/mvcc=0}
r=keyvalues={row009/cf1:dummy/1389113376745/Put/vlen=5/mvcc=0, row009/cf1:port-80/1389113376745/Put/vlen=5/mvcc=0}
r=keyvalues={row010/cf1:dummy/1389113376745/Put/vlen=5/mvcc=0, row010/cf1:port-80/1389113376745/Put/vlen=5/mvcc=0}
{code}

But I only expected following two row got returned
{code:title=Results I expected}
r=keyvalues={row007/cf1:dummy/1389113376745/Put/vlen=5/mvcc=0, row007/cf1:ip/1389113376745/Put/vlen=10/mvcc=0, row007/cf1:port-70/1389113376745/Put/vlen=5/mvcc=0, row007/cf2:dummy1/1389113376745/Put/vlen=6/mvcc=0, row007/cf2:dummy2/1389113376745/Put/vlen=6/mvcc=0}
r=keyvalues={row008/cf1:dummy/1389113376745/Put/vlen=5/mvcc=0, row008/cf1:ip/1389113376745/Put/vlen=10/mvcc=0, row008/cf1:port-80/1389113376745/Put/vlen=5/mvcc=0, row008/cf2:dummy1/1389113376745/Put/vlen=6/mvcc=0, row008/cf2:dummy2/1389113376745/Put/vlen=6/mvcc=0}
{code}

Test code attached to demo this issue."
HBASE-10287	"HRegionServer#addResult() is called by HRegionServer#mutate() where controller parameter could be null.

HRegionServer#addResult() should check whether rpcc is null."
HBASE-10238	"In TestAccessController#verifyDenied there is code attempting to deal with UTEs with AccessDeniedException as a nested cause. Although it would seem the code intends to handle the exception reported by a recently failed test on the 0.98 Hadoop 1.1 Jenkins build here:

{noformat}
java.lang.reflect.UndeclaredThrowableException: Unknown exception in doAs
Caused by: java.security.PrivilegedActionException: com.google.protobuf.ServiceException: Error calling method PingService.noop
Caused by: com.google.protobuf.ServiceException: Error calling method PingService.noop
Caused by: org.apache.hadoop.hbase.security.AccessDeniedExceptionorg.apache.hadoop.hbase.security.AccessDeniedException: Insufficient permissions (user=UserB, scope=testCoprocessorExec, family=, action=EXEC)
Caused by: org.apache.hadoop.hbase.ipc.RemoteWithExtrasException: org.apache.hadoop.hbase.security.AccessDeniedException: Insufficient permissions (user=UserB, scope=testCoprocessorExec, family=, action=EXEC)
...
{noformat}

it didn't work properly."
HBASE-10208	"These are APIs related to split keys avaiable in HTable same can be to HTI
getStartKeys()
getEndKeys()
getStartEndKeys()
getSplitKeys() - New
and 3 more APIs related to find region location when we specify row also can be added to HTI
getRegionLocation(final String row)
getRegionLocation(final byte [] row)
getRegionLocation(final byte [] row, boolean reload)"
HBASE-10192	"In HBase-8101, we added a check and disallowed empty rowkey.  However, it is allowed in 0.94. I was wondering if there is any special reason for that, i.e., is it by intention or by mistake?"
HBASE-2719	"Hudson is failing the tests from HBASE-2400: http://hudson.hbase.org/job/hbase-trunk/119/#showFailuresLink. These tests pass locally, so it's not clear why they are failing with Hudson."
HBASE-9553	"In order to make it easy on the garbage collector and to avoid full compaction phases we should make sure that all (or at least a large percentage) of the HFile blocks as cached in the block cache are exactly the same size.

Currently an HFile block is typically slightly larger than the declared block size, as the block will accommodate that last KV on the block. The padding would be a ColumnFamily option. In many cases 100 bytes would probably be a good value to make all blocks exactly the same size (but of course it depends on the max size of the KVs).

This does not have to be perfect. The more blocks evicted and replaced in the block cache are of the exact same size the easier it should be on the GC.

Thoughts?
"
HBASE-9671	"In our integration test we saw the following:
{code}
2013-09-26 19:29:47,852|beaver.machine|INFO|2013-09-26 19:29:47,852 INFO  [main] client.HBaseAdmin: Started disable of IntegrationTestLoadAndVerify
...
2013-09-26 19:30:03,459|beaver.machine|INFO|2013-09-26 19:30:03,458 DEBUG [Thread-6] actions.Action: Compacting region IntegrationTestLoadAndVerify,\x8B\xC8\x06\x00\x00\x00\x00\x00/000031_0,1380220935462.da93e4f26dbb801b0da03ffc70b6145d.
...
2013-09-26 19:30:03,500|beaver.machine|INFO|2013-09-26 19:30:03,500 WARN  [Thread-6] policies.Policy: Exception occured during performing action: org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region is not online: da93e4f26dbb801b0da03ffc70b6145d
2013-09-26 19:30:03,500|beaver.machine|INFO|at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionByEncodedName(HRegionServer.java:2576)
2013-09-26 19:30:03,501|beaver.machine|INFO|at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:3961)
2013-09-26 19:30:03,501|beaver.machine|INFO|at org.apache.hadoop.hbase.regionserver.HRegionServer.compactRegion(HRegionServer.java:3776)
2013-09-26 19:30:03,501|beaver.machine|INFO|at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$2.callBlockingMethod(AdminProtos.java:19803)
2013-09-26 19:30:03,502|beaver.machine|INFO|at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2146)
2013-09-26 19:30:03,502|beaver.machine|INFO|at org.apache.hadoop.hbase.ipc.RpcServer$Handler.run(RpcServer.java:1851)
2013-09-26 19:30:03,502|beaver.machine|INFO|
2013-09-26 19:30:03,502|beaver.machine|INFO|at sun.reflect.GeneratedConstructorAccessor24.newInstance(Unknown Source)
2013-09-26 19:30:03,503|beaver.machine|INFO|at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
2013-09-26 19:30:03,503|beaver.machine|INFO|at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
2013-09-26 19:30:03,503|beaver.machine|INFO|at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
2013-09-26 19:30:03,503|beaver.machine|INFO|at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)
2013-09-26 19:30:03,503|beaver.machine|INFO|at org.apache.hadoop.hbase.protobuf.ProtobufUtil.getRemoteException(ProtobufUtil.java:235)
2013-09-26 19:30:03,504|beaver.machine|INFO|at org.apache.hadoop.hbase.client.HBaseAdmin.compact(HBaseAdmin.java:1638)
2013-09-26 19:30:03,504|beaver.machine|INFO|at org.apache.hadoop.hbase.client.HBaseAdmin.compact(HBaseAdmin.java:1602)
2013-09-26 19:30:03,504|beaver.machine|INFO|at org.apache.hadoop.hbase.client.HBaseAdmin.compact(HBaseAdmin.java:1495)
2013-09-26 19:30:03,504|beaver.machine|INFO|at org.apache.hadoop.hbase.chaos.actions.CompactRandomRegionOfTableAction.perform(CompactRandomRegionOfTableAction.java:69)
2013-09-26 19:30:03,504|beaver.machine|INFO|at org.apache.hadoop.hbase.chaos.policies.PeriodicRandomActionPolicy.runOneIteration(PeriodicRandomActionPolicy.java:59)
{code}
CompactRandomRegionOfTableAction didn't check that table IntegrationTestLoadAndVerify was enabled before issuing compaction request."
HBASE-6656	"I'm trying to call a Coprocessor Endpoint from within the preGet handler of a RegionObserver, and it's throwing Class Loader issues.
The exact same Coprocessor Endpoint works perfectly from remote Java client, however, fails from within the same Region Server.
For our particular test environment, only 1 Region Server is available, so I guess it's a ""local"" call that fails, and perhaps a remote RegionServer wouldn't fail, but that doesn't justify the issue :).

The Code within the RegionObserver is roughly (way reduced) as follows:

---
	@Override
	public void preGet(ObserverContext<RegionCoprocessorEnvironment> e, Get get, List<KeyValue> results)
     throws IOException {
		Map<byte[], Set<byte[]>> results;

		// scan: for all regions
		try {
			Batch.Call<PlatformStatsIndexEndpointProtocol,Set<byte[]>> batchCall = new Batch.Call<PlatformStatsIndexEndpointProtocol,Set<byte[]>>() {
				  public Set<byte[]> call(PlatformStatsIndexEndpointProtocol instance) throws IOException{
				    return instance.getKeyTokenByPrefix(index, match, additionalMatches);
				  }
				};
			results = indexTable.coprocessorExec(PlatformStatsIndexEndpointProtocol.class, null, null, batchCall);
		} catch (Throwable e1) {
			e1.printStackTrace();
			throw new IOException(e1);
		}
		
		Set<byte[]> finalResultSet = new HashSet<byte[]>();
		for (Map.Entry<byte[], Set<byte[]>> e : results.entrySet()) {
			finalResultSet.addAll(e.getValue());
		}
	}
---

The Code for the Coprocessor Endpoint is irrelevant, as it never gets executed.

This is the Exception I get on the Client side (Server side logged exception below).

---
Thu Aug 23 17:37:45 CST 2012, org.apache.hadoop.hbase.client.HTable$5@26659db7,java.io.IOException: java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: interface com.company.hbase.platformstats.PlatformStatsIndexEndpointProtocol is not visible from class loader
        at com.company.hbase.platformstats.IndexQueryRegionObserver.preGet(IndexQueryRegionObserver.java:100)
        at org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preGet(RegionCoprocessorHost.java:553)
        at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:3737)
        at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:3639)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1785)
        at sun.reflect.GeneratedMethodAccessor53.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1336)
Caused by: java.io.IOException: java.lang.IllegalArgumentException: interface com.company.hbase.platformstats.PlatformStatsIndexEndpointProtocol is not visible from class loader
        at com.company.hbase.platformstats.PlatformStatsIndexer.getKeyTokens(PlatformStatsIndexer.java:390)
        at com.company.hbase.platformstats.PlatformStatsIndexer.getKeyTokensBySubstring(PlatformStatsIndexer.java:348)
        at com.company.hbase.platformstats.IndexQueryRegionObserver.searchTokens(IndexQueryRegionObserver.java:148)
        at com.company.hbase.platformstats.IndexQueryRegionObserver.preGet(IndexQueryRegionObserver.java:97)
        ... 9 more
Caused by: java.lang.IllegalArgumentException: interface com.company.hbase.platformstats.PlatformStatsIndexEndpointProtocol is not visible from class loader
        at java.lang.reflect.Proxy.getProxyClass(Proxy.java:353)
        at java.lang.reflect.Proxy.newProxyInstance(Proxy.java:581)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$4.call(HConnectionManager.java:1451)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)


        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionServerWithRetries(HConnectionManager.java:1345)
        at org.apache.hadoop.hbase.client.HTable.get(HTable.java:657)
        at Test.queryIndex(Test.java:109)
        at Test.main(Test.java:42)
---

This is the Server Side exception logged:

---
2012-08-23 19:37:44,705 ERROR org.apache.hadoop.hbase.regionserver.HRegionServer:
java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: interface com.company.hbase.platformstats.PlatformStatsIndexEndpointProtocol is not
visible from class loader
        at com.company.hbase.platformstats.IndexQueryRegionObserver.preGet(IndexQueryRegionObserver.java:100)
        at org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preGet(RegionCoprocessorHost.java:553)
        at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:3737)
        at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:3639)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1785)
        at sun.reflect.GeneratedMethodAccessor53.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1336)
Caused by: java.io.IOException: java.lang.IllegalArgumentException: interface com.company.hbase.platformstats.PlatformStatsIndexEndpointProtocol is not visible from class loader
        at com.company.hbase.platformstats.PlatformStatsIndexer.getKeyTokens(PlatformStatsIndexer.java:390)
        at com.company.hbase.platformstats.PlatformStatsIndexer.getKeyTokensBySubstring(PlatformStatsIndexer.java:348)
        at com.company.hbase.platformstats.IndexQueryRegionObserver.searchTokens(IndexQueryRegionObserver.java:148)
        at com.company.hbase.platformstats.IndexQueryRegionObserver.preGet(IndexQueryRegionObserver.java:97)
        ... 9 more
Caused by: java.lang.IllegalArgumentException: interface com.company.hbase.platformstats.PlatformStatsIndexEndpointProtocol is not visible from class loader
        at java.lang.reflect.Proxy.getProxyClass(Proxy.java:353)
        at java.lang.reflect.Proxy.newProxyInstance(Proxy.java:581)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$4.call(HConnectionManager.java:1451)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

---

I'm not senior java developer, so this Class Loader visibility issue goes beyond my reach. I have the theory that because it's a local call, the ClassLoaders for the very same Protocol interface may be different (1 reads from the .jar, another from the remote HTable client call); but I have not been able to prove nor fix this behavior."
HBASE-9872	"This issue (if it is an expected behaviour I can close this) exists in all versions.
If i do modifyColumn and change an HCDs parameter I am able to get back the modified HCD with the latest data.
But when i do modifyTable and in that modify an HCD parameter say for eg. the SCOPE of it then as we don't persist the HCD information as in TableModifyFamilyHandler used for modifycolumn
{code}
  HTableDescriptor htd =
      this.masterServices.getMasterFileSystem().modifyColumn(tableName, familyDesc);
{code}
we are not able to get the updated HCD information on the RegionServer.  So incases of replication where I need to modify the HCD's scope we are not able to make the replication happen. "
HBASE-6609	"When starting up the cluster using

	@BeforeClass
	public static void setUp() throws Exception {
		utility = new HBaseTestingUtility();
		utility.startMiniCluster();
	}

I get the following stack trace under Windows in Eclipse.  Note it is looking for ""ls"" which is not present under Windows.

java.lang.RuntimeException: Error while running command to get file permissions : java.io.IOException: Cannot run program ""ls"": CreateProcess error=2, The system cannot find the file specified
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:200)
	at org.apache.hadoop.util.Shell.run(Shell.java:182)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:375)
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:461)
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:444)
	at org.apache.hadoop.fs.FileUtil.execCommand(FileUtil.java:710)
	at org.apache.hadoop.fs.RawLocalFileSystem$RawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:443)
	at org.apache.hadoop.fs.RawLocalFileSystem$RawLocalFileStatus.getPermission(RawLocalFileSystem.java:418)
	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsAndPermissionCheck(DiskChecker.java:146)
	at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:162)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1574)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1521)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1496)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:417)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:280)
	at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniDFSCluster(HBaseTestingUtility.java:430)
	at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:598)
	at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:554)
	at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:523)
	at net.trajano.nosql.hbase.test.FrameworkTest.setUp(FrameworkTest.java:17)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)
Caused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified
	at java.lang.ProcessImpl.create(Native Method)
	at java.lang.ProcessImpl.<init>(Unknown Source)
	at java.lang.ProcessImpl.start(Unknown Source)
	... 37 more

	at org.apache.hadoop.fs.RawLocalFileSystem$RawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:468)
	at org.apache.hadoop.fs.RawLocalFileSystem$RawLocalFileStatus.getPermission(RawLocalFileSystem.java:418)
	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsAndPermissionCheck(DiskChecker.java:146)
	at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:162)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1574)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1521)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1496)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:417)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:280)
	at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniDFSCluster(HBaseTestingUtility.java:430)
	at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:598)
	at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:554)
	at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:523)
	at net.trajano.nosql.hbase.test.FrameworkTest.setUp(FrameworkTest.java:17)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)
"
HBASE-9974	"Rest sometimes return incomplete xml/json data.

We found these exceptions in rest server.

13/11/15 11:40:51 ERROR mortbay.log:/log/1A:23:11:0C:06:22*
javax.ws.rs.WebApplicationException: javax.xml.bind.MarshalException
 - with linked exception:
[org.mortbay.jetty.EofException]
	at com.sun.jersey.core.provider.jaxb.AbstractRootElementProvider.writeTo(AbstractRootElementProvider.java:159)
	at com.sun.jersey.spi.container.ContainerResponse.write(ContainerResponse.java:306)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1437)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)
	at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:699)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:847)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
	at org.apache.hadoop.hbase.rest.filter.GzipFilter.doFilter(GzipFilter.java:73)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:322)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
Caused by: javax.xml.bind.MarshalException
 - with linked exception:
[org.mortbay.jetty.EofException]
	at com.sun.xml.bind.v2.runtime.MarshallerImpl.write(MarshallerImpl.java:325)
	at com.sun.xml.bind.v2.runtime.MarshallerImpl.marshal(MarshallerImpl.java:249)
	at javax.xml.bind.helpers.AbstractMarshallerImpl.marshal(AbstractMarshallerImpl.java:75)
	at com.sun.jersey.json.impl.JSONMarshallerImpl.marshal(JSONMarshallerImpl.java:74)
	at com.sun.jersey.core.provider.jaxb.AbstractRootElementProvider.writeTo(AbstractRootElementProvider.java:179)
	at com.sun.jersey.core.provider.jaxb.AbstractRootElementProvider.writeTo(AbstractRootElementProvider.java:157)
	... 24 more
Caused by: org.mortbay.jetty.EofException
	at org.mortbay.jetty.HttpGenerator.flush(HttpGenerator.java:791)
	at org.mortbay.jetty.AbstractGenerator$Output.blockForOutput(AbstractGenerator.java:551)
	at org.mortbay.jetty.AbstractGenerator$Output.flush(AbstractGenerator.java:572)
	at org.mortbay.jetty.HttpConnection$Output.flush(HttpConnection.java:1012)
	at org.mortbay.jetty.AbstractGenerator$Output.write(AbstractGenerator.java:651)
	at org.mortbay.jetty.AbstractGenerator$Output.write(AbstractGenerator.java:580)
	at com.sun.jersey.spi.container.servlet.WebComponent$Writer.write(WebComponent.java:307)
	at com.sun.jersey.spi.container.ContainerResponse$CommittingOutputStream.write(ContainerResponse.java:134)
	at com.sun.xml.bind.v2.runtime.output.UTF8XmlOutput.flushBuffer(UTF8XmlOutput.java:416)
	at com.sun.xml.bind.v2.runtime.output.UTF8XmlOutput.text(UTF8XmlOutput.java:369)
	at com.sun.xml.bind.v2.runtime.unmarshaller.Base64Data.writeTo(Base64Data.java:303)
	at com.sun.xml.bind.v2.runtime.output.UTF8XmlOutput.text(UTF8XmlOutput.java:310)
	at com.sun.xml.bind.v2.runtime.XMLSerializer.text(XMLSerializer.java:425)
	at com.sun.xml.bind.v2.model.impl.RuntimeBuiltinLeafInfoImpl$PcdataImpl.writeText(RuntimeBuiltinLeafInfoImpl.java:177)
	at com.sun.xml.bind.v2.runtime.reflect.TransducedAccessor$CompositeTransducedAccessorImpl.writeText(TransducedAccessor.java:261)
	at com.sun.xml.bind.v2.runtime.property.ValueProperty.serializeBody(ValueProperty.java:87)
	at com.sun.xml.bind.v2.runtime.ClassBeanInfoImpl.serializeBody(ClassBeanInfoImpl.java:344)
	at com.sun.xml.bind.v2.runtime.XMLSerializer.childAsXsiType(XMLSerializer.java:700)
	at com.sun.xml.bind.v2.runtime.property.ArrayElementNodeProperty.serializeItem(ArrayElementNodeProperty.java:69)
	at com.sun.xml.bind.v2.runtime.property.ArrayElementProperty.serializeListBody(ArrayElementProperty.java:172)
	at com.sun.xml.bind.v2.runtime.property.ArrayERProperty.serializeBody(ArrayERProperty.java:159)
	at com.sun.xml.bind.v2.runtime.ClassBeanInfoImpl.serializeBody(ClassBeanInfoImpl.java:344)
	at com.sun.xml.bind.v2.runtime.XMLSerializer.childAsXsiType(XMLSerializer.java:700)
	at com.sun.xml.bind.v2.runtime.property.ArrayElementNodeProperty.serializeItem(ArrayElementNodeProperty.java:69)
	at com.sun.xml.bind.v2.runtime.property.ArrayElementProperty.serializeListBody(ArrayElementProperty.java:172)
	at com.sun.xml.bind.v2.runtime.property.ArrayERProperty.serializeBody(ArrayERProperty.java:159)
	at com.sun.xml.bind.v2.runtime.ClassBeanInfoImpl.serializeBody(ClassBeanInfoImpl.java:344)
	at com.sun.xml.bind.v2.runtime.XMLSerializer.childAsSoleContent(XMLSerializer.java:597)
	at com.sun.xml.bind.v2.runtime.ClassBeanInfoImpl.serializeRoot(ClassBeanInfoImpl.java:328)
	at com.sun.xml.bind.v2.runtime.XMLSerializer.childAsRoot(XMLSerializer.java:498)
	at com.sun.xml.bind.v2.runtime.MarshallerImpl.write(MarshallerImpl.java:320)
	... 29 more
Caused by: java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcher.write0(Native Method)
	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:29)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:69)
	at sun.nio.ch.IOUtil.write(IOUtil.java:26)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:336)
	at org.mortbay.io.nio.ChannelEndPoint.flush(ChannelEndPoint.java:170)
	at org.mortbay.io.nio.SelectChannelEndPoint.flush(SelectChannelEndPoint.java:221)
	at org.mortbay.jetty.HttpGenerator.flush(HttpGenerator.java:725)
	... 59 more
"
HBASE-9109	"This problem is observed when region server dies while an endpoint coprocessor is executing. On the client side channel.getLastRegion() returns null and we get null pointer exception while updating result map. 
Following stack-trace is seen on client:

Caused by: java.lang.NullPointerException
at org.apache.hadoop.hbase.util.Bytes.compareTo(Bytes.java:981)
at org.apache.hadoop.hbase.util.Bytes$ByteArrayComparator.compare(Bytes.java:128)
at org.apache.hadoop.hbase.util.Bytes$ByteArrayComparator.compare(Bytes.java:119)
at java.util.TreeMap.put(TreeMap.java:530)
at java.util.Collections$SynchronizedMap.put(Collections.java:1979)
at org.apache.hadoop.hbase.client.HTable$17.update(HTable.java:1372)
at org.apache.hadoop.hbase.client.HTable$18.call(HTable.java:1401)
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
at java.util.concurrent.FutureTask.run(FutureTask.java:138)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:662)"
HBASE-9923	"Need to Delete a row based on the column name and value, not using rowkey in hbase. Please provide query.

Need to Delete a row based on partial rowkey in hbase.Please provide query.

Thanks in advance.

"
HBASE-9803	"Use hbase-daemon.sh start regionserver 锛?but no regionserver start锛宼he log is

Fri Oct 18 20:01:47 CST 2013 Starting regionserver on h021046.eos.grid.xxx.com.cn
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 513920
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 65536
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 10240
cpu time               (seconds, -t) unlimited
max user processes              (-u) 65536
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
Fri Oct 18 20:14:21 CST 2013 Starting regionserver on h021046.eos.grid.xxx.com.cn
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 513920
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 65536
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 10240
cpu time               (seconds, -t) unlimited
max user processes              (-u) 65536
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited"
HBASE-9592	"When a downstream project depends on hbase, and uses hadoop.profile in it's build, then maven craps out because the pom file generated by generate-hadoopX-poms.sh will not activate the relevant hadoop profile:

{code}
mvn -Dhadoop.profile=2 dependency:tree 
[INFO] Scanning for projects...
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building foo-app 1.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[WARNING] The POM for org.apache.hbase:hbase-common:jar:0.96.0 is invalid, transitive dependencies (if any) will not be available, enable debug logging for more details
[WARNING] The POM for org.apache.hbase:hbase-common:jar:tests:0.96.0 is invalid, transitive dependencies (if any) will not be available, enable debug logging for more details
[WARNING] The POM for org.apache.hbase:hbase-client:jar:0.96.0 is invalid, transitive dependencies (if any) will not be available, enable debug logging for more details
[WARNING] The POM for org.apache.hbase:hbase-server:jar:0.96.0 is invalid, transitive dependencies (if any) will not be available, enable debug logging for more details
[WARNING] The POM for org.apache.hbase:hbase-server:jar:tests:0.96.0 is invalid, transitive dependencies (if any) will not be available, enable debug logging for more details
[WARNING] The POM for org.apache.hbase:hbase-hadoop-compat:jar:tests:0.96.0 is invalid, transitive dependencies (if any) will not be available, enable debug logging for more details
[WARNING] The POM for org.apache.hbase:hbase-hadoop2-compat:jar:tests:0.96.0 is invalid, transitive dependencies (if any) will not be available, enable debug logging for more details
{code}

Note that a lot of downstream components like flume, mahout, sqoop use this property. "
HBASE-8976	"http://54.241.6.143/job/HBase-TRUNK/424/org.apache.hbase$hbase-server/testReport/junit/org.apache.hadoop.hbase.client/TestFromClientSideWithCoprocessor/testClientPoolThreadLocal/

{code}
Error Message

expected null, but was:<java.lang.AssertionError: The number of versions of '[B@755d828f:[B@4e26b67b did not match 4 expected:<4> but was:<3>>
Stacktrace

java.lang.AssertionError: expected null, but was:<java.lang.AssertionError: The number of versions of '[B@755d828f:[B@4e26b67b did not match 4 expected:<4> but was:<3>>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotNull(Assert.java:664)
	at org.junit.Assert.assertNull(Assert.java:646)
	at org.junit.Assert.assertNull(Assert.java:656)
	at org.apache.hadoop.hbase.client.TestFromClientSide.testClientPoolThreadLocal(TestFromClientSide.java:4687)
...
{code}

I do not think I have seen this fail recently.  Making this issue so can keep an eye on it.  If anyone wants to take this on in meantime, they are welcome.  Thanks."
HBASE-9069	"This is second time I've seen this fail.  Anyone want to take a look?  I am filing this as placeholder to keep account of failures.

https://builds.apache.org/job/HBase-TRUNK/4306/testReport/junit/org.apache.hadoop.hbase.thrift/TestThriftServerCmdLine/testRunThriftServer_18_/

"
HBASE-8765	"I noticed that the current split behavior is rather suboptimal with regard to compactions. On large regions, HFile size limit triggers a split. Split is followed by major compaction to get rid of the partial reference files. However, HFile size limit is surpassed after compaction most of the time.
So, first we rewrite a lot of data into a new file. Then we say ""Oh look! A large file!"", split the region and rewrite everything again.

Perhaps region split should be based on store size, or incoming compaction size - large enough compaction should be converted into splits.

Thoughts? I think basing off store size is a simple fix, and will code it up soon if there are no objections"
HBASE-4814	"I've seen a situation where regions were splitting almost exactly at the same time as an alter command was issued and those regions' daughters were left unaltered. It would even seem that the daughters' daughters also share this situation.

Reopening all the regions fixes the problem."
HBASE-3937	"I describe the scenario of how this problem happened:

1.HMaster assigned the region A to RS1. So the RegionState was set to PENDING_OPEN.
2.For there's too many opening requests, the open process on RS1 was blocked.
3.Some time later, TimeoutMonitor found the assigning of A was timeout. For the RegionState was in PENDING_OPEN, went into the following handler process(Just put the region into an waiting-assigning set):

   case PENDING_OPEN:
      LOG.info(""Region has been PENDING_OPEN for too "" +
          ""long, reassigning region="" +
          regionInfo.getRegionNameAsString());
      assigns.put(regionState.getRegion(), Boolean.TRUE);
      break;
So we can see that, under this case, we consider the ZK node state was OFFLINE. Indeed, in an normal disposal, it's OK.

4.But before the real-assigning, the requests of RS1 was disposed. So that affected the new-assigning. For it update the ZK node state from OFFLINE to OPENING. 

5.The new assigning started, so it send region to open in RS2. But while the opening, it should update the ZK node state from OFFLINE to OPENING. For the current state is OPENING, so this operation failed.
So this region couldn't be open success anymore.

So I think, to void this problem , under the case of PENDING_OPEN of TiemoutMonitor, we should transform the ZK node state to OFFLINE first.

"
HBASE-7135	"2012-11-07 08:35:36,082 ERROR org.apache.hadoop.hbase.io.hfile.slab.SingleSizeCache: Deserializer threw an exception. This may indicate a bug.
java.io.IOException: Invalid HFile block magic: \x00\x00\x00\x00\x00\x00\x00\x00
at org.apache.hadoop.hbase.io.hfile.BlockType.parse(BlockType.java:153)
at org.apache.hadoop.hbase.io.hfile.BlockType.read(BlockType.java:164)
at org.apache.hadoop.hbase.io.hfile.HFileBlock.<init>(HFileBlock.java:254)
at org.apache.hadoop.hbase.io.hfile.HFileBlock$1.deserialize(HFileBlock.java:148)
at org.apache.hadoop.hbase.io.hfile.HFileBlock$1.deserialize(HFileBlock.java:140)
at org.apache.hadoop.hbase.io.hfile.slab.SingleSizeCache.getBlock(SingleSizeCache.java:166)
at org.apache.hadoop.hbase.io.hfile.slab.SlabCache.getBlock(SlabCache.java:245)
at org.apache.hadoop.hbase.io.hfile.DoubleBlockCache.getBlock(DoubleBlockCache.java:100)
at org.apache.hadoop.hbase.io.hfile.HFileReaderV2.getBlockFromCache(HFileReaderV2.java:267)
at org.apache.hadoop.hbase.io.hfile.HFileReaderV2.readBlock(HFileReaderV2.java:349)
at org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader.loadDataBlockWithScanInfo(HFileBlockIndex.java:257)
at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.seekTo(HFileReaderV2.java:498)
at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.seekTo(HFileReaderV2.java:522)
at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seekAtOrAfter(StoreFileScanner.java:226)
at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seek(StoreFileScanner.java:145)
"
HBASE-7627	"While testing snapshot branch I found this exception.  

{code}
2013-01-18 05:42:09,832 ERROR org.apache.hadoop.hbase.master.CatalogJanitor: Caught exception
java.lang.UnsupportedOperationException
	at java.util.AbstractList.remove(AbstractList.java:144)
	at org.apache.hadoop.hbase.client.HTable.delete(HTable.java:738)
	at org.apache.hadoop.hbase.catalog.MetaEditor.deleteFromMetaTable(MetaEditor.java:145)
	at org.apache.hadoop.hbase.catalog.MetaEditor.deleteFromMetaTable(MetaEditor.java:132)
	at org.apache.hadoop.hbase.catalog.MetaEditor.deleteDaughtersReferencesInParent(MetaEditor.java:372)
	at org.apache.hadoop.hbase.master.CatalogJanitor.removeDaughtersFromParent(CatalogJanitor.java:288)
	at org.apache.hadoop.hbase.master.CatalogJanitor.cleanParent(CatalogJanitor.java:239)
	at org.apache.hadoop.hbase.master.CatalogJanitor.scan(CatalogJanitor.java:142)
	at org.apache.hadoop.hbase.master.CatalogJanitor.initialChore(CatalogJanitor.java:74)
	at org.apache.hadoop.hbase.Chore.run(Chore.java:65)
	at java.lang.Thread.run(Thread.java:662)
{code}

This was introduced by HBASE-7365 (currently committed in snapshot branch, soon to be committed to trunk.)"
HBASE-7751	"From https://builds.apache.org/job/HBase-TRUNK/3846/testReport/org.apache.hadoop.hbase.ipc/TestDelayedRpc/testDelayedRpcImmediateReturnValue/:
{code}
java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
	at java.util.ArrayList.rangeCheck(ArrayList.java:604)
	at java.util.ArrayList.get(ArrayList.java:382)
	at org.apache.hadoop.hbase.ipc.TestDelayedRpc.testDelayedRpc(TestDelayedRpc.java:96)
	at org.apache.hadoop.hbase.ipc.TestDelayedRpc.testDelayedRpcImmediateReturnValue(TestDelayedRpc.java:59)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
Standard Output

2013-02-02 07:49:26,693 INFO  [pool-1-thread-1] hbase.ResourceChecker(147): before: ipc.TestDelayedRpc#testDelayedRpcImmediateReturnValue Thread=5, OpenFileDescriptor=81, MaxFileDescriptor=4096, ConnectionCount=0
2013-02-02 07:49:31,746 WARN  [pool-1-thread-1] impl.MetricsSystemImpl(137): Metrics system not started: Cannot locate configuration: tried hadoop-metrics2-hbase.properties, hadoop-metrics2.properties
Exception in thread ""Thread-7"" java.lang.AssertionError: Unexpected exception: null	at org.junit.Assert.fail(Assert.java:88)	at org.apache.hadoop.hbase.ipc.TestDelayedRpc$TestThread.run(TestDelayedRpc.java:249)Exception in thread ""Thread-8"" java.lang.AssertionError: Unexpected exception: null	at org.junit.Assert.fail(Assert.java:88)	at org.apache.hadoop.hbase.ipc.TestDelayedRpc$TestThread.run(TestDelayedRpc.java:249)2013-02-02 07:49:36,410 WARN  [IPC Client (47) connection to /127.0.0.1:36850 from jenkins] ipc.HBaseClient$Connection(640): Unexpected exception receiving call responses
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.receiveResponse(HBaseClient.java:1003)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.run(HBaseClient.java:637)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.receiveResponse(HBaseClient.java:999)
	... 1 more
Exception in thread ""Thread-9"" java.lang.AssertionError: Unexpected exception: null	at org.junit.Assert.fail(Assert.java:88)	at org.apache.hadoop.hbase.ipc.TestDelayedRpc$TestThread.run(TestDelayedRpc.java:249)2013-02-02 07:49:36,449 INFO  [pool-1-thread-1] hbase.ResourceChecker(171): after: ipc.TestDelayedRpc#testDelayedRpcImmediateReturnValue Thread=21 (was 5)
{code}
Here is related code where NPE was thrown:
{code}
            rpcResponseType = ProtobufRpcClientEngine.Invoker.getReturnProtoType(
                getMethod(remoteId.getProtocol(),
                          call.param.getMethodName()));
{code}
"
HBASE-4614	Kinger on IRC found out that it's currently impossible to CopyTable between clusters if there's a zoo.cfg on the classpath as it will take precedence over the --peer.adr or whatever else the user passes.
HBASE-4697	"After the introduction of CacheConfig, over in HBASE-4641 we dealt with a bug of the block cache being instantiated in the master.  A short-term fix was done there.

This is the real fix.  The block cache should not be a static reference, it should be instantiated within HRS construction/initialization."
HBASE-4538	"Saw this in a failed TestAdmin on 0.92

{code}
2011-10-05 01:18:58,890 ERROR [MASTER_OPEN_REGION-sv4r9s38,52146,1317777098450-2] executor.EventHandler(171): Caught throwable while processing event RS_ZK_REGION_OPENED
java.lang.NullPointerException
        at org.apache.hadoop.hbase.master.AssignmentManager.updateTimers(AssignmentManager.java:1053)
        at org.apache.hadoop.hbase.master.AssignmentManager.regionOnline(AssignmentManager.java:1027)
        at org.apache.hadoop.hbase.master.handler.OpenedRegionHandler.process(OpenedRegionHandler.java:108)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:168)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}"
HBASE-5958	"ByteString.copyFrom makes a copy of a byte array in case it is changed in other thread.
In most case, we don't need to worry about that.  We should avoid copying the bytes
for performance issue."
HBASE-6323	"Most of the metrics in replication were written with 1 slave in mind but with multiple slaves the issue really shows. Most of the metrics are set directly:

{code}
public void enqueueLog(Path log) {
  this.queue.put(log);
  this.metrics.sizeOfLogQueue.set(queue.size());
}
{code}

So {{sizeOfLogQueue}} is always showing the size of the queue that updated the metric last.

I'm not sure what's the right way to fix this since we can't have dynamic metrics. Merging them would work here but it wouldn't work so well with {{ageOfLastShippedOp}} since the age can be different and it definitely cannot be summed.

Assigning to Elliott since he seems to dig metrics these days. "
HBASE-7813	"Try a BulkDeleteEndpoint using DeleteType.COLUMN or DeleteType.VERSION and give it a set of Columns which are not in the row.

A Delete is constructed using just the row alone. No family/column/version is added and it is applied, thus deleting the entire row."
HBASE-9479	"Right now, HBase contains so many dependencies, that using the most basic HBase functionality such as HConnection in a larger application is unreasonably hard.  For example, trying to include HBase connectivity in a Spring web app leads to hundreds of JarClassLoader errors such as:

{code}
JarClassLoader: Warning: org/apache/commons/collections/FastHashMap$CollectionView.class in lib/commons-collections-3.2.1.jar is hidden by lib/commons-beanutils-core-1.8.3.jar (with different bytecode)
JarClassLoader: Warning: org/apache/commons/collections/FastHashMap$EntrySet.class in lib/commons-collections-3.2.1.jar is hidden by lib/commons-beanutils-core-1.8.3.jar (with different bytecode)
JarClassLoader: Warning: org/apache/commons/collections/FastHashMap$KeySet.class in lib/commons-collections-3.2.1.jar is hidden by lib/commons-beanutils-core-1.8.3.jar (with different bytecode)
JarClassLoader: Warning: org/apache/commons/collections/FastHashMap$Values.class in lib/commons-collections-3.2.1.jar is hidden by lib/commons-beanutils-core-1.8.3.jar (with different bytecode)
JarClassLoader: Warning: org/apache/commons/collections/FastHashMap.class in lib/commons-collections-3.2.1.jar is hidden by lib/commons-beanutils-core-1.8.3.jar (with different bytecode)
JarClassLoader: Warning: javax/servlet/Filter.class in lib/servlet-api-2.5-6.1.14.jar is hidden by lib/javax.servlet-3.0.0.v201112011016.jar (with different bytecode)
JarClassLoader: Warning: javax/servlet/FilterChain.class in lib/servlet-api-2.5-6.1.14.jar is hidden by lib/javax.servlet-3.0.0.v201112011016.jar (with different bytecode)
JarClassLoader: Warning: javax/servlet/FilterConfig.class in lib/servlet-api-2.5-6.1.14.jar is hidden by lib/javax.servlet-3.0.0.v201112011016.jar (with different bytecode)
JarClassLoader: Warning: javax/servlet/GenericServlet.class in lib/servlet-api-2.5-6.1.14.jar is hidden by lib/javax.servlet-3.0.0.v201112011016.jar (with different bytecode)
JarClassLoader: Warning: javax/servlet/http/Cookie.class in lib/servlet-api-2.5-6.1.14.jar is hidden by lib/javax.servlet-3.0.0.v201112011016.jar (with different bytecode)
{code}


Why is this all bundled together?  Why not have an ""hbase-client"" or ""hbase-client-dev"" package which is friendly for creating applications?

I have spent 2+ days attempting to run a web service which is backed by HBase with no luck.  I have created several stack overflow questions:

http://stackoverflow.com/questions/18703903/java-massive-class-collision

http://stackoverflow.com/questions/18690582/how-to-create-jetty-spring-app-with-hbase-connection

The use of BeanUtils is also known to have a very bad issue:

""The three jars contain wrong classes""
https://issues.apache.org/jira/browse/BEANUTILS-398

Why is this so difficult?  How do I include what I need to make an HBase app.  So far I have tried using Maven, but this approach is draconian, and I have not succeeded.  Am I Pwned?

{code}
   <dependencyManagement>
        <dependencies>
            <dependency>
                <groupId>org.jboss.netty</groupId>
                <artifactId>netty</artifactId>
                <version>3.2.4.Final</version>
            </dependency>
            <dependency>
                <groupId>org.codehaus.jackson</groupId>
                <artifactId>jackson-core-asl</artifactId>
                <version>1.9.12</version>
            </dependency>
            <dependency>
                <groupId>org.codehaus.jackson</groupId>
                <artifactId>jackson-jaxrs</artifactId>
                <version>1.9.12</version>
            </dependency>
            <dependency>
                <groupId>com.sun.xml.bind</groupId>
                <artifactId>jackson-jaxrs</artifactId>
                <version>2.2.6</version>
            </dependency>
            <dependency>
                <groupId>com.sun.xml.bind</groupId>
                <artifactId>jaxb-impl</artifactId>
                <version>2.2.6</version>
            </dependency>
            <dependency>
                <groupId>log4j</groupId>
                <artifactId>log4j</artifactId>
                <version>1.2.16</version>
            </dependency>
            <dependency>
                <groupId>asm</groupId>
                <artifactId>asm</artifactId>
                <version>3.3.1</version>
            </dependency>
            <dependency>
                <groupId>commons-codec</groupId>
                <artifactId>commons-codec</artifactId>
                <version>1.4</version>
            </dependency>
            <dependency>
                <groupId>commons-lang</groupId>
                <artifactId>commons-lang</artifactId>
                <version>2.5</version>
            </dependency>
            <dependency>
                <groupId>org.slf4j</groupId>
                <artifactId>slf4j-api</artifactId>
                <version>1.7.5</version>
            </dependency>
            <dependency>
                <groupId>commons-logging</groupId>
                <artifactId>commons-logging</artifactId>
                <version>1.1.1</version>
            </dependency>
            <dependency>
                <groupId>org.codehaus.jackson</groupId>
                <artifactId>jackson-mapper-asl</artifactId>
                <version>1.9.12</version>
            </dependency>
            <dependency>
                <groupId>org.slf4j</groupId>
                <artifactId>slf4j-log4j12</artifactId>
                <version>1.6.1</version>
            </dependency>
            <dependency>
                <groupId>org.apache.httpcomponents</groupId>
                <artifactId>httpcore</artifactId>
                <version>4.1.3</version>
            </dependency>
            <dependency>
                <groupId>commons-httpclient</groupId>
                <artifactId>commons-httpclient</artifactId>
                <version>3.1</version>
            </dependency>
            <dependency>
                <groupId>org.codehaus.jackson</groupId>
                <artifactId>jackson-xc</artifactId>
                <version>1.9.12</version>
            </dependency>
            <dependency>
                <groupId>commons-beanutils</groupId>
                <artifactId>commons-beanutils-core</artifactId>
                <version>1.8.3</version>
            </dependency>
            <dependency>
                <groupId>commons-beanutils</groupId>
                <artifactId>commons-beanutils</artifactId>
                <version>1.8.3</version>
            </dependency>

        </dependencies>
    </dependencyManagement>
    <dependencies>
        <dependency>
            <groupId>commons-beanutils</groupId>
            <artifactId>commons-beanutils-core</artifactId>
            <version>1.8.3</version>
        </dependency>
        <dependency>
            <groupId>commons-beanutils</groupId>
            <artifactId>commons-beanutils</artifactId>
            <version>1.8.3</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.cxf</groupId>
            <artifactId>cxf-rt-frontend-jaxrs</artifactId>
            <version>2.7.2</version>
        </dependency>
        <dependency>
            <groupId>org.apache.cxf</groupId>
            <artifactId>cxf-rt-frontend-jaxws</artifactId>
            <version>2.7.2</version>
        </dependency>
        <dependency>
            <groupId>org.codehaus.jackson</groupId>
            <artifactId>jackson-core-asl</artifactId>
            <version>1.9.12</version>
        </dependency>
        <dependency>
            <groupId>org.codehaus.jackson</groupId>
            <artifactId>jackson-mapper-asl</artifactId>
            <version>1.9.12</version>
        </dependency>
        <dependency>
            <groupId>org.codehaus.jackson</groupId>
            <artifactId>jackson-jaxrs</artifactId>
            <version>1.9.12</version>
        </dependency>
        <dependency>
            <groupId>org.codehaus.jackson</groupId>
            <artifactId>jackson-xc</artifactId>
            <version>1.9.12</version>
        </dependency>
        <dependency>
            <groupId>org.springframework</groupId>
            <artifactId>spring-core</artifactId>
            <version>3.2.4.RELEASE</version>
        </dependency>
        <dependency>
            <groupId>org.springframework</groupId>
            <artifactId>spring-context</artifactId>
            <version>3.2.4.RELEASE</version>
        </dependency>
        <dependency>
            <groupId>org.springframework</groupId>
            <artifactId>spring-webmvc</artifactId>
            <version>3.2.4.RELEASE</version>
        </dependency>
        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-classic</artifactId>
            <version>1.0.13</version>
        </dependency>
        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-core</artifactId>
            <version>1.0.13</version>
        </dependency>
        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <version>4.11</version>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>net.sf.opencsv</groupId>
            <artifactId>opencsv</artifactId>
            <version>2.3</version>
        </dependency>
        <dependency>
            <groupId>javax.inject</groupId>
            <artifactId>javax.inject</artifactId>
            <version>1</version>
        </dependency>
        <dependency>
            <groupId>org.eclipse.jetty</groupId>
            <artifactId>jetty-server</artifactId>
            <version>9.0.4.v20130625</version>
        </dependency>
        <dependency>
            <groupId>org.eclipse.jetty</groupId>
            <artifactId>jetty-webapp</artifactId>
            <version>9.0.4.v20130625</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hbase</groupId>
            <artifactId>hbase</artifactId>
            <version>0.94.11</version>
            <exclusions>
                <exclusion>
                    <groupId>org.codehaus.jackson</groupId>
                    <artifactId>jackson-core-asl</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.codehaus.jackson</groupId>
                    <artifactId>jackson-jaxrs</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.codehaus.jackson</groupId>
                    <artifactId>jackson-mapper-asl</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.codehaus.jackson</groupId>
                    <artifactId>jackson-xc</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>com.sun.jersey</groupId>
                    <artifactId>jersey-core</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>com.sun.jersey</groupId>
                    <artifactId>jersey-json</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>com.sun.jersey</groupId>
                    <artifactId>jersey-server</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.mortbay.jetty</groupId>
                    <artifactId>jetty</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>com.sun.jersey</groupId>
                    <artifactId>jetty-util</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>com.sun.jersey</groupId>
                    <artifactId>jsp-2.1</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>com.sun.jersey</groupId>
                    <artifactId>jsp-api-2.1</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>com.sun.jersey</groupId>
                    <artifactId>servlet-api-2.5</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>javax.xml.bind</groupId>
                    <artifactId>jaxb-api</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>tomcat</groupId>
                    <artifactId>jasper-compiler</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>tomcat</groupId>
                    <artifactId>jasper-runtime</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.slf4j</groupId>
                    <artifactId>slf4j-api</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.slf4j</groupId>
                    <artifactId>slf4j-log4j12</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.apache.thrift</groupId>
                    <artifactId>libthrift</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-core</artifactId>
            <version>1.2.1</version>
            <exclusions>
                <exclusion>
                    <groupId>com.sun.jersey</groupId>
                    <artifactId>jersey-core</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>com.sun.jersey</groupId>
                    <artifactId>jersey-json</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>com.sun.jersey</groupId>
                    <artifactId>jersey-server</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>tomcat</groupId>
                    <artifactId>jasper-compiler</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>tomcat</groupId>
                    <artifactId>jasper-runtime</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
    </dependencies>
{code}

"
HBASE-9363	REST/Thrift web UI is not working properly any more.
HBASE-8950	"Currently, the HbaseTestingUtility is not importable in versions 0.94.x without Sonatype Aether dynamic classpath modification. Please change this to make the HbaseTestingUtility visible and importable without special classpath modifications.

That is:
{code}
import org.apache.hadoop.hbase.HBaseTestingUtility;
{code}
should succeed after placing org.apache.hbase/hbase ""0.94.X"" and higher on the classpath."
HBASE-9037	"From Stack on the mailing list:
{quote}
I see this every 4ms or so when reading.

21 2013-07-24 10:55:24,594 DEBUG
org.apache.hadoop.hbase.io.hfile.LruBlockCache: Block cache LRU eviction
completed; freed=24.78 MB, total=185.13 MB, single=0 KB, multi=207.88 MB,
memory=0.64 KB
20 2013-07-24 10:55:28,975 DEBUG
org.apache.hadoop.hbase.io.hfile.LruBlockCache: Block cache LRU eviction
started; Attempting to free 24.75 MB of total=209.91 MB

Loads of it.
{quote}
Should either limit the log frequency of this, or change to trace."
HBASE-8973	"While trying to test some things, ended up modifying the rowcounter job a little. Proposing a patch with these changes."
HBASE-8708	"How can i load data in a file that is resides in hdfs in to a hbase table?

Is Hbase have any external table facility like as hive provides?






"
HBASE-8677	"These days, I test one of available clients REST. When I startup two REST servers and put these servers into Cluster instance using the REST client classes, I get the wrong results by getScanner interface. 

Code like this(37, 38 are two rest servers):
        Cluster cluster = new Cluster();
        cluster.add(""10.28.171.37"", 8080);
        cluster.add(""10.28.171.38"", 8080);
        Client client = new Client(cluster);
        RemoteHTable table = new RemoteHTable(client, ""demotime"");
        
        ResultScanner resultScanner = table.getScanner(new Scan());
        for (Result result: resultScanner) {
            System.out.println(""Scan row[""+        Bytes.toString(result.getRow())+""]:""+result);
        }

I find server-side codes of REST maybe the cause. They use ""static final Map<String, ScannerInstanceResource> scanners"" to cache the last scanner context, so if there are multi REST servers, client may navigate to the other server, which has not scanner conext of it and return the wrong results.
"
HBASE-8464	"In FastDiffEncoder
Inside compressSingleKeyValue()
{code}
 currentState.prevOffset = in.position();
    int keyLength = in.getInt();
    int valueOffset =
        currentState.prevOffset + keyLength + KeyValue.ROW_OFFSET;
    int valueLength = in.getInt();
    byte flag = 0;
{code}
Before seeing the bug, whenever we write something into encoders, we take the ByteBuffer that is created by Writer.append().
This basically writes
keyLength, valueLength, keyarray, valuearray, <memstoreTS> 
Now consider a case where the keyarray size is 20 and valuearray size is 20.
As per the above code for the first KV
Read keyLength (4  bytes), value length (4 bytes).
First time the prevOffset is 0 so our value Offset is - 0+20+8 =28.
This is correct.
After the first KV is read when we take up the next KV,
Now the currentState.prevOffset => 28+20 = 48 (the value is also read)
The above calculation will give us
28+20+8 = 56.
But the bytebuffer has only 48 bytes in it.

Why our testcases did not catch this bug?
========================================
It is because in the TestDataBlockEncoders we create a ByteBuffer directly from the KVs and we do not create the way the HFileWriterV2 does it.
See RedundantKVGenerator.convertKvToByteBuffer().

Pls correct me if am wrong.  I can provide a patch for the same if my above analysis is correct."
HBASE-8385	"Expected behavior:
A user should be able to:
1. Take a snapshot of a table
2. Delete that table
3. Use the snapshot to restore that deleted table

Observed behavior:
During a restore, we attempt to create a snapshot of the table should the restore go awry. However, the snapshot fails because the table that we want to snapshot is not present.

{code}
Stack trace:

org.apache.hadoop.hbase.exceptions.SnapshotCreationException: org.apache.hadoop.hbase.exceptions.SnapshotCreationException: Could not build snapshot handler
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:95)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:79)
	at org.apache.hadoop.hbase.ipc.ProtobufRpcClientEngine$Invoker.invoke(ProtobufRpcClientEngine.java:146)
	at $Proxy24.snapshot(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$MasterProtocolHandler.invoke(HConnectionManager.java:1703)
	at org.apache.hadoop.hbase.client.$Proxy25.snapshot(Unknown Source)
	at org.apache.hadoop.hbase.client.HBaseAdmin$18.call(HBaseAdmin.java:2337)
	at org.apache.hadoop.hbase.client.HBaseAdmin$18.call(HBaseAdmin.java:1)
	at org.apache.hadoop.hbase.client.HBaseAdmin.executeCallable(HBaseAdmin.java:2637)
	at org.apache.hadoop.hbase.client.HBaseAdmin.execute(HBaseAdmin.java:2612)
	at org.apache.hadoop.hbase.client.HBaseAdmin.takeSnapshotAsync(HBaseAdmin.java:2334)
	at org.apache.hadoop.hbase.client.HBaseAdmin.snapshot(HBaseAdmin.java:2279)
	at org.apache.hadoop.hbase.client.HBaseAdmin.snapshot(HBaseAdmin.java:2252)
	at org.apache.hadoop.hbase.client.HBaseAdmin.snapshot(HBaseAdmin.java:2204)
	at org.apache.hadoop.hbase.client.HBaseAdmin.restoreSnapshot(HBaseAdmin.java:2417)
	at org.apache.hadoop.hbase.client.TestRestoreSnapshotFromClient.testRestoreSnapshotOfDeleted(TestRestoreSnapshotFromClient.java:266)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)
Caused by: org.apache.hadoop.hbase.ipc.RemoteWithExtrasException: org.apache.hadoop.hbase.exceptions.SnapshotCreationException: Could not build snapshot handler
	at org.apache.hadoop.hbase.master.snapshot.SnapshotManager.snapshotDisabledTable(SnapshotManager.java:573)
	at org.apache.hadoop.hbase.master.snapshot.SnapshotManager.takeSnapshot(SnapshotManager.java:524)
	at org.apache.hadoop.hbase.master.HMaster.snapshot(HMaster.java:2521)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.ProtobufRpcServerEngine$Server.call(ProtobufRpcServerEngine.java:174)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1871)
Caused by: java.io.IOException: HTableDescriptor missing for testtb-1366407250932
	at org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler.loadTableDescriptor(TakeSnapshotHandler.java:125)
	at org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler.prepare(TakeSnapshotHandler.java:136)
	at org.apache.hadoop.hbase.master.snapshot.DisabledTableSnapshotHandler.prepare(DisabledTableSnapshotHandler.java:77)
	at org.apache.hadoop.hbase.master.snapshot.SnapshotManager.snapshotDisabledTable(SnapshotManager.java:557)
	... 8 more

	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:1321)
	at org.apache.hadoop.hbase.ipc.ProtobufRpcClientEngine$Invoker.invoke(ProtobufRpcClientEngine.java:131)
	... 44 more
{code}"
HBASE-8184	"thrift2 is what our thrift interface should be.  It is unfinished though and without an owner.  While in place, it prompts why a thrift2 and a thrift1 questions.  Meantime, thrift1 is what folks use and it is getting bug fixes.  Suggest we remove thrift2 till it gets carried beyond thrift1."
HBASE-8647	"Running tests I see this from time to time:

{code}
 19 2013-05-29 13:42:37,868 INFO  [Thread-476] util.ChaosMonkey$RestartRandomRs(274): Performing action: Restart random region server
 20 2013-05-29 13:42:37,869 WARN  [Thread-476] util.ChaosMonkey$PeriodicRandomActionPolicy(578): Exception occured during performing action: java.lang.NullPointerException
 21 ,...at org.apache.hadoop.hbase.util.ChaosMonkey$Action.getCurrentServers(ChaosMonkey.java:160)
 22 ,...at org.apache.hadoop.hbase.util.ChaosMonkey$RestartRandomRs.perform(ChaosMonkey.java:275)
 23 ,...at org.apache.hadoop.hbase.util.ChaosMonkey$PeriodicRandomActionPolicy.runOneIteration(ChaosMonkey.java:576)
 24 ,...at org.apache.hadoop.hbase.util.ChaosMonkey$PeriodicPolicy.run(ChaosMonkey.java:488)
 25 ,...at org.apache.hadoop.hbase.util.ChaosMonkey$CompositeSequentialPolicy.run(ChaosMonkey.java:458)
 26 ,...at java.lang.Thread.run(Thread.java:680)
{code}

Our monkey has killed everything."
HBASE-6523	During some testing here at Salesforce.com we found another scenario where an HConnectionImplementation would never recover from a lost ZK connection.
HBASE-8554	"Currently Scan.startRow() is only used for determining which Region to look up the row into but we do not seek within the region to the start row. Since its not uncommon to run with large sized regions these days 5G, this is suboptimal."
HBASE-3669	"After going crazy killing region servers after HBASE-3668, most of the cluster recovered except for 3 regions that kept being refused by the region servers.

One the master I would see:
{code}
2011-03-17 22:23:14,828 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  supr_rss_items,ea0a3ac6c8779dab:872333599:ed1a7ad00f076fd98fcd3adcd98b62c6,1285707378709.f11849557c64c4efdbe0498f3fe97a21. state=PENDING_OPEN, ts=1300400554826
2011-03-17 22:23:14,828 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been PENDING_OPEN for too long, reassigning region=supr_rss_items,ea0a3ac6c8779dab:872333599:ed1a7ad00f076fd98fcd3adcd98b62c6,1285707378709.f11849557c64c4efdbe0498f3fe97a21.
2011-03-17 22:23:14,828 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=supr_rss_items,ea0a3ac6c8779dab:872333599:ed1a7ad00f076fd98fcd3adcd98b62c6,1285707378709.f11849557c64c4efdbe0498f3fe97a21. state=PENDING_OPEN, ts=1300400554826
2011-03-17 22:23:14,828 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: No previous transition plan was found (or we are ignoring an existing plan) for supr_rss_items,ea0a3ac6c8779dab:872333599:ed1a7ad00f076fd98fcd3adcd98b62c6,1285707378709.f11849557c64c4efdbe0498f3fe97a21. so generated a random one; hri=supr_rss_items,ea0a3ac6c8779dab:872333599:ed1a7ad00f076fd98fcd3adcd98b62c6,1285707378709.f11849557c64c4efdbe0498f3fe97a21., src=, dest=sv2borg171,60020,1300399357135; 17 (online=17, exclude=null) available servers
2011-03-17 22:23:14,828 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region supr_rss_items,ea0a3ac6c8779dab:872333599:ed1a7ad00f076fd98fcd3adcd98b62c6,1285707378709.f11849557c64c4efdbe0498f3fe97a21. to sv2borg171,60020,1300399357135

{code}

Then on the region server:

{code}
2011-03-17 22:23:14,829 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x22d627c142707d2 Attempting to transition node f11849557c64c4efdbe0498f3fe97a21 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2011-03-17 22:23:14,832 DEBUG org.apache.hadoop.hbase.zookeeper.ZKUtil: regionserver:60020-0x22d627c142707d2 Retrieved 166 byte(s) of data from znode /hbase/unassigned/f11849557c64c4efdbe0498f3fe97a21; data=region=supr_rss_items,ea0a3ac6c8779dab:872333599:ed1a7ad00f076fd98fcd3adcd98b62c6,1285707378709.f11849557c64c4efdbe0498f3fe97a21., server=sv2borg180,60020,1300384550966, state=RS_ZK_REGION_OPENING
2011-03-17 22:23:14,832 WARN org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x22d627c142707d2 Attempt to transition the unassigned node for f11849557c64c4efdbe0498f3fe97a21 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING failed, the node existed but was in the state RS_ZK_REGION_OPENING
2011-03-17 22:23:14,832 WARN org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Failed transition from OFFLINE to OPENING for region=f11849557c64c4efdbe0498f3fe97a21
{code}

I'm not sure I fully understand what was going on... the master was suppose to OFFLINE the znode but then that's not what the region server was seeing? In any case, I was able to recover by doing a force unassign for each region and then assign."
HBASE-2522	"If for any reason the master takes too much time to process a MSG_REPORT_SPLIT_INCLUDES_DAUGHTERS, and that a user successfully disables the table, then the daughter regions will still be assigned and marked as offline in .META.

A test case I'm uploading to HBASE-2515 shows the issue."
HBASE-7539	No point in having stale META record when the server that put it has already closed the region.
HBASE-5912	"I ran oprofile on a YCSB client and found that a large percentage of the CPU time was going to this function:

51991     0.4913  25361.jo                 java                     java.lang.reflect.Method[] java.lang.Class.copyMethods(java.lang.reflect.Method[])
51384     0.4856  25361.jo                 java                     int org.apache.hadoop.hbase.ipc.ProtocolSignature.getFingerprint(java.lang.reflect.Method)
50428     0.4766  25361.jo                 java                     void java.util.Arrays.sort1(int[], int, int)

We should introduce a simple cache to avoid this overhead."
HBASE-4794	"In AssignmentManager.getReopenStatus, it calls a version of MetaReader.getTableRegions that sets excludeOfflinedSplitParents to false meaning that the offline parents are returned. What this means is that if one of them was already closed before the alter command was issued (and I believe there are a few other cases) then the alter will hang until the CatalogJanitor sweeps the parent .META. row.

Since the CJ sleep time is 5 minutes, the worst case scenario is an alter that takes almost 5 minutes.

Here's an example:

{quote}
925/948 regions updated.
920/943 regions updated.
913/934 regions updated.
912/928 regions updated.
912/928 regions updated.

(5 minutes later)

912/928 regions updated.
912/928 regions updated.
905/918 regions updated.
897/906 regions updated.
891/892 regions updated.
891/891 regions updated.
Done.
{quote}

I can confirm with the log that 37 parent regions were cleaned up.

Also it's pretty nice to see how the number fluctuates up and down :)"
HBASE-5086	"I got this twice during the same test.

If the region servers are slow enough and you run an online alter, it's possible for the RS to change the znode status to CLOSED and have the master send an OPEN before the region server is able to remove the region from it's list of RITs.

This is what the master sees:

{quote}
011-12-21 22:24:09,498 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Starting unassignment of region test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f. (offlining)
2011-12-21 22:24:09,498 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:62003-0x134589d3db033f7 Creating unassigned node for 43123e2e3fc83ec25fe2a76b4f09077f in a CLOSING state
2011-12-21 22:24:09,524 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Sent CLOSE to sv4r25s44,62023,1324494325099 for region test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f.
2011-12-21 22:24:15,656 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_CLOSED, server=sv4r25s44,62023,1324494325099, region=43123e2e3fc83ec25fe2a76b4f09077f
2011-12-21 22:24:15,656 DEBUG org.apache.hadoop.hbase.master.handler.ClosedRegionHandler: Handling CLOSED event for 43123e2e3fc83ec25fe2a76b4f09077f
2011-12-21 22:24:15,656 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f. state=CLOSED, ts=1324506255629, server=sv4r25s44,62023,1324494325099
2011-12-21 22:24:15,656 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:62003-0x134589d3db033f7 Creating (or updating) unassigned node for 43123e2e3fc83ec25fe2a76b4f09077f with OFFLINE state
2011-12-21 22:24:15,663 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Found an existing plan for test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f. destination server is + sv4r25s44,62023,1324494325099
2011-12-21 22:24:15,663 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Using pre-existing plan for region test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f.; plan=hri=test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f., src=, dest=sv4r25s44,62023,1324494325099
2011-12-21 22:24:15,663 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f. to sv4r25s44,62023,1324494325099
2011-12-21 22:24:15,664 ERROR org.apache.hadoop.hbase.master.AssignmentManager: Failed assignment in: sv4r25s44,62023,1324494325099 due to org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: Received:OPEN for the region:test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f. ,which we are already trying to CLOSE.
{quote}

After that the master abandons.

And the region server:
{quote}
2011-12-21 22:24:09,523 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Received close region: test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f.
2011-12-21 22:24:09,523 DEBUG org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler: Processing close of test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f.
2011-12-21 22:24:09,524 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Closing test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f.: disabling compactions & flushes
2011-12-21 22:24:09,524 INFO org.apache.hadoop.hbase.regionserver.HRegion: Running close preflush of test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f.
2011-12-21 22:24:09,524 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memstore flush for test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f., current region memstore size 40.5m
2011-12-21 22:24:09,524 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished snapshotting test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f., commencing wait for mvcc, flushsize=42482936
2011-12-21 22:24:13,368 DEBUG org.apache.hadoop.hbase.regionserver.Store: Renaming flushed file at hdfs://sv4r11s38:9100/hbase/test1/43123e2e3fc83ec25fe2a76b4f09077f/.tmp/87d6944c54c7417e9a34a9f9542bcb72 to hdfs://sv4r11s38:9100/hbase/test1/43123e2e3fc83ec25fe2a76b4f09077f/actions/87d6944c54c7417e9a34a9f9542bcb72
2011-12-21 22:24:13,568 INFO org.apache.hadoop.hbase.regionserver.Store: Added hdfs://sv4r11s38:9100/hbase/test1/43123e2e3fc83ec25fe2a76b4f09077f/actions/87d6944c54c7417e9a34a9f9542bcb72, entries=54209, sequenceid=31451012, memsize=40.5m, filesize=31.4m
2011-12-21 22:24:14,381 INFO org.apache.hadoop.hbase.regionserver.HRegion: Finished memstore flush of ~40.5m/42482936, currentsize=218.9k/224128 for region test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f. in 4856ms, sequenceid=31451012, compaction requested=true
2011-12-21 22:24:15,267 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Updates disabled for region test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f.
2011-12-21 22:24:15,267 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memstore flush for test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f., current region memstore size 218.9k
2011-12-21 22:24:15,267 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished snapshotting test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f., commencing wait for mvcc, flushsize=224128
2011-12-21 22:24:15,330 DEBUG org.apache.hadoop.hbase.regionserver.Store: Renaming flushed file at hdfs://sv4r11s38:9100/hbase/test1/43123e2e3fc83ec25fe2a76b4f09077f/.tmp/0a744b85cec5454e873a7c27bf9b3c53 to hdfs://sv4r11s38:9100/hbase/test1/43123e2e3fc83ec25fe2a76b4f09077f/actions/0a744b85cec5454e873a7c27bf9b3c53
2011-12-21 22:24:15,346 INFO org.apache.hadoop.hbase.regionserver.Store: Added hdfs://sv4r11s38:9100/hbase/test1/43123e2e3fc83ec25fe2a76b4f09077f/actions/0a744b85cec5454e873a7c27bf9b3c53, entries=286, sequenceid=31451619, memsize=218.9k, filesize=170.2k
2011-12-21 22:24:15,347 INFO org.apache.hadoop.hbase.regionserver.HRegion: Finished memstore flush of ~218.9k/224128, currentsize=0.0/0 for region test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f. in 80ms, sequenceid=31451619, compaction requested=true
2011-12-21 22:24:15,365 INFO org.apache.hadoop.hbase.regionserver.HRegion: Closed test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f.
2011-12-21 22:24:15,365 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:62023-0x134589d3db03403 Attempting to transition node 43123e2e3fc83ec25fe2a76b4f09077f from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2011-12-21 22:24:15,637 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:62023-0x134589d3db03403 Successfully transitioned node 43123e2e3fc83ec25fe2a76b4f09077f from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2011-12-21 22:24:15,670 DEBUG org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler: set region closed state in zk successfully for region test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f. sn name: sv4r25s44,62023,1324494325099
2011-12-21 22:24:15,670 DEBUG org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler: Closed region test1,db6db6b4,1324501004642.43123e2e3fc83ec25fe2a76b4f09077f.
{quote}

Doing a force unassign in the shell fixes it.

A small-ish fix would be to add to RegionAlreadyInTransitionException which state it's in so that we can detect this case and then just retry or open on another region server.

This is critical for online altering to work, but I don't think it's likely to happen in other situations."
HBASE-1241	"This issue is to batch all of the edits and additions we make to ZooKeeper for its use in HBase. Rather than wasting lots of our (and ZK's) time with little edit patches, we will send them batch patches from here when things stabilize."
HBASE-1193	"I would wait for HBASE-880 which will change the client API
in a way that I think would be easier to create the abstractions
you are looking at.

---
Jim Kellerman, Powerset (Live Search, Microsoft Corporation)
- Hide quoted text -


> -----Original Message-----
> From: edward@udanax.org [mailto:edward@udanax.org] On Behalf Of Edward J.
> Yoon
> Sent: Tuesday, February 03, 2009 1:01 AM
> To: hbase-dev@hadoop.apache.org; hama-dev@incubator.apache.org
> Subject: Hbase client
>
> I would like to have abstraction layers for the HTable and RowResult
> because My project uses the two classes to implement Graph and AdjList
> (or Matrix and Vector). w/o abstraction layers, the codes are becoming
> very messy.
>
> What do you think?
> --
> Best Regards, Edward J. Yoon @ NHN, corp.
> edwardyoon@apache.org
> http://blog.udanax.org
"
HBASE-8445	"when I update a coprocessor jar, then I disable and enable the table with the coprocessor, but the new features in the updated coprocessor jar doesn't make any sense. Follow into the class 'org.apache.hadoop.hbase.coprocessor.CoprocessorHost', I found that there's a coprocessor class loader cache , of which the key is the coprocessor jar path(although the key is a weak reference), so when I disable/enable the table, it got a cached coprocessor class loader from the cache with the jar path, and it didn't try to reload the coprocessor jar from the hdfs. Here I give a patch, in which I add an extra info which is 'FileCheckSum' with the coprocessor class loader cache, if the checksum is changed, try to reload the jar from the hdfs path"
HBASE-2827	"A client on our beta tier was stuck in this exception loop when we issued a new HMaster after the old one died:

Exception while trying to connect hBase
java.lang.reflect.UndeclaredThrowableException
at $Proxy1.getClusterStatus(Unknown Source)
at org.apache.hadoop.hbase.client.HBaseAdmin.getClusterStatus(HBaseAdmin.java:912)
at org.apache.hadoop.hbase.client.HTable.getCurrentNrHRS(HTable.java:170)
at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:143)
...
at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
at java.lang.Thread.run(Thread.java:619)
Caused by: java.net.SocketTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/10.18.34.212:60000]
at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:213)
at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:406)
at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupIOstreams(HBaseClient.java:309)
at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:856)
at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:724)
at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:252)
... 20 more
12:52:55,863 [pool-4-thread-5182] INFO PersistentUtil:153 - Retry after 1 second...

Looking at the client code, the HConnectionManager does not watch ZK for NodeDeleted & NodeCreated of /hbase/master"
HBASE-2887	"Refactored the way the HBaseFsck tool works -- it now loads all the info it can find into RAM first, and then looks over the in-memory structures for inconsistencies. It's still a work in progress, but detects more kinds of problems now (eg multiple assignment, etc) and has some very basic functional tests."
HBASE-2990	"This is something I've seen here since we upgraded to 0.89 since Gets are now Scans, although I don't currently have any strong evidence that it wasn't happening in 0.20.

I saw this when we began getting alerts on the frontend that some requests were taking more than 8 seconds to complete. Even getting a value could take more than 3 minutes in the shell. The first thing I did was major compacting the table that was slow and the problem went away immediately. Looking in the logs, it seems the compaction transformed 2 files of (total) 550MB into 5.2MB. Looking back in the logs for September, it appears that that table was never major compacted and was slowly growing everyday. Some more grepping around showed that quite a few regions were never major compacted.

I'm still looking at the code, but the issue seems to be that the minor compactions are always happening on all store files more than once per day on certain regions, meaning that the oldest timestamp is always smaller than hbase.hregion.majorcompaction and major compactions are never triggered."
HBASE-8120	"When working on HBASE-8116, Enis discovered that TestSnapshotCloneIndependence.java was missing in 0.94

We should add the test in 0.94"
HBASE-8273	"See discussion on dev@ mailing list, entitled 'Does compatibility between versions also mean binary compatibility?'

Synopsis from that thread:

HBASE-5357 ""Use builder pattern in HColumnDescriptor"" changed the
method signatures by changing ""void"" to ""HColumnDescriptor"" so it' not
the same methods anymore.

if you invoke setters
on HColumnDescriptor as you'll get:

java.lang.NoSuchMethodError:
org.apache.hadoop.hbase.HColumnDescriptor.setMaxVersions(I)V"
HBASE-7893	"For disabling auto major compaction, I configured hbase.hregion.majorcompaction = 0 in the config file and restarted the cluster. In spite of this, major compaction continues to run everyday. Here is the config I set:

<property>
  <name>hbase.hregion.majorcompaction</name>
  <value>0</value>
</property>

What other way can I disable auto major compaction? I expected this config to work. What am I doing wrong here? 

Please advice. Thanks a lot."
HBASE-8168	"When I used to start Hbase, for the first time Hmaster is running. If i type JPS for second consecutive time, Hmaster is not working. It is showing some thing like the below.

rg.apache.zookeeper.ClientCnxn: Opening socket connection to server ubuntu-1/172.16.78.122:2222. Will not attempt to authenticate using SASL (unknown error)
 2013-03-20 22:04:33,491 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to ubuntu-1/172.16.78.122:2222, initiating session
 2013-03-20 22:04:33,492 INFO org.apache.zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x0, likely server has closed socket, closing socket connection and attempting reconnect
 2013-03-20 22:04:33,592 WARN org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper exception: org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase
 2013-03-20 22:04:33,593 INFO org.apache.hadoop.hbase.util.RetryCounter: Sleeping 4000ms before retry #2...
 2013-03-20 22:04:33,760 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server CLBBLR-6196/172.16.78.91:2222. Will not attempt to authenticate using SASL (unknown error)
 2013-03-20 22:04:33,760 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to CLBBLR-6196/172.16.78.91:2222, initiating session
 2013-03-20 22:04:33,761 INFO org.apache.zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x0, likely server has closed socket, closing socket connection and attempting reconnect
 2013-03-20 22:04:34,840 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server CLBBLR-6176/172.16.78.136:2222. Will not attempt to authenticate using SASL (unknown error)
 2013-03-20 22:04:34,840 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to CLBBLR-6176/172.16.78.136:2222, initiating session
 2013-03-20 22:04:34,841 INFO org.apache.zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x0, likely server has closed socket, closing socket connection and attempting reconnect
 2013-03-20 22:04:36,026 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server ubuntu-1/172.16.78.122:2222. Will not attempt to authenticate using SASL (unknown error)
 2013-03-20 22:04:36,027 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to ubuntu-1/172.16.78.122:2222, initiating session
 2013-03-20 22:04:36,028 INFO org.apache.zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x0, likely server has closed socket, closing socket connection and attempting reconnect
 2013-03-20 22:04:36,570 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server CLBBLR-6196/172.16.78.91:2222. Will not attempt to authenticate using SASL (unknown error)
 2013-03-20 22:04:36,571 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to CLBBLR-6196/172.16.78.91:2222, initiating session
 2013-03-20 22:04:36,572 INFO org.apache.zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x0, likely server has closed socket, closing socket connection and attempting reconnect
 2013-03-20 22:04:37,505 INFO org.apache.zookeeper.ClientCnxn: Op

As discussed.Hbase is starting properly without any issue. When we are trying to list the tables in Hbase or create the table, HMaster is not running.

 
 
 

"
HBASE-8110	Get#setMaxVersions() -> Get#getMaxVersions(). Looks like a fat finger mistake.
HBASE-7985	4 failures of this test in the last 6 builds.
HBASE-7697	"The user experience for importing data into HBase and getting a dump out of HBase is pretty poor. The existing tools as I understand them include:
- org.apache.hadoop.hbase.mapreduce.Export,
- org.apache.hadoop.hbase.mapreduce.Import,
- org.apache.hadoop.hbase.mapreduce.ImportTsv,
- org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles, and
- org.apache.hadoop.hbase.mapreduce.CopyTable

Each one provides specific features that do not necessarily overlap with the others. For instance, Import and ImportTsv could have most of their logic combined, sharing common driver code and leaving the details of the file-format up to the user to provide via a pluggable mapper. Export and CopyTable both map over a target table; it's only the detail of what they do with the data that is different. Bulk operations via HFiles could be a more common use-case as well, not just a special case of ImportTsv.

The list of [open issues|https://issues.apache.org/jira/issues/?filter=-1&jql=project%20%3D%20HBASE%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened%2C%20%22Patch%20Available%22)%20AND%20text%20~%20%22ImportTsv%22%20ORDER%20BY%20updatedDate%20DESC] against ImportTsv alone indicates users are using the tool, and I certainly advise it for people getting started with a new HBase deployment.

I propose a single interface for getting data into and out of HBase. It would be pluggable, allowing users to override details of their file formats and schemas. We can provide implementations that replicate existing tool behaviors as example modules. These tools are also a reasonable place, IMHO, to include support for creation and loading of snapshots.

I started down the path of a specific tool intended to overcome some of the limitations of ImportTsv and it has since refactored into a more general purpose application. Initial patches forthcoming. Comments strongly encouraged."
HBASE-7973	"{{HFile.getWriterFactory(Configuration)}} has migrated to {{HFile.getWriterFactoryNoCache(Configuration)}} and {{HFile.WriterFactory.createWriter(...)}} is now protected in {{HFile.WriterFactory}}

As the result, Bigtop integration tests won't compile."
HBASE-3907	"Add plumbing needed to add various types of per ColumnFamily metrics. And to start with add a bunch per-CF metrics such as:

1) Blocks read, cache hit, avg time of read for a column family.
2) Similar stats for compaction related reads.
3) Stats for meta block reads per CF
4) Bloom Filter stats per CF
etc.
"
HBASE-7863	"See HBASE-7843, I figured the patch could be split for easier review."
HBASE-7860	"We are currently unable to use ACLs without having Kerberos setup.  That is a pain for testing and environments that have other authentication methods that are not Kerberos-centric.

safety valve:
<property>
     <name>hbase.security.authorization</name>
     <value>true</value>
</property>
<property>
     <name>hbase.coprocessor.master.classes</name>
     <value>org.apache.hadoop.hbase.security.access.AccessController</value>
</property>
<property>
     <name>hbase.coprocessor.region.classes</name>
     <value>org.apache.hadoop.hbase.security.token.TokenProvider,org.apache.hadoop.hbase.security.access.AccessController</value>
</property>

[root@cdh4-oozie-1 ~]# hbase shell
hbase(main):001:0> create 't1', 'cf1'

ERROR: org.apache.hadoop.hbase.security.AccessDeniedException: org.apache.hadoop.hbase.security.AccessDeniedException: Insufficient permissions for user 'null' (global, action=CREATE)
	at org.apache.hadoop.hbase.security.access.AccessController.requirePermission(AccessController.java:402)
	at org.apache.hadoop.hbase.security.access.AccessController.preCreateTable(AccessController.java:525)
	at org.apache.hadoop.hbase.master.MasterCoprocessorHost.preCreateTable(MasterCoprocessorHost.java:89)
	at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.java:1056)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1345)

[root@cdh4-oozie-1 ~]# su hbase
bash-4.1$ hbase shell

hbase(main):001:0> create 't1', 'cf1'

ERROR: org.apache.hadoop.hbase.security.AccessDeniedException: org.apache.hadoop.hbase.security.AccessDeniedException: Insufficient permissions for user 'null' (global, action=CREATE)
	at org.apache.hadoop.hbase.security.access.AccessController.requirePermission(AccessController.java:402)
	at org.apache.hadoop.hbase.security.access.AccessController.preCreateTable(AccessController.java:525)
	at org.apache.hadoop.hbase.master.MasterCoprocessorHost.preCreateTable(MasterCoprocessorHost.java:89)
	at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.java:1056)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1345)

It looks like we are relying on Kerberos to tell us who the user is, but since we are not using authentication, we are just passing NULL.  We should be able to just rely on the local fs account."
HBASE-7762	"When hbase is used to store external table in conjunction with hive, the create table in hive fails with the following error:

{code}
Failed with exception java.lang.RuntimeException: hbase-default.xml file seems to be for and old version of HBase (0.94.2), this version is 0.9
4.5.5.1302010003
{code}

Looking at the classpath I don't see multiple version of hbase jar. Not sure where the 0.94.2 is coming from. It works fine if _hbase.defaults.for.version.skip_ is set to _true_."
HBASE-7668	"I have a problem that I have been trap weeks, can somebody can help me?
 With full of appreciate
The problem is below:
I have writed about 1G data to the hbase ,then it does not work.
when I set the  hadoop dfsadmin -safemode leave ,then the hbase can use ""list"" command,but when i use ""count 'tableTest' or get ,put and so on ,finaly ,It tell me below锛?
hbase(main):002:0> count 'zsfTest'

ERROR: org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region is not online: -ROOT-,,0
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:2862)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getClosestRowBefore(HRegionServer.java:1768)
        at sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1336)

Here is some help for this command:
Courows in a table. This operation may take a LONG
time (Run '$HADOOP_HOME/bin/hadoop jar hbase.jar rowcount' to run a
counting mapreduce job). Current count is shown every 1000 rows by
default. Count interval may be optionally specified. Scan caching
is enabled on count scans by default. Default cache size is 10 rows.
If your rows are small in size, you may want to increase this
parameter. Examples:

 hbase> count 't1'
 hbase> count 't1', INTERVAL => 100000
 hbase> count 't1', CACHE => 1000
 hbase> count 't1', INTERVAL => 10, CACHE => 1000


hbase(main):003:0> "
HBASE-7650	And it strange that everything seems to work despite this...
HBASE-7573	"We have deployed a hbase cluster without kerberos.And for security issue, we want to upgrade it to a hbase cluster with kerberos. After we config hbase cluster with kerberos, we found that the hmaster can not read/write the data in zookeeper the the hbase cluster without kerberos left.

How can we migrate to a hbase cluster with kerberos from  a hbase cluster without kerberos? We want to keep the data in the hbase cluster.
"
HBASE-6324	When handling Thrift calls in the regionserver we should not go through RPC to talk to the local regionserver.
HBASE-7324	"We should always move the logs to .oldlogs instead of deleting them directly. 
The negative effect of this bug may cause data-loss if we enabled replication.
The below code is extracted from SplitLogManager#splitLogDistributed:
{code}
for(Path logDir: logDirs){
      status.setStatus(""Cleaning up log directory..."");
      try {
        if (fs.exists(logDir) && !fs.delete(logDir, false)) {
          LOG.warn(""Unable to delete log src dir. Ignoring. "" + logDir);
        }
      } catch (IOException ioe) {
        FileStatus[] files = fs.listStatus(logDir);
        if (files != null && files.length > 0) {
          LOG.warn(""returning success without actually splitting and "" + 
              ""deleting all the log files in path "" + logDir);
        } else {
          LOG.warn(""Unable to delete log src dir. Ignoring. "" + logDir, ioe);
        }
      }
      tot_mgr_log_split_batch_success.incrementAndGet();
    }
{code}"
HBASE-7313	"ColumnPaginationFilter does not reset count to zero on moving to next row. Hence, if we have already gotten ""limit"" number of columns - the subsequent rows will always return 0 columns."
HBASE-7261	"Put uses batchMutate to do the actual work.  Put calls startRegionOperation/closeRegionOperation.  BatchMutate does the same.  If the same thread already holds the lock, lock it again doesn't increase the lock count.
However, releasing lock is a little different.  If the lock is already released, IllegalMonitorStateException will throw if it is released again.

There could be other calls. I will look into it more."
HBASE-5281	"In {{AssignmentManager}}'s {{CreateUnassignedAsyncCallback}}, we have the following condition:

{code}
if (rc != 0) {
        // Thisis resultcode.  If non-zero, need to resubmit.
        LOG.warn(""rc != 0 for "" + path + "" -- retryable connectionloss -- "" +
          ""FIX see http://wiki.apache.org/hadoop/ZooKeeper/FAQ#A2"");
        this.zkw.abort(""Connectionloss writing unassigned at "" + path +
          "", rc="" + rc, null);
        return;
}
{code}

While a similar structure inside {{ExistsUnassignedAsyncCallback}} (which the above is linked to), does not have such a force abort.

Do we really require the abort statement here, or can we make do without?"
HBASE-7144	"In working on HBASE-7131, we noticed that the client still retries the same server in case a NotServingRegionException.  It should relocate the region instead using the same region server."
HBASE-7117	"UnixOperationSystemMXBean compile  error with open JDK 1.6.

UnixOperatingSystemMXBean doesn't existed in open JDK 1.6, open JDK doesn't have any get*FileDescriptorCount method in OperatingSystemMXBean at all, we need to provide a corresponding method for it.
"
HBASE-6563	"2012-05-05 00:49:43,265 ERROR org.apache.hadoop.hbase.regionserver.HRegionServer$MajorCompactionChecker: Caught exception
java.lang.NullPointerException
at org.apache.hadoop.hbase.regionserver.Store.isMajorCompaction(Store.java:938)
at org.apache.hadoop.hbase.regionserver.Store.isMajorCompaction(Store.java:917)
at org.apache.hadoop.hbase.regionserver.HRegion.isMajorCompaction(HRegion.java:3250)
at org.apache.hadoop.hbase.regionserver.HRegionServer$MajorCompactionChecker.chore(HRegionServer.java:1222)
at org.apache.hadoop.hbase.Chore.run(Chore.java:66)"
HBASE-7102	"The culster is started normally. It can work when the I/O pressure is small.

However, when I run a large pressure job (with a lot of threads, each of which writes and reads frequently) about one hour, one of the region server will crash.

I investigated the logs of the HRegionServer, they didn't contain any exception log.

Specifically, the log of the down server ends with some normal log (info level log).

This bug can be replayed easily, and each time the crashed server is different. Even more, the log of the crashed server ends with different information for different experiment.

I really don't know why the process of HRegionServer disappear so weirdly.
If the process is crashed due to my configuration or the enviornment, the log should contain some exception information, right?

So, I doubt the reason is the JVM crashed. But I didn't find any error log in JVM.

How to go on the test to find the reason?"
HBASE-7094	"In previous jira I made locks and leases usable from coprocessors.
I think the same should be a possible for MVCC.
As seens in HBASE-4583 and HBASE-7051 it is not possible anymore to implement atomic operations correctly without some control over MVCC."
HBASE-6615	"when we close region锛宻torefile.closeReader(true) is invoked, it seems that hbase.rs.evictblocksonclose is ineffective."
HBASE-2407	"PE --nomapred randomWrite 15

{noformat}
Exception in thread ""13"" java.lang.NullPointerException
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.deleteCachedLocation(HConnectionManager.java:876)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.processBatchOfPuts(HConnectionManager.java:1402)
        at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:645)
        at org.apache.hadoop.hbase.client.HTable.put(HTable.java:511)
        at org.apache.hadoop.hbase.PerformanceEvaluation$RandomWriteTest.testRow(PerformanceEvaluation.java:940)
        at org.apache.hadoop.hbase.PerformanceEvaluation$Test.testTimed(PerformanceEvaluation.java:781)
        at org.apache.hadoop.hbase.PerformanceEvaluation$Test.test(PerformanceEvaluation.java:766)
        at org.apache.hadoop.hbase.PerformanceEvaluation.runOneClient(PerformanceEvaluation.java:1099)
        at org.apache.hadoop.hbase.PerformanceEvaluation$1.run(PerformanceEvaluation.java:510)
{noformat}
"
HBASE-5853	"2012-04-23 12:51:07,474 WARN org.apache.hadoop.ipc.Client: Unexpected error reading responses on connection Thread[IPC Client (1260987126) connection to server121/172.16.40.121:9000 from smp,5,main]
java.lang.RuntimeException: readObject can't find class org.apache.hadoop.hdfs.protocol.HdfsFileStatus
	at org.apache.hadoop.io.ObjectWritable.loadClass(ObjectWritable.java:372)
	at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:223)
	at org.apache.hadoop.io.ObjectWritable.readFields(ObjectWritable.java:75)
	at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:832)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:756)
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.hdfs.protocol.HdfsFileStatus not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1151)
	at org.apache.hadoop.io.ObjectWritable.loadClass(ObjectWritable.java:368)
	... 4 more
2012-04-23 12:51:07,797 FATAL org.apache.hadoop.hbase.regionserver.HRegionServer: ABORTING region server server124,60020,1335152900476: Replay of HLog required. Forcing server shutdown
org.apache.hadoop.hbase.DroppedSnapshotException: region: hbase_cdr,e0072b2b-5e19-431f-bb69-a6427765eac4,1334902272934.8365a7cbf90dd558f297d70224113c8a.
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1278)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1162)
	at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:1104)
	at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:400)
	at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushOneForGlobalPressure(MemStoreFlusher.java:202)
	at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.run(MemStoreFlusher.java:223)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Failed on local exception: java.io.IOException: Error reading responses; Host Details : local host is: ""server124/172.16.40.124""; destination host is: """"server121"":9000; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:724)
	at org.apache.hadoop.ipc.Client.call(Client.java:1094)
	at org.apache.hadoop.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:193)
	at $Proxy10.getFileInfo(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:100)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:65)
	at $Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1172)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:725)
	at org.apache.hadoop.hbase.regionserver.StoreFile.computeHDFSBlockDistribution(StoreFile.java:449)
	at org.apache.hadoop.hbase.regionserver.StoreFile.open(StoreFile.java:473)
	at org.apache.hadoop.hbase.regionserver.StoreFile.createReader(StoreFile.java:548)
	at org.apache.hadoop.hbase.regionserver.Store.internalFlushCache(Store.java:595)
	at org.apache.hadoop.hbase.regionserver.Store.flushCache(Store.java:506)
	at org.apache.hadoop.hbase.regionserver.Store.access$100(Store.java:89)
	at org.apache.hadoop.hbase.regionserver.Store$StoreFlusherImpl.flushCache(Store.java:1905)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1254)
	... 6 more
Caused by: java.io.IOException: Error reading responses
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:763)
Caused by: java.lang.RuntimeException: readObject can't find class org.apache.hadoop.hdfs.protocol.HdfsFileStatus
	at org.apache.hadoop.io.ObjectWritable.loadClass(ObjectWritable.java:372)
	at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:223)
	at org.apache.hadoop.io.ObjectWritable.readFields(ObjectWritable.java:75)
	at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:832)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:756)
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.hdfs.protocol.HdfsFileStatus not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1151)
	at org.apache.hadoop.io.ObjectWritable.loadClass(ObjectWritable.java:368)
	... 4 more"
HBASE-5331	"It looks like there's an off by one bug in the OfflineMerger constructor in util.HMerge:

      InternalScanner rootScanner =
        root.getScanner(scan);

      try {
        List<KeyValue> results = new ArrayList<KeyValue>();
        while(rootScanner.next(results)) {
          for(KeyValue kv: results) {
            HRegionInfo info = Writables.getHRegionInfoOrNull(kv.getValue());
            if (info != null) {
              metaRegions.add(info);
            }
          }
        }
      } finally {
	...
      }

That call to InternalScanner.next() in the while condition returns true if there's another result *after* the one it just loaded into the out param.  That is, after it reads the last row into the 'results' collection, it returns false and the loop exits with that last row unread.  It probably wants to be structured more like this:

final InternalScanner metaScanner = meta.getScanner(scan); List<KeyValue> results = Lists.newArrayList();

while (true) {

	boolean hasMore = metaScanner.next(results);

	for (KeyValue kv : results) {
		HRegionInfo hri = Writables.getHRegionInfoOrNull(kv.getValue());
		if (hri != null) {
			regionInfo.add(hri);
		}
	}
					
	if (!hasMore) {
		break;
	}
}

The loop in util.HMerge is scanning ROOT for META regions.  So this bug will only be hit when there is more than one region of META.  Personally, I don't have any installations with more than one META region, and I'm not sure if anyone does, so this might be a moot point.
"
HBASE-6699	"We recently had a requirement where we need to log the information about various users who were using non-secure HBase cluster. 
The user level logging is supported as part of security, but in 0.92, 0.94 security related code is separate. This jira is about adding that support in non-secure code.

This feature is already there in trunk, after we merge the security related code."
HBASE-3801	"When the HMaster crash, the Backup HMaster blocked for waiting the ZK notify.
The Backup HMaster's thread stack is :
""master-hp1:60000"" prio=10 tid=0x00000000484c6800 nid=0x4b56 waiting on condition [0x0000000040209000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.hbase.master.HMaster.stallIfBackupMaster(HMaster.java:251)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:279)

   Locked ownable synchronizers:
        - None"
HBASE-6745	"Currently, memstore flusher always chooses the biggest memstore region to flush.

Suppose I have two tables: one is very actively updated, while the other is periodically updated. The active one has biggest memstore all the time and is flushed all the time.  But the in-active one never gets a chance to flush.  Since it is not flushed, the hlog file can't be archived, although there are lots of hlog files.

If the active table happens to have big updates all the time, the hlog files could cause huge disk space pressure.

Other than the memstore size, periodically flushing regions based on hlog roll time is helpful in hlog archiving/replication.
"
HBASE-3918	"This one came up in the case where the data was copied from one cluster to another.  The first cluster was running 0.89.x.  The second 0.90.x.  On startup of 0.90.x, it wanted to verify .META. was in the location -ROOT- said it was at, so it tried connect to the FIRST cluster.  The attempt failed because of mismatched RPCs.  The master then actually aborted.

{code}
org.apache.hadoop.hbase.ipc.HBaseRPC$VersionMismatch: Protocol org.apache.hadoop.hbase.ipc.HRegionInterface version mismatch. (client = 27, server = 24)
at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:424)
at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:393)
at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:444)
at org.apache.hadoop.hbase.ipc.HBaseRPC.waitForProxy(HBaseRPC.java:349)
at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:965)
at org.apache.hadoop.hbase.catalog.CatalogTracker.getCachedConnection(CatalogTracker.java:386)
at org.apache.hadoop.hbase.catalog.CatalogTracker.getMetaServerConnection(CatalogTracker.java:285)
at org.apache.hadoop.hbase.catalog.CatalogTracker.verifyMetaRegionLocation(CatalogTracker.java:486)
at org.apache.hadoop.hbase.master.HMaster.assignRootAndMeta(HMaster.java:442)
at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:389)
at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:283)
2011-05-23 22:38:07,720 INFO org.apache.hadoop.hbase.master.HMaster: Aborting
{code}"
HBASE-846	"Looking in log, I see:

{code}
2008-08-26 18:57:23,602 INFO org.apache.hadoop.dfs.DFSClient: org.apache.hadoop.ipc.RemoteException: java.io.IOException: File /hbase/aa0-000-8.u.powerset.com/log_208.76.45.95_1218666613846_60020/hlog.dat.1219776799293 could only be replicated to 0 nodes, instead of 1
        at org.apache.hadoop.dfs.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1145)
        at org.apache.hadoop.dfs.NameNode.addBlock(NameNode.java:300)
        at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:446)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:896)

        at org.apache.hadoop.ipc.Client.call(Client.java:557)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:212)
        at org.apache.hadoop.dfs.$Proxy1.addBlock(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
        at org.apache.hadoop.dfs.$Proxy1.addBlock(Unknown Source)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.locateFollowingBlock(DFSClient.java:2335)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2220)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.access$1700(DFSClient.java:1702)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:1842)
{code}

... and then:

{code}
2008-08-26 18:57:28,423 WARN org.apache.hadoop.dfs.DFSClient: Error Recovery for block null bad datanode[0]
2008-08-26 18:57:28,424 FATAL org.apache.hadoop.hbase.regionserver.HLog: Could not append. Requesting close of log
java.io.IOException: Could not get block locations. Aborting... 
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:2081)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.access$1300(DFSClient.java:1702)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:1818)
2008-08-26 18:57:28,424 INFO org.apache.hadoop.hbase.regionserver.LogRoller: Rolling hlog. Number of entries: 127
2008-08-26 18:57:28,424 ERROR org.apache.hadoop.hbase.regionserver.LogRoller: Log rolling failed
java.io.IOException: Could not get block locations. Aborting...
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:2081)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.access$1300(DFSClient.java:1702)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:1818)
...
{code}
... and so on.

Meantime clients are trying to do updates and getting below:

{code}
2008-08-26 22:49:42,834 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 60020, call batchUpdate([B@40e830f1, row => IKwQLMJ3rKRvtAv_ZkQlAk==, {column => page:url, value => '...', column => page:contents, value => '...'}) from 208.76.45.3:51164: error: java.io.IOException: Could not get block locations. Aborting... 
java.io.IOException: Could not get block locations. Aborting...
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:2081)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.access$1300(DFSClient.java:1702)
        at org.apache.hadoop.dfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:1818)
..
{code}

DFSClient seems horked.

Need to be able to ride out these kind of event.  

Restart is needed.

Test this by filling HDFS."
HBASE-4888	"A ResultScanner is created in client side, If the user doesn't invoke the ""ResultScanner.close()"" ,it happens that the memory of RegionServer increase rapidly and hold for a long time. Finally,the cluster goes to an abnormal status
 
"
HBASE-6100	"Fix the flaky tests in 0.94 branch after #209.  Many test cases like the org.apache.hadoop.hbase.TestLocalHBaseCluster.testLocalHBaseCluster
org.apache.hadoop.hbase.TestZooKeeper.testClientSessionExpired 
org.apache.hadoop.hbase.regionserver.TestServerCustomProtocol.testSingleMethod

are failing frequently.
"
HBASE-6566	"current, hfile and hlog use the same block replication, some times we can set different block replication, for example in some cluster that not every important, we may set the block replication to 2 for hlog to improve performance."
HBASE-6511	"When I did HBASE-6334 and HBASE-6379, I only focused on the unit test and didn't think to test the system test as well.

The problem is that those JIRAs introduced a reliance on the miniCluster:
{code}
    // Add a flusher
    ctx.addThread(new RepeatingTestThread(ctx) {
      public void doAnAction() throws Exception {
        util.flush();
      }
    });
{code}

util.flush requires a miniCluster, so doesn't work with a system test."
HBASE-6472	"This is visible in the logs:

{noformat}
2012-07-17 00:00:29,081 INFO org.apache.hadoop.hbase.regionserver.Store: Started compaction of 1 file(s) in cf=m into hdfs://nn:9000/hbase/t1/27889b3c9257e8dff4cddae5eedfd064/.tmp, seqid=1918186410, totalSize=124.7m 
2012-07-17 00:00:29,081 DEBUG org.apache.hadoop.hbase.regionserver.Store: Compacting hdfs://nn:9000/hbase/t1/27889b3c9257e8dff4cddae5eedfd064/m/5478928132066935241, keycount=1435991, bloomtype=NONE, size=124.7m 
2012-07-17 00:00:34,538 INFO org.apache.hadoop.hbase.regionserver.Store: Completed major compaction of 1 file(s), new file=hdfs://nn:9000/hbase/t1/27889b3c9257e8dff4cddae5eedfd064/m/7554093203376406441, size=124.7m; total size for store is 124.7m
{noformat}

I assume the reason is that the Store.java check is reversed:

{code}
      // skip selection algorithm if we don't have enough files
      if (compactSelection.getFilesToCompact().size() < this.minFilesToCompact) {
        if(LOG.isDebugEnabled()) {
          LOG.debug(""Not compacting files because we only have "" +
            compactSelection.getFilesToCompact().size() +
            "" files ready for compaction.  Need "" + this.minFilesToCompact + "" to initiate."");
        }
        compactSelection.emptyFileList();
        return compactSelection;
      }

      // remove bulk import files that request to be excluded from minors
      compactSelection.getFilesToCompact().removeAll(Collections2.filter(
          compactSelection.getFilesToCompact(),
          new Predicate<StoreFile>() {
            public boolean apply(StoreFile input) {
              return input.excludeFromMinorCompaction();
            }
          }));
{code}

The bulk files should be removed first, and +then+ it should check if there are enough files.

There are other places doing the same things, i.e. removing the bulk files and check what needs to be done, so this needs some extra care to get it all right."
HBASE-6418	"Timestamp updation in Delete flow is not considering all flavors (Delete record, Delete Family, Delete Column) of Delete API. Currently its considering Delete Record only. 
 
org.apache.hadoop.hbase.regionserver.HRegion.prepareDeleteTimestamps(Delete, byte[])

{code}
      for (KeyValue kv: kvs) {
        //  Check if time is LATEST, change to time of most recent addition if so
        //  This is expensive.
        if (kv.isLatestTimestamp() && kv.isDeleteType()) {
{code}

Basically used a wrong API.
kv.isDeleteType() should be KeyValue.isDelete(type);
"
HBASE-6171	HBck related testcases are frequently failing from build #245.
HBASE-6346	I've only started looking at this but became concerned when a completely quiescent cluster loads up 20% CPU (system+user) once HBase comes up.
HBASE-6349	"I am trying to compile the latest svn checkout of HBase source 
code using Maven.
This is the error I am facing 

[ERROR] Failed to execute goal on project hbase-server: Could not resolve dependencies for project org.apache.hbase:hbase-server:jar:0.95-SNAPSHOT: Could not find artifact org.apache.hbase:hbase-common:jar:0.95-SNAPSHOT in cloudbees netty (http://repository-netty.forge.cloudbees.com/snapshot/)"
HBASE-6339	"I noticed that right now, under a bulkLoadHFiles call to an RS, we grab the HRegion write lock as soon as we determine that it is a multi-family bulk load we'll be attempting. The file copy from the caller's source FS is done after holding the lock.

This doesn't seem right. For instance, we had a recent use-case where the bulk load running cluster is a separate HDFS instance/cluster than the one that runs HBase and the transfers between these FSes can get slower than an intra-cluster transfer. Hence I think we should begin to hold the write lock only after we've got a successful destinationFS copy of the requested file, and thereby allow more write throughput to pass.

Does this sound reasonable to do?"
HBASE-6298	"Despite regions being unbalanced, the load balancer takes no action. On my cluster the least-loaded regionserver has 33 regions and the most-loaded regionserver has 44 regions. My cluster has 1084 regions and 29 servers. It might be relevant that a 30th server used to belong to the cluster but was removed.

The master log has some strange entries when the balancer runs. The attached log file was generated by restarting the master, then running ""balancer"" in the shell."
HBASE-6304	"mvn test -Dhadoop.profile=2.0 -Dtest=TestPBOnWritrableRpc

{code}
java.lang.NoSuchMethodError: org.apache.hadoop.net.NetUtils.getInputStream(Ljava/net/S
ocket;)Ljava/io/InputStream;"" type=""java.io.IOException"">java.io.IOException: java.lang.NoSuchMethodError:
 org.apache.hadoop.net.NetUtils.getInputStream(Ljava/net/Socket;)Ljava/io/InputStream;
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine.getProxy(WritableRpcEngine.java:211)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:336)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:313)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:365)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.waitForProxy(HBaseRPC.java:237)
        at org.apache.hadoop.hbase.ipc.TestPBOnWritableRpc.testCallsInternal(TestPBOnWritableRpc.java:98)
        at org.apache.hadoop.hbase.ipc.TestPBOnWritableRpc.testCalls(TestPBOnWritableRpc.java:80)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
        at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:62)
Caused by: java.lang.NoSuchMethodError: org.apache.hadoop.net.NetUtils.getInputStream(Ljava/net/Socket;)Lj
ava/io/InputStream;
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupIOstreams(HBaseClient.java:676)
        at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:1286)
        at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:1138)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:152)
        at $Proxy10.getProtocolVersion(Unknown Source)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine.getProxy(WritableRpcEngine.java:196)
        ... 15 more
{code}"
HBASE-5632	"The first cut of HBASE-5128 failed fast and tried to avoid cases handling partial recovery failures.  We can improve this.

- collecting faults and recovering from them
- Handling RejectedExectionException exceptions when using executor.execute."
HBASE-6208	"2012-06-14 15:06:54,902 WARN org.apache.hadoop.hbase.regionserver.wal.HLog: IPC Server handler 188 on 60020 took 8872 ms appending an edit to hlog; editcount=15730, len~=2.2k

we have many this log , and the write request will be blocked .  i find no solution."
HBASE-6172	TestHRegion.java for eg does not run from #245. There are few more testcases that does not run.
HBASE-5988	"we use thrift to access the hbase. but The problems we for a long time.
the hbase would be blocked writing for a few seconds. we check the log but find nothing.
eachtime, it will effect 48 or 96 request.

hbase have 13 regionserver, 3 zookeeper and 3 masterserver
hadoop have 16 datanodeserver.

"
HBASE-4559	"TestAvroServer is a beefy test, spins up a mini cluster, does a large series of manipulations and then spins it down. It take about 2 mins to run on a local machine, which on the high side for a 'unit' test. 

This is part of the implentation discussed in http://search-hadoop.com/m/L9OzBNEOJK1"
HBASE-5951	"FilterList listOfFilters = new FilterList (FilterList.Operator.MUST_PASS_ALL);
FilterList listOfFilters1 = new FilterList (FilterList.Operator.MUST_PASS_ALL);
FilterList listOfFilters2 = new FilterList (FilterList.Operator.MUST_PASS_ALL);

SingleColumnValueFilter SingleFilter1 = new
SingleColumnValueFilter(Bytes.toBytes(""cf""),
Bytes.toBytes(""country""), CompareOp.EQUAL,
Bytes.toBytes(""USA""));
listOfFilters.addFilter(SingleFilter1);

ValueFilter VF1= new ValueFilter (CompareOp.EQUAL, 
new SubstringComparator(""ABC""));
ColumnPrefixFilter CF1= new ColumnPrefixFilter(Bytes.toBytes(""name""));
listOfFilters1.addFilter(CF1);
listOfFilters1.addFilter(VF1);
listOfFilters.addFilter(listOfFilters1);

ValueFilter VF2= new ValueFilter (CompareOp.EQUAL, 
new SubstringComparator(""ED""));
ColumnPrefixFilter CF2= new ColumnPrefixFilter (Bytes.toBytes(""CRS""));
listOfFilters2.addFilter(CF2);
listOfFilters2.addFilter(VF2);
listOfFilters.addFilter(listOfFilters2);

When i do a combibation of SingleFilter1 and listOfFilters1
 the result is correct, same way the combination of 
SingleFilter1 and listOfFilters2 is returing correct result.
But when all the three is combined im not getting any result..

Is it the problem with multiple ColumnPrefixFilter??? 
Value ""ABC"" exist in name.0 and value ""ED"" exist in CRS.0 and it is in
the same row under same Column Family."
HBASE-5917	"i want to custom filter hbase.
i created jar file by eclipse, copy to sever and in file hbase-env.xml i set ""export HBASE_CLASSPATH=/cldo/hadoop/conf;/cldo/customfilter.jar

but when start have error 

/cldo/hbase/bin/../conf/hbase-env.sh: line 29: /cldo/customfilter.jar: cannot execute binary file
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/hbase/filter/FilterBase
at java.lang.ClassLoader.defineClass1(Native Method)
at java.lang.ClassLoader.defineClassCond(ClassLoader.java:631)
at java.lang.ClassLoader.defineClass(ClassLoader.java:615)
at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141)
at java.net.URLClassLoader.defineClass(URLClassLoader.java:283)
at java.net.URLClassLoader.access$000(URLClassLoader.java:58)
at java.net.URLClassLoader$1.run(URLClassLoader.java:197)
at java.security.AccessController.doPrivileged(Native Method)
at java.net.URLClassLoader.findClass(URLClassLoader.java:190)

thank
"
HBASE-5501	"In a live cluster, we do the following step
1.kill the master;
1.start the master, and master is initializing锛?3.master complete splitLog
4.kill the META server
5.master start assigning ROOT and META
6.Now meta region data will loss since we may assign meta region before SSH finish split log for dead META server."
HBASE-5424	"We meet NPE when call getRegionInfo() in testing environment.
Exception in thread ""main"" java.lang.NullPointerException
at org.apache.hadoop.hbase.util.Writables.getWritable(Writables.java:75)
at org.apache.hadoop.hbase.util.Writables.getHRegionInfo(Writables.java:119)
at org.apache.hadoop.hbase.client.HTable$2.processRow(HTable.java:395)
at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:190)
at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:95)
at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:73)
at org.apache.hadoop.hbase.client.HTable.getRegionsInfo(HTable.java:418)

This NPE also make the table.jsp can't show the region information of this table.
"
HBASE-5429	"HCatStorageHandler was moved for the ""storageHandler"" package to ""mapreduce"". For some reason it wasn't move but copied. I need to remove this as well as the HCatStorageHandlerImpl which I believe is no longer used. There is one class which depends on this instead of the mapreduce one. I need to fix that as well."
HBASE-5381	"Currently the region server will flush mem store of the region based on the limitation of the global mem store flush size and global low water mark. However, It will cause the hot tables, which serve more write traffic, to flush too frequently even though the overall mem store heap usage is quite low. Too frequently flush would also contribute to too many minor compactions. 
So if we can make memstore.flush.size as a table level configuration, it would be more flexible to config different tables with different desired mem store flush size based on compaction ratio, recovery time and put ops."
HBASE-2628	"getRowOrBefore, if I point it a table other than .META., does the below....

{code}
java.io.IOException: java.io.IOException: java.lang.StackOverflowError
        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:887)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:877)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getClosestRowBefore(HRegionServer.java:1734)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:657)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:915)
Caused by: java.lang.StackOverflowError
        at org.apache.hadoop.hbase.KeyValue$KeyComparator.compare(KeyValue.java:1776)
        at org.apache.hadoop.hbase.io.HalfHFileReader$1.seekBefore(HalfHFileReader.java:148)
        at org.apache.hadoop.hbase.io.HalfHFileReader$1.seekBefore(HalfHFileReader.java:150)
        at org.apache.hadoop.hbase.io.HalfHFileReader$1.seekBefore(HalfHFileReader.java:150)
        at org.apache.hadoop.hbase.io.HalfHFileReader$1.seekBefore(HalfHFileReader.java:150)
        at org.apache.hadoop.hbase.io.HalfHFileReader$1.seekBefore(HalfHFileReader.java:150)
        at org.apache.hadoop.hbase.io.HalfHFileReader$1.seekBefore(HalfHFileReader.java:150)
        at org.apache.hadoop.hbase.io.HalfHFileReader$1.seekBefore(HalfHFileReader.java:150)
        at org.apache.hadoop.hbase.io.HalfHFileReader$1.seekBefore(HalfHFileReader.java:150)
        at org.apache.hadoop.hbase.io.HalfHFileReader$1.seekBefore(HalfHFileReader.java:150)
        at org.apache.hadoop.hbase.io.HalfHFileReader$1.seekBefore(HalfHFileReader.java:150)
        at org.apache.hadoop.hbase.io.HalfHFileReader$1.seekBefore(HalfHFileReader.java:150)
        at org.apache.hadoop.hbase.io.HalfHFileReader$1.seekBefore(HalfHFileReader.java:150)
        at org.apache.hadoop.hbase.io.HalfHFileReader$1.seekBefore(HalfHFileReader.java:150)
        at org.apache.hadoop.hbase.io.HalfHFileReader$1.seekBefore(HalfHFileReader.java:150)
{code}

Here is some code to do it:

{code}
hbase(main):018:0> t = HTable.new(@configuration, ""TestTable"")    
hbase(main):018:0> r = t.getRowOrBefore(Bytes.toString(''), Bytes.toString('info'))     
... spew....
{code}"
HBASE-5126	"[Notice this in 89 branch. Possibly an issue in trunk also.]

A test which does a columnPrefixFilter(""tag0"") AND columnPrefixFilter(""tag1"") should return 0 kvs; instead it returns kvs with prefix ""tag0"".

{code}
table = HTable.new(conf, tableName)

put = Put.new(Bytes.toBytes(""row""))
put.add(cf1name, Bytes.toBytes(""tag0""), Bytes.toBytes(""value0""))
put.add(cf1name, Bytes.toBytes(""tag1""), Bytes.toBytes(""value1""))
put.add(cf1name, Bytes.toBytes(""tag2""), Bytes.toBytes(""value2""))

table.put(put)

# Test for AND Two Column Prefix Filters                                                                                                                                                   
filter1 = ColumnPrefixFilter.new(Bytes.toBytes(""tag0""));
filter2 = ColumnPrefixFilter.new(Bytes.toBytes(""tag2""));

filters = FilterList.new(FilterList::Operator::MUST_PASS_ALL);
filters.addFilter(filter1);
filters.addFilter(filter2);

get = Get.new(Bytes.toBytes(""row""))
get.setFilter(filters)
get.setMaxVersions();
keyValues = table.get(get).raw()

keyValues.each do |keyValue|
  puts ""Key=#{Bytes.toStringBinary(keyValue.getQualifier())}; Value=#{Bytes.toStringBinary(keyValue.getValue())}; Timestamp=#{keyValue.getTimestamp()}"" 
end
{code}

outputs:

{code}
Key=tag0; Value=value0; Timestamp=1325719223523
{code}
"
HBASE-5095	"When I put one row into Hbase Table, it throws IOException.(NullPointerException). I could make sure that I have add values into the Row. But when it runs at table.put(row). The exception comes out.

Here is part of my code.

public HbaseInterface() {
        HBaseConfiguration config = new HBaseConfiguration();
        try {
            config.set(""hbase.zookeeper.quorum"", ""127.0.0.1"");
            config.set(""hbase.zookeeper.property.clientPort"", ""2222"");
            table = new HTable(config, urlstable);
        } catch (Exception ex) {
            if (LOGGER.isErrorEnabled()) {
                LOGGER.error(""error"", ex);
            }
        }
    }

public boolean put(String url, String category) {
        
        Put row = new Put(Bytes.toBytes(url));
        row.add(Bytes.toBytes(lifetime), null, Bytes.toBytes(""99999""));
        row.add(Bytes.toBytes(categories), null, Bytes.toBytes(category));
        row.add(Bytes.toBytes(digest), null, Bytes.toBytes(m_encry.encrypt(url)));
        try {
            table.put(row);
        } catch (Exception ex) {
            if (LOGGER.isErrorEnabled()) {
                LOGGER.error(""hbase put error :"", ex);
            }
            return false;
        }
        return true;
    }

This is the table description.
{NAME => 'urls', FAMILIES => [{NAME => 'categories', COMPRESSION => 'N true    
 ONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '65536', IN_M         
 EMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'digest', COMPRESSIO         
 N => 'NONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '65536         
 ', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'lifetime', C         
 OMPRESSION => 'NONE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE          
 => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}   
"
HBASE-5037	"same as https://issues.apache.org/jira/browse/HBASE-2849
thrift log:
org.apache.zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x1343b1b1fbf0005, likely server has closed socket, closing socket connection and attempting reconnect
hbase log:
2011-12-15 14:58:03,793 INFO org.apache.zookeeper.server.NIOServerCnxn: Accepted socket connection from /127.0.0.1:19035
2011-12-15 14:58:03,793 INFO org.apache.zookeeper.server.NIOServerCnxn: Refusing session request for client /127.0.0.1:19035 as it has seen zxid 0x169 our last zxid is 0x15c client must try another server
2011-12-15 14:58:03,793 INFO org.apache.zookeeper.server.NIOServerCnxn: Closed socket connection for client /127.0.0.1:19035 (no session established for client)
"
HBASE-4901	"I create a class named HBaseInterfaceHolder who will hold ThreadLocal<HBaseInterface> hBaseInterfaceHolder. But when the program run at the line (HBaseConfiguration config = new HBaseConfiguration();), it throws a java.io.IOException:config()
at 
org.apache.hadoop.conf.Configuration.<init>(Configuration.java:211)
org.apache.hadoop.conf.Configuration.<init>(Configuration.java:198)
org.apache.hadoop.hbase.HBaseConfiguration.<init>(Configuration.java:33)
com.netentsec.niudian.phoenix.web2.hbaseinterface.HBaseInterfaceHolder$1.initialValue(HBaseInterfaceHolder.java:24) this line is my own program which is the content in previous bracket.

I search online to find out the reason. But no forum related it."
HBASE-4546	"HBase is still depending on 3.3.1.  There many critical bug fixes in 3.3.2 and two more critical fixes in 3.3.3.

We recently tripped on ZOOKEEPER-822 which was fixed in 3.3.2."
HBASE-4355	"This unit test has been failing on my machine with the following error.

testTableWithStringName(org.apache.hadoop.hbase.client.TestHTablePool$TestHTableThreadLocalPool): Cluster already running at /hbase-core-trunk/target/test-data/2e41efb9-7b96-4ab3-abec-c58f467b220c/af01017e-ee3c-46fc-b908-078a3a4e8b52/bfd8e9b4-66da-4322-96bd-6db4564d8f41/d9a97e3d-8ffb-4945-a71e-d059e3bc7274/6cdf0b73-b9a0-45f4-856d-53cd02ecebce/34c41612-9311-4199-9902-cf30a9cb7b9d/33e7bfd5-2519-4349-9a44-d05000e00526/dbc60fd9-756d-4263-9ed1-bbff69ec7a80/0e1bde7e-c966-4c3e-a01c-50ded9cb166b/415e8d51-46f2-4d50-879a-870298a9e1f8/fb165bb9-7d6c-4cf8-970a-e281b9818e97

It looks like TestHTablePool uses nested classes TestHTableReusablePool and TestHTableThreadLocalPool. Both classes could be instantiated by junit framework in mulitple threads fashion. Both classes call HBaseTestingUtilility.startMiniCluster. HBaseTestingUtilility.isRunningCluster throws this exception.


Is the understanding about junit framework correctly? I don't know why others haven't got such error."
HBASE-4502	"I have set up hadoop 0.20.2 as Pseduo distributed. This is running fine.

And trying to integrate hbase-0.90.4/3 with hadoop. But while starting, getting following error at Master log.


2011-09-27 15:01:16,295 INFO org.apache.zookeeper.server.NIOServerCnxn: Client attempting to establish new session at /127.0.0.1:37643
2011-09-27 15:01:16,296 INFO org.apache.zookeeper.server.NIOServerCnxn: Established session 0x132ace7f4fa0002 with negotiated timeout 40000 for client /127.0.0.1:37643
2011-09-27 15:01:16,296 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x132ace7f4fa0002, negotiated timeout = 40000
2011-09-27 15:01:16,468 FATAL org.apache.hadoop.hbase.master.HMaster: Unhandled exception. Starting shutdown.
java.io.IOException: Call to localhost/127.0.0.1:8020 failed on local exception: java.io.EOFException
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1139)
	at org.apache.hadoop.ipc.Client.call(Client.java:1107)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:226)
	at $Proxy6.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:398)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:384)
	at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:111)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:213)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:180)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:89)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1514)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:67)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:1548)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1530)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:228)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:183)
	at org.apache.hadoop.hbase.util.FSUtils.getRootDir(FSUtils.java:364)
	at org.apache.hadoop.hbase.master.MasterFileSystem.<init>(MasterFileSystem.java:81)
	at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:346)
	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:282)
	at org.apache.hadoop.hbase.master.HMasterCommandLine$LocalHMaster.run(HMasterCommandLine.java:193)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:375)
	at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:812)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:720)
2011-09-27 15:01:16,470 INFO org.apache.hadoop.hbase.master.HMaster: Aborting
2011-09-27 15:01:16,470 DEBUG org.apache.hadoop.hbase.master.HMaster: Stopping service threads
2011-09-27 15:01:16,470 INFO org.apache.hadoop.ipc.HBaseServer: Stopping server on 51567



hbase-site.xml file structure is as follows....

<configuration>
<property>
    <name>hbase.rootdir</name>
    <value>hdfs://localhost:8020/hbase</value>
  </property>

<property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>
</configuration>


Please help me to fix this error............ 
"
HBASE-4372	"Currently, IS_SECURE_HADOOP is set to true if UserGroupInformation has a method isSecurityEnabled. It should be set to true only if security is turned on as well."
HBASE-2158	A Ryan Rawson suggestion.  See HBASE-2149 for more context.
HBASE-4332	"On hbase 0.90.3, describing a table which is disabled, on hbase shell shows the table as being ENABLED i.e.

{noformat}
hbase> create 't1','c1'
hbase> describe 't1'
       DESCRIPTION ENABLED   
hbase> drop 't1'
       ERROR: Table t1 is enabled. Disable it first.'
hbase> disable 't1'
hbase> describe 't1'
       DESCRIPTION ENABLED   
hbase> drop 't1'
Table dropped

{noformat}

The describe option can be useful to know if the table is disabled/enabled. I looked through jiras to find if an issue already exists but could not find one. Please mark it as duplicate if one already exists.
"
HBASE-4111	"    We found a strange problem in our read test. 

    It is a 5 nodes cluster.Four of our 5 regionservers set ""hfile.block.cache.size""=0.4, one of them is 0.1(we call it node A). When we random read from a 2TB data table we found node A's network reached 100MB, and others are less than 10MB. So the read speed is low.

    We set node A's ""hfile.block.cache.size""=0.2, then all the nodes's network are 10MB, that's right. To find why is this we debug with btrace and find ""readBlock"" in HFile.Reader become abnormal.We know hbase read a block which is 64 KB from disks and put it into blockcache. But when we set ""hfile.block.cache.size""=0.1, it is not 64KB, it is 5~6MB one time after about 1 minute we restart hbase.

    Why not 64 KB? The btrace code and results are in the attachments. "
HBASE-4042	"There's a couple of issues going on here.  Its taken me a while to figure whats up and am still not done.  Here's what I found so far:

+ The last test testWorkerAbort has been hanging because there is no one to process the log split the final assert is expecting completed; we've killed the lone RS that this test put up.  This test passes most of the time for me locally; luck has the log processed before the RS with its splitLogWorker processes the log before we go out.  Putting up a new RS with a splitLogWorker makes this test pass for me most of the time now but I've seen an error in testing so need to dig in still.
+ The first test, testThreeRSAbort is a good test.  Its turning up a issue that has nothing to do w/ log splitting.  If .META. is on one of the RSs that goes down -- we launch w/ 6 RSs -- then we get stuck in catalog tracker waiting on meta to be up again (though it deploys fin -- we don't notice its deploy in master).  I'm on this one at mo. "
HBASE-4018	"Currently, block caches are limited by heap size, which is limited by garbage collection times in Java.

We can get around this by using memcached w/JNI as a secondary block cache. This should be faster than the linux file system's caching, and allow us to very quickly gain access to a high quality slab allocated cache."
HBASE-3977	An unintended change of HBASE-3873 (sorry about that)
HBASE-2290	We need to add maven made jars to bin/hbase CLASSPATH
HBASE-3822	"The regionserver is not able to exit because the rs thread is stuck here



""regionserver60020"" prio=10 tid=0x00002ab2b039e000 nid=0x760a waiting on condition [0x000000004365e000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.hbase.util.Threads.sleep(Threads.java:126)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.waitOnAllRegionsToClose(HRegionServer.java:736)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:689)
        at java.lang.Thread.run(Thread.java:619)


===

In CloseRegionHandler.process() we do not call removeFromOnlineRegions() if there is an exception. (In this case I suspect there was a log-rolling exception because of another issue)

    // Close the region
    try {
      // TODO: If we need to keep updating CLOSING stamp to prevent against
      // a timeout if this is long-running, need to spin up a thread?
      if (region.close(abort) == null) {
        // This region got closed.  Most likely due to a split. So instead
        // of doing the setClosedState() below, let's just ignore and continue.
        // The split message will clean up the master state.
        LOG.warn(""Can't close region: was already closed during close(): "" +
          regionInfo.getRegionNameAsString());
        return;
      }
    } catch (IOException e) {
      LOG.error(""Unrecoverable exception while closing region "" +
        regionInfo.getRegionNameAsString() + "", still finishing close"", e);
    }

    this.rsServices.removeFromOnlineRegions(regionInfo.getEncodedName());


===

I think we set the closing flag on the region, it won't be taking any more requests, it is as good as offline.

Either we should refine the check in waitOnAllRegionsToClose() or CloseRegionHandler.process() should remove the region from online-regions set."
HBASE-3824	"When replaying a large log file, mestore flushes can happen. But there is no Progressible report being sent during memstore flushes. That can lead to master timing out the region server during region open.

===
Another related issue and Jonathan's response

> So if a region server that is handed a region for opening and has done part of
> the work ... it has created some HFiles (because the logs were so huge that
> the mestore got flushed while the logs were being replayed) ... and then it is
> asked to give up because the master thought the region server was taking
> too long to open the region.
> 
> When the region server gives up on the region then will it make sure that it
> removes all the HFiles it had created for that region?


Will need to check the code, but would it matter?  One issue is whether it cleans up after itself (I'm guessing not).  Another issue is whether the replay is idempotent (duplicate KVs across files shouldn't matter in most cases).

===

2011-04-25 09:11:36,844 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_RS_OPEN_REGION
java.lang.NullPointerException
at org.apache.hadoop.hbase.util.Writables.getWritable(Writables.java:75)
at org.apache.hadoop.hbase.executor.RegionTransitionData.fromBytes(RegionTransitionData.java:198)
at org.apache.hadoop.hbase.zookeeper.ZKAssign.transitionNode(ZKAssign.java:672)
at org.apache.hadoop.hbase.zookeeper.ZKAssign.transitionNodeOpened(ZKAssign.java:621)
at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:168)
at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:151)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:619)

byte [] existingBytes =
ZKUtil.getDataNoWatch(zkw, node, stat);
RegionTransitionData existingData =
RegionTransitionData.fromBytes(existingBytes);

existingBytes can be null. have to return -1 if null.


===

master logs

2011-04-25 05:24:03,250 DEBUG org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Creating writer path=hdfs://pumahbase002-snc5-dfs.data.facebook.com:9000/PUMAHBASE002-SNC5-HBASE/realtime_domain_imps_urls/e7a478b4bd164525052f1dedb832de0a/recovered.edits/0000000057528037047 region=e7a478b4bd164525052f1dedb832de0a
2011-04-25 09:09:19,246 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs://pumahbase002-snc5-dfs.data.facebook.com:9000/PUMAHBASE002-SNC5-HBASE/realtime_domain_imps_urls/e7a478b4bd164525052f1dedb832de0a/recovered.edits/0000000057528037047 (wrote 4342690 edits in 46904ms)
2011-04-25 09:09:26,134 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x32f7bb74e8a0000 Creating (or updating) unassigned node for e7a478b4bd164525052f1dedb832de0a with OFFLINE state
2011-04-25 09:09:26,136 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: No previous transition plan was found (or we are ignoring an existing plan) for realtime_domain_imps_urls,afffffbe,1295556905482.e7a478b4bd164525052f1dedb832de0a. so generated a random one; hri=realtime_domain_imps_urls,afffffbe,1295556905482.e7a478b4bd164525052f1dedb832de0a., src=, dest=pumahbase107.snc5.facebook.com,60020,1303450731227; 70 (online=70, exclude=null) available servers
2011-04-25 09:09:26,136 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region realtime_domain_imps_urls,afffffbe,1295556905482.e7a478b4bd164525052f1dedb832de0a. to pumahbase107.snc5.facebook.com,60020,1303450731227
2011-04-25 09:09:26,139 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=pumahbase107.snc5.facebook.com,60020,1303450731227, region=e7a478b4bd164525052f1dedb832de0a
2011-04-25 09:09:44,045 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=pumahbase107.snc5.facebook.com,60020,1303450731227, region=e7a478b4bd164525052f1dedb832de0a
2011-04-25 09:09:59,050 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=pumahbase107.snc5.facebook.com,60020,1303450731227, region=e7a478b4bd164525052f1dedb832de0a
2011-04-25 09:10:14,054 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=pumahbase107.snc5.facebook.com,60020,1303450731227, region=e7a478b4bd164525052f1dedb832de0a
2011-04-25 09:10:29,055 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=pumahbase107.snc5.facebook.com,60020,1303450731227, region=e7a478b4bd164525052f1dedb832de0a
2011-04-25 09:10:44,060 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=pumahbase107.snc5.facebook.com,60020,1303450731227, region=e7a478b4bd164525052f1dedb832de0a
2011-04-25 09:10:59,062 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=pumahbase107.snc5.facebook.com,60020,1303450731227, region=e7a478b4bd164525052f1dedb832de0a
2011-04-25 09:10:59,388 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=pumahbase107.snc5.facebook.com,60020,1303450731227, region=e7a478b4bd164525052f1dedb832de0a
2011-04-25 09:11:33,411 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out: realtime_domain_imps_urls,afffffbe,1295556905482.e7a478b4bd164525052f1dedb832de0a. state=OPENING, ts=1303747859386
2011-04-25 09:11:33,411 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been OPENING for too long, reassigning region=realtime_domain_imps_urls,afffffbe,1295556905482.e7a478b4bd164525052f1dedb832de0a.
2011-04-25 09:11:33,412 INFO org.apache.hadoop.hbase.master.AssignmentManager: Successfully transitioned region=realtime_domain_imps_urls,afffffbe,1295556905482.e7a478b4bd164525052f1dedb832de0a. into OFFLINE and forcing a new assignment
2011-04-25 09:11:33,412 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: No previous transition plan was found (or we are ignoring an existing plan) for realtime_domain_imps_urls,afffffbe,1295556905482.e7a478b4bd164525052f1dedb832de0a. so generated a random one; hri=realtime_domain_imps_urls,afffffbe,1295556905482.e7a478b4bd164525052f1dedb832de0a., src=, dest=pumahbase150.snc5.facebook.com,60020,1303450731207; 70 (online=70, exclude=null) available servers
2011-04-25 09:11:33,413 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region realtime_domain_imps_urls,afffffbe,1295556905482.e7a478b4bd164525052f1dedb832de0a. to pumahbase150.snc5.facebook.com,60020,1303450731207
2011-04-25 09:11:33,414 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=M_ZK_REGION_OFFLINE, server=pumahbase162.snc5.facebook.com:60000, region=e7a478b4bd164525052f1dedb832de0a
2011-04-25 09:11:33,416 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=pumahbase150.snc5.facebook.com,60020,1303450731207, region=e7a478b4bd164525052f1dedb832de0a
2011-04-25 09:11:33,825 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=pumahbase150.snc5.facebook.com,60020,1303450731207, region=e7a478b4bd164525052f1dedb832de0a
2011-04-25 09:11:36,804 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENED, server=pumahbase150.snc5.facebook.com,60020,1303450731207, region=e7a478b4bd164525052f1dedb832de0a
2011-04-25 09:11:36,804 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Handling OPENED event for e7a478b4bd164525052f1dedb832de0a; deleting unassigned node
2011-04-25 09:11:36,804 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x32f7bb74e8a0000 Deleting existing unassigned node for e7a478b4bd164525052f1dedb832de0a that is in expected state RS_ZK_REGION_OPENED
2011-04-25 09:11:36,805 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x32f7bb74e8a0000 Successfully deleted unassigned node for region e7a478b4bd164525052f1dedb832de0a in expected state RS_ZK_REGION_OPENED
2011-04-25 09:11:36,805 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Opened region realtime_domain_imps_urls,afffffbe,1295556905482.e7a478b4bd164525052f1dedb832de0a. on pumahbase150.snc5.facebook.com,60020,1303450731207


===

region server log

2011-04-25 09:09:26,136 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Received request to open region: realtime_domain_imps_urls,afffffbe,1295556905482.e7a478b4bd164525052f1dedb832de0a.
2011-04-25 09:09:26,136 DEBUG org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Processing open of realtime_domain_imps_urls,afffffbe,1295556905482.e7a478b4bd164525052f1dedb832de0a.
2011-04-25 09:09:26,136 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x22f7bb74e8e000a Attempting to transition node e7a478b4bd164525052f1dedb832de0a from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2011-04-25 09:09:26,138 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x22f7bb74e8e000a Successfully transitioned node e7a478b4bd164525052f1dedb832de0a from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2011-04-25 09:09:26,139 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Opening region: REGION => {NAME => 'realtime_domain_imps_urls,afffffbe,1295556905482.e7a478b4bd164525052f1dedb832de0a.', STARTKEY => 'afffffbe', ENDKEY => 'b040ebdbf81a3750f0eaa71842ff09dagr.insomnia li 294576344998', ENCODED => e7a478b4bd164525052f1dedb832de0a, TABLE => {{NAME => 'realtime_domain_imps_urls', FAMILIES => [{NAME => 'COUNTERS_0', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', COMPRESSION => 'LZO', VERSIONS => '1', TTL => '2147483647', BLOCKSIZE => '16384', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'COUNTERS_3600', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', COMPRESSION => 'LZO', VERSIONS => '1', TTL => '172800', BLOCKSIZE => '16384', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'COUNTERS_86400', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', COMPRESSION => 'LZO', VERSIONS => '1', TTL => '2592000', BLOCKSIZE => '16384', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}
2011-04-25 09:09:26,140 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Instantiated realtime_domain_imps_urls,afffffbe,1295556905482.e7a478b4bd164525052f1dedb832de0a.
2011-04-25 09:09:26,140 INFO org.apache.hadoop.hbase.regionserver.logger.HRegionLogger: HRegionLogger for region realtime_domain_imps_urls,afffffbe,1295556905482.e7a478b4bd164525052f1dedb832de0a., category nectar_titan_hbase_updates
2011-04-25 09:09:26,190 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://pumahbase002-snc5-dfs.data.facebook.com:9000/PUMAHBASE002-SNC5-HBASE/realtime_domain_imps_urls/e7a478b4bd164525052f1dedb832de0a/COUNTERS_0/87cf11f68701404b862d631ba3d20213, isReference=false, isBulkLoadResult=false, seqid=58137870275, majorCompaction=false
2011-04-25 09:09:28,572 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://pumahbase002-snc5-dfs.data.facebook.com:9000/PUMAHBASE002-SNC5-HBASE/realtime_domain_imps_urls/e7a478b4bd164525052f1dedb832de0a/COUNTERS_0/c58d9a33653a4e5e966c426b33a57598, isReference=false, isBulkLoadResult=false, seqid=57847795298, majorCompaction=true
2011-04-25 09:09:28,796 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://pumahbase002-snc5-dfs.data.facebook.com:9000/PUMAHBASE002-SNC5-HBASE/realtime_domain_imps_urls/e7a478b4bd164525052f1dedb832de0a/COUNTERS_3600/1bc513e915b741d59d4f113c273e67d4, isReference=false, isBulkLoadResult=false, seqid=58137870275, majorCompaction=false
2011-04-25 09:09:28,826 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://pumahbase002-snc5-dfs.data.facebook.com:9000/PUMAHBASE002-SNC5-HBASE/realtime_domain_imps_urls/e7a478b4bd164525052f1dedb832de0a/COUNTERS_3600/5a50b17f3ad045368229680414987ae7, isReference=false, isBulkLoadResult=false, seqid=57847795298, majorCompaction=true
2011-04-25 09:09:28,850 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://pumahbase002-snc5-dfs.data.facebook.com:9000/PUMAHBASE002-SNC5-HBASE/realtime_domain_imps_urls/e7a478b4bd164525052f1dedb832de0a/COUNTERS_86400/23648dce8d4243c5b804c0e766f5a235, isReference=false, isBulkLoadResult=false, seqid=58137870275, majorCompaction=false
2011-04-25 09:09:29,016 DEBUG org.apache.hadoop.hbase.regionserver.Store: loaded hdfs://pumahbase002-snc5-dfs.data.facebook.com:9000/PUMAHBASE002-SNC5-HBASE/realtime_domain_imps_urls/e7a478b4bd164525052f1dedb832de0a/COUNTERS_86400/d6820d86715049c087fa5c3f4355f2fc, isReference=false, isBulkLoadResult=false, seqid=57847795298, majorCompaction=true
2011-04-25 09:09:29,017 INFO org.apache.hadoop.hbase.regionserver.HRegion: Replaying edits from hdfs://pumahbase002-snc5-dfs.data.facebook.com:9000/PUMAHBASE002-SNC5-HBASE/realtime_domain_imps_urls/e7a478b4bd164525052f1dedb832de0a/recovered.edits/0000000057528037047; minSequenceid=58137870275
2011-04-25 09:09:44,041 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x22f7bb74e8e000a Attempting to transition node e7a478b4bd164525052f1dedb832de0a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2011-04-25 09:09:44,044 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x22f7bb74e8e000a Successfully transitioned node e7a478b4bd164525052f1dedb832de0a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2011-04-25 09:09:59,046 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x22f7bb74e8e000a Attempting to transition node e7a478b4bd164525052f1dedb832de0a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2011-04-25 09:09:59,050 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x22f7bb74e8e000a Successfully transitioned node e7a478b4bd164525052f1dedb832de0a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2011-04-25 09:10:14,050 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x22f7bb74e8e000a Attempting to transition node e7a478b4bd164525052f1dedb832de0a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING

832de0a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2011-04-25 09:10:29,055 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x22f7bb74e8e000a Successfully transitioned node e7a478b4bd164525052f1dedb832de0a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2011-04-25 09:10:44,056 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x22f7bb74e8e000a Attempting to transition node e7a478b4bd164525052f1dedb832de0a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2011-04-25 09:10:44,059 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x22f7bb74e8e000a Successfully transitioned node e7a478b4bd164525052f1dedb832de0a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2011-04-25 09:10:59,057 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x22f7bb74e8e000a Attempting to transition node e7a478b4bd164525052f1dedb832de0a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2011-04-25 09:10:59,062 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x22f7bb74e8e000a Successfully transitioned node e7a478b4bd164525052f1dedb832de0a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2011-04-25 09:10:59,154 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memstore flush for realtime_domain_imps_urls,afffffbe,1295556905482.e7a478b4bd164525052f1dedb832de0a., current region memstore size 18.9m; wal is null, using passed sequenceid=58140219688
2011-04-25 09:10:59,277 INFO org.apache.hadoop.hbase.regionserver.Store: Flushed , sequenceid=58140219688, memsize=12.4m, into tmp file hdfs://pumahbase002-snc5-dfs.data.facebook.com:9000/PUMAHBASE002-SNC5-HBASE/realtime_domain_imps_urls/e7a478b4bd164525052f1dedb832de0a/.tmp/a1df4f1ac37a4c7fbf4067f3bcff49a5
2011-04-25 09:10:59,312 INFO org.apache.hadoop.hbase.regionserver.Store: Flushed , sequenceid=58140219688, memsize=3.2m, into tmp file hdfs://pumahbase002-snc5-dfs.data.facebook.com:9000/PUMAHBASE002-SNC5-HBASE/realtime_domain_imps_urls/e7a478b4bd164525052f1dedb832de0a/.tmp/eefd1540ca644f06b251201894ef5dd8
2011-04-25 09:10:59,357 INFO org.apache.hadoop.hbase.regionserver.Store: Flushed , sequenceid=58140219688, memsize=3.2m, into tmp file hdfs://pumahbase002-snc5-dfs.data.facebook.com:9000/PUMAHBASE002-SNC5-HBASE/realtime_domain_imps_urls/e7a478b4bd164525052f1dedb832de0a/.tmp/83c87fcdbf7147ceac873a1e97e2990f
2011-04-25 09:10:59,357 INFO org.apache.hadoop.hbase.regionserver.Store: Renaming flushed file at hdfs://pumahbase002-snc5-dfs.data.facebook.com:9000/PUMAHBASE002-SNC5-HBASE/realtime_domain_imps_urls/e7a478b4bd164525052f1dedb832de0a/.tmp/a1df4f1ac37a4c7fbf4067f3bcff49a5 to hdfs://pumahbase002-snc5-dfs.data.facebook.com:9000/PUMAHBASE002-SNC5-HBASE/realtime_domain_imps_urls/e7a478b4bd164525052f1dedb832de0a/COUNTERS_0/a1df4f1ac37a4c7fbf4067f3bcff49a5
2011-04-25 09:10:59,372 INFO org.apache.hadoop.hbase.regionserver.Store: Added hdfs://pumahbase002-snc5-dfs.data.facebook.com:9000/PUMAHBASE002-SNC5-HBASE/realtime_domain_imps_urls/e7a478b4bd164525052f1dedb832de0a/COUNTERS_0/a1df4f1ac37a4c7fbf4067f3bcff49a5, entries=45626, sequenceid=58140219688, filesize=549.1k
2011-04-25 09:10:59,372 INFO org.apache.hadoop.hbase.regionserver.Store: Renaming flushed file at hdfs://pumahbase002-snc5-dfs.data.facebook.com:9000/PUMAHBASE002-SNC5-HBASE/realtime_domain_imps_urls/e7a478b4bd164525052f1dedb832de0a/.tmp/eefd1540ca644f06b251201894ef5dd8 to hdfs://pumahbase002-snc5-dfs.data.facebook.com:9000/PUMAHBASE002-SNC5-HBASE/realtime_domain_imps_urls/e7a478b4bd164525052f1dedb832de0a/COUNTERS_3600/eefd1540ca644f06b251201894ef5dd8
2011-04-25 09:10:59,375 INFO org.apache.hadoop.hbase.regionserver.Store: Added hdfs://pumahbase002-snc5-dfs.data.facebook.com:9000/PUMAHBASE002-SNC5-HBASE/realtime_domain_imps_urls/e7a478b4bd164525052f1dedb832de0a/COUNTERS_3600/eefd1540ca644f06b251201894ef5dd8, entries=11411, sequenceid=58140219688, filesize=167.9k
2011-04-25 09:10:59,375 INFO org.apache.hadoop.hbase.regionserver.Store: Renaming flushed file at hdfs://pumahbase002-snc5-dfs.data.facebook.com:9000/PUMAHBASE002-SNC5-HBASE/realtime_domain_imps_urls/e7a478b4bd164525052f1dedb832de0a/.tmp/83c87fcdbf7147ceac873a1e97e2990f to hdfs://pumahbase002-snc5-dfs.data.facebook.com:9000/PUMAHBASE002-SNC5-HBASE/realtime_domain_imps_urls/e7a478b4bd164525052f1dedb832de0a/COUNTERS_86400/83c87fcdbf7147ceac873a1e97e2990f
2011-04-25 09:10:59,378 INFO org.apache.hadoop.hbase.regionserver.Store: Added hdfs://pumahbase002-snc5-dfs.data.facebook.com:9000/PUMAHBASE002-SNC5-HBASE/realtime_domain_imps_urls/e7a478b4bd164525052f1dedb832de0a/COUNTERS_86400/83c87fcdbf7147ceac873a1e97e2990f, entries=11404, sequenceid=58140219688, filesize=161.5k
2011-04-25 09:10:59,378 INFO org.apache.hadoop.hbase.regionserver.HRegion: Finished memstore flush of ~18.9m for region realtime_domain_imps_urls,afffffbe,1295556905482.e7a478b4bd164525052f1dedb832de0a. in 224ms, sequenceid=58140219688, compaction requested=true; wal=null
2011-04-25 09:10:59,385 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Deleted recovered.edits file=hdfs://pumahbase002-snc5-dfs.data.facebook.com:9000/PUMAHBASE002-SNC5-HBASE/realtime_domain_imps_urls/e7a478b4bd164525052f1dedb832de0a/recovered.edits/0000000057528037047
2011-04-25 09:10:59,385 INFO org.apache.hadoop.hbase.regionserver.HRegion: Onlined realtime_domain_imps_urls,afffffbe,1295556905482.e7a478b4bd164525052f1dedb832de0a.; next sequenceid=58140219689
2011-04-25 09:10:59,385 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x22f7bb74e8e000a Attempting to transition node e7a478b4bd164525052f1dedb832de0a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2011-04-25 09:10:59,388 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x22f7bb74e8e000a Successfully transitioned node e7a478b4bd164525052f1dedb832de0a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2011-04-25 09:11:36,843 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Updated row realtime_domain_imps_urls,afffffbe,1295556905482.e7a478b4bd164525052f1dedb832de0a. in region .META.,,1 with server=pumahbase107.snc5.facebook.com:60020, startcode=1303450731227
2011-04-25 09:11:36,843 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x22f7bb74e8e000a Attempting to transition node e7a478b4bd164525052f1dedb832de0a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
java.lang.NullPointerException

==="
HBASE-1992	"Here is our fail reporting a split.  Lets make sure that this kinda event is not dropped as part of master rewrite
{code}

2009-11-17 20:57:51,943 INFO org.apache.hadoop.hbase.regionserver.HRegion: region dds-test,k\x12\xB3\xA6\x02\xE9H\xDC,1258520203767/459329844 available; sequence id is 25187748
2009-11-17 20:57:51,944 INFO org.apache.hadoop.hbase.regionserver.HRegion: Closed dds-test,k\x12\xB3\xA6\x02\xE9H\xDC,1258520203767
2009-11-17 20:57:52,007 INFO org.apache.hadoop.hbase.regionserver.CompactSplitThread: region split, META updated, and report to master all successful. Old region=REGION => {NAME => 'dds-test,j\xFD\xD7e\x8B\x1
0\xFC\xFE,1258446041463', STARTKEY => 'j\xFD\xD7e\x8B\x10\xFC\xFE', ENDKEY => 'k\x27q6\xCA\xC8\x15,', ENCODED => 1069248233, OFFLINE => true, SPLIT => true, TABLE => {{NAME => 'dds-test', FAMILIES => [{NAME =
> 'data', COMPRESSION => 'NONE', VERSIONS => '1', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'meta', COMPRESSION => 'NONE', VERSIONS => '1', TTL => '21474
83647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}, new regions: dds-test,j\xFD\xD7e\x8B\x10\xFC\xFE,1258520203767, dds-test,k\x12\xB3\xA6\x02\xE9H\xDC,1258520203767. Split took 1min
s, 8sec
2009-11-17 20:57:52,007 INFO org.apache.hadoop.hbase.regionserver.HRegion: Starting compaction on region dds-test,\x7C\x98\xB4\x91\xE7L\xDC\xC7,1258436623505
2009-11-17 20:57:54,779 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: MSG_REGION_OPEN: dds-test,k\x12\xB3\xA6\x02\xE9H\xDC,1258520203767
2009-11-17 20:57:54,780 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Worker: MSG_REGION_OPEN: dds-test,k\x12\xB3\xA6\x02\xE9H\xDC,1258520203767
2009-11-17 20:57:54,864 INFO org.apache.hadoop.hbase.regionserver.HRegion: region dds-test,k\x12\xB3\xA6\x02\xE9H\xDC,1258520203767/459329844 available; sequence id is 25187748
2009-11-17 20:58:16,166 WARN org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 6 on 60020 took 2967ms appending an edit to hlog; editcount=58
2009-11-17 20:58:16,166 WARN org.apache.hadoop.hbase.regionserver.HLog: regionserver/10.10.0.180:60020.logFlusher took 2854ms optional sync'ing hlog; editcount=59
2009-11-17 20:58:34,340 WARN org.apache.hadoop.hbase.regionserver.HLog: IPC Server handler 6 on 60020 took 7615ms appending an edit to hlog; editcount=820
2009-11-17 20:58:34,340 WARN org.apache.hadoop.hbase.regionserver.HLog: regionserver/10.10.0.180:60020.logFlusher took 1019ms optional sync'ing hlog; editcount=821
2009-11-17 21:00:19,366 INFO org.apache.hadoop.hbase.regionserver.HRegion: compaction completed on region dds-test,\x7C\x98\xB4\x91\xE7L\xDC\xC7,1258436623505 in 2mins, 27sec
2009-11-17 21:00:19,367 INFO org.apache.hadoop.hbase.regionserver.HRegion: Starting split of region dds-test,\x7C\x98\xB4\x91\xE7L\xDC\xC7,1258436623505
2009-11-17 21:00:22,237 INFO org.apache.hadoop.hbase.regionserver.HRegion: Closed dds-test,\x7C\x98\xB4\x91\xE7L\xDC\xC7,1258436623505
2009-11-17 21:00:32,829 INFO org.apache.hadoop.hbase.regionserver.HRegion: region dds-test,\x7C\x98\xB4\x91\xE7L\xDC\xC7,1258520419408/538348405 available; sequence id is 25188602
2009-11-17 21:00:32,829 INFO org.apache.hadoop.hbase.regionserver.HRegion: Closed dds-test,\x7C\x98\xB4\x91\xE7L\xDC\xC7,1258520419408
2009-11-17 21:00:33,002 INFO org.apache.hadoop.hbase.regionserver.HRegion: region dds-test,\x7C\xAD\xCF\x27\x93\x10\x9A5,1258520419408/1604966836 available; sequence id is 25188603
2009-11-17 21:00:33,003 INFO org.apache.hadoop.hbase.regionserver.HRegion: Closed dds-test,\x7C\xAD\xCF\x27\x93\x10\x9A5,1258520419408
2009-11-17 21:01:51,211 ERROR org.apache.hadoop.hbase.regionserver.CompactSplitThread: Compaction/Split failed for region dds-test,\x7C\x98\xB4\x91\xE7L\xDC\xC7,1258436623505
org.apache.hadoop.hbase.client.RetriesExhaustedException: Trying to contact region server 10.10.0.189:60020 for region .META.,,1, row 'dds-test,\x7C\x98\xB4\x91\xE7L\xDC\xC7,1258436623505', but failed after 1
0 attempts.
Exceptions:
java.net.ConnectException: Connection refused
java.net.ConnectException: Connection refused
java.net.ConnectException: Connection refused
java.net.ConnectException: Connection refused
java.net.ConnectException: Connection refused
java.net.ConnectException: Connection refused
java.net.ConnectException: Connection refused
java.net.ConnectException: Connection refused
java.net.ConnectException: Connection refused
java.net.ConnectException: Connection refused

        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.getRegionServerWithRetries(HConnectionManager.java:1001)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers$2.doCall(HConnectionManager.java:1192)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers$Batch.process(HConnectionManager.java:1114)
        at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.processBatchOfRows(HConnectionManager.java:1200)
        at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:605)
        at org.apache.hadoop.hbase.client.HTable.put(HTable.java:470)
        at org.apache.hadoop.hbase.regionserver.CompactSplitThread.split(CompactSplitThread.java:211)
        at org.apache.hadoop.hbase.regionserver.CompactSplitThread.run(CompactSplitThread.java:106)
2009-11-17 21:02:00,709 INFO org.apache.hadoop.hbase.regionserver.HRegion: Starting compaction on region dds-test,d\xC7s\x1B\xD3\x23we,1258519740878
2009-11-17 21:02:11,147 INFO org.apache.hadoop.hbase.regionserver.HRegion: compaction completed on region dds-test,d\xC7s\x1B\xD3\x23we,1258519740878 in 10sec
2009-11-17 21:02:11,147 INFO org.apache.hadoop.hbase.regionserver.HRegion: Starting compaction on region dds-test,a\x96\x85\xB6fV\xE1\xFB,1258519760278
2009-11-17 21:02:30,950 INFO org.apache.hadoop.hbase.regionserver.HLog: Roll /hbase/.logs/sl0127,60020,1258512074419/hlog.dat.1258520252642, entries=853, calcsize=1428860, filesize=1342144. New hlog /hbase/.l
ogs/sl0127,60020,1258512074419/hlog.dat.1258520550914
2009-11-17 21:02:30,950 WARN org.apache.hadoop.hbase.regionserver.HLog: regionserver/10.10.0.180:60020.logFlusher took 7413ms optional sync'ing hlog; editcount=0
2009-11-17 21:03:29,990 INFO org.apache.hadoop.hbase.regionserver.HRegion: compaction completed on region dds-test,a\x96\x85\xB6fV\xE1\xFB,1258519760278 in 1mins, 18sec
{code}"
HBASE-1557	"From Irfan up on hbase-users@

I shutdown hbase using the following command

{{$ HBASE_HOME/bin/stop-hbase.sh}}

and it shuts down properly after a few seconds.

here is my observation since last night. If I user TableRecordWriter1 uses HTable.incrementColumnValue, then the table data is not persisted across restarts but if I use TableRecordWriter2 then the table data is persisted across the restarts.

Any clues ...

Thanks,
Irfan

{code}
    protected static class TableRecordWriter extends RecordWriter<ImmutableBytesWritable, Put> {
        ...
        ...
        ...

        /**
        * {@inheritDoc}
        */
        @Override
        public void write(ImmutableBytesWritable key, Put put) throws IOException
        {
            byte[] row = put.getRow();

            for (Map.Entry<byte[], List<KeyValue>> familyEntry : put.getFamilyMap().entrySet()) {
                byte[] family = familyEntry.getKey();

                for (KeyValue keyValue : familyEntry.getValue()) {
                    byte[] qualifier = keyValue.getQualifier();

                    long amount = Bytes.toLong(keyValue.getValue());

                    this.table_.incrementColumnValue(row, family, qualifier, amount);
                }
            }
        }
    }
{code}

------

{code}
    protected static class TableRecordWriter2 extends RecordWriter<ImmutableBytesWritable, Put> {
        ...
        ...
        ...

        /**
        * {@inheritDoc}
        */
        public void write(ImmutableBytesWritable key, Put put) throws IOException {
            this.table_.put(new Put(put));
        }
    }
{code}"
HBASE-3399	"org.apache.hadoop.hbase.regionserver.MemStore.upsert(KeyValue) doesn't match family before deciding to remove a kv in the memstore

      // if the qualifier matches and it's a put, remove it
      if (kv.matchingQualifier(cur)) {

        // to be extra safe we only remove Puts that have a memstoreTS==0
        if (kv.getType() == KeyValue.Type.Put.getCode() &&
            kv.getMemstoreTS() == 0) {
          // false means there was a change, so give us the size.
          addedSize -= heapSizeChange(kv, true);
          it.remove();
        }

shouldn't it be ""if the family and qualifier match and it's a Put, remove it""?

"
HBASE-3398	"In org.apache.hadoop.hbase.regionserver.HRegion.increment(Increment, Integer, boolean) the following loop assumes that the result from geLastIncrement() has a single entry for a given <family, qualifier>. But that is not necessarily true. getLastIncrement() does a union of all entries found in each of the store files ... and multiple versions of the same key are quite possible.

          List<KeyValue> results = getLastIncrement(get);

          // Iterate the input columns and update existing values if they were
          // found, otherwise add new column initialized to the increment amount
          int idx = 0;
          for (Map.Entry<byte [], Long> column : family.getValue().entrySet()) {
            long amount = column.getValue();
            if (idx < results.size() &&
                results.get(idx).matchingQualifier(column.getKey())) {
              amount += Bytes.toLong(results.get(idx).getValue());
              idx++;
            }"
HBASE-3396	"In getLastIncrement() there is an assumption that memstore only scan will never return multiple versions of a kv

    // found everything we were looking for, done
    if (results.size() == expected) {
      return results;
    }


Based on this assumption the code does an early out after it finds the expected number of key-value pairs in the memstore. But what if there were multiple versions of the same kv returned by the memstore scan? I think it is possible when the memstore has a snapshot pending to be written out. A version of the key can be returned each from the online and from the snapshot memory.
"
HBASE-1988	"RegionServers tend to die with an OutOfMemoryError under load. I expected this problem to go away with the fix for HBASE-1927 in 0.20.2 RC1, but it's still happening. Also, when this happens the cluster becomes unresponsive, even once the load on the machines has gone back down. Interestingly there are lots and lots of scanner lease expired messages right before the OOM."
HBASE-1955	"When i start up HBase cluster, master logs shows zookeeper complaining about being in safe-mode.  Manually deleting the safe-mode file from zk using hbase shell, the WARN logs disappear:

shell> zk 'delete /hbase/safe-mode'



"
HBASE-1802	"During recovery, the master had opened all the logfiles, but when it went to open the destination files, it crashed.  The logfile is missing, the edits did not get applied.

looks like there is a hole whereby we delete the original logfiles before we confirm the new output logs were written. oops!"
HBASE-1721	"JGray RS was stuck doing the below:

{code}
IOException: Cannot append; log is closed
{code}

Just kept going on and on.

Was after a zk session timeout.  Regionserver had restarted itself and had been taking on new regions just fine.  I saw this entry from HLog:

{code}
2009-07-29 08:13:13,493 INFO org.apache.hadoop.hbase.regionserver.HLog: HLog configuration: blocksize=67108864, rollsize=63753420, enabled=true, flushlogentries=100, optionallogflushinternal=10000ms
2009-07-29 08:13:13,495 INFO org.apache.hadoop.hbase.regionserver.HLog: New hlog /hbase/.logs/hb2,60020,1248880393481/hlog.dat.1248880393493
{code}

Then two minutes later I saw the 'Cannot append'.

I do not see any close nor on a cursory glance, how this situation might arise -- somethign to do with the restart?"
HBASE-1711	"On deletion or truncation of a table (including major compacting the META), the entries for that table should get deleted in the META table. That doesnt happen and the entries remain. This causes Region Not Hosting exceptions when doing insertions into the table later on. The files for the deleted table do get deleted from the FS though."
HBASE-1681	"Reproduce: 
1. populate hbase with 100 m records: bin/hadop jar hbase-dev-test.jar --rows=1000000 sequtialWrite 100
2. populate hbase with 10 m records (random writes): bin/hadoop jar hbase-dev-test.jar --rows=1000000 randomWrite 10
3. scan 10 m records: bin/hadoop jar hbase-dev-test.jar --rows=1000000 scan 10
2 scan mapper task failed with NSRE exception for one region:

org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: TestTable,0001724032,1248204794507
	at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:2251)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.openScanner(HRegionServer.java:1862)
	at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:650)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:913)

Grep master log for TestTable,0001724032,1248204794507:

2009-07-21 12:33:18,275 INFO org.apache.hadoop.hbase.master.ServerManager: Recei
ved MSG_REPORT_SPLIT: TestTable,0001724032,1248141721258: Daughters; TestTable,0
001724032,1248204794507, TestTable,0001780000,1248204794507 from snv-it-lin-010.
projectrialto.com,60020,1248115451722; 1 of 3
2009-07-21 12:33:19,169 INFO org.apache.hadoop.hbase.master.RegionManager: Assig
ning region TestTable,0001724032,1248204794507 to snv-it-lin-011.projectrialto.c
om,60020,1248115452051
2009-07-21 12:33:21,464 DEBUG org.apache.hadoop.hbase.master.BaseScanner: Curren
t assignment of TestTable,0001724032,1248204794507 is not valid;  Server '' star
tCode: 0 unknown.
2009-07-21 12:33:22,207 INFO org.apache.hadoop.hbase.master.ServerManager: Recei
ved MSG_REPORT_PROCESS_OPEN: TestTable,0001724032,1248204794507 from snv-it-lin-
011.projectrialto.com,60020,1248115452051; 1 of 1
2009-07-21 12:33:22,208 INFO org.apache.hadoop.hbase.master.RegionManager: Assig
ning region TestTable,0001724032,1248204794507 to snv-it-lin-011.projectrialto.c
om,60020,1248115452051
2009-07-21 12:33:25,245 INFO org.apache.hadoop.hbase.master.ServerManager: Recei
ved MSG_REPORT_PROCESS_OPEN: TestTable,0001724032,1248204794507 from snv-it-lin-
011.projectrialto.com,60020,1248115452051; 1 of 3
2009-07-21 12:33:25,245 INFO org.apache.hadoop.hbase.master.ServerManager: Recei
ved MSG_REPORT_PROCESS_OPEN: TestTable,0001724032,1248204794507 from snv-it-lin-
011.projectrialto.com,60020,1248115452051; 3 of 3
2009-07-21 12:33:28,283 INFO org.apache.hadoop.hbase.master.ServerManager: Recei
ved MSG_REPORT_PROCESS_OPEN: TestTable,0001724032,1248204794507 from snv-it-lin-
011.projectrialto.com,60020,1248115452051; 1 of 7
2009-07-21 12:33:28,283 INFO org.apache.hadoop.hbase.master.ServerManager: Recei
ved MSG_REPORT_PROCESS_OPEN: TestTable,0001724032,1248204794507 from snv-it-lin-
011.projectrialto.com,60020,1248115452051; 3 of 7
2009-07-21 12:33:28,283 INFO org.apache.hadoop.hbase.master.ServerManager: Recei
ved MSG_REPORT_OPEN: TestTable,0001724032,1248204794507 from snv-it-lin-011.proj
ectrialto.com,60020,1248115452051; 5 of 7
2009-07-21 12:33:28,284 INFO org.apache.hadoop.hbase.master.ServerManager: Received MSG_REPORT_OPEN: TestTable,0001724032,1248204794507 from snv-it-lin-011.proj
ectrialto.com,60020,1248115452051; 5 of 7
2009-07-21 12:33:28,284 INFO org.apache.hadoop.hbase.master.ServerManager: Recei
ved MSG_REPORT_OPEN: TestTable,0001724032,1248204794507 from snv-it-lin-011.proj
ectrialto.com,60020,1248115452051; 7 of 7
2009-07-21 12:33:28,284 DEBUG org.apache.hadoop.hbase.master.ServerManager: regi
on server 10.10.30.105:60020 should not have opened region TestTable,0001724032,
1248204794507
2009-07-21 12:33:28,289 INFO org.apache.hadoop.hbase.master.RegionServerOperatio
n: TestTable,0001724032,1248204794507 open on 10.10.30.105:60020
2009-07-21 12:33:28,289 INFO org.apache.hadoop.hbase.master.RegionServerOperatio
n: updating row TestTable,0001724032,1248204794507 in region .META.,,1 with star
tcode 1248115452051 and server 10.10.30.105:60020


Grep region server log for TestTable,0001724032,1248204794507: 

2009-07-21 12:33:19,163 INFO org.apache.hadoop.hbase.regionserver.HRegionServer:
 MSG_REGION_OPEN: TestTable,0001724032,1248204794507
2009-07-21 12:33:22,202 INFO org.apache.hadoop.hbase.regionserver.HRegionServer:
 MSG_REGION_OPEN: TestTable,0001724032,1248204794507
2009-07-21 12:33:26,183 INFO org.apache.hadoop.hbase.regionserver.HRegionServer:
 Worker: MSG_REGION_OPEN: TestTable,0001724032,1248204794507
2009-07-21 12:33:26,184 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Open
ing region TestTable,0001724032,1248204794507, encoded=3313355
2009-07-21 12:33:26,204 INFO org.apache.hadoop.hbase.regionserver.HRegion: regio
n TestTable,0001724032,1248204794507/3313355 available; sequence id is 45707088
2009-07-21 12:33:26,204 DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitT
hread: Compaction requested for region TestTable,0001724032,1248204794507/331335
5 because: Region has references on open
2009-07-21 12:33:26,204 INFO org.apache.hadoop.hbase.regionserver.HRegionServer:
 Worker: MSG_REGION_OPEN: TestTable,0001724032,1248204794507
2009-07-21 12:33:28,278 INFO org.apache.hadoop.hbase.regionserver.HRegionServer:
 MSG_REGION_CLOSE_WITHOUT_REPORT: TestTable,0001724032,1248204794507: Duplicate
assignment
2009-07-21 12:33:28,279 INFO org.apache.hadoop.hbase.regionserver.HRegionServer:
 Worker: MSG_REGION_CLOSE_WITHOUT_REPORT: TestTable,0001724032,1248204794507: Du
plicate assignment
2009-07-21 12:33:28,279 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Clos
ing TestTable,0001724032,1248204794507: compactions & flushes disabled
2009-07-21 12:33:28,279 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Upda
tes disabled for region, no outstanding scanners on TestTable,0001724032,1248204
794507
2009-07-21 12:33:28,279 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: No m
ore row locks outstanding on region TestTable,0001724032,1248204794507
2009-07-21 12:33:28,279 INFO org.apache.hadoop.hbase.regionserver.HRegion: Close
d TestTable,0001724032,1248204794507
2009-07-21 12:34:45,728 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Skip
ping compaction on TestTable,0001724032,1248204794507 because closing/closed
org.apache.hadoop.hbase.NotServingRegionException: TestTable,0001724032,12482047
94507
2009-07-21 13:24:35,902 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handl
er 27 on 60020, call openScanner([B@1756ef1, startRow=0001700000, stopRow=, maxV
ersions=1, timeRange=[0,9223372036854775807), families={(family=info, columns={d
ata}}) from 10.10.30.105:50797: error: org.apache.hadoop.hbase.NotServingRegionE
xception: TestTable,0001724032,1248204794507
org.apache.hadoop.hbase.NotServingRegionException: TestTable,0001724032,12482047
94507
org.apache.hadoop.hbase.NotServingRegionException: TestTable,0001724032,12482047
94507
2009-07-21 13:24:37,908 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handl
er 19 on 60020, call openScanner([B@25b414, startRow=0001700000, stopRow=, maxVe
rsions=1, timeRange=[0,9223372036854775807), families={(family=info, columns={da
ta}}) from 10.10.30.105:50797: error: org.apache.hadoop.hbase.NotServingRegionEx
ception: TestTable,0001724032,1248204794507
org.apache.hadoop.hbase.NotServingRegionException: TestTable,0001724032,12482047
94507
org.apache.hadoop.hbase.NotServingRegionException: TestTable,0001724032,12482047
94507
"
HBASE-1673	"Getting 500 in UI:

{code}
org.apache.hadoop.hbase.client.RetriesExhaustedException: Trying to contact region server null for region , row '', but failed after 3 attempts.
Exceptions:
java.net.ConnectException: Call to /208.76.44.141:60020 failed on connection exception: java.net.ConnectException: Connection refused
...
{code}

Doesn't recover.

I think issue is here in HCM:
{code}
        } catch (IOException e) {
          if (e instanceof RemoteException) {
            e = RemoteExceptionHandler.decodeRemoteException(
                (RemoteException) e);
          }
          if (tries < numRetries - 1) {
            if (LOG.isDebugEnabled()) {
              LOG.debug(""locateRegionInMeta attempt "" + tries + "" of "" +
                this.numRetries + "" failed; retrying after sleep of "" +
                getPauseTime(tries), e);
            }
            relocateRegion(parentTable, metaKey);
          } else {
...
{code}

The call to relocateRegion is going to result in an attempt at finding the .META.,,1 region in .META. which will get a ConnectionException again.

On ConnectionException, should be backing up and going to -ROOT- to find new location of .META."
HBASE-1228	"After an exception that forced an HRegionServer to shut down, I'm seeing it hang in the following method for at least a few minutes:

""regionserver/0:0:0:0:0:0:0:0:60020"" prio=10 tid=0x00002aaaf41a9000 nid=0x10f6 in Object.wait() [0x00000000422dd000..0x00000000422ddb10]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	at java.lang.Object.wait(Object.java:485)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.flushInternal(DFSClient.java:3025)
	- locked <0x00002aaad8fa2410> (a java.util.LinkedList)
	- locked <0x00002aaad8fa2078> (a org.apache.hadoop.hdfs.DFSClient$DFSOutputStream)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.closeInternal(DFSClient.java:3105)
	- locked <0x00002aaad8fa2078> (a org.apache.hadoop.hdfs.DFSClient$DFSOutputStream)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.close(DFSClient.java:3054)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:61)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:86)
	at org.apache.hadoop.io.SequenceFile$Writer.close(SequenceFile.java:959)
	- locked <0x00002aaad8fa1f10> (a org.apache.hadoop.io.SequenceFile$Writer)
	at org.apache.hadoop.hbase.regionserver.HLog.close(HLog.java:431)
	- locked <0x00002aaab378b290> (a java.lang.Integer)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:498)
	at java.lang.Thread.run(Thread.java:619)

I believe the file system may have been closed and thus there is trouble flushing the HLog. The HLog should be pro actively closed before shutdown begins, to maximize the chances of it surviving the crash."
HBASE-1168	"After a HRS goes down on OOME and is restarted, the master acknowledges it but does not assign any regions to it. Stack dump on HRS shows it is up and waiting for work. Relevant lines from tail of master log is:

2009-01-31 03:30:54,377 DEBUG org.apache.hadoop.hbase.master.RegionServerOperation: Removed 10.30.94.38:60020 from deadservers Map
2009-01-31 03:32:49,955 DEBUG org.apache.hadoop.hbase.master.ServerManager: received server report from unknown server: 10.30.94.38:60020
2009-01-31 03:50:37,025 INFO org.apache.hadoop.hbase.master.ServerManager: Received start message from: 10.30.94.38:60020
2009-01-31 04:03:59,822 INFO org.apache.hadoop.hbase.master.ServerManager: Cancelling lease for 10.30.94.38:60020
2009-01-31 04:03:59,823 INFO org.apache.hadoop.hbase.master.ServerManager: Region server 10.30.94.38:60020: MSG_REPORT_EXITING -- lease cancelled
2009-01-31 04:05:31,061 INFO org.apache.hadoop.hbase.master.ServerManager: Received start message from: 10.30.94.38:60020
"
HBASE-1149	"To reproduce:

1) Run HBase in standalone mode
2)

create 'foo', 'bar'

3) kill -9 the HBase server

4) Restart hbase

The table 'foo' will not exist.

Apparently this problem happens because the master and region servers die at the same time. To me that suggests a fairly large flaw -- if your cluster has a systematic failure (say, a power outage) it would cause data loss."
HBASE-1115	"When compaction fails the affected region is left in an open and writable state, but scanners fail construction. Later a manual reassignment via close_region brings the region all the way back up.

Should there be rollback after a failed compaction somehow?

org.apache.hadoop.hbase.client.RetriesExhaustedException: Trying to contact region server 10.30.94.50:60020 for region content,c84bbfc94b2143e41ba119d159be2958,1231518442461, row 'c84bbfc94b2143e41ba119d159be2958', but failed after 10 attempts.
Exceptions:
java.io.IOException: java.io.IOException: HStoreScanner failed construction
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.(StoreFileScanner.java:70)
	at org.apache.hadoop.hbase.regionserver.HStoreScanner.(HStoreScanner.java:84)
	at org.apache.hadoop.hbase.regionserver.HStore.getScanner(HStore.java:2119)
	at org.apache.hadoop.hbase.regionserver.HRegion$HScanner.(HRegion.java:1878)
	at org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:1162)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.openScanner(HRegionServer.java:1673)
	at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:632)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:894)
Caused by: java.io.FileNotFoundException: File does not exist: hdfs://sjdc-atr-dc-1.atr.trendmicro.com:50000/data/hbase/content/1707725801/url/mapfiles/7039742044868774100/data
	at org.apache.hadoop.dfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:394)
	at org.apache.hadoop.fs.FileSystem.getLength(FileSystem.java:695)
	at org.apache.hadoop.hbase.io.SequenceFile$Reader.(SequenceFile.java:1431)
	at org.apache.hadoop.hbase.io.SequenceFile$Reader.(SequenceFile.java:1426)
	at org.apache.hadoop.hbase.io.MapFile$Reader.createDataFileReader(MapFile.java:310)
	at org.apache.hadoop.hbase.io.HBaseMapFile$HBaseReader.createDataFileReader(HBaseMapFile.java:96)
	at org.apache.hadoop.hbase.io.MapFile$Reader.open(MapFile.java:292)
	at org.apache.hadoop.hbase.io.HBaseMapFile$HBaseReader.(HBaseMapFile.java:79)
	at org.apache.hadoop.hbase.io.BloomFilterMapFile$Reader.(BloomFilterMapFile.java:65)
	at org.apache.hadoop.hbase.io.HalfMapFileReader.(HalfMapFileReader.java:86)
	at org.apache.hadoop.hbase.regionserver.HStoreFile.getReader(HStoreFile.java:438)
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.openReaders(StoreFileScanner.java:96)
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.(StoreFileScanner.java:67)
	... 10 more
"
HBASE-549	"We assign a region to a server.  It takes too long to open (HBASE-505).  Region gets assigned to another server.  Meantime original host returns a MSG_REPORT_CLOSE (because other regions opening messes it up moving files on disk out from under it).  We queue a shutdown which marks the region as needing reassignment.  Second server reports in that it successfully opened the region.  Master tells it it should not have opened it.  Churn ensues.

Fix is to ignore the CLOSE if its reported server/startcode does not match that of the server currently trying to open region.  Fix is not easy because currently we don't keep list of server info in unassigned regions.

Here's master log snippet showing problem:
{code}
...
2008-03-25 19:16:43,711 INFO org.apache.hadoop.hbase.HMaster: assigning region enwiki_080103,iLStZ0yTnfVUziYcNVVxWV==,1205393076482 to server XX.XX.XX.220:60020
2008-03-25 19:16:46,725 DEBUG org.apache.hadoop.hbase.HMaster: Received MSG_REPORT_PROCESS_OPEN : enwiki_080103,iLStZ0yTnfVUziYcNVVxWV==,1205393076482 from XX.XX.XX.220:60020
2008-03-25 19:18:06,411 DEBUG org.apache.hadoop.hbase.HMaster: shutdown scanner looking at enwiki_080103,iLStZ0yTnfVUziYcNVVxWV==,1205393076482
2008-03-25 19:18:06,811 DEBUG org.apache.hadoop.hbase.HMaster: shutdown scanner looking at enwiki_080103,iLStZ0yTnfVUziYcNVVxWV==,1205393076482
2008-03-25 19:19:46,841 INFO org.apache.hadoop.hbase.HMaster: assigning region enwiki_080103,iLStZ0yTnfVUziYcNVVxWV==,1205393076482 to server XX.XX.XX.221:60020
2008-03-25 19:19:49,849 DEBUG org.apache.hadoop.hbase.HMaster: Received MSG_REPORT_PROCESS_OPEN : enwiki_080103,iLStZ0yTnfVUziYcNVVxWV==,1205393076482 from XX.XX.XX.221:60020
2008-03-25 19:19:56,883 DEBUG org.apache.hadoop.hbase.HMaster: Received MSG_REPORT_CLOSE : enwiki_080103,iLStZ0yTnfVUziYcNVVxWV==,1205393076482 from XX.XX.XX.220:60020
2008-03-25 19:19:56,883 INFO org.apache.hadoop.hbase.HMaster: XX.XX.XX.220:60020 no longer serving regionname: enwiki_080103,iLStZ0yTnfVUziYcNVVxWV==,1205393076482, startKey: <iLStZ0yTnfVUziYcNVVxWV==>, endKey: <jLB27Q4hKls4tSvp64rJfF==
>, encodedName: 1857033608, tableDesc: {name: enwiki_080103, families: {alternate_title:={name: alternate_title, max versions: 3, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}, alternate_url:={name: al
ternate_url, max versions: 3, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}, anchor:={name: anchor, max versions: 3, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}, mi
sc:={name: misc, max versions: 3, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}, page:={name: page, max versions: 3, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}, re
direct:={name: redirect, max versions: 3, compression: NONE, in memory: false, max length: 2147483647, bloom filter: none}}}
2008-03-25 19:19:56,885 DEBUG org.apache.hadoop.hbase.HMaster: Main processing loop: ProcessRegionClose of enwiki_080103,iLStZ0yTnfVUziYcNVVxWV==,1205393076482, true, false
2008-03-25 19:19:56,885 INFO org.apache.hadoop.hbase.HMaster: region closed: enwiki_080103,iLStZ0yTnfVUziYcNVVxWV==,1205393076482
2008-03-25 19:19:56,887 INFO org.apache.hadoop.hbase.HMaster: reassign region: enwiki_080103,iLStZ0yTnfVUziYcNVVxWV==,1205393076482
2008-03-25 19:19:57,288 INFO org.apache.hadoop.hbase.HMaster: assigning region enwiki_080103,iLStZ0yTnfVUziYcNVVxWV==,1205393076482 to server XX.XX.XX.189:60020
2008-03-25 19:20:00,296 DEBUG org.apache.hadoop.hbase.HMaster: Received MSG_REPORT_PROCESS_OPEN : enwiki_080103,iLStZ0yTnfVUziYcNVVxWV==,1205393076482 from XX.XX.XX.189:60020
2008-03-25 19:20:16,885 DEBUG org.apache.hadoop.hbase.HMaster: Received MSG_REPORT_OPEN : enwiki_080103,iLStZ0yTnfVUziYcNVVxWV==,1205393076482 from XX.XX.XX.221:60020
2008-03-25 19:20:16,885 DEBUG org.apache.hadoop.hbase.HMaster: region server XX.XX.XX.221:60020 should not have opened region enwiki_080103,iLStZ0yTnfVUziYcNVVxWV==,1205393076482
2008-03-25 19:23:51,707 DEBUG org.apache.hadoop.hbase.HMaster: shutdown scanner looking at enwiki_080103,iLStZ0yTnfVUziYcNVVxWV==,1205393076482
2008-03-25 19:23:51,834 DEBUG org.apache.hadoop.hbase.HMaster: shutdown scanner looking at enwiki_080103,iLStZ0yTnfVUziYcNVVxWV==,1205393076482
2008-03-25 19:23:53,947 INFO org.apache.hadoop.hbase.HMaster: assigning region enwiki_080103,iLStZ0yTnfVUziYcNVVxWV==,1205393076482 to server XX.XX.XX.97:60020
...
{code}"
HBASE-70	"Each Store has a Memcache of edits that is flushed on a fixed period (It used to be flushed when it grew beyond a limit). A Region can be made up of N Stores.  A regionserver has no upper bound on the number of regions that can be deployed to it currently.  Add to this that per mapfile, we have read the index into memory.  We're also talking about adding caching of blocks and cells.

We need a means of keeping an account of memory usage adjusting cache sizes and flush rates (or sizes) dynamically -- using References where possible -- to accomodate deployment of added regions.  If memory is strained, we should reject regions proffered by the master with a resouce-constrained, or some such, message.

The manual sizing we currently do ain't going to cut it for clusters of any decent size."
HBASE-1851	"Master crashed, SIGSEGV (0xb) at pc=0x00000031a40fea07, pid=14689, tid=1133910336.  Four other masters running ready to take the failover.  I see where we move to new master but there is an error:

{code}
2009-09-13 22:07:02,061 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: Wrote master address XX.XX.XX.251:20000 to ZooKeeper
2009-09-13 22:07:02,064 WARN org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: Failed to set state node in ZooKeeper
org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /hbase/shutdown
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:110)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
        at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:522)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.setClusterState(ZooKeeperWrapper.java:279)
        at org.apache.hadoop.hbase.master.HMaster.writeAddressToZooKeeper(HMaster.java:270)
        at org.apache.hadoop.hbase.master.HMaster.<init>(HMaster.java:255)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
        at java.lang.reflect.Constructor.newInstance(Unknown Source)
        at org.apache.hadoop.hbase.master.HMaster.doMain(HMaster.java:1200)
        at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:1241)
{code}

Is /hbase/shutdown now ephemeral?

Otherwise, the transition went off well it seems.

Except, if I look in zk -- this is a good while after teh event -- I do not see a master.. its empty.  Do we not record in zk on failover?

But then a split comes in:

2009-09-17 05:50:05,070 INFO org.apache.hadoop.hbase.master.BaseScanner: All 1 .META. region(s) scanned
2009-09-17 05:50:30,745 INFO org.apache.hadoop.hbase.master.ServerManager: Processing MSG_REPORT_SPLIT: enwikibase_dumpurls,,1253145470066: Daughters; enwikibase_dum
purls,,1253166628107, enwikibase_dumpurls,EzAdzwPBtG_o9BLsEqu4bV\x3D\x3D,1253166628107 from aa0-018-6.u.powerset.com,20020,1251458355425; 1 of 3
2009-09-17 05:50:30,745 DEBUG org.apache.hadoop.hbase.master.RegionManager: Assigning for address: XX.XX.44.91:20020, startcode: 1251458355425, load: (requests=0, r
egions=3, usedHeap=490, maxHeap=2031): total nregions to assign=2, nregions to reach balance=4, isMetaAssign=false
2009-09-17 05:50:30,838 DEBUG org.apache.hadoop.hbase.master.RegionManager: Assigning for address: XX.XX.44.49:20020, startcode: 1250638276455, load: (requests=3, r
egions=4, usedHeap=134, maxHeap=2031): total nregions to assign=2, nregions to reach balance=4, isMetaAssign=false
2009-09-17 05:50:31,117 DEBUG org.apache.hadoop.hbase.master.RegionManager: Assigning for address: XX.XX.45.128:20020, startcode: 1250638269214, load: (requests=5, 
regions=4, usedHeap=130, maxHeap=2031): total nregions to assign=2, nregions to reach balance=4, isMetaAssign=false
2009-09-17 05:50:31,119 DEBUG org.apache.hadoop.hbase.master.RegionManager: Assigning for address: XX.XX.45.221:20020, startcode: 1250638268709, load: (requests=5, 
regions=4, usedHeap=82, maxHeap=2031): total nregions to assign=2, nregions to reach balance=4, isMetaAssign=false
2009-09-17 05:50:31,150 DEBUG org.apache.hadoop.hbase.master.RegionManager: Assigning for address: XX.XX.44.75:20020, startcode: 1250638276632, load: (requests=9, r
egions=4, usedHeap=284, maxHeap=2031): total nregions to assign=2, nregions to reach balance=4, isMetaAssign=false
2009-09-17 05:50:31,215 DEBUG org.apache.hadoop.hbase.master.RegionManager: Assigning for address: XX.XX.45.180:20020, startcode: 1250638269143, load: (requests=11,
 regions=4, usedHeap=132, maxHeap=2031): total nregions to assign=2, nregions to reach balance=4, isMetaAssign=false
2009-09-17 05:50:31,265 DEBUG org.apache.hadoop.hbase.master.RegionManager: Assigning for address: XX.XX.45.121:20020, startcode: 1250638269297, load: (requests=5, 
regions=4, usedHeap=54, maxHeap=2031): total nregions to assign=2, nregions to reach balance=4, isMetaAssign=false
...

And we never recover from the above (12 hours and still at it).

"
HBASE-1086	"On the jgray cluster, datanodes died.  Subsequently, could read from the table fine but attempts at writing into regions hosted by regionservers that sat beside dead datanodes fail -- connection refused. 

This issue should be easy to replicate, just kill adjacent datanode."
HBASE-3061	Requirements documentation update. 
HBASE-1468	"if we want to hash KeyValue, we will need a hash that takes int offset, int length."
HBASE-3004	"Fix would be pretty simple.  In the major compaction look, if file is already major compacted, add check of newest and oldest entries.   If has data older than TTL, re-major compact."
HBASE-2682	"Imran M Yousuf  writes: (http://permalink.gmane.org/gmane.comp.java.hadoop.hbase.user/10525)

""I am trying to use HBase as maven dependency and am running in an
error for 0.21-SNAPSHOT :(. I have attached the DEBUG maven output and
Maven, Java versions and relevant POMs are as follows.


Maven & Java version:
-------------------------------

Apache Maven 2.1.0 (r755702; 2009-03-19 01:10:27+0600)
Java version: 1.6.0_13
Java home: /opt/jdk1.6.0_13/jre
Default locale: en_US, platform encoding: UTF-8
OS name: ""linux"" version: ""2.6.28-18-generic"" arch: ""i386"" Family: ""unix""
Ubuntu 9.04

Relevant POM files:
----------------------------

http://github.com/imyousuf/smart-dao/blob/master/smart-hbase-dao/pom.xml
http://github.com/imyousuf/smart-dao/blob/master/pom.xml (Line 178)

I would be grateful if someone would kindly help getting it to work.
Please feel free to ask me anything in this regard.

Thank you.""

See link for aforementioned maven output."
HBASE-1873	"HRegionServer on lines 459 - 463 (part of run()) accesses outboundMsgs in a synchronized fashion, but other uses of the object are not synchronized.

Specifically, the code is

{noformat}
459          synchronized(this.outboundMsgs) {
460            outboundArray =
461              this.outboundMsgs.toArray(new HMsg[outboundMsgs.size()]); 
462            this.outboundMsgs.clear();
463          }
{noformat}

Whereas things are added to this list from calls like

{noformat}
  private void reportOpen(HRegionInfo region) {
    outboundMsgs.add(new HMsg(HMsg.Type.MSG_REPORT_OPEN, region));
  }
{noformat}

And

{noformat}
  void reportSplit(HRegionInfo oldRegion, HRegionInfo newRegionA,
      HRegionInfo newRegionB) {
    outboundMsgs.add(new HMsg(HMsg.Type.MSG_REPORT_SPLIT, oldRegion,
      (""Daughters; "" +
        newRegionA.getRegionNameAsString() + "", "" +
        newRegionB.getRegionNameAsString()).getBytes()));
    outboundMsgs.add(new HMsg(HMsg.Type.MSG_REPORT_OPEN, newRegionA));
    outboundMsgs.add(new HMsg(HMsg.Type.MSG_REPORT_OPEN, newRegionB));
  }
{noformat}

It looks like the object is initialized as

{noformat}
  private final List<HMsg> outboundMsgs =
    Collections.synchronizedList(new ArrayList<HMsg>());
{noformat}

Which would appear to provide security, but it doesn't actually prevent an insert from happening between lines 461 and 462, which would subsequently get removed from the call to clear().  At least, from the Sun HotSpot source code, it looks like Collections.synchronizedList() does the right thing and synchronizes on an inner mutex object instead of synchronizing on the externally visible list object itself.  That means, however, that the synchronized() on line 459 is largely meaningless.

I'm not sure how often this race condition would occur in the wild, but every thread waiting on the mutex around the toArray() call increases the probability that the next person to get the mutex is someone who wants to add something to the List, rather than the thread calling clear().

Simple fix would be to do external synchronization around all accesses to the List.  Barring that, perhaps a SynchronizedList implementation with a ""emptyToArray()"" method that encapsulates the toArray() and subsequent clear()."
HBASE-1463	"I've seen post-hbase-1457 that if regionserver hosting -ROOT- goes down and then is reassigned, though .META. is happy where it is, and even though -ROOT- edits get picked up and are applied, I see that rootscanner complains the  .META. assignment is invalid because the server and startcode are empty.  Something's up.  Take a look."
HBASE-1909	"fake on irc found a bug where he truncated his tables and they all ended up getting assigned to the same regionserver.

4 total nodes, root and meta each ended up on one node, another node had 0 regions, and the final one ended up with all 5 regions of user tables."
HBASE-2900	"I installed the hbase and hadoop under the Pseudo-Distributed mode:

I configure the server side with following configuration:

hadoop: core-site.xml
<property>
    <name>fs.default.name</name>
    <value>hdfs://cluster1.office:9000</value>
  </property>

hadoop:mapred-site.xml
<property>
    <name>mapred.job.tracker</name>
    <value>localhost:9001</value>
  </property>

hadoop:hdfs-site.xml
  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>

hbase:hbase-site.xml
 <property>
    <name>hbase.rootdir</name>
    <value>hdfs://cluster1.office:9000/hbase</value>
    <description>The directory shared by region servers.
    </description>
  </property>

I configure my client side(this is not in the same server as the server side) with following configuration:
  <property>
    <name>hbase.rootdir</name>
    <value>hdfs://cluster1.office/hbase</value>
  </property>
  <property>
    <name>hbase.zookeeper.quorum</name>
    <value>cluster1.office</value>
  </property>

And then I connect to server side by java API described in the following page:
http://hbase.apache.org/docs/current/api/org/apache/hadoop/hbase/client/package-summary.html#package_description

Unfortunately I get the Out of memory error:

2010-08-04 17:57:49,636 INFO org.apache.hadoop.hbase.regionserver.HRegion: Starting compaction on region -ROOT-,,0
2010-08-04 17:57:49,686 INFO org.apache.hadoop.hbase.master.ServerManager: 1 region servers, 0 dead, average load 4.0
2010-08-04 17:57:49,815 DEBUG org.apache.hadoop.hbase.regionserver.Store: Compaction size of info: 4.0k; Skipped 1 file(s), size: 1849
2010-08-04 17:57:49,815 INFO org.apache.hadoop.hbase.regionserver.Store: Started compaction of 4 file(s) in info of -ROOT-,,0  into /hbase/-ROOT-/compaction.dir/70236052, seqid=112
2010-08-04 17:57:49,921 INFO org.apache.hadoop.hbase.regionserver.Store: Completed compaction of 4 file(s) in info of -ROOT-,,0; new storefile is hdfs://cluster1.jsw.office:9000/hbase/-ROOT-/70236052/info/1636337967443011476; store size is 3.0k
2010-08-04 17:57:49,926 INFO org.apache.hadoop.hbase.regionserver.HRegion: compaction completed on region -ROOT-,,0 in 0sec
2010-08-04 17:57:49,926 INFO org.apache.hadoop.hbase.regionserver.HRegion: Starting compaction on region .META.,,1
2010-08-04 17:57:49,928 DEBUG org.apache.hadoop.hbase.regionserver.Store: historian: no store files to compact
2010-08-04 17:57:49,937 DEBUG org.apache.hadoop.hbase.regionserver.Store: Compaction size of info: 5.5k; Skipped 0 file(s), size: 0
2010-08-04 17:57:49,937 INFO org.apache.hadoop.hbase.regionserver.Store: Started compaction of 5 file(s) in info of .META.,,1  into /hbase/.META./compaction.dir/1028785192, seqid=116
2010-08-04 17:57:50,000 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.rootScanner scanning meta region {server: 192.168.1.81:39859, regionname: -ROOT-,,0, startKey: <>}
2010-08-04 17:57:50,037 INFO org.apache.hadoop.hbase.regionserver.Store: Completed compaction of 5 file(s) in info of .META.,,1; new storefile is hdfs://cluster1.jsw.office:9000/hbase/.META./1028785192/info/254940474272935789; store size is 4.0k
2010-08-04 17:57:50,045 INFO org.apache.hadoop.hbase.regionserver.HRegion: compaction completed on region .META.,,1 in 0sec
2010-08-04 17:57:50,053 INFO org.apache.hadoop.hbase.master.BaseScanner: RegionManager.rootScanner scan of 1 row(s) of meta region {server: 192.168.1.81:39859, regionname: -ROOT-,,0, startKey: <>} complete
2010-08-04 17:57:57,278 FATAL org.apache.hadoop.hbase.regionserver.HRegionServer: OutOfMemoryError, aborting.
java.lang.OutOfMemoryError: Java heap space
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Invocation.readFields(HBaseRPC.java:175)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.processData(HBaseServer.java:867)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.readAndProcess(HBaseServer.java:835)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.doRead(HBaseServer.java:419)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.run(HBaseServer.java:318)
2010-08-04 17:57:57,279 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Dump of metrics: request=0.0, regions=4, stores=5, storefiles=4, storefileIndexSize=0, memstoreSize=0, compactionQueueSize=0, usedHeap=23, maxHeap=695, blockCacheSize=1208688, blockCacheFree=144726880, blockCacheCount=11, blockCacheHitRatio=82, fsReadLatency=0, fsWriteLatency=0, fsSyncLatency=0
2010-08-04 17:57:57,279 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server listener on 39859: exiting on OOME
2010-08-04 17:57:57,793 INFO org.apache.hadoop.ipc.HBaseServer: Stopping server on 39859
2010-08-04 17:57:57,794 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 24 on 39859: exiting
2010-08-04 17:57:57,794 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 0 on 39859: exiting
2010-08-04 17:57:57,794 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 2 on 39859: exiting
2010-08-04 17:57:57,794 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 20 on 39859: exiting
2010-08-04 17:57:57,795 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 21 on 39859: exiting
2010-08-04 17:57:57,795 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Stopping infoServer
2010-08-04 17:57:57,796 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 1 on 39859: exiting
2010-08-04 17:57:57,796 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 3 on 39859: exiting
2010-08-04 17:57:57,796 INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 4 on 39859: exiting"
HBASE-2795	"I committed the hbase-2707 patch yesterday but on second thoughts, it has a flaw in that if nothing in the todo queue, we then poll the delayedtodo queue.  If we fall into the latter and it has not elements, then we'll never come out; there are no notifyalls going on to wake us up.  Patch coming."
HBASE-2216	Add mapred-test to cp 
HBASE-1884	"Coming back after a crash, we have region A, and region for __GLOBAL_TRX_LOG. Assuming that there was a pending trx in A at the time of commit, it will find that in the WAL, and need to ask __GLOBAL_TRX_LOG__ what happend with that transaction. However, if we are opening region A before region __GLOBAL_TRX_LOG__ then we will fail to be able to query as we are blocking the opening of the __GLOBAL_TRX_LOG__ region, but we need it to open region A.

"
HBASE-2508	"Currently lockIdGenerator is an int. In obtainRowLock(), retry is needed in case of lockId collisions.

We can declare lockIdGenerator as AtomicInteger and use incrementAndGet() to get the next lock Id."
HBASE-2504	"regionserver log:

Thu Apr 29 13:34:27 CEST 2010 Starting regionserver on dell102
...
2010-04-29 13:34:29,656 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server dell147/10.1.3.147:2181
2010-04-29 13:34:29,657 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to dell147/10.1.3.147:2181, initiating session
2010-04-29 13:34:29,678 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server dell147/10.1.3.147:2181, sessionid = 0x284958aa550001, negotiated timeout = 60000
...
2010-04-29 14:13:30,096 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=dell149:2181,dell148:2181,dell147:2181 sessionTimeout=60000 watcher=org.apache.hadoop.hbase.client.HConnectionManager$ClientZKWatcher@46d895e1
2010-04-29 14:13:30,096 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server dell147/10.1.3.147:2181
2010-04-29 14:13:30,161 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to dell147/10.1.3.147:2181, initiating session
2010-04-29 14:13:30,194 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server dell147/10.1.3.147:2181, sessionid = 0x284958aa550014, negotiated timeout = 60000
2010-04-29 14:13:30,195 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: Read ZNode /hbase/root-region-server got 10.1.3.123:60020
2010-04-29 14:13:30,226 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$TableServers: Found ROOT at 10.1.3.123:60020
2010-04-29 14:13:30,243 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$TableServers: Cached location for .META.,,1 is 10.1.3.125:60020
2010-04-29 14:13:30,247 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: Read ZNode /hbase/master got 10.1.3.150:60000
...
2010-04-29 22:06:01,637 INFO org.apache.zookeeper.ZooKeeper: Session: 0x284958aa550014 closed
2010-04-29 22:06:02,012 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper: Closed connection with ZooKeeper

Clearly the reinitializeZooKeeper() method was called for some reason.

Unfortunately:
hbase(main):005:0> zk 'get /hbase/rs/1272540873161'
10.1.3.102:60020
cZxid = 0x5f0000002b
ctime = Thu Apr 29 13:34:33 CEST 2010
mZxid = 0x5f0000003e
mtime = Thu Apr 29 13:34:33 CEST 2010
pZxid = 0x5f0000002b
cversion = 0
dataVersion = 1
aclVersion = 0
ephemeralOwner = 0x284958aa550001
dataLength = 16
numChildren = 0

The owner of the zookeeper node is the first session which was never closed.

"
HBASE-2477	"Memstore flushes are triggered today if a memstore exceeds a certain size or there is memory pressure.  However, there is no timer based flush for a memstore. This means a single column family or table getting a very slow rate of writes could hold up old HLogs from getting reclaimed for long periods of time-- which in turn increases recovery time for a failed region server since there are a lot more logs to process.

META is an example of a table which is likely to get very few writes. But even if we special cased META somehow, it wouldn't be good enough, since an application could genuinely have a mix of slow and fast changing tables or column families.

What about also triggering flushes on a timer (in addition to the current mechanism) to bound recovery times?
"
HBASE-2050	"What do people think of making ""."" a valid path in HDFS? The motivation is to allow users to create symlinks to the current directory in HDFS-245, eg see the following test for the current behavior:
{code}
  @Test
  /** Test create symlink to . */
  public void testCreateLinkToDot() throws IOException {
    Path dir  = new Path(""/test"");
    Path link = new Path(""/test/linkToDot"");
    fc.mkdir(dir, FileContext.DEFAULT_PERM, true);        
    fc.setWorkingDirectory(dir);
    try {
      fc.createSymlink(new Path("".""), link);
      fail(""Created symlink to dot"");
      readFile(new Path(""/test/linkToDot/file""));
    } catch (IOException x) {
      // Expected. Path(""."") resolves to """" because URI normalizes
      // the dot away and AbstractFileSystem considers """" invalid.  
    }
    fc.delete(dir, true);
  }
{code}

This involves trade offs since Hadoop Paths represent URIs (rather than the path component of a URI) and in URIs dot normalizes away. Some options:
# Make symlinks to ""."" an exception per the above, though it seems odd to consider ""."", "".."", ""/"" etc invalid paths (and the latter two happen to work based on how the Path constructor initializes the URI even though isValidName in AbstractFileSystem would consider them invalid names.
# Making ""."" immediately parse to an absolute path would be poor symlink semantics (eg a link to ""."" should not break if you rename the link's parent directory). 
# Making Path special case this so ""."" doesn't normalize away would be a weird one off case where Path and URIs differ.

Other alternatives? Of the above I'd prefer the last one."
HBASE-1044	"From IRC (with some improving text added -- finishes on a good point made by jgray):

{code}
18:53 < St^Ack> The coupling in a MR cluster is looser than it is in hbase cluster
18:53 < St^Ack> TTs only need report in every ten minutes and a task can fail and be restarted
18:54 < St^Ack> Whereas with hbase, it must report in at least every two minutes (IIRC) and cannot 'redo' lost edit -- no chance of a redo
18:54 < St^Ack> So, your MR job can be rougher about getting to the finish line; messier.
18:55 < tim_s> yeah. 
18:55 < St^Ack> If MR is running on same nodes as those hosting hbase, then it can rob resources from hbase in a way that damages hbase but not TT
18:56 < St^Ack> So, maybe we need to look at the hbase config; make it more tolerant when its running beside a hogging TT job
18:57 < tim_s> hmm, that would be lovely
18:57 < St^Ack> Need to look at HDFS; see how 'fragile' it is too; hbase should be at least that 'fragile'
18:57 < jgray> yeah, most issues we see come from resource issues on shared hdfs/tt/rs nodes
18:57 < tim_s> so is it common to host hbase elsewhere?
18:57 < St^Ack> Let me make an issue on it because this is common failure case for hbase (Setup hbase then run your old MR job as though nothing has changed -- then surprise when the little hbase lady faints)
18:57 < jgray> tim_s: currently, no.  common practice is shared
18:58 < St^Ack> ... and its better if shared -- locality benefits
18:58 < tim_s> would that be a good idea though? cause I don't really need to have hadoop local I guess.
18:58 < tim_s> ahh
18:58 < apurtell> we share also
...
18:59 < jgray> beyond locality, sharing makes sense as hdfs and hbase nodes have different requirements... hdfs being heaviest on io (where hbase has no use), hbase heavy in memory, TTs vary greatly but most often heavy in cpu/io
{code}"
HBASE-1752	"When a regionserver aborts, there is a window where clients are continuing to try to write to it, but the writes are failing:

{noformat}
Aug 4, 2009 6:47:25 AM net.iridiant.heritrix.writer.HBaseWriterProcessor innerProcessResult
SEVERE: Failed write of Record: http://www.rdc.udel.edu/reports/development/profdev.pdf (in thread 'ToeThread #3: http://www.rdc.udel.edu/reports/development/profdev.pdf'; in processor 'Archiver')
java.io.IOException: org.apache.hadoop.hbase.client.RetriesExhaustedException: Trying to contact region server 172.20.3.229:60020 for region content,23d6b92bc7eb100fc1294e6b124b7e75,1249198098627, row '23d6b92bc7eb100fc1294e6b124b7e75', but failed after 10 attempts.
Exceptions:
java.net.ConnectException: Call to /172.20.3.229:60020 failed on connection exception: java.net.ConnectException: Connection refused
java.net.ConnectException: Call to /172.20.3.229:60020 failed on connection exception: java.net.ConnectException: Connection refused
java.net.ConnectException: Call to /172.20.3.229:60020 failed on connection exception: java.net.ConnectException: Connection refused
java.net.ConnectException: Call to /172.20.3.229:60020 failed on connection exception: java.net.ConnectException: Connection refused
java.net.ConnectException: Call to /172.20.3.229:60020 failed on connection exception: java.net.ConnectException: Connection refused
java.net.ConnectException: Call to /172.20.3.229:60020 failed on connection exception: java.net.ConnectException: Connection refused
java.net.ConnectException: Call to /172.20.3.229:60020 failed on connection exception: java.net.ConnectException: Connection refused
java.net.ConnectException: Call to /172.20.3.229:60020 failed on connection exception: java.net.ConnectException: Connection refused
java.net.ConnectException: Call to /172.20.3.229:60020 failed on connection exception: java.net.ConnectException: Connection refused
java.net.ConnectException: Call to /172.20.3.229:60020 failed on connection exception: java.net.ConnectException: Connection refused
	at net.iridiant.heritrix.writer.HBaseWriter.write(Unknown Source)
	at net.iridiant.heritrix.writer.HBaseWriterProcessor.write(Unknown Source)
	at net.iridiant.heritrix.writer.HBaseWriterProcessor.innerProcessResult(Unknown Source)
	at org.archive.modules.Processor.process(Unknown Source)
	at org.archive.crawler.framework.ToeThread.processCrawlUri(Unknown Source)
	at org.archive.crawler.framework.ToeThread.run(Unknown Source)
{noformat}

The client does not retry, so the data is lost. Maybe we can get this in for RC2?"
HBASE-1631	"{noformat}
HTTP ERROR: 500

Expected static method org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.getQuorumServers()Ljava/lang/String;

RequestURI=/master.jsp
Caused by:

java.lang.IncompatibleClassChangeError: Expected static method org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.getQuorumServers()Ljava/lang/String;
	at org.apache.hadoop.hbase.generated.master.master_jsp._jspService(master_jsp.java:103)
	at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:97)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:502)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:363)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:417)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:324)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:534)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:864)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:533)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:207)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:403)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:522)
{noformat}

This method is no longer static now that each ZooKeeperWrapper maintains their own quorum so that it can be programatically changed."
HBASE-1533	I see this during bulk upload into TRUNK.  Happens often enough.  Odd is that its INFO level and the compaction seems to succeed.  Figure out whats going on here.  The log is disturbing.
HBASE-1519	"Running on a hacked up TRUNK -- none of my patches touched hfile -- I got below (I have the original 1513 patch in place):

{code}
2009-06-14 02:42:28,853 [regionserver/0:0:0:0:0:0:0:0:60021.compactor] DEBUG org.apache.hadoop.hbase.regionserver.Store: Completed compaction of historian; store size is 47.2k
2009-06-14 02:42:28,855 [IPC Server handler 7 on 60021] INFO org.apache.hadoop.ipc.HBaseServer: IPC Server handler 7 on 60021, call get([B@6a8bfc4f, row=TestTable,0006569959,1244947258659, maxVersions=1, timeRange=[0,9223372036854775807), families={(family=historian, columns=ALL), (family=info, columns=ALL}) from 208.76.44.139:47644: error: java.io.IOException: java.lang.NullPointerException
java.io.IOException: java.lang.NullPointerException
    at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:832)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:822)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1723)
    at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:643)
    at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:913)
Caused by: java.lang.NullPointerException
    at org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream.read(BoundedRangeFileInputStream.java:97)
    at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:100)
    at org.apache.hadoop.hbase.io.hfile.HFile$Reader.decompress(HFile.java:950)
    at org.apache.hadoop.hbase.io.hfile.HFile$Reader.readBlock(HFile.java:906)
    at org.apache.hadoop.hbase.io.hfile.HFile$Reader$Scanner.loadBlock(HFile.java:1222)
    at org.apache.hadoop.hbase.io.hfile.HFile$Reader$Scanner.seekTo(HFile.java:1105)
    at org.apache.hadoop.hbase.regionserver.StoreFileGetScan.getStoreFile(StoreFileGetScan.java:80)
    at org.apache.hadoop.hbase.regionserver.StoreFileGetScan.get(StoreFileGetScan.java:65)
    at org.apache.hadoop.hbase.regionserver.Store.get(Store.java:1485)
    at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:2251)
    at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:2240)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1721)
    ... 5 more

{code}"
HBASE-1426	"From Clint Morgan: ""Rather that adding new methods to the existing interface and deprecating the old ones, I'd propose creating a new interface, deprecating the old one, and using a wrapper that takes a deprecated filter and makes it implement the new interface. That way the regionserver logic would only have to honor the new interface.""

If we're going to make changes to filters for 0.20.0, lets entertain Clints idea.  We might do as Clint suggests anyways."
HBASE-1378	"HBASE-1318, svn checkin 771087, included the following bad change:

{code}
diff --git a/conf/hbase-env.sh b/conf/hbase-env.sh
index 64fa4c5..b260c6c 100644
--- a/conf/hbase-env.sh
+++ b/conf/hbase-env.sh
@@ -23,6 +23,7 @@
 
 # The java implementation to use.  Java 1.6 required.
 # export JAVA_HOME=/usr/java/jdk1.6.0/
+export JAVA_HOME=/usr/lib/jvm/java-6-sun-1.6.0.07/
 
 # Extra Java CLASSPATH elements.  Optional.
 # export HBASE_CLASSPATH=
{code}"
HBASE-1326	"I get this in the logfile:

2009-04-16 23:49:20,518 INFO org.apache.hadoop.hbase.regionserver.MemcacheFlusher: Forced flushing of <table redacted>,1239954535262 because global memcache limit of 3.2m exceeded; currently 3.2m and flushing till 2.0m

With the experimental G1 GC, the heap size is created differently, and the way we calculate the size of memcache is flawed under this GC.  This affects performance and makes things reallll slow.
"
HBASE-1208	"On irc this evening, another victim of too few xceivers."
HBASE-1047	"The patch for HBASE-1039 allows regions to continue to be served even after a bloomfilter related exception/failure happens during compaction. Over time, eventually all regions containing column families with bloomfilters enabled will accumulate these errors. "
HBASE-1724	"I have a 3 node cluster setup each running hadoop and hbase. I have created 'accounts' table and loaded some data into it (about 3000 rows).
Some days later one of the region servers died and after i restarted it there were no records in the table at all. I saw HLOG file with my records
in HDFS but as I understand the file was not used by HBase to recover the table.

I tried to emulate the situation and uploaded another 2000 records into my table and killed the region server holding 'accounts' table region.
In the HDFS I found file with my records but some time later it was replaced by another empty directory. As i suspected after killed region 
server startup the data was not recovered.

Everything is lost again and there is no any exceptions in the logs..."
HBASE-616	"Just saw the below in a log... all in a row on the one server.

{code}
   4493 2008-05-05 18:08:17,512 WARN org.apache.hadoop.hbase.util.Sleeper: We slept 34557ms, ten times longer than scheduled: 3000
   4494 2008-05-05 18:11:08,879 WARN org.apache.hadoop.hbase.util.Sleeper: We slept 30576ms, ten times longer than scheduled: 3000
   4495 2008-05-05 18:30:45,056 WARN org.apache.hadoop.hbase.util.Sleeper: We slept 1091720ms, ten times longer than scheduled: 3000
   4496 2008-05-05 18:30:45,056 WARN org.apache.hadoop.hbase.util.Sleeper: We slept 1094209ms, ten times longer than scheduled: 10000
   4497 2008-05-05 18:30:45,429 FATAL org.apache.hadoop.hbase.HRegionServer: unable to report to master for 1092093 milliseconds - aborting server
{code}

We're seeing these kinda outages pretty frequently.  In the case above, it was small cluster that was using TableReduce to insert.  The MR, HDFS and HBase were all running on same nodes."
HBASE-1570	"[Sorry, pressed Ctrl-Enter accidentally. Please remove this issue]"
HBASE-1571	"Suppose that two threads are concurrently calling HBaseClient.getConnection() to an address to which no connection has been made yet, or one existed but has been dropped due to an error.
One of the threads creates a Connection object, puts it into the 'connections' map, and there, starts doing setupIOStreams, and there goes a context switch.
The second thread also calls getConnection and gets a connection from the 'connections' map, whose 'out' stream is only going to be initialized by the second thread, but has not yet been. There, at synchronized(this.out), goes an NPE."
HBASE-1135	"{code}
2009-01-18 22:16:35,740 [regionserver/0:0:0:0:0:0:0:0:60020] INFO org.apache.hadoop.hbase.regionserver.HRegionServer: MSG_REGION_OPEN: TestTable,0037764196,1232260341993
2009-01-18 22:16:35,741 [regionserver/0:0:0:0:0:0:0:0:60020.worker] INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Worker: MSG_REGION_OPEN: TestTable,0037764196,1232260341993
2009-01-18 22:16:35,749 [regionserver/0:0:0:0:0:0:0:0:60020.worker] INFO org.apache.hadoop.hbase.regionserver.HStore: HSTORE_LOGINFOFILE 308435520/info/1230499821085743170-1612503414/6361963498845991946/bottom does not contain a sequence number - ignoring
2009-01-18 22:16:35,757 [regionserver/0:0:0:0:0:0:0:0:60020.worker] ERROR org.apache.hadoop.hbase.regionserver.HRegionServer: Error opening TestTable,0037764196,1232260341993
java.io.FileNotFoundException: File does not exist: hdfs://aa0-000-12.u.powerset.com:9000/hbasetrunk2/TestTable/1612503414/info/mapfiles/6361963498845991946/data
    at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:394)
    at org.apache.hadoop.hbase.regionserver.HStoreFile.length(HStoreFile.java:477)
    at org.apache.hadoop.hbase.regionserver.HStore.loadHStoreFiles(HStore.java:487)
    at org.apache.hadoop.hbase.regionserver.HStore.<init>(HStore.java:230)
    at org.apache.hadoop.hbase.regionserver.HRegion.instantiateHStore(HRegion.java:1764)
    at org.apache.hadoop.hbase.regionserver.HRegion.initialize(HRegion.java:276)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.instantiateRegion(HRegionServer.java:1367)
    at org.apache.hadoop.hbase.regionserver.HRegionServer.openRegion(HRegionServer.java:1338)
    at org.apache.hadoop.hbase.regionserver.HRegionServer$Worker.run(HRegionServer.java:1253)
    at java.lang.Thread.run(Thread.java:619)
{code}

Above happens over and over again.

HBase should heal itself."
HBASE-1071	"From Andrew Purtell note up on list:

""Later, maybe it would make sense to dynamically set the index
interval based on the distribution of cell sizes in the 
mapfile at some future time, according to some parameterized
formula that could be adjusted with config variable(s). This
could be done during compaction. Would make sense to also
consider the distribution of key lengths. Or there could be
other similar tricks implemented to keep index sizes down. """
HBASE-1376	I get NPEs when no LRU block cache enabled
HBASE-1312	"Does the master watch its own znode? Right around the time of regionserver problems described in HBASE-1311, clients could no longer find the master, but according to its log it was up and functionling normally. I think the master and regionserver sessions expired at the same time, as they were started within seconds of each other."
HBASE-911	"This issue is about looking into how much space in filesystem hbases uses.  Daniel Ploeg suggests that hbase is profligate in its use of space in hdfs.   Given that block sizes by default are 64MB, and that every time hbase writes a store file that its accompanied by an index file and a very small metadata file, thats 3*64MB even if the file is empty (TODO: Prove this).  The situation is aggrevated by the fact that hbase does a flush of whatever is in memory every 30 minutes to minimize loss in the absence of appends; this latter action makes for lots of small files.

The solution to the above is implement append so optional flush is not necessary and a file format that aggregates info, index and data all in the one file.   Short-term, we should set block size on the info/metadata file down to 4k or some such small size and look into doing likewise for the mapfile index."
HBASE-827	"Background introduction :
I was intent to implement a large queue-liked structure based on HBase.
Different from normal queue structure, in my structure, a pop action means 'randomly' pick one elements(which is key/value pair from a row) in queue, and decrease the queueSize of that queue.

For performance consideration, I implemented pop action by following steps :
1.get the queue size
2.random a number between 0~queueSize
3.take step2 as a qualifier, get the value of that element, and put the last element's value to the original qualifier.
4.decrease the queue size. (I didn't delete the last elements since storage space is not my primary consideration)

When the queueSize and number of pop action become large, hbase failed to open a scanner. (Even the data of database is correct)

My program can be split into few steps:
1.create a large queue.
2.pop until the queue empty
3.keep popping the queue 
4.delete the queue ( delete a row with deleteAll )
5.create a small queue with same name (i.e: same row)

After create a small queue, I checked every key/value in the hbase and found it is correct. But when I tried to open a scanner, it got Unknown Exception as follows:

In shell :
NativeException: org.apache.hadoop.hbase.client.RetriesExhaustedException: Trying to contact region server 127.0.0.1:54141 for region pleaseCrash,,1218621377152, row '', but failed after 3 attempts.
Exceptions:
java.net.SocketTimeoutException: timed out waiting for rpc response
java.net.SocketTimeoutException: timed out waiting for rpc response
java.net.SocketTimeoutException: timed out waiting for rpc response

        from org/apache/hadoop/hbase/client/HConnectionManager.java:873:in `getRegionServerWithRetries'
        from org/apache/hadoop/hbase/client/HTable.java:1415:in `nextScanner'
        from org/apache/hadoop/hbase/client/HTable.java:1360:in `<init>'
        from org/apache/hadoop/hbase/client/HTable.java:985:in `getScanner'
        from org/apache/hadoop/hbase/client/HTable.java:779:in `getScanner'
        from org/apache/hadoop/hbase/client/HTable.java:706:in `getScanner'
        from sun/reflect/NativeMethodAccessorImpl.java:-2:in `invoke0'
        from sun/reflect/NativeMethodAccessorImpl.java:39:in `invoke'
        from sun/reflect/DelegatingMethodAccessorImpl.java:25:in `invoke'
        from java/lang/reflect/Method.java:597:in `invoke'
        from org/jruby/javasupport/JavaMethod.java:250:in `invokeWithExceptionHandling'
        from org/jruby/javasupport/JavaMethod.java:219:in `invoke'
        from org/jruby/javasupport/JavaClass.java:416:in `execute'
        from org/jruby/internal/runtime/methods/SimpleCallbackMethod.java:67:in `call'
        from org/jruby/internal/runtime/methods/DynamicMethod.java:78:in `call'
        from org/jruby/runtime/CallSite.java:155:in `cacheAndCall'
... 128 levels...
        from ruby.home.itspeter.hbase_minus_0_dot_3_dot_0_minus_dev.bin.hirbInvokermethod__23$RUBY$startOpt:-1:in `call'
        from org/jruby/internal/runtime/methods/DynamicMethod.java:74:in `call'
        from org/jruby/internal/runtime/methods/CompiledMethod.java:48:in `call'
        from org/jruby/runtime/CallSite.java:123:in `cacheAndCall'
        from org/jruby/runtime/CallSite.java:298:in `call'
        from ruby/home/itspeter/hbase_minus_0_dot_3_dot_0_minus_dev/bin//home/itspeter/hbase-0.3.0-dev/bin/../bin/hirb.rb:350:in `__file__'
        from ruby/home/itspeter/hbase_minus_0_dot_3_dot_0_minus_dev/bin//home/itspeter/hbase-0.3.0-dev/bin/../bin/hirb.rb:-1:in `__file__'
        from ruby/home/itspeter/hbase_minus_0_dot_3_dot_0_minus_dev/bin//home/itspeter/hbase-0.3.0-dev/bin/../bin/hirb.rb:-1:in `load'
        from org/jruby/Ruby.java:512:in `runScript'
        from org/jruby/Ruby.java:432:in `runNormally'
        from org/jruby/Ruby.java:312:in `runFromMain'
        from org/jruby/Main.java:144:in `run'
        from org/jruby/Main.java:89:in `run'
        from org/jruby/Main.java:80:in `main'
        from /home/itspeter/hbase-0.3.0-dev/bin/../bin/hirb.rb:271:in `scan'
        from (hbase):3:in `binding'

Side effects :
	After scan got time out, I discovered that I also failed to disable the table (But not every time), somehow more strange is that table will be dropped successfully even disable failed (but not every time).

System Resources:
	Hbase will rob the cpu...
	In my platform(4-core), cpu usage will raise from 10% -> 50% ..100% -> 200% -> 300% ..

Procedure to reproduce :
Run the attached java file. 
Try to type ""scan 'pleaseCrash'"" in hbase shell after program finished.


"
HBASE-804	"In many failed map tasks we see things like this:
{code}
INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 64020, call next(-8330618580557781998) from 192.168.1.84:59678: error: org.apache.hadoop.hbase.UnknownScannerException: Name: -8330618580557781998
{code}

Here is an excerpt of the regionserver log:
{code}
2008-08-08 10:49:13,276 INFO org.apache.hadoop.hbase.regionserver.HRegion: Starting split of region entities,North_America-United_States-Missouri-Oran-b9c7e44a-4a2b-11dd-abd4-1231390079f2,1218205066272
2008-08-08 10:49:13,310 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Compactions and cache flushes disabled for region entities,North_America-United_States-Missouri-Oran-b9c7e44a-4a2b-11dd-abd4-1231390079f2,1218205066272
2008-08-08 10:49:13,310 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Updates and scanners disabled for region entities,North_America-United_States-Missouri-Oran-b9c7e44a-4a2b-11dd-abd4-1231390079f2,1218205066272
2008-08-08 10:49:13,310 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: waiting for 1 scanners to finish
2008-08-08 10:50:13,306 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Scanner -8330618580557781998 lease expired
2008-08-08 10:50:13,307 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: No more active scanners for region entities,North_America-United_States-Missouri-Oran-b9c7e44a-4a2b-11dd-abd4-1231390079f2,1218205066272
2008-08-08 10:50:13,307 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: No more row locks outstanding on region entities,North_America-United_States-Missouri-Oran-b9c7e44a-4a2b-11dd-abd4-1231390079f2,1218205066272
2008-08-08 10:50:13,307 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memcache flush for region entities,North_America-United_States-Missouri-Oran-b9c7e44a-4a2b-11dd-abd4-1231390079f2,1218205066272. Current region memcache size 3.7m
2008-08-08 10:50:13,821 DEBUG org.apache.hadoop.hbase.regionserver.HStore: Added /hbase/amsterdam_migration/entities/858230020/attribute/mapfiles/5987693228942080783 with 54576 entries, sequence id 9333266, data size 3.7m, file size 4.5m
2008-08-08 10:50:13,821 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region entities,North_America-United_States-Missouri-Oran-b9c7e44a-4a2b-11dd-abd4-1231390079f2,1218205066272 in 514ms, sequence id=9333266, compaction requested=true
2008-08-08 10:50:13,821 DEBUG org.apache.hadoop.hbase.regionserver.HStore: closed 858230020/attribute
2008-08-08 10:50:13,822 DEBUG org.apache.hadoop.hbase.regionserver.HStore: closed 858230020/context
2008-08-08 10:50:13,822 DEBUG org.apache.hadoop.hbase.regionserver.HStore: closed 858230020/view
2008-08-08 10:50:13,822 INFO org.apache.hadoop.hbase.regionserver.HRegion: closed entities,North_America-United_States-Missouri-Oran-b9c7e44a-4a2b-11dd-abd4-1231390079f2,1218205066272
...
2008-08-08 10:50:23,437 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 64020, call next(-8330618580557781998) from 192.168.1.84:59678: error: org.apache.hadoop.hbase.UnknownScannerException: Name: -8330618580557781998
org.apache.hadoop.hbase.UnknownScannerException: Name: -8330618580557781998
  at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1115)
  at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
  at java.lang.reflect.Method.invoke(Method.java:597)
  at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:473)
  at org.apache.hadoop.ipc.Server$Handler.run(Server.java:896)
{code} 

We have to have some way of telling the client code that it's scanner lease was canceled because of a split so that it stays hidden from the user."
HBASE-778	Renaud Delbri up on the list and Jon Gray just now ran into issue where getting length of an HSF inside in the HStore constructor failed with FNFE.   Test of length is done after supposed tests that file exists so odd.
HBASE-508	"When I look in the log of XX.XX.XX.92, it shows only log rolling activity of following form:

{code}
...
2008-03-13 07:04:17,596 DEBUG org.apache.hadoop.hbase.HLog: Closing current log writer hdfs://coral-dfs.cluster.powerset.com:10000/hbase/XX.XX.XX-2.u.powerset.com/log_XX.XX.XX.92_1205384328364_60020/hlog.dat.022 to get a new one
2008-03-13 07:04:17,599 INFO org.apache.hadoop.hbase.HLog: new log writer created at hdfs://coral-dfs.cluster.powerset.com:10000/hbase/XX.XX.XX-2.u.powerset.com/log_XX.XX.XX.92_1205384328364_60020/hlog.dat.023
2008-03-13 07:08:48,425 INFO org.apache.hadoop.hbase.HRegionServer: Rolling hlog. Number of entries: 30004
2008-03-13 07:08:48,472 DEBUG org.apache.hadoop.hbase.HLog: Closing current log writer hdfs://coral-dfs.cluster.powerset.com:10000/hbase/XX.XX.XX-2.u.powerset.com/log_XX.XX.XX.92_1205384328364_60020/hlog.dat.023 to get a new one
2008-03-13 07:08:48,484 INFO org.apache.hadoop.hbase.HLog: new log writer created at hdfs://coral-dfs.cluster.powerset.com:10000/hbase/XX.XX.XX-2.u.powerset.com/log_XX.XX.XX.92_1205384328364_60020/hlog.dat.024
~        
...                                                                                                                                                                                                           
{code}

.. which is odd because if we're taking on this many edits, you'd think there'd be flushing and compacting going on.  There's none.
"
HBASE-96	"Using a patched version of 0.15.0, my regionserver freezes up, saying repeatedly:

2008-01-22 23:10:12,617 WARN org.apache.hadoop.ipc.Server: IPC Server handler 1 on 60020, call getProtocolVersion(org.
apache.hadoop.hbase.HRegionInterface, 1) from xxx.xxx.xxx.178:55116: discarded for being too old (46464)

Then I get:

2008-01-22 23:11:12,644 WARN org.apache.hadoop.ipc.Server: IPC Server handler 7 on 60020, call openScanner(-ROOT-,,0, [Lorg.apache.hadoop.io.Text;@19958bf9, .META., 1201043326058, null) from xxx.xxx.45.192:44993: output error
java.nio.channels.ClosedChannelException
        at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(Unknown Source)
        at sun.nio.ch.SocketChannelImpl.write(Unknown Source)
        at org.apache.hadoop.ipc.SocketChannelOutputStream.flushBuffer(SocketChannelOutputStream.java:108)
        at org.apache.hadoop.ipc.SocketChannelOutputStream.write(SocketChannelOutputStream.java:89)
        at java.io.BufferedOutputStream.flushBuffer(Unknown Source)
        at java.io.BufferedOutputStream.flush(Unknown Source)
        at java.io.DataOutputStream.flush(Unknown Source)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:615)
2008-01-22 23:11:12,660 INFO org.apache.hadoop.ipc.Server: Process Thread Dump: Discarding call getProtocolVersion(org.apache.hadoop.hbase.HRegionInterface, 1) from xxx.xxx.45.224:34506

29 active threads
Thread 4329 (IPC Client connection to xxx.xxx.44.135:10000):
  State: WAITING
  Blocked count: 1
  Waited count: 4
  Waiting on org.apache.hadoop.ipc.Client$Connection@21e1962d
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.Object.wait(Unknown Source)
    org.apache.hadoop.ipc.Client$Connection.waitForWork(Client.java:216)
    org.apache.hadoop.ipc.Client$Connection.run(Client.java:255)
Thread 37 (IPC Server handler 9 on 60020):
  State: BLOCKED
  Blocked count: 1
  Waited count: 202
  Blocked on org.apache.hadoop.hbase.HStoreFile@72e8a021
  Blocked by 29 (IPC Server handler 1 on 60020)
  Stack:
    org.apache.hadoop.hbase.HStoreFile.getReader(HStoreFile.java:936)
    org.apache.hadoop.hbase.HStore$StoreFileScanner.<init>(HStore.java:2057)
    org.apache.hadoop.hbase.HStore$HStoreScanner.<init>(HStore.java:2203)
    org.apache.hadoop.hbase.HStore.getScanner(HStore.java:2025)
    org.apache.hadoop.hbase.HRegion$HScanner.<init>(HRegion.java:1699)
    org.apache.hadoop.hbase.HRegion.getScanner(HRegion.java:1271)
    org.apache.hadoop.hbase.HRegionServer.openScanner(HRegionServer.java:1499)
    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    java.lang.reflect.Method.invoke(Unknown Source)
    org.apache.hadoop.ipc.RPC$Server.call(RPC.java:379)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:596)
Thread 36 (IPC Server handler 8 on 60020):
  State: BLOCKED
  Blocked count: 1
  Waited count: 201
  Blocked on org.apache.hadoop.hbase.HStoreFile@72e8a021
  Blocked by 29 (IPC Server handler 1 on 60020)
  Stack:
    org.apache.hadoop.hbase.HStoreFile.getReader(HStoreFile.java:936)
    org.apache.hadoop.hbase.HStore$StoreFileScanner.<init>(HStore.java:2057)
    org.apache.hadoop.hbase.HStore$HStoreScanner.<init>(HStore.java:2203)
    org.apache.hadoop.hbase.HStore.getScanner(HStore.java:2025)
    org.apache.hadoop.hbase.HRegion$HScanner.<init>(HRegion.java:1699)
    org.apache.hadoop.hbase.HRegion.getScanner(HRegion.java:1271)
    org.apache.hadoop.hbase.HRegionServer.openScanner(HRegionServer.java:1499)
    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    java.lang.reflect.Method.invoke(Unknown Source)
    org.apache.hadoop.ipc.RPC$Server.call(RPC.java:379)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:596)
Thread 35 (IPC Server handler 7 on 60020):
  State: RUNNABLE
  Blocked count: 4
  Waited count: 240
  Stack:
    sun.management.ThreadImpl.getThreadInfo0(Native Method)
    sun.management.ThreadImpl.getThreadInfo(Unknown Source)
    sun.management.ThreadImpl.getThreadInfo(Unknown Source)
    org.apache.hadoop.util.ReflectionUtils.printThreadInfo(ReflectionUtils.java:114)
    org.apache.hadoop.util.ReflectionUtils.logThreadInfo(ReflectionUtils.java:162)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:579)
Thread 34 (IPC Server handler 6 on 60020):
  State: BLOCKED
  Blocked count: 1
  Waited count: 203
  Blocked on org.apache.hadoop.hbase.HStoreFile@72e8a021
  Blocked by 29 (IPC Server handler 1 on 60020)
  Stack:
    org.apache.hadoop.hbase.HStoreFile.getReader(HStoreFile.java:936)
    org.apache.hadoop.hbase.HStore$StoreFileScanner.<init>(HStore.java:2057)
    org.apache.hadoop.hbase.HStore$HStoreScanner.<init>(HStore.java:2203)
    org.apache.hadoop.hbase.HStore.getScanner(HStore.java:2025)
    org.apache.hadoop.hbase.HRegion$HScanner.<init>(HRegion.java:1699)
    org.apache.hadoop.hbase.HRegion.getScanner(HRegion.java:1271)
    org.apache.hadoop.hbase.HRegionServer.openScanner(HRegionServer.java:1499)
    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    java.lang.reflect.Method.invoke(Unknown Source)
    org.apache.hadoop.ipc.RPC$Server.call(RPC.java:379)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:596)
Thread 33 (IPC Server handler 5 on 60020):
  State: BLOCKED
  Blocked count: 1
  Waited count: 212
  Blocked on org.apache.hadoop.hbase.HStoreFile@72e8a021
  Blocked by 29 (IPC Server handler 1 on 60020)
  Stack:
    org.apache.hadoop.hbase.HStoreFile.getReader(HStoreFile.java:936)
    org.apache.hadoop.hbase.HStore$StoreFileScanner.<init>(HStore.java:2057)
    org.apache.hadoop.hbase.HStore$HStoreScanner.<init>(HStore.java:2203)
    org.apache.hadoop.hbase.HStore.getScanner(HStore.java:2025)
    org.apache.hadoop.hbase.HRegion$HScanner.<init>(HRegion.java:1699)
    org.apache.hadoop.hbase.HRegion.getScanner(HRegion.java:1271)
    org.apache.hadoop.hbase.HRegionServer.openScanner(HRegionServer.java:1499)
    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    java.lang.reflect.Method.invoke(Unknown Source)
    org.apache.hadoop.ipc.RPC$Server.call(RPC.java:379)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:596)
Thread 32 (IPC Server handler 4 on 60020):
  State: BLOCKED
  Blocked count: 1
  Waited count: 202
  Blocked on org.apache.hadoop.hbase.HStoreFile@72e8a021
  Blocked by 29 (IPC Server handler 1 on 60020)
  Stack:
    org.apache.hadoop.hbase.HStoreFile.getReader(HStoreFile.java:936)
    org.apache.hadoop.hbase.HStore$StoreFileScanner.<init>(HStore.java:2057)
    org.apache.hadoop.hbase.HStore$HStoreScanner.<init>(HStore.java:2203)
    org.apache.hadoop.hbase.HStore.getScanner(HStore.java:2025)
    org.apache.hadoop.hbase.HRegion$HScanner.<init>(HRegion.java:1699)
    org.apache.hadoop.hbase.HRegion.getScanner(HRegion.java:1271)
    org.apache.hadoop.hbase.HRegionServer.openScanner(HRegionServer.java:1499)
    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    java.lang.reflect.Method.invoke(Unknown Source)
    org.apache.hadoop.ipc.RPC$Server.call(RPC.java:379)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:596)
Thread 31 (IPC Server handler 3 on 60020):
  State: BLOCKED
  Blocked count: 1
  Waited count: 210
  Blocked on org.apache.hadoop.hbase.HStoreFile@72e8a021
  Blocked by 29 (IPC Server handler 1 on 60020)
  Stack:
    org.apache.hadoop.hbase.HStoreFile.getReader(HStoreFile.java:936)
    org.apache.hadoop.hbase.HStore$StoreFileScanner.<init>(HStore.java:2057)
    org.apache.hadoop.hbase.HStore$HStoreScanner.<init>(HStore.java:2203)
    org.apache.hadoop.hbase.HStore.getScanner(HStore.java:2025)
    org.apache.hadoop.hbase.HRegion$HScanner.<init>(HRegion.java:1699)
    org.apache.hadoop.hbase.HRegion.getScanner(HRegion.java:1271)
    org.apache.hadoop.hbase.HRegionServer.openScanner(HRegionServer.java:1499)
    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    java.lang.reflect.Method.invoke(Unknown Source)
    org.apache.hadoop.ipc.RPC$Server.call(RPC.java:379)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:596)
Thread 30 (IPC Server handler 2 on 60020):
  State: BLOCKED
  Blocked count: 1
  Waited count: 216
  Blocked on org.apache.hadoop.hbase.HStoreFile@72e8a021
  Blocked by 29 (IPC Server handler 1 on 60020)
  Stack:
    org.apache.hadoop.hbase.HStoreFile.getReader(HStoreFile.java:936)
    org.apache.hadoop.hbase.HStore$StoreFileScanner.<init>(HStore.java:2057)
    org.apache.hadoop.hbase.HStore$HStoreScanner.<init>(HStore.java:2203)
    org.apache.hadoop.hbase.HStore.getScanner(HStore.java:2025)
    org.apache.hadoop.hbase.HRegion$HScanner.<init>(HRegion.java:1699)
    org.apache.hadoop.hbase.HRegion.getScanner(HRegion.java:1271)
    org.apache.hadoop.hbase.HRegionServer.openScanner(HRegionServer.java:1499)
    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    java.lang.reflect.Method.invoke(Unknown Source)
    org.apache.hadoop.ipc.RPC$Server.call(RPC.java:379)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:596)
Thread 29 (IPC Server handler 1 on 60020):
  State: RUNNABLE
  Blocked count: 3
  Waited count: 229
  Stack:
    java.net.SocketInputStream.socketRead0(Native Method)
    java.net.SocketInputStream.read(Unknown Source)
    java.io.BufferedInputStream.fill(Unknown Source)
    java.io.BufferedInputStream.read(Unknown Source)
    java.io.DataInputStream.readShort(Unknown Source)
    org.apache.hadoop.dfs.DFSClient$BlockReader.newBlockReader(DFSClient.java:773)
    org.apache.hadoop.dfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:1000)
    org.apache.hadoop.dfs.DFSClient$DFSInputStream.read(DFSClient.java:1096)
    java.io.DataInputStream.readFully(Unknown Source)
    java.io.DataInputStream.readFully(Unknown Source)
    org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:1383)
    org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1360)
    org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1349)
    org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1344)
    org.apache.hadoop.io.MapFile$Reader.<init>(MapFile.java:263)
    org.apache.hadoop.io.MapFile$Reader.<init>(MapFile.java:242)
    org.apache.hadoop.hbase.HStoreFile$BloomFilterMapFile$Reader.<init>(HStoreFile.java:816)
    org.apache.hadoop.hbase.HStoreFile.getReader(HStoreFile.java:936)
    org.apache.hadoop.hbase.HStore$StoreFileScanner.<init>(HStore.java:2057)
    org.apache.hadoop.hbase.HStore$HStoreScanner.<init>(HStore.java:2203)
Thread 28 (IPC Server handler 0 on 60020):
  State: BLOCKED
  Blocked count: 1
  Waited count: 200
  Blocked on org.apache.hadoop.hbase.HStoreFile@72e8a021
  Blocked by 29 (IPC Server handler 1 on 60020)
  Stack:
    org.apache.hadoop.hbase.HStoreFile.getReader(HStoreFile.java:936)
    org.apache.hadoop.hbase.HStore$StoreFileScanner.<init>(HStore.java:2057)
    org.apache.hadoop.hbase.HStore$HStoreScanner.<init>(HStore.java:2203)
    org.apache.hadoop.hbase.HStore.getScanner(HStore.java:2025)
    org.apache.hadoop.hbase.HRegion$HScanner.<init>(HRegion.java:1699)
    org.apache.hadoop.hbase.HRegion.getScanner(HRegion.java:1271)
    org.apache.hadoop.hbase.HRegionServer.openScanner(HRegionServer.java:1499)
    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    java.lang.reflect.Method.invoke(Unknown Source)
    org.apache.hadoop.ipc.RPC$Server.call(RPC.java:379)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:596)
Thread 14 (IPC Server listener on 60020):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.PollArrayWrapper.poll0(Native Method)
    sun.nio.ch.PollArrayWrapper.poll(Unknown Source)
    sun.nio.ch.PollSelectorImpl.doSelect(Unknown Source)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(Unknown Source)
    sun.nio.ch.SelectorImpl.select(Unknown Source)
    sun.nio.ch.SelectorImpl.select(Unknown Source)
    org.apache.hadoop.ipc.Server$Listener.run(Server.java:285)

... etc.

Note that most of the threads are blocked on thread 29, which is in turn blocked on a socket read.  This node was hosting the ROOT region, so when it got wedged it made hbase unusable (since the master won't reassign ROOT until the server actually goes down)."
HBASE-4	"We're running a high load of random reads through a single hbase rest server.  When I look at the 60010 master servlet, there are a very high number of hits to the server hosting .META.,,1.  That server has 4000 requests in the last ""3 seconds"", the server hosting ROOT (and two other regions which shouldn't be getting reads) has 80 requests, and all the other regionservers have less.  

This doesn't seem right: shouldn't the single hbase client be caching table lookups?

This appears to be a hurdle for scaling our random read rate.
"
HBASE-540	HBase trunk region server gets an out of memory exception in the Performance Evaluation (sequential write) test at map 37% reduce 12%
HBASE-21	I don't think it'll be found at this location.  Verify.
HBASE-179	"I am uploading logs for this

webdata has more then one region loaded that have the same starting row key but different end row keys

This happens after a split sometimes they both had null as the starting row key

PE1750-3 started with the table then once split the new splits went to PE1750-1
"
HBASE-160	"TestTable fails in Hudson build

    [junit] Running org.apache.hadoop.hbase.TestTable
    ....
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
"
HBASE-21506	"{code:java}
[WARNING] Expected all dependencies to require Scala version: 2.11.8
[WARNING] org.apache.phoenix:phoenix-spark:5.1.0-HBase-2.0-SNAPSHOT requires scala version: 2.11.8
[WARNING] com.twitter:chill_2.11:0.8.4 requires scala version: 2.11.8
[WARNING] org.apache.spark:spark-core_2.11:2.3.0 requires scala version: 2.11.8
[WARNING] org.json4s:json4s-jackson_2.11:3.2.11 requires scala version: 2.11.0
[WARNING] Multiple versions of scala libraries detected!{code}"
HBASE-21332	"When using scan with pagefilter to get data from hbase, the scanner will skip{color:#ff0000} 'non-edge'{color} regions.The code i use comes from the book _HBase: Definitive Guide, Example 4.8, PageFilter example._聽Difference is i use scan with startRow and stopRow.

Say i have regions with start and end聽keys like \{'111', '222', '333', '444'}, which means i have 3 regions \{111, 222}, \{222, 333}, \{333, 444} and they are in different region servers. When scan with startRow '111' and stopRow '444' , most data in region \{222, 333} will be skiped and won't be returned by ResultScanner.Region \{111,222} or \{333,444} works just fine and because region \{222,333} doesn't contain startRowkey or stopRowkey i call it non-edge region.

Below is some explanation聽with log:

聽
{code:java}
// Here scanner works just fine in region {111,222}, it gets exactly {pageSize} rows each time, which is 1000
...
2018-10-17 21:25:57.810 INFO 213872 [ main] c.p.s.c.HBaseTest : Test: results from [2139718600001069] to [2179067497952422], sum [1000 : 64000], cost: [77ms]
2018-10-17 21:25:57.885 INFO 213872 [ main] c.p.s.c.HBaseTest : Test: results from [2179098921079755] to [21c2879280113661], sum [1000 : 65000], cost: [75ms]
2018-10-17 21:25:57.962 INFO 213872 [ main] c.p.s.c.HBaseTest : Test: results from [21c2899018774688] to [2203180876471552], sum [1000 : 66000], cost: [77ms]

// Here scanner goes from region {111,222} to {222,333}. As you can see, the scanner gets 2405 rows with stopRow '3373621463365126'.The scanner moves to regin {333,444} too early and most data in {222,333} are skiped.
2018-10-17 21:25:58.321 INFO 213872 [ main] c.p.s.c.HBaseTest : Test: results from [2203223414254308] to [3373621463365126], sum [2405 : 68405], cost: [359ms]

// Now the scanner is in region {333,444}, everything works just fine
2018-10-17 21:25:58.396 INFO 213872 [ main] c.p.s.c.HBaseTest : Test: results from [3373764408525604] to [33b3849714659525], sum [1000 : 69405], cost: [74ms]
2018-10-17 21:25:58.467 INFO 213872 [ main] c.p.s.c.HBaseTest : Test: results from [33b3882378177107] to [33f5221377695765], sum [1000 : 70405], cost: [71ms]
...{code}
聽"
HBASE-20473	"{code}
  // Disable verbose INFO logging from org.apache.hadoop.io.compress.CodecPool
  static {
    System.setProperty(""org.apache.commons.logging.Log"",
      ""org.apache.commons.logging.impl.SimpleLog"");
{code}
The above code has no effect since we're migrating away from commons-logging."
HBASE-18827	"I'm trying to make a release. I'm in a tizzy as is usual around these times*. 1.5 hours seems totally over-the-top. I think [~misty]'s lovely automation has hidden this fact from us but needs digging on why we are taking so long. The cycle seems to be provoked by hbase-archetypes module.... but I got 'mvn log glaze disease' as soon as I tried digging in.

Filing issue in case someone else wants to have a go at this before I. Also filing if only to note that the thing does eventually finish in case I forget....


* I go to build the doc/site and the build never seems to end. First there is the issue HBASE-18821 where we were NPE'ing after a bunch of time had passed. My fix for HBASE-18821 then got me further but after what seemed hours, it failed with a cryptic message (Thanks [~Apache9] for figuring that I'd made an incorrect fix over in HBASE-18821).  Next up I'm looking at a cycle that never seems to end... only it eventually does after 90minutes."
HBASE-18650	"When I use HBase replication with Master-Master  model, install like  below:
1>  with cluster 1  I create table ""repliTest"",  and put some data on it .

2>  after about 6 month, I have cluster 2 ,and install master-master model with two cluster.

3> I put  cluster 1 some data and It replication  to cluster 2, it's correct.

4>  I put  cluster 2 some data and it replication to cluster 1, It's correct also.

5> The issue is : when I run     command 
 ""hbase org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication  clusterID  repliTest  ""    on each cluster.

the result  is different :
on cluster 1  the result is 
	org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication$Verifier$Counters
		BADROWS=33
		GOODROWS=20
		ONLY_IN_PEER_TABLE_ROWS=4
		ONLY_IN_SOURCE_TABLE_ROWS=29

on cluster 2 the result is:

	org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication$Verifier$Counters
		BADROWS=32
		GOODROWS=20
		ONLY_IN_PEER_TABLE_ROWS=28
		ONLY_IN_SOURCE_TABLE_ROWS=4

It means one record is lost on cluster 2  verifierreplication .  I check the table and get the recode  is one recode I have put it  6 month ago.  
I delete this record like this:

hbase(main):017:0> delete 'repliTest','3','score:english'

and after that,   when I run hbase verifyreplication  is also correct on every cluster. 



environment :

HBase 1.16  
hdfs  2.7.1 

ps: I am not good at english  ,sorry about that. "
HBASE-18613	"Noticed the following in some internal testing (line numbers likely are skewed)

{noformat}
2017-08-16 21:20:25,557| 2017-08-16 21:20:25,553 WARN  [main] client.ConnectionManager$HConnectionImplementation: Checking master connection
2017-08-16 21:20:25,557| com.google.protobuf.ServiceException: org.apache.hadoop.hbase.exceptions.ConnectionClosingException: Call to master1.domain.com/10.0.2.131:16000 failed on local exception: org.apache.hadoop.hbase.exceptions.ConnectionClosingException: Connection to master1.domain.com/10.0.2.131:16000 is closing. Call id=581, waitTime=1
2017-08-16 21:20:25,557| at org.apache.hadoop.hbase.ipc.AbstractRpcClient.callBlockingMethod(AbstractRpcClient.java:223)
2017-08-16 21:20:25,558| at org.apache.hadoop.hbase.ipc.AbstractRpcClient$BlockingRpcChannelImplementation.callBlockingMethod(AbstractRpcClient.java:287)
2017-08-16 21:20:25,560| at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$BlockingStub.isMasterRunning(MasterProtos.java:62739)
2017-08-16 21:20:25,560| at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation$MasterServiceState.isMasterRunning(ConnectionManager.java:1448)
2017-08-16 21:20:25,561| at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.isKeepAliveMasterConnectedAndRunning(ConnectionManag
er.java:2124)
2017-08-16 21:20:25,561| at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.getKeepAliveMasterService(ConnectionManager.java:1712)
2017-08-16 21:20:25,562| at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.getMaster(ConnectionManager.java:1701)
2017-08-16 21:20:25,562| at org.apache.hadoop.hbase.DistributedHBaseCluster.getMasterAdminService(DistributedHBaseCluster.java:153)
2017-08-16 21:20:25,563| at org.apache.hadoop.hbase.DistributedHBaseCluster.waitForActiveAndReadyMaster(DistributedHBaseCluster.java:184)
2017-08-16 21:20:25,563| at org.apache.hadoop.hbase.HBaseCluster.waitForActiveAndReadyMaster(HBaseCluster.java:204)
2017-08-16 21:20:25,563| at org.apache.hadoop.hbase.DistributedHBaseCluster.restoreMasters(DistributedHBaseCluster.java:278)
2017-08-16 21:20:25,563| at org.apache.hadoop.hbase.DistributedHBaseCluster.restoreClusterStatus(DistributedHBaseCluster.java:239)
2017-08-16 21:20:25,563| at org.apache.hadoop.hbase.HBaseCluster.restoreInitialStatus(HBaseCluster.java:235)
2017-08-16 21:20:25,564| at org.apache.hadoop.hbase.IntegrationTestingUtility.restoreCluster(IntegrationTestingUtility.java:99)
2017-08-16 21:20:25,564| at org.apache.hadoop.hbase.IntegrationTestBase.cleanUpCluster(IntegrationTestBase.java:200)
2017-08-16 21:20:25,564| at org.apache.hadoop.hbase.IntegrationTestDDLMasterFailover.cleanUpCluster(IntegrationTestDDLMasterFailover.java:146)
2017-08-16 21:20:25,564| at org.apache.hadoop.hbase.IntegrationTestBase.cleanUp(IntegrationTestBase.java:140)
2017-08-16 21:20:25,564| at org.apache.hadoop.hbase.IntegrationTestBase.doWork(IntegrationTestBase.java:125)
2017-08-16 21:20:25,565| at org.apache.hadoop.hbase.util.AbstractHBaseTool.run(AbstractHBaseTool.java:112)
2017-08-16 21:20:25,565| at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
2017-08-16 21:20:25,565| at org.apache.hadoop.hbase.IntegrationTestDDLMasterFailover.main(IntegrationTestDDLMasterFailover.java:832)
2017-08-16 21:20:25,566| Caused by: org.apache.hadoop.hbase.exceptions.ConnectionClosingException: Call to master1.domain.com/10.0.2.131:16000 failed on local exception: org.apache.hadoop.hbase.exceptions.ConnectionClosingException: Connection to master1.domain.com/10.0.2.131:16000 is closing. Call id=581, waitTime=1
2017-08-16 21:20:25,566| at org.apache.hadoop.hbase.ipc.RpcClientImpl.wrapException(RpcClientImpl.java:1258)
2017-08-16 21:20:25,566| at org.apache.hadoop.hbase.ipc.RpcClientImpl.call(RpcClientImpl.java:1229)
2017-08-16 21:20:25,566| at org.apache.hadoop.hbase.ipc.AbstractRpcClient.callBlockingMethod(AbstractRpcClient.java:213)
2017-08-16 21:20:25,566| ... 20 more
2017-08-16 21:20:25,566| Caused by: org.apache.hadoop.hbase.exceptions.ConnectionClosingException: Connection to master1.domain.com/10.0.2.131:16000 is closing. Call id=581, waitTime=1
2017-08-16 21:20:25,567| at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.cleanupCalls(RpcClientImpl.java:1047)
2017-08-16 21:20:25,567| at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.close(RpcClientImpl.java:846)
2017-08-16 21:20:25,567| at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.run(RpcClientImpl.java:574)
{noformat}

This is when the IntegrationTest harness is resetting the state of the distributed cluster. When dealing with ""slow"" nodes, the restart of the previously active master could be delayed which cause the test code to see a ConnectionClosingException (wrapped in a ServiceException).

I think we want to just consume this Exception, same as MasterNotRunningException and ZooKeeperConnectionException, in {{DistributedHBaseCluster#waitForActiveAndReadyMaster(long)}}."
HBASE-18363	"We run into cases that with read replica, after split, sometimes, the parent replica region is left in  master's in memory onlineRegion list. This results in the region got assigned to a region server. Though the root cause will be fixed by HBASE-18025. We need to enhance hbck tool to fix this in-memory state. Currently, hbck only allows the fix for primary region (in this case, the primary region is gone) with fixAssignment option, please see the following line of code. We will enhance it so it can be applied to replica region as well.

https://github.com/apache/hbase/blob/master/hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java#L2216"
HBASE-20291	"receiving message
{code:java}
The POM for net.minidev:json-smart:jar:2.3-SNAPSHOT is missing, no dependency information available{code}
when running with
{code:java}
mvn clean install -DHBasePatchProcess -Dhadoop-three.version=3.0.0 -Dhadoop.profile=3.0 -DskipTests{code}"
HBASE-19192	"{code}
  public int getRegionServerInfoPort(final ServerName sn) {
    RegionServerInfo info = this.regionServerTracker.getRegionServerInfo(sn);
    if (info == null || info.getInfoPort() == 0) {
      return conf.getInt(HConstants.REGIONSERVER_INFO_PORT,
        HConstants.DEFAULT_REGIONSERVER_INFOPORT);
    }
    return info.getInfoPort();
{code}
hbase.regionserver.info.port config is only checked when regionServerTracker doesn't have info port.
When hbase.regionserver.info.port is set to -1 by user, we should respect the config value and disable UI."
HBASE-12630	"Chatting with Manukranth, online config update feature should handle the case where certain ConfigurationObserver(s) constantly produce exception when notified of config update.

We can handle such ConfigurationObserver by sidelining it after configurable number of exceptions seen from the ConfigurationObserver.
"
HBASE-12656	"TestVisibilityLabelsWithDeletes.testVisibilityLabelsWithDeleteFamilyWithMultipleVersionsNoTimestamp is flaky.  I fear if Visibility delete is broken some where. I have some idea on the reason but will provide a patch for it tomorrow after complete analysis. If the bug is only a test case bug will lower the priority of the bug - but I don't think so.
testVisibilityLabelsWithDeleteFamilyWithMultipleVersionsNoTimestamp does not fail in any of the jenkins build but fails in local randomly."
HBASE-12203	"With patch HBASE-7767 we moved table statuses from ZK to HDFS. That was a good cleanup, but we put additional (substantial) load on master. Some client requests use checks for table state (for example HBASE-12035). 
Thats is why patch was not back ported to branch1 (HBASE-11978)

Lets replicate state back to zk, but as a mirror of table states.

What can be done:
1. TableStateManager would push table state changes to zk
2. Return back ZKTableStateClientSideReader.

Alternative way:
1. Move table statuses to separate table like namespaces
2. Issue statuses requests against this table

Alternative way2:
1. Extend RS api with getTableState() call
2. Each RS will be able to cache table states
3. Clients will call RS instead of master or zk"
HBASE-5224	"HFile's midkey() is implemented as the first key of the middle index block both HFile v1 and HFile v2 (the middle leaf index block is used in v2). However, in HFile v2 midkey() currently grabs 12 more bytes from the next leaf index entry, representing the offset and compressed size of the data block pointed to by that entry. While this probably does not affect the interpretation of the returned buffer as an HBase key (the last 12 bytes are simply discarded), this has to be cleaned up. "
HBASE-3737	"I just realized that htable.delete(List<Delete>) doesn't use the writebuffer and processes the list immediately, but htable.put(List<Put>) does use the writebuffer (i.e., send when filled). Likewise, htable.delete(Delete) sends immediately.
 
Out of sheer curiosity, why?  With the 'batch' methods now in place, it seems like it would be consistent for 'delete' and 'put' to use the writebuffer (assuming it is expanded to hold more than Puts), whereas 'batch' methods process immediately.

This isn't a huge issue, but it does seem a little inconsistent. "
HBASE-11249	"At line 4883:
{code}
            Store store = getStore(kv);
            if (store == null) {
              checkFamily(CellUtil.cloneFamily(kv));
              // unreachable
            }
{code}
Exception would be thrown from checkFamily() if store is null.
In the finally block:
{code}
      } finally {
        if (!mutations.isEmpty() && !walSyncSuccessful) {
          LOG.warn(""Wal sync failed. Roll back "" + mutations.size() +
              "" memstore keyvalues for row(s):"" + StringUtils.byteToHexString(
              processor.getRowsToLock().iterator().next()) + ""..."");
          for (KeyValue kv : mutations) {
            getStore(kv).rollback(kv);
          }
{code}
There is no corresponding null check for return value of getStore() above, potentially leading to partially rolled back state in memstore."
HBASE-12211	"In the hbase-daemon.sh file line 199, it has the content:
{quote}echo ""`ulimit -a`"" >> $loglog 2>&1{quote}
The variable loglog is defined as:
{quote}
logout=$HBASE_LOG_DIR/$HBASE_LOG_PREFIX.out
loggc=$HBASE_LOG_DIR/$HBASE_LOG_PREFIX.gc
loglog=""${HBASE_LOG_DIR}/${HBASE_LOGFILE}""
{quote}
For my understanding, this information should be printed to the ""logout"" variable; we should not mix this ""ulimit"" information with the actual log printed by hbase java program."
HBASE-15737	"HBASE-14963 removed reference to Guava Stopwatch from hbase-client module.

However, there're still 3 classes referring to Guava Stopwatch :

hbase-client/src/test/java/org/apache/hadoop/hbase/client/TestClientNoCluster.java:import com.google.common.base.Stopwatch;
hbase-server/src/main/java/org/apache/hadoop/hbase/util/JvmPauseMonitor.java:import com.google.common.base.Stopwatch;
hbase-server/src/test/java/org/apache/hadoop/hbase/ScanPerformanceEvaluation.java:import com.google.common.base.Stopwatch;

We should remove reference to Guava Stopwatch.

hadoop is no longer referencing Guava Stopwatch."
HBASE-17393	"In the code,

https://github.com/apache/hbase/blob/master/hbase-common/src/main/java/org/apache/hadoop/hbase/io/TimeRange.java#L181
 
return getMin() < tr.getMax() && getMax() >= tr.getMin();

Given that TimeRange is defined as [minStamp,maxStamp), 

Assume that TimeRange(4, 5). includesTimeRange(TimeRange(5,6)).
So it will be (4 < 6) && (5 >=5), the result is true.

Same for TimeRangeTracker#includesTimeRange"
HBASE-17034	"{code:title=HTable.java|borderStyle=solid}
private Result get(Get get, final boolean checkExistenceOnly) throws IOException {
    if (get.isCheckExistenceOnly() != checkExistenceOnly || get.getConsistency() == null) {
      get = ReflectionUtils.newInstance(get.getClass(), get);
      get.setCheckExistenceOnly(checkExistenceOnly);
      if (get.getConsistency() == null){
        get.setConsistency(defaultConsistency);
      }
    }
  ...
}
{code}
Can the passed Get be modified? If so, we can just change the passed Get. If not, we can record the values returned by isCheckExistenceOnly() and getConsistency() for avoiding the Get copy.

It seems to me that it is ok to modify the passed Get.

Any comment? Thanks."
HBASE-16575	"It seems to me that RRCI#callWithRetries and RRCI#callWithoutRetries should have the same logic if the maxAttempts is configured to one. But there are some difference are shown below:
1) timeout
2) failure handle
The quick solution is that we always call the RRCI#callWithRetries in the RRCI#callWithoutRetries when the maxAttempts is configured to one.
Any comment? Thanks."
HBASE-17670	When VictimHandler is used we use the bucket cache to cache those blocks that are evicted. So in case of where we close the hfile and call evictBlocksByHfileName - I think it still makes sense to call evict on the vicitmHandler also. Else the victimHandler is going to just occupy the space till the eviction thread in that vicitm handler clears it. 
HBASE-17431	"Here is related code:
{code}
    public R get() {
      if (super.size() < maxSize) {
        return null;
      }
      nextResource %= super.size();
{code}
Since super.size() is involved in modulo operation after the check, it seems the check should compare against 0 instead of maxSize.

Looks like a copy-paste error from put() method."
HBASE-17394	"It is supposed to be [minimumTimestamp, maximumTimestamp), the following logic suggests  [minimumTimestamp, maximumTimestamp]

https://github.com/apache/hbase/blob/master/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/TimeRangeTracker.java#L81

needs to be modified to set(l, l + 1)"
HBASE-6709	"Current naming convention about QOS is confusing. For first, there is no high or low priority, rather it works based on whether the operation is a root/meta lookup or a pure client side one. 
With addition of ReplicationHandlers, we introduced a new level of QOS. 
I think we should standarized QOS levels that reflects how they actually works.
"
HBASE-16717	"Every time to generateChecksums or validateChecksum will create a new DataChecksum, and DataChecksum is not thread safe, so use ThreadLocal to reuse it and keep thread safe."
HBASE-15829	"The comment of hbase.client.retries.number is:
{code}
  /**  
   * Parameter name for maximum retries, used as maximum for all retryable
   * operations such as fetching of the root region from root region server,
   * getting a cell's value, starting a row update, etc.
   */
  public static final String HBASE_CLIENT_RETRIES_NUMBER = ""hbase.client.retries.number"";
{code}

In branch-1, the max attempts number equals with hbase.client.retries.number. But in master, the max attempts number equals with hbase.client.retries.number + 1.

For RpcRetryingCaller.
{code}
this.retries = retries; // branch-1
{code}
{code}
this.maxAttempts = retries + 1; // master
{code}

For AsyncProcess:
{code}
    this.numTries = conf.getInt(HConstants.HBASE_CLIENT_RETRIES_NUMBER,
        HConstants.DEFAULT_HBASE_CLIENT_RETRIES_NUMBER); // branch-1
{code}
{code}
    // how many times we could try in total, one more than retry number
    this.numTries = conf.getInt(HConstants.HBASE_CLIENT_RETRIES_NUMBER,
        HConstants.DEFAULT_HBASE_CLIENT_RETRIES_NUMBER) + 1; // master
{code}"
HBASE-16590	TestRestoreSnapshotFromClientWithRegionReplicas uses Consistency.STRONG when doing a scan for counting rows. This is not right as it will not send out scan requests to replicas. It caused issue when read replica logic is corrected in HBASE-16345.
HBASE-16564	"0.98 compiled with hadoop 2.2.0,   so it has some compatibility issues with hadoop 2.7.2 (it seems 2.5.0+ has the same issue),  some counter has been removed.  

IMO we should catch the exception so our ITBLL could go on.

{code}
16/09/06 15:39:33 INFO hbase.HBaseCluster: Added new HBaseAdmin
16/09/06 15:39:33 INFO hbase.HBaseCluster: Restoring cluster - done
16/09/06 15:39:33 INFO hbase.HBaseCommonTestingUtility: Stopping mini mapreduce cluster...
16/09/06 15:39:33 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
16/09/06 15:39:33 INFO hbase.HBaseCommonTestingUtility: Mini mapreduce cluster stopped
16/09/06 15:39:33 ERROR util.AbstractHBaseTool: Error running command-line tool
java.lang.IllegalArgumentException: No enum constant org.apache.hadoop.mapreduce.JobCounter.MB_MILLIS_MAPS
       	at java.lang.Enum.valueOf(Enum.java:238)
       	at org.apache.hadoop.mapreduce.counters.FrameworkCounterGroup.valueOf(FrameworkCounterGroup.java:148)
       	at org.apache.hadoop.mapreduce.counters.FrameworkCounterGroup.findCounter(FrameworkCounterGroup.java:182)
       	at org.apache.hadoop.mapreduce.counters.AbstractCounters.findCounter(AbstractCounters.java:154)
       	at org.apache.hadoop.mapreduce.TypeConverter.fromYarn(TypeConverter.java:240)
       	at org.apache.hadoop.mapred.ClientServiceDelegate.getJobCounters(ClientServiceDelegate.java:370)
       	at org.apache.hadoop.mapred.YARNRunner.getJobCounters(YARNRunner.java:511)
       	at org.apache.hadoop.mapreduce.Job$7.run(Job.java:756)
       	at org.apache.hadoop.mapreduce.Job$7.run(Job.java:753)
       	at java.security.AccessController.doPrivileged(Native Method)
       	at javax.security.auth.Subject.doAs(Subject.java:422)
       	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)
       	at org.apache.hadoop.mapreduce.Job.getCounters(Job.java:753)
       	at org.apache.hadoop.mapreduce.Job.monitorAndPrintJob(Job.java:1361)
       	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1289)
       	at org.apache.hadoop.hbase.test.IntegrationTestBigLinkedList$Generator.jobCompletion(IntegrationTestBigLinkedList.java:543)
       	at org.apache.hadoop.hbase.test.IntegrationTestBigLinkedList$Generator.runRandomInputGenerator(IntegrationTestBigLinkedList.java:505)
       	at org.apache.hadoop.hbase.test.IntegrationTestBigLinkedList$Generator.run(IntegrationTestBigLinkedList.java:553)
       	at org.apache.hadoop.hbase.test.IntegrationTestBigLinkedList$Loop.runGenerator(IntegrationTestBigLinkedList.java:842)
       	at org.apache.hadoop.hbase.test.IntegrationTestBigLinkedList$Loop.run(IntegrationTestBigLinkedList.java:892)
       	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
       	at org.apache.hadoop.hbase.test.IntegrationTestBigLinkedList.runTestFromCommandLine(IntegrationTestBigLinkedList.java:1237)
       	at org.apache.hadoop.hbase.IntegrationTestBase.doWork(IntegrationTestBase.java:115)
       	at org.apache.hadoop.hbase.util.AbstractHBaseTool.run(AbstractHBaseTool.java:112)
       	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
       	at org.apache.hadoop.hbase.test.IntegrationTestBigLinkedList.main(IntegrationTestBigLinkedList.java:1272)
{code}"
HBASE-16504	"The Replication procedure in ReplicationSink.replicateEntries() method is not preserving the timestamp of the cell. 

Pointer to the code:
m = CellUtil.isDelete(cell) ? new Delete(cell.getRowArray(), cell.getRowOffset(),cell.getRowLength()) : new Put(cell.getRowArray(), cell.getRowOffset(),cell.getRowLength());

The Put and Delete constructors called here assign the timestamp with HConstants.LATEST_TIMESTAMP. Instead we need to keep the timestamp of the cell here.

Also there doesn't seem to be a test which checks if replication is preserving the timestamp. 


"
HBASE-16383	"I was recently trying to set up an MR job initialized with TableMapReduceUtil to be ran by Oozie. Oozie offers a way to configure the job by providing an implementation of org.apache.oozie.action.hadoop.OozieActionConfigurator. Unfortunately, this interface is using org.apache.hadoop.mapred.JobConf, not Job. I may be wrong but I believe that everything TableMapReduceUtil does actually maps to the job configuration so it does not really need the Job itself. If this is true, probably it would be more appropriate to use JobConf in TableMapReduceUtil instead of Job? Or provide the methods for both."
HBASE-15660	"In debugging an ITBLL job which has numerous failures, I saw that instead of the Verify job completing and reporting that there were a large number of UNDEF nodes, the reducers in the Verify were failing due to lack of progress.

The reducer's syslog file was filled with information from the {{dumpExtraInfoOnRefs()}} method. I believe that when a reducer is repeatedly doing these lookups, the MR framework doesn't realize that any progress is being made (nothing is being written to the context) and eventually kills the reducer task. This ultimately causes the entire Verify job to fail because the reducer fails in the same manner each time.

We should make sure to invoke {{context.progress()}} when we do these lookups to let the framework know that we're still doing ""our thing""."
HBASE-15261	"In the region split process, daughter regions are opened in different threads, Throwable t is set in these threads and it is checked in the calling thread. Need to make it volatile so the checking will not miss any exceptions from opening daughter regions."
HBASE-3521	"We have test a cluster which have more than 30,000 regions, max size of a region is 512MB. At this situation, data no more growing, but remove some old data and insert new, and regions will be more and more. And some regions may be very small or empty. This occupies too much heapsize, and will be more if regions cannot be merged. This will limit hbase running for a long time. 
A script that does a survey to remove empty regions, or pick out adjacent small regions that then does the online merge up seems like it would be useful. "
HBASE-15450	"I have difficulties to start HBase using 'install_local_hadoop' each time I have a new workspace. 
Most of the time, I simply remove the sscc coprocessor from hbase-site.xml and hbase can start well.
Since SSCC has not been maintained for a long time, and need more effort to test it again. So I think we should disable it totally and remove it from hbase-site.xml"
HBASE-15313	"Getting this error while compiling hbase-0.98.8 using this command *mvn package -DskipTests""* with JAVA8.

{quote} PoolMap.java error: name clash: remove(K,V) in PoolMap and remove(Object,Object) in Map have the same erasure, yet neither overrides the other {quote}

However its working when tried Java7. Sorry If I sound less informative about error. I am a newbie and not very familiar with the issue posting practice :) "
HBASE-10877	"Example where retries do not make sense:
{noformat}
2014-03-31 20:54:27,765 WARN [InputInitializer [Map 1] #0] org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation: Encountered problems when prefetch hbase:meta table: 
org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=35, exceptions:
Mon Mar 31 20:45:17 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: class com.google.protobuf.HBaseZeroCopyByteString cannot access its superclass com.google.protobuf.LiteralByteString
Mon Mar 31 20:45:17 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:45:17 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:45:18 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:45:20 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:45:24 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:45:34 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:45:45 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:45:55 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:46:05 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:46:25 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:46:45 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:47:05 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:47:25 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:47:45 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:48:05 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:48:25 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:48:46 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:49:06 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:49:26 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:49:46 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:50:06 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:50:26 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:50:46 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:51:06 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:51:26 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:51:46 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:52:06 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:52:27 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:52:47 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:53:07 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:53:27 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:53:47 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:54:07 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString
Mon Mar 31 20:54:27 UTC 2014, org.apache.hadoop.hbase.client.RpcRetryingCaller@343d511e, java.lang.IllegalAccessError: com/google/protobuf/HBaseZeroCopyByteString

	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:136)
	at org.apache.hadoop.hbase.client.HTable.getRowOrBefore(HTable.java:751)
	at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:147)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.prefetchRegionCache(ConnectionManager.java:1167)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegionInMeta(ConnectionManager.java:1232)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1119)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1102)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1061)
	at org.apache.hadoop.hbase.client.HTable.finishSetup(HTable.java:347)
{noformat}"
HBASE-15089	"Previously in 0.98 HTable#flushCommits throws InterruptedIOException and RetriesExhaustedWithDetailsException, but now in 1.1.2 this method signature has been changed to throw IOException, which will force application code changes for exception handling (previous catch on InterruptedIOException and RetriesExhaustedWithDetailsException become invalid). HTable#put has the same problem.

After a check, the compatibility issue was introduced by HBASE-12728. Will recover the compatibility In this JIRA."
HBASE-2972	"Red Hat's version of Eclipse barfs all over the place on HBase code. The problem is technically inappropriate use of the @Override decorator. See http://download.oracle.com/javase/1.5.0/docs/api/java/lang/Override.html. Should only be used when a method declaration is intended to override a method declaration in a superclass, NOT an interface. 

Attached janitorial patches removes all inappropriate @Override decorators on interface methods on trunk and 0.20 branch."
HBASE-2243	"The Regions on FS metric is almost always bigger than the real number of regions, it means we somehow not always clean region directories and they pile up. I guess it shouldn't be too hard spending some time loading a system until it gets wrong and then see why."
HBASE-14444	"At home page of Master/RegionServer/Thrift UI, ""Home"" link is redirecting to jsp file. Home link is working fine from other page.
We need to keep it same as ""HBase Logo"" link.

In MasterStatusTmpl.jamon, ""/master-status"" should be configured instead of ""/""
{code}
<li class=""active""><a href=""/"">Home</a></li>
{code}


In RSStatusTmpl.jamon, ""/rs-status"" should be configured instead of ""/""
{code}
<li class=""active""><a href=""/"">Home</a></li>
{code}


In thrift.jsp, ""/thrift.jsp"" should be configured instead of ""/""
{code}
<li class=""active""><a href=""/"">Home</a></li>
{code}"
HBASE-14412	"Created replication peer POB3-HBASE-LL-A750A. By mistake used hyphens in the replication peer name.
- Did {{remove_peer POB3-HBASE-LL-A}}
- Created a new peer POB3_HBASE_LL_A
- Verified zk for entries
{noformat}
ls /hbase/replication/peers 
[POB3_HBASE_LL_A"
HBASE-14399	"In method run() of class HRegionServer( hbase-server\src\main\java\org\apache\hadoop\hbase\regionserver\HRegionServer.java).

The first dummy catch block catch (KeeperException.NoNodeException nn)
performs no actions to handle its expected exception, which makes itself useless. 

To fix this bug, developers should add more code into the catch block to handle this exception.

public void run() {
...
try {
 deleteMyEphemeralNode();
}catch (KeeperException.NoNodeException nn){
}catch (KeeperException e) {
 LOG.warn(""Failed deleting my ephemeral node"",e);
}
...
}"
HBASE-5249	"After acquiring an explicit row lock, if one attempts to send {{Put}} after the row lock has expired, an NPE is triggered in the RegionServer, instead of throwing an {{UnknownRowLockException}} back to the client.
{code}
2012-01-20 17:09:54,074 ERROR
org.apache.hadoop.hbase.regionserver.HRegionServer: Error obtaining
row lock (fsOk: true)
java.lang.NullPointerException
       at java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:881)
       at org.apache.hadoop.hbase.regionserver.HRegionServer.addRowLock(HRegionServer.java:2313)
       at org.apache.hadoop.hbase.regionserver.HRegionServer.lockRow(HRegionServer.java:2299)
       at sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
       at java.lang.reflect.Method.invoke(Method.java:597)
       at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364)
       at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1327)

It happened only once out of thousands of RPCs that grabbed and
released a row lock.
{code}"
HBASE-6452	HConnectionManager has processSingleMultiPut and processBatchOfMultiPut which are both very similar. Integrating both.
HBASE-14179	"In method ""markRegionsRecovering()"" of class: hbase-1.1.1\hbase-server\src\main\java\org\apache\hadoop\hbase\coordination\ZKSplitLogManagerCoordination.java
""InterruptedException"" is catched twice.

  public void markRegionsRecovering(final ServerName serverName, Set<HRegionInfo> userRegions)
      throws IOException, InterruptedIOException {
...
   try {
            Thread.sleep(20);
          } catch (InterruptedException e1) {
            throw new InterruptedIOException();
          }
        } catch (InterruptedException e) {
          throw new InterruptedIOException();
        }
...
}"
HBASE-14088	We never close connection in LoadTestTool#applyColumnFamilyOptions
HBASE-13785	"We already have metrics for average response time, will be nice to have metrics for average response size. Time might vary a lot depending on the size."
HBASE-13404	"Bytes has these 4 functions for a lot of data types, might make sense to add missing ones/fix existing ones related to vint and vlong.
1) read X from byte[]
2) read X from offset in byte[]
3) put X in byte[] at a given offset
4) return byte[] for X

Also add tests for these new/changed functions."
HBASE-13521	"Currently, the readRequestsCount records the request count for both get request(random read) and scan#next request(sequential read). However, the cost of get request and scan#next request are different, and usually the get request is much more heavy than the scan#next. Is it reasonable to create a metric getRequestsCount to record the get request count specifically? Then, we can trigger an alert if getRequestsCount grows too fast because large number of random read requests will cause cluster overload more easily(The readRequestsCount will easily grow fast if there is a scan, however, this may not cause the system overload because sequential read is much more fast). Discussions and suggestions are welcomed! Thanks."
HBASE-5110	"The HLog class (method findMemstoresWithEditsEqualOrOlderThan) has unnecessary if check in a loop.

 static byte [][] findMemstoresWithEditsEqualOrOlderThan(final long oldestWALseqid,
      final Map<byte [], Long> regionsToSeqids) {
    //  This method is static so it can be unit tested the easier.
    List<byte []> regions = null;
    for (Map.Entry<byte [], Long> e: regionsToSeqids.entrySet()) {
      if (e.getValue().longValue() <= oldestWALseqid) {
        if (regions == null) regions = new ArrayList<byte []>();
        regions.add(e.getKey());
      }
    }
    return regions == null?
      null: regions.toArray(new byte [][] {HConstants.EMPTY_BYTE_ARRAY});
  }

The following change is suggested

  static byte [][] findMemstoresWithEditsEqualOrOlderThan(final long oldestWALseqid,
      final Map<byte [], Long> regionsToSeqids) {
    //  This method is static so it can be unit tested the easier.
    List<byte []> regions = new ArrayList<byte []>();
    for (Map.Entry<byte [], Long> e: regionsToSeqids.entrySet()) {
      if (e.getValue().longValue() <= oldestWALseqid) {
        regions.add(e.getKey());
      }
    }
    return regions.size() == 0?
      null: regions.toArray(new byte [][] {HConstants.EMPTY_BYTE_ARRAY});
  }"
HBASE-6072	"We have a MR job that is very memory bound.  It reads a potentially large row from hbase, then deserializes it into an (even larger) object representation, then does a fair amount of computation requiring memory.  After converting the Result into our object representation we want to free the memory holding the Result to be available for the actual computation of output values.

Currently we have our own custom modified copy of TableRecordReaderImpl to be able to set the Result value to null after reading it, but it's almost entirely a duplicate of hbase's TableRecordReaderImpl so we have to manually keep it up to date with changes to the hbase version.  If the value field of TableRecordReaderImpl were protected instead of private we could use a very simple subclass instead.

Are there any philosophical guidelines about what parts of HBase should or should not be easily extensible?"
HBASE-5899	"In 0.92, HBase local cluster won't start because of trying to connect to local HDFS. This error does not happen in 0.90.
We should not need to connect to HDFS to run a local cluster.

Here is my hbase-site.xml
{code:xml}
<configuration>
  <property>
    <name>hbase.rootdir</name>
    <value>file:///usr/local/hbase/var/hbase</value>
  </property>
</configuration>
{code}

This is the error:
{noformat}
2012-04-30 11:32:22,225 ERROR org.apache.hadoop.hbase.master.HMasterCommandLine: Failed to start master
java.net.ConnectException: Call to localhost/127.0.0.1:8020 failed on connection exception: java.net.ConnectException: Connection refused
    at org.apache.hadoop.ipc.Client.wrapException(Client.java:1095)
    at org.apache.hadoop.ipc.Client.call(Client.java:1071)
    at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)
    at $Proxy11.getProtocolVersion(Unknown Source)
    at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:396)
    at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:379)
    at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:119)
    at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:238)
    at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:203)
    at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:89)
    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1386)
    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1404)
    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)
    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:123)
    at org.apache.hadoop.hbase.util.JVMClusterUtil.startup(JVMClusterUtil.java:185)
    at org.apache.hadoop.hbase.LocalHBaseCluster.startup(LocalHBaseCluster.java:418)
    at org.apache.hadoop.hbase.master.HMasterCommandLine.startMaster(HMasterCommandLine.java:141)
    at org.apache.hadoop.hbase.master.HMasterCommandLine.run(HMasterCommandLine.java:103)
    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
    at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:76)
    at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:1637)
Caused by: java.net.ConnectException: Connection refused
    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
    at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
    at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:489)
    at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:434)
    at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:560)
    at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:184)
    at org.apache.hadoop.ipc.Client.getConnection(Client.java:1202)
    at org.apache.hadoop.ipc.Client.call(Client.java:1046)
    ... 20 more
{noformat}"
HBASE-5896	hbase.ipc.warn.response.time and hbase.ipc.warn.response.size need documentation
HBASE-5859	"There is a graceful_stop script. This algorithm:

{noformat}
for i = 0 to servers.size {
 regionsInServer = servers[i].regions
 move servers[i].regions to random
 stop servers[i]
 start servers[i]
 move regionsInServer to servers[i] //filled back with the same regions
}
{noformat}

It would be possible to optimize it while keeping data locality with

{noformat}
for i = 0 to servers.size {
 start servers[i*2+1] on the computer of servers[i] // Two RS on the same box
 move servers[i].regions to servers[i*2+1]  // The one on the same box
 stop servers[i]
}
{noformat}

There would be an impact with a fixed port configuration. To fix this, we could:
- use a range of port instead of a single port. This could be an issue for the web port.
- start on a port then reuse the fixed ones when they become available. This is not very elegant if a client code is already using the previous code. Moreover the region server code is written in the meta table.
- do a mix of the two solutions: a range for the server itself, while waiting for the web port to be available.


To be discussed...
"
HBASE-5858	"Since HBase tries to find locations of the Hadoop jars from the environment in certain cases it ends up with extra SLF4j jars coming from Hadoop classpath. When that happens ""SLF4j:CLASSPATH contains multiple SLF4j binding"" warning gets displayed.

The good news here is that the warning is just that -- a warning. The bad news is that it seems this issue will be tricky to fix properly. On one hand we would like Hadoop itself to give us transitive closure of the set of jar files required (computing that in the HBase script is a maintenance nightmare). On the other hand that set of jar files could contain some of the very same jars shipped with HBase.

This problem existed for quiet some time. SLF4j is just the first component to complain."
HBASE-5710	"In the MiniCluster test environment, an NPE occurs while scanning regions
of a pre-split table with multiple column families. Without this working
in the test environment, you cannot write unit tests for these types of
scenarios.

Add the following to TestMetaScanner to repro:

   @Test
   public void testMultiFamilyMultiRegionMetaScanner() throws Exception {
     LOG.info(""Starting testMetaScanner"");
     final byte[] TABLENAME = Bytes.toBytes(""testMetaScanner"");
     final byte[] FAMILY1 = Bytes.toBytes(""family1"");
     final byte[] FAMILY2 = Bytes.toBytes(""family2"");
     TEST_UTIL.createTable(TABLENAME, new byte[][] {FAMILY1,FAMILY2});
     Configuration conf = TEST_UTIL.getConfiguration();
     HTable table = new HTable(conf, TABLENAME);
     TEST_UTIL.createMultiRegions(conf, table, FAMILY1,
         new byte[][]{
           HConstants.EMPTY_START_ROW,
           Bytes.toBytes(""region_a""),
           Bytes.toBytes(""region_b"")});
     TEST_UTIL.createMultiRegions(conf, table, FAMILY2,
             new byte[][]{
               HConstants.EMPTY_START_ROW,
               Bytes.toBytes(""region_a""),
               Bytes.toBytes(""region_b"")});
     // Make sure all the regions are deployed
     TEST_UTIL.countRows(table);

     // This fails with an NPE currently
     MetaScanner.allTableRegions(conf, TABLENAME, false).keySet();
     table.close();
   }


"
HBASE-4151	"I have generated HFiles using importtsv and tried to bulk load them using completebulkload, even though i have specified the ZK quorum ensemble and client port in hbase-site.xml, completebulkload looks for ZK ensemble and client port in zoo.cfg, even after i have specified parameters in zoo.cfg, i was getting NullPointerException at line 167 in ZKConfig.java

{code}
 if (conf.get(HConstants.CLUSTER_DISTRIBUTED).equals(HConstants.CLUSTER_IS_DISTRIBUTED)
            && value.startsWith(""localhost"")) {
{code}"
HBASE-5043	"Easily reproducible:
  (1) Start habse shell
  (2) Press Ctrl-z, to suspend process
  (3) Run ""fg"", to resume

Now when you type nothing shows up.

An irb shell behaves much more gracefully on Suspend/Resume."
HBASE-5025	"There are two issues with these threads:
 - there should have a better name.  pool-x-thread-y is the default name given by java.util.concurrent.Executors
 - these threads should not survive to a minicluster shutdown

They are created by org.apache.hadoop.ipc.Server$Listener (with version 0.20.205.0; the code was different before), so the first issue is a hadoop common one. It's unclear for the second one, it could be hadoop-common as well. 

Constructor for org.apache.hadoop.ipc.Server$Listener:
{noformat}
    public Listener() throws IOException {
      //...
      readPool = Executors.newFixedThreadPool(readThreads);  // Lack a ThreadFactory to set the names
      //...
    }
{noformat}


Server#stop shutdowns the thread pool.

"
HBASE-5023	"If the minicluster is started/stopped multiple time, there is a new LeaseChecker each time. This thread is not created by HBase but by hdfs.

This is likely to be HDFS-1840, solved in 0.23"
HBASE-4865	"The javadoc states is asynchronous, but we can see in the implementation on HMaster that the implementation does not use executorService but calls directly process(). This is not true for all methods: enableTable, modifyTable, disableTable are truly asynchronous.

The other impact is that the listeners are not called, as this is done by the executorService.


I don't known if we have to change the documentation or the implementation. For consistency; I would change the implementation, but it may breaks existing code.


Two other comments:
1) There is no real naming pattern here, while it would be useful:
HBaseAdmin#createTable is synchrounous and calls the asynchronous HMaster#createTable 
HBaseAdmin#createTableAsync is asynchrounous and calls the asynchronous HMaster#createTable 
HBaseAdmin#modifyTable is asynchrounous and calls the asynchronous HMaster#modifyTable 
HBaseAdmin#modifyColumn is documented as asynchrounous and calls the synchronous HMaster#modifyColumn

2) the coprocessor ""post"" semantic is not consistent across the services.
- when the service is synchronous, post is called after the services execution (ex: addColumn with the current implementation).
- when the service is asynchronous, post is called after the executorService has registered the service to execute, but the service itself is not executed yet.

"
HBASE-4427	"It would be extremely helpful to have standalone HBase default to a non-standard port for running its embedded ZK. This would help to run HBase on the same host where a legitimate fully distributed ZK server, etc.

It seems that the following addition to hbase-default.xml would be enough to make it happen:

{noformat}
+  <property>
+    <name>hbase.zookeeper.property.clientPort</name>
+    <value>4181</value>
+  </property>
{noformat}

This will take care of the master/client for HBase and can be overridden in hbase-site if needed.

Thoughts?"
HBASE-4103	"It would be nice, if possible, to get the region-server metrics in another column in the RegionServer table at the bottom of master.jsp.  For instance, seeing the compaction-queues across all the RegionServers, etc., would be useful.

Granted, frameworks like OpenTSDB can probably do this all out of the box, but having a simple cross-cluster view that is built into HBase would be helpful."
HBASE-4243	"Now that HBASE-3465 has been integrated, perhaps we should try to auto-detect the HADOOP_HOME setting if it is not given explicitly. Something along the lines of:

{noformat}
# check for hadoop in the path
141	HADOOP_IN_PATH=`which hadoop 2>/dev/null`
142	if [ -f ${HADOOP_IN_PATH} ]; then
143	  HADOOP_DIR=`dirname ""$HADOOP_IN_PATH""`/..
144	fi
145	# HADOOP_HOME env variable overrides hadoop in the path
146	HADOOP_HOME=${HADOOP_HOME:-$HADOOP_DIR}
147	if [ ""$HADOOP_HOME"" == """" ]; then
148	  echo ""Cannot find hadoop installation: \$HADOOP_HOME must be set or hadoop must be in the path"";
149	  exit 4;
150	fi
{noformat}

Thoughts?"
HBASE-11599	BloomFilterFactory.IO_STOREFILE_DELETEFAMILY_BLOOM_ENABLED is not used anywhere. I think we should clean it.
HBASE-13403	"We currently wait whatever is the configured value of hbase.server.thread.wakefrequency or the default 10 seconds. We should have a configuration to control how long we wait until the HDFS is no longer in safe mode, since using the existing hbase.server.thread.wakefrequency property to tune that can have adverse side effects. My proposal is to add a new property called hbase.master.waitonsafemode and start with the current default."
HBASE-8311	Adding an option to output summaries related to each table into a file in json format. The table information would contain the following details total/online/offline (not deployed)/missing/skipped regions. Also each table would contain a list of errors for the regions for that table.
HBASE-9833	"So far, when we try to insert to HTableMultiplexer, it returns false if the buffer is full. This forces one to write wrapper code to retry inserting into the buffer. If someone is okay with the put getting blocked while the buffer is flushed, we should block."
HBASE-9000	This is to address the linear reseek in MemStoreScanner. Currently reseek iterates over the kvset and the snapshot linearly by just calling next repeatedly. The new solution is to do this linear seek up to a configurable maximum amount of times then if the seek is not yet complete fall back to logarithmic seek.
HBASE-11389	"Over time, features people work get buried in the large heap of config parameters that we have. This task is to find and document all the config parameters in hbase and tag them. This should be automated so that we can run it every now and then and find out the lost configs.

Ideas on how these can be found :
1. Look for Configuration objects and identify where we are setting values.
2. Identify constants/strings that are the parameter names.
3. Identify the constants that are default values.
4. Identify where in the code they are present and tag them appropriately, for example if a constant is being read in a bunch of places like HRegionServer.class, HRegion.class, we can tag it with HRegionServer and HRegion. We can go a bit further and subtag them with the functions they are displayed in.
5. Obtain the comments around the parameter name constants when they are defined.

A few pointers to traversing the java source tree :
http://stackoverflow.com/questions/2183488/traversing-through-the-ast-node
http://www.vogella.com/tutorials/EclipseJDT/article.html"
HBASE-10109	"When a new table is created we place the primary locations for the regions in a round robin fashion. Because we do this for every newly created table it is possible that some regionservers end up with more regions than the others and if there are more tables in the cluster the difference between a min and max #regions per RS can be larger. 
One small optimization is to sort the RS in ascending order before assigning them regions."
HBASE-12105	"Exists does a proper Get, it does not need to get all the KVs for a row to do an existence check. The first KV itself is a good enough indicator. This will speed up exists()."
HBASE-9704	"Fixing the following unit tests:
TestCompaction"
HBASE-11138	"HBaseAdmin#checkSplitKeys() doesn't check for empty split keys, which can cause multiple regions with the same start key (i.e., """"). This diff fixes that."
HBASE-10538	"In current a Scan for Get, which was created by Scan(Get) and isGetScan returns true, is implemented by setting startRow and stopRow the same, which breaks the definition of Scan for general usage. This fix will set stopRow to a value next to startRow, which makes it accord to normal definition."
HBASE-10869	"Motivation of doing this is two-folded:

1. local file system is more stable than `MiniDFSCluster`, so for cases only testing on HBase logic, using this may avoid unstable problems caused by DFS unstable problem.
1. Starting of local file systme is much faster.

Currently only a few of the testcases is switch to LFS mode for testing. Some other diff may switch more.

Implementaiton

For `FS_TYPE_DFS`, same as before.
For `FS_TYPE_LFS`, use `LocalFileSystem`"
HBASE-8775	Throttle the open and close of the regions after an online schema change
HBASE-7179	Can save up to 30 secs during shutdown. No point waiting for these threads.
HBASE-10808	This fix simplifies the Leases to use a ConcurrentHashMap instead of a DelayQueue so as to avoid the Synchronization cost. The LeaseChecker now should check the ConcurrentHashMap periodically to check if a scanner has been inactive for atleast the max time. This has a weaker guarantee on when the lease will be expired but is more efficient and decreases the amount of contention.
HBASE-9143	"In order to achieve online configurable, some of key HBase configuration knobs needed to be updated on the fly. 
Currently, HBase can be configured to flush multiple Region's memstore in parallel.  And this task is to make the number of flush thread be updated in an online fashion (hbase.regionserver.flusher.count).  "
HBASE-8524	"We've seen in a couple of replication use cases that the client thread keeps waiting for a response but waits for ever, as it does not get a response.

The client waits indefintely even if a rpcTimeout is specified.

1) Need to find out what is causing this. 

2) Convert the unconditional wait() in HBaseClient into a timed wait, so that the client can bail out if it waits longer than the rpcTimeout"
HBASE-11106	"If we don't change the assignment plan accordingly, the moving of the region may be igonred."
HBASE-8942	"This is a similar issue as discussed in HBASE-8228

1) A scanner holds the Store.ReadLock() while opening the store files ... encounters errors. Thus, takes a long time to finish.

2) A flush is completed, in the mean while. It needs the write lock to commit(), and update scanners. Hence ends up waiting.

3+) All Puts (and also Gets) to the CF, which will need a read lock, will have to wait for 1) and 2) to complete. Thus blocking updates to the system for the DFS timeout.

Fix:
 Open Store files outside the read lock. getScanners() already tries to do this optimisation. However, Store.getScanner() which calls this functions through the StoreScanner constructor, redundantly tries to grab the readLock. Causing the readLock to be held while the storeFiles are being opened, and seeked.

 We should get rid of the readLock() in Store.getScanner(). This is not required. The constructor for StoreScanner calls getScanners(xxx, xxx, xxx). This has the required locking already."
HBASE-8292	"Currently, any changes to the configuration options: compaction tuning/etc. requires that we restart the regionserver.

Perhaps, some of the configs can be updated on the fly, without having to restart the region server.
"
HBASE-11881	With introduction of swift we have to build composite data structures that can represent a list of KeyValues in a more efficient manner. This can be used both in Put and the Result objects.
HBASE-11712	"isLegalFamilyName might have a zero length string as the argument, but current code does not check its length and directly try to access."
HBASE-8221	"Currently, the RS stops responding as soon as the stop is requested. We then go in and close all regions in a 2-flush mechanism.

Ensure that RS will first close the regions, and then stop taking client requests. This will reduce the number of errors seen by the client."
HBASE-11602	"In case the election znode is deleted (manually for example), the active master should kill itself."
HBASE-11883	ZookeeperWrapper contains a lot of confusing methods. Combine and redefine them.
HBASE-8194	"The test creates regions in the meta region and waits for the master to allot them to regionservers. When the favored nodes were set, the timout was not completely reliable to say that they will be set. Instead setting the favored nodes while creating the regions directly in the starting would ensure that the master creates the regions assigning selected favored nodes."
HBASE-8245	"When a dead region server comes up, the regions that belonged to the region server are assigned to it very aggressively. This causes a increase in the get latencies over all these regions which are under movement and hurt the performance. Instead spreading out this process over a period of time will be beneficial."
HBASE-13360	"I'm very interested in the Raft implementation (in particular, whether it can be reused elsewhere, given that this effort seems likely to produce the canonical Raft implementation).

I'd like to try it standalone, but I'm not yet having much success.

I start a cluster of 3 servers (N=0,1,2):

java -cp ""conf:hbase-consensus/target/hbase-consensus-2.0.0-SNAPSHOT.jar:hbase-consensus/target/dependency/*"" org.apache.hadoop.hbase.consensus.server.LocalConsensusServer -region 1 -servers 127.0.0.1:10000,127.0.0.1:10001,127.0.0.1:10002  -debug DEBUG -localIndex ${N}

And then I try running the load-test client:

java -cp ""hbase-consensus/target/hbase-consensus-2.0.0-SNAPSHOT.jar:hbase-consensus/target/dependency/*"" org.apache.hadoop.hbase.consensus.client.QuorumLoadTestClient -region 1 -servers 127.0.0.1:10000,127.0.0.1:10001,127.0.0.1:10002

That gives an immediate NPE at org.apache.hadoop.hbase.consensus.quorum.QuorumInfo.populateInternalMaps(QuorumInfo.java:315).

Are there any docs on how to get started playing with running this branch?  Thanks!"
HBASE-12803	"http://hbase.apache.org/book/ops_mgt.html#copytable

The page incorrectly shows some text between two unrelated single quotes, in red. See attached picture.
"
HBASE-10776	"HConnectionManager is too large to effectively maintain. This Jira records some refactoring jobs:

1. Move TableServers out as a standalone class
2. Move region-locating code as a class"
HBASE-13180	"Noticing the test failure frequently on internal test rig. 
{code}
FAILED:  org.apache.hadoop.hbase.regionserver.TestRegionReplicas.testGetOnTargetRegionReplicaError Message:
1

Stack Trace:
java.lang.ArrayIndexOutOfBoundsException: 1
        at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas$ResultBoundedCompletionService.submit(RpcRetryingCallerWithReadReplicas.java:420)
        at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.addCallsForReplica(RpcRetryingCallerWithReadReplicas.java:280)
        at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.call(RpcRetryingCallerWithReadReplicas.java:199)
        at org.apache.hadoop.hbase.client.HTable.get(HTable.java:890)
        at org.apache.hadoop.hbase.regionserver.TestRegionReplicas.testGetOnTargetRegionReplica(TestRegionReplicas.java:193)
{code}"
HBASE-11967	"Got this testing 0.99.0 RC.

I started it up after unbundling and got this:

{code}
 99 2014-09-12 14:50:42,761 FATAL [ActiveMasterManager] master.HMaster: Unhandled exception. Starting shutdown.
100 org.apache.hadoop.hbase.util.FileSystemVersionException: HBase file layout needs to be upgraded. You have version null and I want version 8. Consult http://hbase.apache.org/book.html for further information about upgrading HBase. Is your hbase.rootdir valid? If so, you may need to run 'hbase hbck -fixVersionFile'.
101   at org.apache.hadoop.hbase.util.FSUtils.checkVersion(FSUtils.java:587)
....
{code}

But now my master is stuck trying to become master in here:

{code}
 46 ""main"" prio=5 tid=0x00007fe825001000 nid=0x1303 waiting on condition [0x0000000107383000]
 47    java.lang.Thread.State: TIMED_WAITING (sleeping)
 48   at java.lang.Thread.sleep(Native Method)
 49   at org.apache.hadoop.hbase.util.JVMClusterUtil.startup(JVMClusterUtil.java:218)
 50   at org.apache.hadoop.hbase.LocalHBaseCluster.startup(LocalHBaseCluster.java:438)
 51   at org.apache.hadoop.hbase.master.HMasterCommandLine.startMaster(HMasterCommandLine.java:188)
 52   at org.apache.hadoop.hbase.master.HMasterCommandLine.run(HMasterCommandLine.java:139)
 53   at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
 54   at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)
 55   at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:1794)
...

{code}

Any ideas [~jxiang]?  Thanks."
HBASE-10603	RegionSplitter is a utility for partitioning a table based on some split algorithm. Those same algorithms are exposed via the shell create command. There's no value in having two ways to access the same functionality. Ensure the main method doesn't provide any functionality absent from the shell and remove it.
HBASE-10846	"Links between active and backup masters are broken for the the blanks before info port in the url.
{code}
href=""//wcc-hadoop-tst-ct01.bj: 12501/master-status""
{code}"
HBASE-11582	"Recent precommit builds shows some javadoc warnings here
{code}
[WARNING] Javadoc Warnings
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/hbase-server/src/main/java/org/apache/hadoop/hbase/io/DataInputInputStream.java:32: warning - Tag @see: reference not found: DataOutputOutputStream
[WARNING] /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/CacheConfig.java:99: warning - Tag @link: can't find BUCKET_CACHE_COMBINED_PERCENTAGE_KEY in org.apache.hadoop.hbase.io.hfile.CacheConfig
{code}
"
HBASE-13033	Currently in MemstoreFlusher the check for maximum allowed memstore size is set to 90% and it should be 80%
HBASE-12977	"If enable multi callQueues , handlers may not be distributed evenly among multi queues, which mean the queue's capacity is not the same. Should we make handler's distribution even? "
HBASE-3599	"The Result(List<KeyValue>) and Result(KeyValue[]) constructor assume that the input is already sorted (HBASE-3073, HBASE-2753).  This works for normal use cases, but not for hand constructed Result objects (e.g. unit tests).

I encountered this when upgrading to 0.90 that some unit tests now failed due to this.  One fix is to add some documentation on the constructors that says the input MUST be sorted.  The other would be to explicitly sort the items (not so desirable).


"
HBASE-8121	"This doesn't seems to cause any issue, but should not ReplicationSink#batch() take a look at the batch call results?"
HBASE-6121	"We shouldn't clear an ArrayList that might be iterated on by another thread.

Specifically, multiput() calls clear() on ArrayList (to free up some memory) while MultiPut.toMap is iterating over that ArrayList in a different thread (called from MonitorTasks UI)"
HBASE-6760	"The issue seems to exists due to oversight during the rewrite. In line 1289, the variable 'plans' is created as a 'new ArrayList<RegionPlan>()' and then in line 1298, balancerRan is calculated as (plans != null) which for obvious reason, will always return true.

{code:title=HMaster.java (trunk:1383496)}
....
1289        List<RegionPlan> plans = new ArrayList<RegionPlan>();
1290        //Give the balancer the current cluster state.
1291        this.balancer.setClusterStatus(getClusterStatus());
1292        for (Map<ServerName, List<HRegionInfo>> assignments : assignmentsByTable.values()) {
1293          List<RegionPlan> partialPlans = this.balancer.balanceCluster(assignments);
1294          if (partialPlans != null) plans.addAll(partialPlans);
1295        }
1296        int rpCount = 0;  // number of RegionPlans balanced so far
1297        long totalRegPlanExecTime = 0;
1298        balancerRan = plans != null;
1299        if (plans != null && !plans.isEmpty()) {
....
{code}

A simple fix is to initialize 'balancerRan' to 'false', remove ""balancerRan = plans != null"" and add ""balancerRan = true"" after ""if (plans != null && !plans.isEmpty()) {"".

However, a question remains that should we call ""this.cpHost.postBalance();"" if the balancer did not run at this point?

I'll attach the patch shortly if I get a confirmation on this."
HBASE-7275	[89-fb] Fixing some minor bugs in 89-fb branch based on the findBugs report
HBASE-4201	Delayed RPC crashes if return value is not delayed and endDelay is called before the actual function returns a value.
HBASE-6042	"Given the implementation of ScanQueryMatcher, we check 
filter.filterKeyValue()

before determining weather we are going to include the KV in the
result or not. Thus, if the scan/get were to specify columns other
than the very first column in the row, they get nothing because
the filter removes everything else."
HBASE-5756	This can be a point of concern when on a certain day the logging happens more because of more and more activity. In that case the log file for that day can grow huge. These logs can not be opened for analysis since size is more.
HBASE-5082	"I was trying to use the ColumnAggregationEndPoint.sum().
 
In my sample is just created a column family but did not use any qualifier and inserted some data.
 
I tried to use  ColumnAggregationEndPoint.sum(qualifier, null).  When i did this inside the ColumnAggregationEndPoint we do 
scan.addColumn().  This is adding the [null] array in the scan object.  Later in the scanQueryMatcher it is throwing nullpointer exception.  
I can understand that addColumn() is to specifiy the qualifier.
Do we need to document somewhere saying qualifier should not be null? I think coprocessors can be used even in places where we don't have qualifiers. If that is the case this sample ColumnAggregationEndPoint may not work."
HBASE-4975	"Hadoop QA generated comments based on patches submitted to JIRAs; for example:

https://issues.apache.org/jira/browse/HBASE-4960?focusedCommentId=13163191&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13163191

There are some spurious -1's given to the patch. The patch only affects documentation, not source code, but Hadoop QA says that:
{noformat}
-1 findbugs. The patch appears to introduce 72 new Findbugs (version 1.3.9)
warnings.
{noformat}

Evidently Hadoop QA is not able to recall the set of Findbugs warnings from the previous build.

(Of course the Findbugs warnings themselves should be addressed, but this patch could not have added to them).

{noformat}
-1 javadoc. The javadoc tool appears to have generated -160 warning
messages.
{noformat}

This should be ""160 warning messages"", not ""-160 warning messages"".

Thanks to NKeywal for suggesting that the relevant file is {{dev-support/test-patch.sh}}.
"
HBASE-12805	"When I added hbase-client as a dependency in Nutch's ivy.xml. 

{code}
<dependency org=""org.apache.hbase"" name=""hbase-client"" rev=""0.98.8-hadoop2"" conf=""*->default"">
{code}
Ivy can not resolve hbase-common and hbase-annotations in compile scope rather than hbase-protocol. It sees these dependencies in test scope and map to runtime and master scope. they looks like below in hbase-client resolved ~/.ivy2/cache/org.apache.hbase/hbase-client/ivy-0.98.8-hadoop2.xml file :

{code}
<dependency org=""org.apache.hbase"" name=""hbase-annotations"" rev=""0.98.8-hadoop2"" force=""true"" conf=""test->runtime(*),master(*)""/>
<dependency org=""org.apache.hbase"" name=""hbase-annotations"" rev=""0.98.8-hadoop2"" force=""true"" conf=""test->runtime(*),master(*)"">
  <artifact name=""hbase-annotations"" type=""test-jar"" ext=""jar"" conf="""" m:classifier=""tests""/>
</dependency>
<dependency org=""org.apache.hbase"" name=""hbase-common"" rev=""0.98.8-hadoop2"" force=""true"" conf=""test->runtime(*),master(*)""/>
<dependency org=""org.apache.hbase"" name=""hbase-common"" rev=""0.98.8-hadoop2"" force=""true"" conf=""test->runtime(*),master(*)"">
  <artifact name=""hbase-common"" type=""test-jar"" ext=""jar"" conf="""" m:classifier=""tests""/>
</dependency>
<dependency org=""org.apache.hbase"" name=""hbase-protocol"" rev=""0.98.8-hadoop2"" force=""true"" conf=""compile->compile(*),master(*);runtime->runtime(*)""/>
{code} 

Not only Hbase-client. Other Hbase modules have same problem. Do you have any idea ?"
HBASE-4038	"We should provide a basic way for end users to operationally diagnose hot row problems.  Thinking about a 2-phase approach:

1. Diagnose hot regions
2. Inspect those regions/servers to find the hot rows.

To diagnose hot regions, we could query the master or regionservers for these regions + sort.  To inspect the regions for hot rows, we could write another script to analyze the HLogs on a server and basically do: sort log|uniq -n|sort -n|top"
HBASE-3761	"Creating hundreds of tables with hundreds of regions I ran into this issue doing mass table delete:

{code}
11/04/09 10:26:06 WARN client.HConnectionManager$HConnectionImplementation: Encountered problems when prefetch META table:
org.apache.hadoop.hbase.TableNotFoundException: Cannot find row in .META. for table: bbb_48, row=bbb_48,T锟紻锟斤拷iy%锟斤拷,99999999999999
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:136)
        at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:95)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.prefetchRegionCache(HConnectionManager.java:648)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:702)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:593)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.relocateRegion(HConnectionManager.java:564)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionLocation(HConnectionManager.java:415)
        at org.apache.hadoop.hbase.client.ServerCallable.instantiateServer(ServerCallable.java:57)
        at org.apache.hadoop.hbase.client.ScannerCallable.instantiateServer(ScannerCallable.java:63)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionServerWithRetries(HConnectionManager.java:1011)
        at org.apache.hadoop.hbase.client.HTable$ClientScanner.nextScanner(HTable.java:1077)
        at org.apache.hadoop.hbase.client.HTable$ClientScanner.initialize(HTable.java:1000)
        at org.apache.hadoop.hbase.client.HTable.getScanner(HTable.java:533)
{code}

The table was probably removed between start of prefetch scan and next invocation."
HBASE-4615	"In checking our package manifests for BigTop, we found a number of source files are set with incorrect permissions:

hbase-0.90.4-cdh3u2/docs/apidocs/org// are set to +x
hbase-0.90.4-cdh3u2/docs/* are set to +x "
HBASE-4306	"It is possible for the LoadBalancer to try to assign an offline/split region while it is waiting to be CatalogJanitor'ed. It goes like this:

{quote}
2011-08-25 00:32:07,137 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: parent: Daughters; d1, d2 from sv4r22s16,60020,1314211225331
...
(cleaning never happens or whatever)
...
2011-08-29 13:45:14,561 INFO org.apache.hadoop.hbase.master.HMaster: balance hri=parent, src=sv4r22s16,60020,1314211225331, dest=sv4r19s17,60020,1314218170402
2011-08-29 13:45:14,561 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Starting unassignment of region parent (offlining)
2011-08-29 13:45:14,588 INFO org.apache.hadoop.hbase.master.AssignmentManager: Server serverName=sv4r22s16,60020,1314211225331, load=(requests=0, regions=0, usedHeap=0, maxHeap=0) returned org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Received close for parent but we are not serving it for parent
{quote}

Here it took 4 days of balancing to finally get to try to balance the parent (that was never deleted because of HBASE-4238), but it can also happen if the balancer decides to balance the parent just before it's cleaned. The end effect is that the balancer will be disabled _forever_ until that's fixed.

The culprit here is that the master keeps the region ""online"" until AssignmentManager.regionOffline is called by the CJ, which means it's still treated like any other region although it's offline."
HBASE-3592	"Running some of the bulk load tools (importtsv, completebulkload, etc.) is a bit awkward from the user perspective. When a user is used to interacting with HBase from the shell, the expectation is kind of set that the shell is the definitive place to be working with their data. However, in order to do bulk load operations, they need to switch gears and submit a MR job. Not a big deal for everyday Hadoop users, but it gets confusing for the HBase-only user."
HBASE-11962	"This issue is to backport the commands for appending and removing peer table-cfs for replication to 0.98

Two new commands, append_peer_tableCFs and remove_peer_tableCFs, are added to do the operation of adding and removing a table/table-column family."
HBASE-12132	mvn install -Dtest=<testclass> runs tests in it and they time out. This Jira is intended to fix that behavior. 
HBASE-12177	"TestIPv6NIOServerSocketChannel hangs consistently on Java 8u20. jstack dump attached. This is on a 64-bit Ubuntu 14.04 system kernel ""Linux version 3.13.0-29-generic (buildd@toyol) (gcc version 4.8.2 (Ubuntu 4.8.2-19ubuntu1) ) #53-Ubuntu SMP Wed Jun 4 21:00:20 UTC 2014""

{noformat}
""pool-1-thread-1"" #9 prio=5 os_prio=0 tid=0x00007ff330707000 nid=0x41a8 runnable [0x00007ff309e8f000]
   java.lang.Thread.State: RUNNABLE
	at java.net.PlainSocketImpl.socketBind(Native Method)
	at java.net.AbstractPlainSocketImpl.bind(AbstractPlainSocketImpl.java:382)
	- locked <0x00000000e1c004e0> (a java.net.SocksSocketImpl)
	at java.net.ServerSocket.bind(ServerSocket.java:375)
	at java.net.ServerSocket.bind(ServerSocket.java:329)
	at org.apache.hadoop.hbase.TestIPv6NIOServerSocketChannel.bindServerSocket(TestIPv6NIOServerSocketChannel.java:60)
	at org.apache.hadoop.hbase.TestIPv6NIOServerSocketChannel.testServerSocketFromLocalhostResolution(TestIPv6NIOServerSocketChannel.java:149){noformat}

I am going to disable this test in 0.98 branch for now."
HBASE-12179	TestConstraints intermittently hangs on Java 8. There are a few things this test could be doing better. Looking at it.
HBASE-8619	"In our rolling upgrade testing, running ImportTsv failed in the job submission phase when the master was down. This was when the master was failing over to the backup master. In this case, a retry would have been helpful and made sure that the job would get submitted.

A good solution would be to refresh the master information before placing the call to getHTableDescriptor.

Command:
{code} sudo -u hbase hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=HBASE_ROW_KEY,c1,c2,c3 -Dimporttsv.bulk.output=/user/hbase/storeFiles2_2/import2_table1369439156 import2_table1369439156 /user/hbase/tsv2{code}

Here is the stack trace:

{code} 13/05/24 16:55:49 INFO compress.CodecPool: Got brand-new compressor [.deflate]
16:45:44  Exception in thread ""main"" java.lang.reflect.UndeclaredThrowableException
16:45:44  	at $Proxy7.getHTableDescriptors(Unknown Source)
16:45:44  	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHTableDescriptor(HConnectionManager.java:1861)
16:45:44  	at org.apache.hadoop.hbase.client.HTable.getTableDescriptor(HTable.java:440)
16:45:44  	at org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.configureCompression(HFileOutputFormat.java:458)
16:45:44  	at org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.configureIncrementalLoad(HFileOutputFormat.java:375)
16:45:44  	at org.apache.hadoop.hbase.mapreduce.ImportTsv.createSubmittableJob(ImportTsv.java:280)
16:45:44  	at org.apache.hadoop.hbase.mapreduce.ImportTsv.main(ImportTsv.java:424)
16:45:44  Caused by: java.io.IOException: Call to hbase-rolling-6.ent.cloudera.com/10.20.186.99:22001 failed on local exception: java.io.EOFException
16:45:44  	at org.apache.hadoop.hbase.ipc.HBaseClient.wrapException(HBaseClient.java:1030)
16:45:44  	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:999)
16:45:44  	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:86)
16:45:44  	... 7 more
16:45:44  Caused by: java.io.EOFException
16:45:44  	at java.io.DataInputStream.readInt(DataInputStream.java:375)
16:45:44  	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.receiveResponse(HBaseClient.java:646)
16:45:44  	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.run(HBaseClient.java:580){code}"
HBASE-11614	MAX_FILESIZE = 0 doesn't make sense. We should avoid creating such tables.
HBASE-3558	"Last night we ran into a problem with the times on RSs being out of sync by 1 minute. The times were being reset by ~70s often because we were getting different responses from pool.ntpd.org.

This caused lost ZK sessions and problems writing to datanodes,  so all the RSs kept shutting down.

I think it would be useful to have HBaseFsck check to see if the times on the region servers are out of sync. Or maybe put a warning on the master web ui or something. 

This seems related to HBASE-3168, but applies when region servers become out of sync once they already joined the cluster (due to NTP issues or something else)."
HBASE-11695	"We just ran into a flush storm caused by the PeriodicFlusher.
Many memstore became eligible for flushing at exactly the same time, the effect we've seen is that the exact same region was flushed multiple times, because the flusher wakes up too often (every 10s). The jitter of 20s is larger than that and it takes some time to actually flush the memstore.

Here's one example. We've seen 100's of these, monopolizing the flush queue and preventing ""important"" flushes from happening.

{code}
06-Aug-2014 20:11:56  [regionserver60020.periodicFlusher] INFO  org.apache.hadoop.hbase.regionserver.HRegionServer[1397]-- regionserver60020.periodicFlusher requesting flush for region tsdb,\x00\x00\x0AO\xCF* \x00\x00\x01\x00\x01\x1F\x00\x00\x03\x00\x00\x0C,1340147003629.ef4a680b962592de910d0fdeb376dfc2. after a delay of 13449
06-Aug-2014 20:12:06  [regionserver60020.periodicFlusher] INFO  org.apache.hadoop.hbase.regionserver.HRegionServer[1397]-- regionserver60020.periodicFlusher requesting flush for region tsdb,\x00\x00\x0AO\xCF* \x00\x00\x01\x00\x01\x1F\x00\x00\x03\x00\x00\x0C,1340147003629.ef4a680b962592de910d0fdeb376dfc2. after a delay of 14060
{code}

So we need to increase the period of the PeriodicFlusher to at least the random jitter, also increase the default random jitter (20s does not help with many regions).
"
HBASE-6517	"Hadoop common is adding a JUnit run listener which prints full thread dump into System.err when a test is failed due to timeout. See HDFS-3762.

Suggest pulling in their {{TestTimedOutListener}} once it is committed."
HBASE-11638	So single server dev clusters don't inexplicably fail to start. 
HBASE-3442	"We've got our servers running on Amazon EC2 and nodes will go through some shutdown scripts if/when we want to take them out of the mix.  Ended up shutting down one of the nodes, in this case Node98, which cased the immediate crash of the master server.  Upon restarting the master, it would attempt to contact the missing node, and then stop it's startup process.  I believe the node removed itself from the DNS server first, then ran a stop on the datanode, and regionserver.  The missing node was also removed from any slave/regionserver list on the master server.  I finally put in a bogus entry in the /etc/hosts file for the missing node, pointing it back to 127.0.0.1, and the master server finally marked it as a dead node, ignored it, and finished the startup process.

Going to try and replicate it again and save some more logs, the following log is the only thing I saved from the first occurrence;  It's the master failing to start up while checking for the missing node:  http://pastebin.com/ZyQMQm91"
HBASE-3414	"I have a project assuming dynamic HBase schema and I would like to be able to create tables and families on the fly. For performance reasons, I was going to react to exceptions like NoSuchColumnFamilyException rather than checking for family validity, alter the table appropriately and repeat the put(). However, it turned out that this exception only contains a human-readable message. Please add two more fields to this class: table and family names."
HBASE-3319	"A useful check to add to HBCK is a way to estimate the max # of files that a cluster should have and raise a warning/error if the file count goes above that threshold.   We ran into an issue this week where our "".oldlogs"" folder filled up to 100k files because 'hbase.master.logcleaner.maxdeletedlogs"" was set too low.  We found this because of faulty region count metric in the HTTP server that actually showed the file count.  Adding an HBCK check would provide an extra layer of detection to find leaks from new features or conservatively configured cleanup thresholds."
HBASE-11300	"For the checkAndPut operation, the AccessController only checks the read and write permission for the family and qualifier to check, but ignores the write permission for the family map of ""put"". What's more,  we don't need the write permission for the family and qualifier to check.

See the code AccessController.java #1538
{code}
    Map<byte[],? extends Collection<byte[]>> families = makeFamilyMap(family, qualifier);
    User user = getActiveUser();
    AuthResult authResult = permissionGranted(OpType.CHECK_AND_PUT, user, env, families,
      Action.READ, Action.WRITE);
{code}

Same problem for checkAndDelete operation.
"
HBASE-3188	"AssignmentManager.regionPlans is currently a ConcurrentSkipListMap so does not require synchronization.  But sometimes we do multiple operations and we synchronize on it.  But other times we don't synchronize on it at all.

Let's review and make sure we're doing the right thing.

Also see if we still need this AssignmentManager.updateTimers().  Don't we disable load balancer / expiration during startup or no?"
HBASE-3078	"It happens very rare (was seen only once yet), but looks like it was caused inside HBase TableOutputFormat class while running periodic mapreduce job on my cluster.

Here is a task log for failed task:

2010-10-04 12:08:00,339 WARN org.apache.hadoop.mapred.TaskTracker: Error running child
java.lang.IndexOutOfBoundsException: toIndex = 850
	at java.util.SubList.<init>(Unknown Source)
	at java.util.RandomAccessSubList.<init>(Unknown Source)
	at java.util.AbstractList.subList(Unknown Source)
	at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:611)
	at org.apache.hadoop.hbase.mapreduce.TableOutputFormat$TableRecordWriter.close(TableOutputFormat.java:80)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:567)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)
	at org.apache.hadoop.mapred.Child.main(Child.java:170)
2010-10-04 12:08:00,343 INFO org.apache.hadoop.mapred.TaskRunner: Runnning cleanup for the task

"
HBASE-3029	"During a scan if the HFile layer throws a unrecoverable IOE, for example, checksum exception, right now we throw the exception, and abort that RPC.  But the scanner is not marked as closed, and the HFileScanner is left in a weird state.  Subsequent calls get weird exceptions about ByteBuffers but this is an artifact of being left pointing at the end of the previous block when we should be into the next block.

If the DFSClient throws an exception we have a choice:
- make some efforts to retry
- assume DFSClient has already tried, and thus this is a fatal type error

The former case might be hard to implement, and the latter case needs to be handled so that subsequent calls to the scanner throw meaningful exceptions.  Right now there is no way to early terminate a scanner from the server-side... HRegion$RegionScanner doesn't have a 'closed' flag nor does it have the ability to realize the scanner is now closed.  The client side takes care of not iterating past the end of a scanner so in the normal case we dont iterate anymore once a scanner returns 'false'.

"
HBASE-2877	"When {{HbaseObjectWritable#writeObject}} serializes a {{Writable}} RPC parameter, it writes its ""class code"" twice to the wire.  {{writeClassCode}} is already called once unconditionally at the beginning of the method, and for {{Writable}} arguments, it's called a second time towards the end of the method.  It seems that the code is trying to deal with the ""declared type"" vs. ""actual type"" of a parameter.  The Hadoop RPC code was already doing this before Stack changed it to use codes in r608738 for HADOOP-2519.  It's not documented when this is useful though, and I couldn't find any use case.  Every RPC I've seen so far just ends up with the same byte sent twice to the wire."
HBASE-2826	"As per users@hbase.apache.org thread: http://mail-archives.apache.org/mod_mbox/hbase-user/201007.mbox/%3CAANLkTimyCtWiuNxWw9P2IFAy6Y4Q7X8oLZSZNdggV3Nn@mail.gmail.com%3E

I setup HBase (and as such, Zookeeper) on Ubuntu Karmic using the Cloudera Karmic CDH3 distribution. Zookeeper has installed fine, however when it comes to starting an hbase master, it falls over with the following exception:

(stack trace summarised to last bit)
Caused by: java.io.EOFException
   at java.io.DataInputStream.readInt(DataInputStream.java:375)
   at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:508)
   at org.apache.hadoop.ipc.Client$Connection.run(Client.java:446)

The cause was that the hbase.rootdir was incorrectly set to hdfs://master:50071/hbase, when it should have been set to port 50070. After remedying this hbase master started fine.

I was asked to raise this as a JIRA to investigate the possibility of improving the handling of the EOFException to perhaps indicate more clearly to the user that an incorrect address has been specified for the rootdir, rather than the very unclear statement above."
HBASE-2778	Seeing a very occasional (maybe one in 40) failure of this unit test.
HBASE-2773	"I had some failed test due to this last weekend, but can't find the trace now."
HBASE-2637	"If a table is disabled when HBase is started up, then ""HTTP ERROR  500"" will happen when view this table in table.jsp"
HBASE-2304	"When there is a error given in the UI/JSPs, the HTTP status code is not set accordingly.

For example, if the page output for master.jsp is:

{{HTTP ERROR: 500
Trying to contact region server null for region , row '', but failed after 3
attempts.
Exceptions:
org.apache.hadoop.hbase.NotServingRegionException:
org.apache.hadoop.hbase.NotServingRegionException: -ROOT-,,0}}

The HTTP status code sent with the response is still ""HTTP/1.1 200 OK"".  This is a red herring for some types of monitoring, though it is debatable if an HTTP status code of 4xx/5xx reflect the server state or the state of the system being monitored.  It may well be that HTTP 200 is OK, since the HTTP UI is responsive and correctly indicating a failed HBase cluster.

"
HBASE-2292	"I'm grabbing my own {{RowLock}} on a given row and then I want to do an ICV on the row (among other things) while I hold the lock.  Unlike {{Get}}, {{Put}} and whatnot, there's no way to specify your own {{RowLock}} when doing an ICV.  Technically, you can live without this since you can get just a {{Get}} followed by a {{Put}} after having manually incremented the value, which is guaranteed to work as you hold the {{RowLock}} on that row, but it would be nice to be able to just do an ICV.

Right now, {{incrementColumnValue}} attempts to lock the row again (which is normal), thus causing my client to ""deadlock"" itself until its {{RowLock}}'s lease expires."
HBASE-2071	"When adding columns to a Scan, using ""addColumns"" or ""addColumn"" (with single argument) the order in which they are added matters.  When adding columns as distinct family and qualifier (2 arguments) this is not a problem.  It is only a problem when adding as a single argument.

For example:
user> (scan ""t1"" {:columns [ ""f1:"" ""f1:hello"" ] })
cols are: [f1: f1:hello]
col: f1:  --  
col: f1:hello  --  world

user> (scan ""t1"" {:columns [ ""f1:hello"" ""f1:"" ] })
cols are: [f1:hello f1:]
col: f1:hello  --  world
col: f1:  --  v1

In the first call to ""scan,"" the arguments are in lexicographic order, and this results in the value associated with ""f1"" to be left out.  In the second call to ""scan,"" the arguments are in reverse order, and the results are valid.

Sorry the example is in Clojure.  In essence, this is what's happending:

This doesn't work:
myScan.addColumn(""f1:"")
myScan.addColumn(""f1:hello"")

This works:
myScan.addColumn(""f1:hello"")
myScan.addColumn(""f1:"")

I have omitted the code which creates a scanner with this scan, returns the results, and then prints them.
"
HBASE-1565	"Every developer that works with HBase ends up inevitably writing all sorts of rsync and kill scripts. These scripts usually end up failing from time to time as our system changes. I think we should maintain some of these capabilities in our bin/ scripts directly to ease the work of our users/developers.

For the record Hadoop has recently put in similar sort of stuff in bin/hadoop-daemon.sh:

{code}
    if [ ""$HADOOP_MASTER"" != """" ]; then
      echo rsync from $HADOOP_MASTER
      rsync -a -e ssh --delete --exclude=.svn --exclude='logs/*' --exclude='contrib/hod/logs/*' $HADOOP_MASTER/ ""$HADOOP_HOME""
    fi
{code}

Whatever we add can be extra options, like this $HADOOP_MASTER variable above, so that the normal operation of the scripts is not affected."
HBASE-1452	"From an irc conversation
(12:43:54 PM) posix4e: ok sorry to keep on bothering y'all but I seem to have more than one directory per regionserver in the .logs file. 
(12:44:00 PM) posix4e: but one of them is empty
(12:44:04 PM) posix4e: soo if the node is called foo
(12:44:07 PM) iand left the room (quit: Connection timed out).
(12:44:24 PM) posix4e: foo_1243299289601_60020
(12:44:24 PM) posix4e: foo_1243299289601_60020
(12:44:30 PM) posix4e: would both exist, and one would be empty
(12:59:16 PM) St^Ack: both named the same?
(1:27:42 PM) posix4e: St^Ack: same hostname yea
(1:29:45 PM) St^Ack: posix4e: all is the same?  Even the startcode?
(1:29:50 PM) St^Ack: how can you have to dirnames sam?
(1:30:08 PM) nitay: St^Ack, sounds good :)
(1:30:37 PM) posix4e: so if the the name of the directory looks like
(1:30:37 PM) posix4e: hostname__stuff_port
(1:30:44 PM) posix4e: then stuff would be different and that's it
(1:33:58 PM) St^Ack: the 'stuff' is different in each case?
(1:34:03 PM) posix4e: yea
(1:34:07 PM) ***St^Ack the 'stuff' is the server startcode
(1:34:08 PM) St^Ack: ok...
(1:34:21 PM) St^Ack: well, here is what I speculate....
(1:34:33 PM) St^Ack: The server was restarted
(1:34:37 PM) St^Ack: got a new startcode
(1:34:39 PM) St^Ack: made a new dir
(1:35:10 PM) St^Ack: the old one, if it had logs in it -- i.e. wasn't shutdown clean -- then its logs were replayed by master (or should have been)
(1:35:19 PM) St^Ack: master should have cleaned it up
(1:35:24 PM) St^Ack: when done
(1:35:34 PM) St^Ack: or regionserver on clean shutdown should have cleaned it up when done
(1:35:39 PM) St^Ack: posix4e: please make an issue
(1:35:49 PM) posix4e: St^Ack: kk
(1:35:56 PM) St^Ack: else our logs dir will be flooded w/ empty dirs
"
HBASE-420	"Region merge functionality exists in HBase today, but merges are triggered manually (in theory only, because there is no admin tool for doing so). Instead of relying on an admin to note and merge regions, the Master should detect adjacent undersized regions and automatically merge them.

Other than the case when a table has exactly one region, region sizes should always be between 1/2x and 1x the split size. For instance, if the max file size is 256MB, steady-state, regions will be between 128 and 256MB. If we find two regions near each other that are less than some threshold when summed together, they are candidates for merging. For instance, we could set the threshold to 1/2x max file size, so if one region was 50MB and the other was 16MB, they would be mergeable. 

The only time that regions small enough to merge should exist is when there have been significant deletions. Otherwise, regions will always stay in the 1/2 to 1x range. "
HBASE-77	"> Right now the only interface for insertion is via a byte[], which severely limits the flexibility of the system. Added 2007-10-08 by stuhood.

Tell us more Stu why you need this feature? (Because you want to write BLOBs of multiple Gigabytes into an hbase cell?)

Added as part of migrating new feature requests from the obsoleted http://wiki.apache.org/hadoop/Hbase/HbaseFeatureRequests"
HBASE-68	"Today, HStoreFiles keep the entire serialized HStoreKey objects around for every cell in the HStore. Since HStores are 1-1 with column families, this is really unnecessary - you can always surmise the column family by looking at the HStore it belongs to. (This information would ostensibly come from the file name or a header section.) This means that we could remove the column family part of the HStoreKeys we put into the HStoreFile, reducing the size of data stored. This would be a space-saving benefit, removing redundant data, and could be a speed benefit, as you have to scan over less data in memory and transfer less data over the network.
"
HBASE-10559	"Investigate if it's possible to autogenerate the CHANGES.txt file from the 'release' Maven profile, using the JIRA API."
HBASE-10063	"In SecureWALCellCodec#EncryptedKvEncoder#write we get the IV for the entry from the secure RNG. This can be a heavyweight operation if not using an accelerated RNG. Consider something lighter weight. One option could be to create a random IV only once, store it in the header, and then increment it per cell. "
HBASE-6221	"As discussed in the HBase BoF today, a patch backporting HBASE-6135 UI improvements to 0.94 may be interesting even if not actually committed to that branch."
HBASE-2055	"There was some advocacy of using Avro for serialization of HBase WAL records up on hbase-dev@. Idea is Hadoop core is getting away from Writables and Avro is the blessed replacement. 

I think we have this criteria for its use:
1) Performance of writing Avro records is no worse than that for writing Writables into a SequenceFile.
2) Space consumed by Avro serialization is no worse than that of Writables
3) File format is amenable to appends (cannot require valid trailers, etc.)

I'll put up a patch so we can try it out. "
HBASE-7395	The security profile in the 0.94 POM shouldn't set hadoop.version. This is done elsewhere. It isn't necessary anymore and I think depending on the order of Maven command line environments may do the wrong thing.
HBASE-9305	"The assertion failures are like this:

{noformat}
java.lang.AssertionError: expected:<2089> but was:<2109>
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.failNotEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:128)
	at org.junit.Assert.assertEquals(Assert.java:472)
	at org.junit.Assert.assertEquals(Assert.java:456)
	at org.apache.hadoop.hbase.client.TestFromClientSide.testCacheOnWriteEvictOnClose(TestFromClientSide.java:4248)
{noformat}

Also:

{noformat}
expected:<2067> but was:<2087>
{noformat}

{noformat}
expected:<2070> but was:<2090>
{noformat}

The test saves off the current block cache stats - block count and hits and misses - then puts a value and gets it back:

{code}
4242: Put put = new Put(ROW);
4243: put.add(FAMILY, QUALIFIER, data);
4244: table.put(put);
4245: assertTrue(Bytes.equals(table.get(new Get(ROW)).value(), data));
{code}

then we have these asserts:

{code}
4246: //data was in memstore so don't expect any changes
4247: assertEquals(startBlockCount, cache.getBlockCount());
4248: assertEquals(startBlockHits, cache.getStats().getHitCount());
4249: assertEquals(startBlockMiss, cache.getStats().getMissCount());
{code}

There are exactly 20 more hits than expected every time. In the log looks like there's a meta scan happening around the same time. "
HBASE-9361	"The error:

{noformat}
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:92)
	at org.junit.Assert.assertTrue(Assert.java:43)
	at org.junit.Assert.assertTrue(Assert.java:54)
	at org.apache.hadoop.hbase.master.TestDistributedLogSplitting.testThreeRSAbort(TestDistributedLogSplitting.java:132)
{noformat}

Here the test has aborted three regionservers but not all of them have terminated after 60 seconds."
HBASE-9362	"An example failure:

{noformat}
java.lang.AssertionError: expected:<0> but was:<200>
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.failNotEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:128)
	at org.junit.Assert.assertEquals(Assert.java:472)
	at org.junit.Assert.assertEquals(Assert.java:456)
	at org.apache.hadoop.hbase.regionserver.TestColumnSeeking.testDuplicateVersions(TestColumnSeeking.java:160)
{noformat}
"
HBASE-11065	"mvn site fails
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.2:site (default-site) on project hbase: SiteToolException: ArtifactResolutionException: Unable to find skin: Could not transfer artifact org.apache.maven.skins:maven-stylus-skin:jar:1.5 from/to central (http//repo.maven.apache.org/maven2/): No connector available to access repository central (http//repo.maven.apache.org/maven2/) of type default using the available factories WagonRepositoryConnectorFactory
[ERROR] org.apache.maven.skins:maven-stylus-skin:jar:RELEASE

"
HBASE-6030	"The HBase AccessController can emit a detailed trace of every action, and whether it succeeded or failed, but this is expected to be used only for debugging an application in a staging environment. In production a RegionServer may have thousands of requests per second, logging the audit trace just isn't viable. However, we might want to log the AccessDeniedExceptions which result from access failures in the daemon logs like Hadoop does, e.g. the NameNode log."
HBASE-11043	"AccessController#preGetTableDescriptors only allow users with admin or create permission to get table's description.
{quote}
        requirePermission(""getTableDescriptors"", nameAsBytes, null, null,
          Permission.Action.ADMIN, Permission.Action.CREATE);
{quote}

I think Users with table's read/write permission should also be able to get table's description. 

Eg: when create a hive table on HBase,  hive will get the table description to check if the mapping is right. Usually the hive users only have the read permission of table."
HBASE-7759	"HADOOP-9252 slightly changes the format of some StringUtils outputs.  It may cause test failures.

Also, some methods were deprecated by HADOOP-9252.  The use of them should be replaced with the new methods."
HBASE-3530	"Currently we always start the LogSyncer thread, even when not necessary because the ""deferred flush"" is set to false and the sync() is called on every record. We could disable the thread and not have it flush every n secs unnecessarily."
HBASE-800	"From Billy Pearson on hbase-users@

Hey Andrew

Do we have plans to include setMaxFileSize for the shell,thrift,rest?
So non java users can change this as needed with out having to learn java.

Billy

""Andrew Purtell"" <apurtell@yahoo.com> wrote in message news:189371.9860.qm@web65516.mail.ac4.yahoo.com...
> Hello David,
>
> Current trunk (upcoming 0.2.0) has support for per-table metadata. See
> https://issues.apache.org/jira/browse/HBASE-42 and
> https://issues.apache.org/jira/browse/HBASE-62.
>
> So maybe you can set the split threshold quite low for the table in
> question?
>
> The default is 256MB (268435456), set globally for all tables in the HBase
> configuration as ""hbase.hregion.max.filesize"". However it's reasonable to
> set it as low as the DFS blocksize. The guidance for a typical HBase
> installation is to set the DFS blocksize to 8MB (8388608), instead of the
> default 64MB.
>
> At create time:
>
>  HTableDescriptor htd = new HTableDescriptor(""foo"");
>  htd.setMaxFileSize(8388608);
>  ...
>  HBaseAdmin admin = new HBaseAdmin(hconf);
>  admin.createTable(htd);
>
> If the table already exists:
>
>  HTable table = new HTable(hconf, ""foo"");
>  admin.disableTable(""foo"");
>  // make a read-write descriptor
>  HTableDescriptor htd =
>    new HTableDescriptor(table.getTableDescriptor());
>  htd.setMaxFileSize(83388608);
>  admin.modifyTableMeta(""foo"", htd);
>  admin.enableTable(""foo"");
>
> Hope this helps,
>
>   - Andy
>
>> From: David Alves
>> <dr-alves@criticalsoftware.com>
>> Subject: Region Splits
>> To: ""hbase-user@hadoop.apache.org""
>> <hbase-user@hadoop.apache.org>
>> Date: Thursday, July 31, 2008, 6:06 AM
> [...]
>> I use hbase (amongst other things) to crawl some repos of infomation
>> and util now I've been using the Nutch segment generation paradigm.
>> I would very much like to skip the segment generation step using
>> hbase as source and sink directly but in order to do that I would
>> need to either allow more that one split to be generated for a
>> single region or make the regions in this particular table split
>> with much less entries than other tables."
HBASE-10386	"I was doing load test on a 0.98 cluster and saw the following in output:
{code}
2014-01-20 20:44:52,567 [Thread-2] INFO  client.HBaseAdmin (HBaseAdmin.java:enableTable(761)) - Enabled table test
Starting to write data...
Failed to write keys: 0
{code}
The above was from call to System.out.println()

There is LOG field in MultiThreadedWriter which is used in other methods except for waitForFinish()
waitForFinish() should utilize LOG as well."
HBASE-7147	"I worked on a MapReduce to archive some old rows from a table to place
them on another one. I think it can be interesting for some other. So
I'm sharing it here.

Feel free to comment if any update is required, or just close the JIRA
if you think it's not useful enough to be integrated."
HBASE-10190	"Rolling restart is giving some words as hosts names. (I have added some output to the trace below),

{code}
hbase@hbasetest1:~$ time ./bin/rolling-restart.sh --graceful
Gracefully restarting: Command
2013-12-17T15:06:08 Disabling load balancer
2013-12-17 15:06:11,817 INFO  [main] Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
2013-12-17T15:06:15 Previous balancer state was true
2013-12-17T15:06:15 Unloading Command region(s)
2013-12-17 15:06:19,991 INFO  [main] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x6dabf4b connecting to ZooKeeper ensemble=hbasetest1.domain.com:2181
2013-12-17 15:06:20,301 INFO  [main] region_mover: Looking for Command
2013-12-17 15:06:20,302 INFO  [main] region_mover: Checking hbasetest4,60020,1387308979399
2013-12-17 15:06:20,302 INFO  [main] region_mover: getHostnameFromServerName(server) = hbasetest4
2013-12-17 15:06:20,302 INFO  [main] region_mover: Checking hbasetest3,60020,1387308979395
2013-12-17 15:06:20,303 INFO  [main] region_mover: getHostnameFromServerName(server) = hbasetest3
2013-12-17 15:06:20,303 INFO  [main] region_mover: Checking hbasetest2,60020,1387310715952
2013-12-17 15:06:20,303 INFO  [main] region_mover: getHostnameFromServerName(server) = hbasetest2
RuntimeError: Server Command not online
    stripServer at /home/hbase/bin/region_mover.rb:224
  unloadRegions at /home/hbase/bin/region_mover.rb:330
         (root) at /home/hbase/bin/region_mover.rb:484
2013-12-17T15:06:20 Unloaded Command region(s)
2013-12-17T15:06:20 Stopping regionserver
Command: ssh: Could not resolve hostname Command: Name or service not known
2013-12-17T15:06:20 Restarting regionserver
Command: ssh: Could not resolve hostname Command: Name or service not known
2013-12-17T15:06:21 Reloading Command region(s)
{code}"
HBASE-10133	"Noticed in the logs we had lines like this: 

2013-12-11 00:02:00,343 DEBUG org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: currentNbOperations:-1341767084 and seenEntries:0 and size: 0

Maybe this value should be reset after we ship our edits this value should get adjusted.  Either that or convert from an int to a long.  

As this is a jmx metric I feel its important to get this correct."
HBASE-9787	"See HBASE-9775:

Some comment on the retry time limit, we may need to fix it.
It was introduced for server-specific retry fallback, which I hope is not broken by recent changes to HCM. That is the logic where we go to one server, retry, wait, retry, wait more, retry, wait more, then we learn that region went to different server. Here, we don't need to wait, because we can assume by default the different server is healthy; but the old code would carry on with wait sequence.
However, if region moves around (which is common in aggressive CM IT tests), retry count can quickly be exhausted as we go to each new server a few times and never reach higher multipliers. It was especially pronounced w/10 retries, where some request could fail in just a few seconds in case of double server failure where region is recovered twice; w/31-35 now it's probably less pronounced but still possible.
So, the time limit based on original retries is supposed to prevent these fast failures, by allowing the retries to go on for as long as we would have retried ""as if"" we were just using the multiplier sequence to its ""full potential"".
It should not serve as lower limit, we might want to change code to check that both time AND count are exhaused, in this case."
HBASE-8048	"Does anyone use this? Do we advocate it's use for monitoring? We have a proper metrics infra now, so I don't think the latency statistic is produces should be a primary data source. Maybe move it to hbase-examples? At the very least, put it in util package next to all the other tools."
HBASE-9878	"AbstractHBaseTool catches any uncaught exceptions, logs them, and explicitly exists. I believe the reason for this is to explicitly set a non-0 exit code for the process. However, this means one cannot attach a debugger and break on uncaught exceptions."
HBASE-8929	"When running the test repeatedly on the same cluster one can sometimes see unexpected reference count, where the number found is (in the observed case) a multiple of the number expected, so instead ok 2.5m nodes it finds 12.5m, for example. It looks like it's reading the data from the old run. Setup should delete that (not cleanup, as the data may be used for debugging after the test)"
HBASE-10068	"Ran the following:
create 'Person' 'Table', 'Demography'

was expecting the shell would throw a syntax error,however, it created the table with the  name  PersonTable. There is a space between 'Person' and 'Table'.

I've pasted the screen text:

hbase(main):002:0> create 'Person' 'Table', 'Demography'
0 row(s) in 1.2050 seconds

hbase(main):003:0> list
TABLE
PersonTable
1 row(s) in 0.0330 seconds

"
HBASE-9935	"Here's an example:
{code}
KeyValue.createLastOnRow(
          kv.getBuffer(), kv.getRowOffset(), kv.getRowLength(),
          kv.getBuffer(), kv.getFamilyOffset(), kv.getFamilyLength(),
           kv.getBuffer(), kv.getQualifierOffset(), kv.getQualifierLength());
{code}

Looks harmless enough, but that actually recalculates the rowlength 5 times. And each time it needs to decode the rowlength again from the bytes of the KV."
HBASE-9589	"Starting with line 1177:
{code}
          if (diff != 0) {
            if (!littleEndian) {
              return lessThanUnsigned(lw, rw) ? -1 : 1;
            }

            // Use binary search
            int n = 0;
            int y;
            int x = (int) diff;
            if (x == 0) {
              x = (int) (diff >>> 32);
{code}
The value of ""x"" cannot be equal to 0."
HBASE-8589	"Sometimes there's need to write an integration test that only makes sense for large amount of data, cluster, etc. It doesn't make sense to run such test on minicluster on single machine. We need to add an attribute or something like that to indicate that, and make sure mvn verify and IntegrationTestDriver do not run such tests by default, unless name is explicitly specified"
HBASE-9571	"mvn dependency:analyze is strange on this one. You can remove or add the dependency, it won't complain about ""used but undeclared"" or ""undeclared but used"" dependencies. But it does a difference in dependency:tree or for the client application."
HBASE-6657	"After HBASE-6477, Filter is an abstract class, as is FilterBase.  It probably doesn't make much sense to keep both.

See Review Request for more info:
https://reviews.apache.org/r/6670/"
HBASE-9357	"When there's no ZK available, running `bin/hbase rest` must timeout before printing usage information. Initiating a connection should happen after parsing CLI options, not before."
HBASE-6687	"If a clusterkey for a replication peer cluster can not be deserialized, we log a warning and continue. This should probably be a stop the world event, because we can no longer connect to the peer cluster."
HBASE-8793	"hbase-regionserver startup script always returns 0 (exit 0 at the end of the script) this is wrong behaviour which causes issues when trying to recognise true status of the service.
Replacing it with 'exit $?' seems to fix the problem, looking at hbase master return codes are assigned to RETVAL variable which is used with exit.

Not sure if the problem exist in other versions.

> /etc/init.d/hbase-regionserver.orig status
hbase-regionserver is not running.
> echo $?

After fix:

> /etc/init.d/hbase-regionserver status
hbase-regionserver is not running.
> echo $?
1
"
HBASE-8759	"On table with VERSIONS => '1', KEEP_DELETED_CELLS => 'true'. Family Delete Markers does not get purged after put > delete > major compaction (they keep on incrementing after every put > delete > major compaction)

Following is the raw scan output after 10 iterations of put > delete > major compaction.

ROW                                       COLUMN+CELL                                                                                                             
A                                        column=CF:, timestamp=1371512706683, type=DeleteFamily                                                                  
A                                        column=CF:, timestamp=1371512706394, type=DeleteFamily                                                                  
A                                        column=CF:, timestamp=1371512706054, type=DeleteFamily                                                                  
A                                        column=CF:, timestamp=1371512705763, type=DeleteFamily                                                                  
A                                        column=CF:, timestamp=1371512705457, type=DeleteFamily                                                                  
A                                        column=CF:, timestamp=1371512705149, type=DeleteFamily                                                                  
A                                        column=CF:, timestamp=1371512704836, type=DeleteFamily                                                                  
A                                        column=CF:, timestamp=1371512704518, type=DeleteFamily                                                                  
A                                        column=CF:, timestamp=1371512704162, type=DeleteFamily                                                                  
A                                        column=CF:, timestamp=1371512703779, type=DeleteFamily                                                                  
A                                        column=CF:COL, timestamp=1371512706682, value=X 


[~lhofhansl]

Code to repro this issue:
http://phoenix-bin.github.io/client/code/delete.java"
HBASE-2883	"Now that the region servers can heartbeat back to the master as soon as they have a message (region open, close, etc), they also recompute their metrics. I see two problems:

 - The metrics are recomputed way too often, and by the looks of that code it seems quite intense.
 - Our UIs are expecting that the value returned by the metrics computation is per second, but when regions are moving a RS can ping back a few times within a single second. This gives a very wrong view of the load.

I think it's time we move metrics computation in it's own separate thread."
HBASE-2317	"HRS.lockRow doesn't manage exceptions like all the other methods: 

{code}
    try {
      HRegion region = getRegion(regionName);
      Integer r = region.obtainRowLock(row);
      long lockId = addRowLock(r,region);
      LOG.debug(""Row lock "" + lockId + "" explicitly acquired by client"");
      return lockId;
    } catch (Throwable t) {
      throw convertThrowableToIOE(cleanup(t,
        ""Error obtaining row lock (fsOk: "" + this.fsOk + "")""));
    }
{code}

Also it throws a special message and shows if fs is ok, although it already calls checkOpen() at the beginning of that method.

Fix by making it behaving like all the other calls."
HBASE-2188	"In the same spirit as group commit down in HLog, incrementColumnValue should be grouping all requests waiting on the row lock into one in HRegion. This means that 10 clients waiting on the lock should be all summed up and then applied only once."
HBASE-3021	"We've been running replication for a little while now and the tool I wrote to verify replicated data (will be posted in HBASE-3013) works great, unless that data is counters. The reason is that in HRegion.incrementColumnValue we append to the WAL a KV with a timestamp that we're not reusing for the MemStore (since we do some processing on it in updateColumnValue), so the replicated KV is different and comparing sets of rows using time ranges usually fails because one row will almost surely be cut in half on the edges (at least this is what my testing shows).

I guess this is also an issue on the slave cluster since if we don't want values on the same ts on the master cluster, why would we want it elsewhere?"
HBASE-7831	"There are times in the life of many a region server when it realizes that it cannot go on any longer in the hostile, inhospitable environment. Or it might realize that there's something inherently wrong with it on the inside (probably because a developer screwed up). The only way out is killing itself.
There's code in some places currently that does Runtime halt in such cases, and there may be code that calls HRegionServer method to do this.
I think we need some easy-to-access (lightweight interface, or static) way to trigger reliable (no catching exceptions on the upper levels, etc.) RS death in such cases."
HBASE-8499	"{code}
    if (this.in.available() <= 0) {
      this.hasNext = false;
      return this.hasNext;
    }
{code}

Javadoc for available:
{quote}
Returns an estimate of the number of bytes that can be read (or skipped over) from this input stream without blocking by the next invocation of a method for this input stream. The next invocation might be the same thread or another thread. A single read or skip of this many bytes will not block, but may read or skip fewer bytes.

Note that while some implementations of InputStream will return the total number of bytes in the stream, many will not.
{quote}

Not a big deal if we always use it with correct streams now, but rather dangerous."
HBASE-553	"Hadoop is incorporating the bloom filter code that is in HBase to make a BloomFilterMapFile (HADOOP-3063). Once this is committed to Hadoop trunk, the bloom filter code can be removed from HBase."
HBASE-8418	"Looking at the profile seems that Bytes.toShort() is called a lot.
the calls are in ScanQueryMatcher.match() and KeyValue.KeyComparator.compare()

getKeyLength() is already cached, but getRowLength() is not.
"
HBASE-8037	"RegionMovedException is currently thrown on global level, and due to how ProtobufUtil does things, it fails the entire multi-request, see HBASE-8036. RME also doesn't specify the region.
Thus, if it's thrown for one region and there are multiple regions in the request, HCM applies it to all of them, which causes clients to become confused temporarily. We should either fix HBASE-8036 or add region encoded name in the description. "
HBASE-8087	"Pls refer to the comments

https://issues.apache.org/jira/browse/HBASE-7992?focusedCommentId=13600513&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13600513

https://issues.apache.org/jira/browse/HBASE-7992?focusedCommentId=13600780&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13600780

Raised this issue to solve that comments."
HBASE-8046	"HBaseConfTool looks like it could be handy for people who don't feel comfortable digging into ruby/shell and don't have a configuration manager like CM/Ambari.

If no one uses it, we should just delete it.

If we want to keep it around, clean it up to use the Tool interface, GenericOptionParser, etc; change the interface audience to Public, maybe add support for wild-card matching, or at least dumping the entire config; document it in the book."
HBASE-7500	"From https://builds.apache.org/job/HBase-TRUNK/3702/testReport/org.apache.hadoop.hbase/TestNodeHealthCheckChore/testHealthChecker/:
{code}
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:92)
	at org.junit.Assert.assertTrue(Assert.java:43)
	at org.junit.Assert.assertTrue(Assert.java:54)
	at org.apache.hadoop.hbase.TestNodeHealthCheckChore.testHealthChecker(TestNodeHealthCheckChore.java:71)
...
2013-01-06 04:29:23,889 INFO  [pool-1-thread-1] hbase.HealthCheckChore(68): Health status at 377068hrs, 29mins, 23sec : ERROR
Server not healthy
{code}
Here is related code:
{code}
    assertTrue(report.getStatus() == HealthCheckerExitStatus.SUCCESS);
    LOG.info(""Health Status:"" + checker);
{code}
The health status log should be moved before the assertion. I think report should be in the log instead of checker."
HBASE-7805	"If multiple scans to different parts of the same region are executed in parallel, they are processed serially if the table has a region observer coprocessor.
"
HBASE-7792	I don't see any judicial use of these two parameters in the code base. Why not remove them?
HBASE-6776	"For opened region of disabled table, it should be added to online region list, and then closed.  We should not just ignore them.
"
HBASE-1648	"Getting some early experience with the G1 GC.

Running with -Xmx1000m and HBASE_OPTS set to ""-XX:+UnlockExperimentalVMOpts -XX:+UseG1GC"".  Regionserver heap use reports are not of much use:

{noformat}
hbase(main):001:0> status 'simple'
3 live servers
    test3:60020 1247389283042
        requests=0, regions=1, usedHeap=0, maxHeap=41
    test2:60020 1247389219994
        requests=0, regions=1, usedHeap=0, maxHeap=41
    test4:60020 1247389324563
        requests=0, regions=2, usedHeap=0, maxHeap=41
0 dead servers
{noformat}

top is about right:

{noformat}
  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
23988 hadoop    24   0 1492m  98m  10m S  0.0  2.5   0:02.65 java
{noformat}

Incidentally, don't try -XX:+UseG1GC and -XX:+DoEscapeAnalysis together or the JVM will rapidly segfault."
HBASE-7131	"For a table that is re-enabled shortly after it is disabled, regions that are reported to be online are not. This is manifested by a flush attempt throwing a NotServingRegion exception despite all regions from the original table reporting that they are online.

I have a test in place that verifies this flaky behavior. "
HBASE-7090	"This has been there for some time. It is a surprise to find it is reversed actually.  Did I missing anything?

I think it really means to do it if the server is online.

{noformat}
  private void splitLogAndExpireIfOnline(final ServerName sn)
      throws IOException {
    if (sn == null || !serverManager.isServerOnline(sn)) {
      return;
    }
    LOG.info(""Forcing splitLog and expire of "" + sn);
    fileSystemManager.splitLog(sn);
    serverManager.expireServer(sn);
  }
{noformat}"
HBASE-2444	"If a region server that has already been declared dead reports to the master, the master throws LeaseStillHeldException. This is not a very descriptive exception for this case - we should either add a new exception for this purpose, or make a general exception like RegionServerStateException and use a descriptive message."
HBASE-4522	"The motivation for diff is that we want to override some config change for any specific cluster easily by just adding the config entries in the hbase-site-custom.xml for that cluster. This change adds the hbase-site-custom.xml configuration file into HBaseConfiguration.
"
HBASE-4046	"We can add the following statistics to the master. Some stats that can be added are: 
1. number of logs split 
2. size of logs split
3. number of region servers online 
4. number of region servers opened
5. number of region servers expired 
"
HBASE-5691	"I was trying to run importtsv from a servlet. Everytime after the completion of job, the tomcat server was shutdown."
HBASE-4858	"Under Linux with OpenJDK 1.6, using a file:///XX URL in the config file creates a directory called 'file:' in the hbase root directory. If I use a standard Unix absolute path, it works as expected. This may work on other platforms, but it would be good to add a note in the example:

{code}
<?xml version=""1.0""?>
<?xml-stylesheet type=""text/xsl"" href=""configuration.xsl""?>
<configuration>
  <property>
    <name>hbase.rootdir</name>
    <!-- Depending on your platform, this may create a 'file:' directory
         in hbase home instead of the desired behavior. Try using a standard
         platform specific absolute path instead. -->
    <value>file:///DIRECTORY/hbase</value>
  </property>
</configuration>
{code}"
HBASE-5999	AggregationClient throws an exception when the startRow is set and stopRow is not set in scan object. AggregationClient should not throw the exception in this case because the user might want to scan the entire table starting from the startRow.
HBASE-5236	"I can't start HMaster :( 
Please help me.. second day about this error
Exception in thread ""main"" java.lang.RuntimeException: Failed construction of Regionserver: class org.apache.hadoop.hbase.regionserver.HRegionServer

Unable to start master
Has already worked well hadoop cluster installation. Wait 30 sec before start hbase.

I followed this tutorial http://hbase.apache.org/book/example_config.html#d0e2432
Change system configuration in required section ulimit and nproc

Have: 1 master, 4 slaves

Here all diagnostic information

Java java version ""1.6.0_26""
Java(TM) SE Runtime Environment (build 1.6.0_26-b03)
Java HotSpot(TM) 64-Bit Server VM (build 20.1-b02, mixed mode)
Debian 6.03 Linux slave1 2.6.32-5-amd64

Copy hadoop-core to hbase/lib on each machine
hduser@slave1:/usr/local/hbase$ ls lib/hadoo*
lib/hadoop-core-1.0.0.jar

Hbase: hbase-0.90.5

LOG
http://pastie.org/private/s9hjy3hsfebzfltvxee6qa

LOG FROM SLAVE1
http://pastie.org/private/ebzulfseaotfcehz0cqua

LOG FROM ZOOKEPER ON SLAVE1
http://pastie.org/private/y7j11uuhzpebrottsx3ug

HBASE 
hbase-site.xml
http://pastie.org/private/6giqrpeeysidqnjsyimg

hbase-env.xml
http://pastie.org/private/9q0abbvmtsgi4kava8zm9w

regionservers
http://pastie.org/private/eic5waqafmsm73j9h0nra


HADOOP
hadoop-env.sh
http://pastie.org/private/6lmz3xhmcovwag7vmpecjg


core-site.xml
http://pastie.org/private/udskadmow3khd3jqavawww

hdfs-site.xml
http://pastie.org/private/b7g1tkpzukkivhjiwgmsfw


masters and slaves
http://pastie.org/private/l8o0luukig6jnnba6mpryg
"
HBASE-1911	"There are a number of instances of ad hoc versioning of Writables: HTD, HCD, etc. We should convert our classes to inherit VersionedWritable. See http://hadoop.apache.org/common/docs/r0.19.1/api/org/apache/hadoop/io/VersionedWritable.html"
HBASE-4146	"HBase doesn't shut down using hbase-stop.sh on OS X 10.7 (""Lion"") most of the time, though it did work for me once."
HBASE-2240	"I'm running a mapreduce job.  I see a region split and its daughters come on line and then 8 seconds later, master judges the regionserver overloaded and so closes the just-opened region.  This messes up clients.   They may have just picked up the new location for their row query and now its moved again and client has to go hunting anew. 

We need to assign the vintage regions first.

I'm going to change balancer slop.  Its not sloppy enough and balancing cuts in too early."
HBASE-1967	"Testcase: testPutPutScan took 15.822 sec FAILED
expected:<299> but was:<199>

Not sure exactly how the test is supposed to work but it seems that sometimes the two Put are on the same timestamp so the value returned is 199. I will commit a temporary fix to branch in order to release 0.20.2"
HBASE-1313	"2009-04-06 17:34:13,357 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: MSG_REGION_OPEN: urls,,1239039250933
2009-04-06 17:34:13,357 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Worker: MSG_REGION_OPEN: urls,,1239039250933
2009-04-06 17:34:13,358 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Opening region urls,,1239039250933/39686773
2009-04-06 17:34:13,360 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Next sequence id for region urls,,1239039250933 is 0
2009-04-06 17:34:13,360 INFO org.apache.hadoop.hbase.regionserver.HRegion: region urls,,1239039250933/39686773 available
2009-04-06 17:34:13,360 DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitThread: Compaction requested for region urls,,1239039250933/39686773 because: Region open check
2009-04-06 17:34:13,360 INFO org.apache.hadoop.hbase.regionserver.HRegion: starting  compaction on region urls,,1239039250933
2009-04-06 17:34:13,360 DEBUG org.apache.hadoop.hbase.regionserver.Store: 39686773/info: no store files to compact
2009-04-06 17:34:13,361 INFO org.apache.hadoop.hbase.regionserver.HRegion: compaction completed on region urls,,1239039250933 in 0sec
2009-04-06 17:34:16,368 WARN org.apache.hadoop.hbase.regionserver.HRegionServer: Processing message (Retry: 0)
java.io.IOException: java.io.IOException: java.lang.IllegalStateException: Cannot set a region as open if it has not been pending. State: name=urls,,1239039250933, unassigned=true, pendingOpen=false, open=false, closing=false, pendingClose=false, closed=false, offlined=false
	at org.apache.hadoop.hbase.master.RegionManager$RegionState.setOpen(RegionManager.java:1236)
	at org.apache.hadoop.hbase.master.RegionManager.setOpen(RegionManager.java:805)
	at org.apache.hadoop.hbase.master.ServerManager.processRegionOpen(ServerManager.java:524)
	at org.apache.hadoop.hbase.master.ServerManager.processMsgs(ServerManager.java:390)
	at org.apache.hadoop.hbase.master.ServerManager.processRegionServerAllsWell(ServerManager.java:361)
	at org.apache.hadoop.hbase.master.ServerManager.regionServerReport(ServerManager.java:269)
	at org.apache.hadoop.hbase.master.HMaster.regionServerReport(HMaster.java:601)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:632)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:909)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.hadoop.hbase.RemoteExceptionHandler.decodeRemoteException(RemoteExceptionHandler.java:94)
	at org.apache.hadoop.hbase.RemoteExceptionHandler.checkThrowable(RemoteExceptionHandler.java:48)
	at org.apache.hadoop.hbase.RemoteExceptionHandler.checkIOException(RemoteExceptionHandler.java:66)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:493)
	at java.lang.Thread.run(Thread.java:619)
2009-04-06 17:34:16,376 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: MSG_REGION_OPEN: urls,,1239039250933
2009-04-06 17:34:16,376 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Worker: MSG_REGION_OPEN: urls,,1239039250933
"
HBASE-767	HBASE-646 and HBASE-766 are about handlers for the case where store file 'data' goes missing.  This issue is about figuring why/how they disappear
HBASE-29	"Ok, this one is a little tricky. Let's say that you write a row with some value without a timestamp, thus meaning right now. Then, the memcache gets flushed out to a MapFile. Then, you write another value to the same row, this time with a timestamp that is in the past, ie, before the ""now"" timestamp of the first put. 

Some time later, but before there is a compaction, if you do a get for this row, and only ask for a single version, you will logically be expecting the latest version of the cell, which you would assume would be the one written at ""now"" time. Instead, you will get the value written into the ""past"" cell, because even though it is tagged as having happened in the past, it actually *was written* after the ""now"" cell, and thus when #get searches for satisfying values, it runs into the one most recently written first. 

The result of this problem is inconsistent data results. Note that this problem only ever exists when there's an uncompacted HStore, because during compaction, these cells will all get sorted into the correct order by timestamp and such. In a way, this actually makes the problem worse, because then you could easily get inconsistent results from HBase about the same (unchanged) row depending on whether there's been a flush/compaction.

The only solution I can think of for this problem at the moment is to scan all the MapFiles and Memcache for possible results, sort them, and then select the desired number of versions off of the top. This is unfortunate because it means you never get the snazzy shortcircuit logic except within a single mapfile or memcache. "
HBASE-58	"Currently, the only way to tell what is really going on with an HBase cluster is to enable DEBUG level logging. Unfortunately, this also generates a lot of 'noise' messages. We need to review log messages and see which DEBUG messages should be promoted to INFO and if any current INFO messages should be demoted to debug.

In addition, some messages are very verbose and don't really need to be. This should be fixed too.

A good starting point for review would be to look at the output from test-contrib. Although that is not everything, it is a place to start working from."
HBASE-2005	"The hbase shell has 'delete' and 'deleteall' for deleting a cell/cells for a given row.

I don't see any equivalent of the RDBMS equivalent of 'truncate' that would delete all the rows in a table, however (i.e., based on my understanding of the shell, 'deleteall' applies horizontally, but not vertically).

At the very least, this could be useful for development purposes when you want to quickly remove test data from a table.  

Nice to have - something like:

truncate mytable    (no column families specified...  so it removes all the rows from every column family)

truncate mytable family1    (remove all rows from table 'mytable' but only for column family 'family1').

Something like that...  just a suggestion.

"
HBASE-509	"A couple of times during an upload, hdfs complains it is corrupt.  Complaint is as following:

{code}
/hbase/XX.XX.XX-2.u.powerset.com/log_XX.XX.XX.92_1205384328364_60020/hlog.dat.025:  Replica placement policy is violated for blk_2712323855504360379. Block should be additionally replicated on 2 more rack(s).
/hbase/XX.XX.XX-2.u.powerset.com/log_XX.XX.XX.92_1205384328364_60020/hlog.dat.025: MISSING 1 blocks of total size 0 B.
{code}

Now the odd thing is that the next time I do a fsck, I see that log number its complaining about for the above server has increased inline with a new file just rolled as in:

{code}
......92_1205384328364_60020/hlog.dat.026:  Replica placement policy is violated for blk_4062204433046618058. Block should be additionally replicated on 2 more rack(s).
/hbase/aa0-005-2.u.powerset.com/log_XX.XX.XX.92_1205384328364_60020/hlog.dat.026: MISSING 1 blocks of total size 0 B.
{code}

Its no longer complaining about hlog.dat.025.  If I do an fsck on the file hlog.dat.020, it says its healthy, replicated 7M file.

Likely an hdfs issue.  Or its the way we're doing our logging? Restart reports cluster HEALTHY (didn't run fsck with remove 'bad' blocks or files).



"
HBASE-968	"I get a java.lang.ArrayIndexOutOfBoundsException from cell iterator()
It has a logic error on the count

"
HBASE-866	"I've been testing running biggish MR jobs uploading into hbase.  My jobs consistently fail with child task timing out its ten minute period.  Adding logging, was able to see that we're actual stuck in a commit.  Following the thread of the row we're committing, I see this in the log:

{code}
...
2008-09-03 18:37:03,446 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Flush requested on TestTable,0029377106,1220466998108
2008-09-03 18:37:03,446 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memcache flush for region TestTable,0029377106,1220466998108. Current region memcache size 64.0m
2008-09-03 18:37:03,446 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 1 on 60020' on region TestTable,0029377106,1220466998108: Memcache size 64.0m is >= than blocking 64.0m size
2008-09-03 18:37:13,450 INFO org.apache.hadoop.hbase.regionserver.HRegion: Unblocking updates for region TestTable,0029377106,1220466998108 'IPC Server handler 1 on 60020'
2008-09-03 18:37:16,089 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 16 on 60020' on region TestTable,0029377106,1220466998108: Memcache size 64.0m is >= than blocking 64.0m size
2008-09-03 18:37:16,090 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 1 on 60020' on region TestTable,0029377106,1220466998108: Memcache size 64.0m is >= than blocking 64.0m size
2008-09-03 18:37:16,090 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 4 on 60020' on region TestTable,0029377106,1220466998108: Memcache size 64.0m is >= than blocking 64.0m size
2008-09-03 18:37:16,090 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 6 on 60020' on region TestTable,0029377106,1220466998108: Memcache size 64.0m is >= than blocking 64.0m size
2008-09-03 18:37:16,090 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 2 on 60020' on region TestTable,0029377106,1220466998108: Memcache size 64.0m is >= than blocking 64.0m size
2008-09-03 18:37:16,090 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 12 on 60020' on region TestTable,0029377106,1220466998108: Memcache size 64.0m is >= than blocking 64.0m size
2008-09-03 18:37:16,090 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 9 on 60020' on region TestTable,0029377106,1220466998108: Memcache size 64.0m is >= than blocking 64.0m size
2008-09-03 18:37:16,091 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 7 on 60020' on region TestTable,0029377106,1220466998108: Memcache size 64.0m is >= than blocking 64.0m size
2008-09-03 18:37:21,984 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region TestTable,0029377106,1220466998108 in 18538ms, sequence id=2852547, compaction requested=false
2008-09-03 18:47:06,241 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memcache flush for region TestTable,0029377106,1220466998108. Current region memcache size 64.0m
2008-09-03 18:47:10,031 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished memcache flush for region TestTable,0029377106,1220466998108 in 3790ms, sequence id=2919208, compaction requested=true
2008-09-03 18:47:10,031 INFO org.apache.hadoop.hbase.regionserver.HRegion: Unblocking updates for region TestTable,0029377106,1220466998108 'IPC Server handler 9 on 60020'
2008-09-03 18:47:10,031 DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitThread: Compaction requested for region: TestTable,0029377106,1220466998108
2008-09-03 18:47:10,031 INFO org.apache.hadoop.hbase.regionserver.HRegion: Unblocking updates for region TestTable,0029377106,1220466998108 'IPC Server handler 12 on 60020'
2008-09-03 18:47:10,032 INFO org.apache.hadoop.hbase.regionserver.HRegion: starting compaction on region TestTable,0029377106,1220466998108
2008-09-03 18:47:10,032 INFO org.apache.hadoop.hbase.regionserver.HRegion: Unblocking updates for region TestTable,0029377106,1220466998108 'IPC Server handler 7 on 60020'
2008-09-03 18:47:10,035 INFO org.apache.hadoop.hbase.regionserver.HRegion: Unblocking updates for region TestTable,0029377106,1220466998108 'IPC Server handler 6 on 60020'
2008-09-03 18:47:10,035 INFO org.apache.hadoop.hbase.regionserver.HRegion: Unblocking updates for region TestTable,0029377106,1220466998108 'IPC Server handler 4 on 60020'
2008-09-03 18:47:10,035 INFO org.apache.hadoop.hbase.regionserver.HRegion: Unblocking updates for region TestTable,0029377106,1220466998108 'IPC Server handler 2 on 60020'
2008-09-03 18:47:10,037 INFO org.apache.hadoop.hbase.regionserver.HRegion: Unblocking updates for region TestTable,0029377106,1220466998108 'IPC Server handler 16 on 60020'
2008-09-03 18:47:10,043 INFO org.apache.hadoop.hbase.regionserver.HRegion: Unblocking updates for region TestTable,0029377106,1220466998108 'IPC Server handler 1 on 60020'
2008-09-03 18:47:18,403 INFO org.apache.hadoop.hbase.regionserver.HRegion: compaction completed on region TestTable,0029377106,1220466998108 in 8sec
...
{code}

Notice how we're blocked for ten minutes until new flush runs.  My guess is that the flush that is going on concurrent with the blocking is clearing the flag "
HBASE-828	"Process:

One the 1st run of the application, i use tableExists( new Text(""TestTable5"")) & tableCreate to create the table.

On the 2nd run, I call tableExists(new Text(""TestTable5"")), which returns false, then i call disableTable, deleteTable, and finally createTable.


Workaround for now is to avoid using Text.   tableExists(byte[]) works when i tested it.

To test this, you have to run the program 2 times.  Once to create the table that doesnt exist, then the 2nd time to see the error.
"
HBASE-777	"Using the old shell I was able to use CREATE TABLE to create a table named 'RelatedAlbums' and then access it from Java using new HTable(conf, new Text(""RelatedAlbums"")).

Using the new shell, trying to use 'create' from the irb shell and then that same constructor results in the table not being found."
HBASE-760	"HCD creation in HBase.rb was brittle and nonfunctional. 

[hadoop@sjdc-atr-dns tmp]$ hbase shell --master=10.30.94.1:60000
(eval):1 warning: already initialized constant MEMCACHE_FLUSHSIZE
(eval):1 warning: already initialized constant IN_MEMORY
(eval):1 warning: already initialized constant VERSIONS
HBase Shell; enter 'help<RETURN>' for list of supported commands.
Version: 0.2.0-dev, r678650, Tue Jul 22 06:15:47 UTC 2008
hbase(main):001:0> create 'content', {NAME => 'url', VERSIONS => 1, TTL => 2592000}, {NAME => 'info', VERSIONS => 1, TTL => 2592000, BLOCKCACHE => 'true'}, {NAME => 'content', VERSIONS => 1, COMPRESSION => 'RECORD', TTL => 2592000,BLOCKCACHE => 'true' }
NameError: no constructor with arguments matching [class [B, class java.lang.Long, class org.apache.hadoop.hbase.HColumnDescriptor$CompressionType, class java.l
ang.Boolean, class java.lang.String, class java.lang.Long, class java.lang.Long,
 class java.lang.Boolean] on object JavaUtilities
        from file:/opt/hadoop-0.17.1/contrib/hbase/lib/jruby-complete-1.1.2.jar!
/builtin/javasupport/proxy/concrete.rb:23:in `__jcreate!'
        from file:/opt/hadoop-0.17.1/contrib/hbase/lib/jruby-complete-1.1.2.jar!
/builtin/javasupport/proxy/concrete.rb:23:in `initialize'
        from file:/opt/hadoop-0.17.1/contrib/hbase/lib/jruby-complete-1.1.2.jar!
/builtin/javasupport/proxy/concrete.rb:6:in `new'
        from file:/opt/hadoop-0.17.1/contrib/hbase/lib/jruby-complete-1.1.2.jar!
/builtin/javasupport/proxy/concrete.rb:6:in `new'
        from /opt/hadoop/contrib/hbase/bin/../bin/HBase.rb:156:in `hcd'
        from /opt/hadoop/contrib/hbase/bin/../bin/HBase.rb:112:in `create'
        from /opt/hadoop/contrib/hbase/bin/../bin/HBase.rb:106:in `each'
        from /opt/hadoop/contrib/hbase/bin/../bin/HBase.rb:106:in `create'
        from /opt/hadoop/contrib/hbase/bin/../bin/hirb.rb:223:in `create'
        from wtp.create.rb:3:in `binding'
"
HBASE-657	"If an invalid column name is passed to the Thrift server, the InvalidColumnNameException is not caught and turned into a Thrift exception. Debug output:

08/05/29 20:34:30 DEBUG thrift.ThriftServer$HBaseHandler: get: table=test_table, row=todd, col=foobar
08/05/29 20:34:30 DEBUG hbase.HTable: reloading table servers because: org.apache.hadoop.hbase.InvalidColumnNameException: foobar is missing the colon family/qualifier separator
        at org.apache.hadoop.hbase.HStoreKey.getColonOffset(HStoreKey.java:335)
        at org.apache.hadoop.hbase.HStoreKey.extractFamily(HStoreKey.java:295)
        at org.apache.hadoop.hbase.HRegion.checkColumn(HRegion.java:1676)
        at org.apache.hadoop.hbase.HRegion.get(HRegion.java:1191)
        at org.apache.hadoop.hbase.HRegion.get(HRegion.java:1154)
        at org.apache.hadoop.hbase.HRegionServer.get(HRegionServer.java:1402)
        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:413)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:901)

08/05/29 20:34:40 DEBUG hbase.HTable: reloading table servers because: org.apache.hadoop.hbase.InvalidColumnNameException: foobar is missing the colon family/qualifier separator
        at org.apache.hadoop.hbase.HStoreKey.getColonOffset(HStoreKey.java:335)
        at org.apache.hadoop.hbase.HStoreKey.extractFamily(HStoreKey.java:295)
        at org.apache.hadoop.hbase.HRegion.checkColumn(HRegion.java:1676)
        at org.apache.hadoop.hbase.HRegion.get(HRegion.java:1191)
        at org.apache.hadoop.hbase.HRegion.get(HRegion.java:1154)
        at org.apache.hadoop.hbase.HRegionServer.get(HRegionServer.java:1402)
        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:413)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:901)

08/05/29 20:34:50 DEBUG hbase.HTable: reloading table servers because: org.apache.hadoop.hbase.InvalidColumnNameException: foobar is missing the colon family/qualifier separator
        at org.apache.hadoop.hbase.HStoreKey.getColonOffset(HStoreKey.java:335)
        at org.apache.hadoop.hbase.HStoreKey.extractFamily(HStoreKey.java:295)
        at org.apache.hadoop.hbase.HRegion.checkColumn(HRegion.java:1676)
        at org.apache.hadoop.hbase.HRegion.get(HRegion.java:1191)
        at org.apache.hadoop.hbase.HRegion.get(HRegion.java:1154)
        at org.apache.hadoop.hbase.HRegionServer.get(HRegionServer.java:1402)
        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:413)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:901)

08/05/29 20:35:00 DEBUG hbase.HTable: reloading table servers because: org.apache.hadoop.hbase.InvalidColumnNameException: foobar is missing the colon family/qualifier separator
        at org.apache.hadoop.hbase.HStoreKey.getColonOffset(HStoreKey.java:335)
        at org.apache.hadoop.hbase.HStoreKey.extractFamily(HStoreKey.java:295)
        at org.apache.hadoop.hbase.HRegion.checkColumn(HRegion.java:1676)
        at org.apache.hadoop.hbase.HRegion.get(HRegion.java:1191)
        at org.apache.hadoop.hbase.HRegion.get(HRegion.java:1154)
        at org.apache.hadoop.hbase.HRegionServer.get(HRegionServer.java:1402)
        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:413)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:901)

08/05/29 20:35:10 DEBUG hbase.HTable: Trying to contact region server for row 'todd', but failed after 5 attempts.
Exception 1:
org.apache.hadoop.hbase.InvalidColumnNameException: org.apache.hadoop.hbase.InvalidColumnNameException: foobar is missing the colon family/qualifier separator
        at org.apache.hadoop.hbase.HStoreKey.getColonOffset(HStoreKey.java:335)
        at org.apache.hadoop.hbase.HStoreKey.extractFamily(HStoreKey.java:295)
        at org.apache.hadoop.hbase.HRegion.checkColumn(HRegion.java:1676)
        at org.apache.hadoop.hbase.HRegion.get(HRegion.java:1191)
        at org.apache.hadoop.hbase.HRegion.get(HRegion.java:1154)
        at org.apache.hadoop.hbase.HRegionServer.get(HRegionServer.java:1402)
        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:413)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:901)
Exception 1:
org.apache.hadoop.hbase.InvalidColumnNameException: org.apache.hadoop.hbase.InvalidColumnNameException: foobar is missing the colon family/qualifier separator
        at org.apache.hadoop.hbase.HStoreKey.getColonOffset(HStoreKey.java:335)
        at org.apache.hadoop.hbase.HStoreKey.extractFamily(HStoreKey.java:295)
        at org.apache.hadoop.hbase.HRegion.checkColumn(HRegion.java:1676)
        at org.apache.hadoop.hbase.HRegion.get(HRegion.java:1191)
        at org.apache.hadoop.hbase.HRegion.get(HRegion.java:1154)
        at org.apache.hadoop.hbase.HRegionServer.get(HRegionServer.java:1402)
        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:413)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:901)
Exception 1:
org.apache.hadoop.hbase.InvalidColumnNameException: org.apache.hadoop.hbase.InvalidColumnNameException: foobar is missing the colon family/qualifier separator
        at org.apache.hadoop.hbase.HStoreKey.getColonOffset(HStoreKey.java:335)
        at org.apache.hadoop.hbase.HStoreKey.extractFamily(HStoreKey.java:295)
        at org.apache.hadoop.hbase.HRegion.checkColumn(HRegion.java:1676)
        at org.apache.hadoop.hbase.HRegion.get(HRegion.java:1191)
        at org.apache.hadoop.hbase.HRegion.get(HRegion.java:1154)
        at org.apache.hadoop.hbase.HRegionServer.get(HRegionServer.java:1402)
        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:413)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:901)
Exception 1:
org.apache.hadoop.hbase.InvalidColumnNameException: org.apache.hadoop.hbase.InvalidColumnNameException: foobar is missing the colon family/qualifier separator
        at org.apache.hadoop.hbase.HStoreKey.getColonOffset(HStoreKey.java:335)
        at org.apache.hadoop.hbase.HStoreKey.extractFamily(HStoreKey.java:295)
        at org.apache.hadoop.hbase.HRegion.checkColumn(HRegion.java:1676)
        at org.apache.hadoop.hbase.HRegion.get(HRegion.java:1191)
        at org.apache.hadoop.hbase.HRegion.get(HRegion.java:1154)
        at org.apache.hadoop.hbase.HRegionServer.get(HRegionServer.java:1402)
        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Server.call(HbaseRPC.java:413)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:901)
"
HBASE-591	"My timezone is in UTC+8, but HBase's log uses PST time. It makes me a little annoying when reading the logs. i think it should either use UTC or localtime in log which makes reading logs more easy."
HBASE-432	"no matter where I tell the scanner to start it always starts with this key
%20%20.gr.counseling.www/greekatalog/index.php-c=481.htm:http
"
HBASE-55	"We would like the master's region assignment function to take into account more factors when choosing where to assign regions.
 
- More advanced accounting of load on regionserver - memory, # requests, etc
- Don't deploy both daughter regions to the same regionserver
- Assign regions where the underlying DFS blocks are hosted if possible

Please add additional ideas in comments as they come up."
HBASE-253	"I get this on my region servers logs sometimes when I shutdown the cluster it repeats several times sometimes trying to find the master
I thank we need to look at the master and make sure we do not stop the master on exit before all region servers report down.

I am not sure if there could be data loss or not but we should not leave region servers looking for the master unless it has failed on it own.

{code}
2008-01-18 17:40:42,009 WARN org.apache.hadoop.hbase.HRegionServer: Failed to send exiting message to master:
java.net.ConnectException: Connection refused
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
        at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
        at java.net.Socket.connect(Socket.java:519)
        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:159)
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:575)
        at org.apache.hadoop.ipc.Client.call(Client.java:498)
        at org.apache.hadoop.hbase.ipc.HbaseRPC$Invoker.invoke(HbaseRPC.java:210)
        at $Proxy0.regionServerReport(Unknown Source)
        at org.apache.hadoop.hbase.HRegionServer.run(HRegionServer.java:898)
        at java.lang.Thread.run(Thread.java:595)
{code}"
HBASE-213	"From the command-line, list all tables:

{code}
./src/contrib/hbase/bin/hbase client listTables
{code}

Delete a table:

{code}
./src/contrib/hbase/bin/hbase client deleteTable testStringCursor
{code}

List tables again. Notice how the 'deleted' table still shows.  Even after restart, its still present."
HBASE-149	"After the JAR command runs successfully, it kills the shell, but any exceptions cause the shell to remain open.

Instead the shell should remain open in all cases so that it is not necessary to relaunch the shell to rerun the JAR.
"
HBASE-106	"below shows a memcache size of 64M but when you add up the regions I get
149.3K =  152883.2B
153B = 153B
153B = 153B
3.8M = 3984588.8B
2.6M = 2726297.6B
153B = 153B
129K = 132096B
2.6M = 2726297.6B
153B = 153B

total  = 9569891.4B = 9.12M not 64M

{code}
2008-01-21 23:51:30,812 DEBUG org.apache.hadoop.hbase.HRegion: Started memcache flush for region webdata,,1200970130265. Size 64.0m
2008-01-21 23:51:31,694 DEBUG org.apache.hadoop.hbase.HStore: Added 540402979/size/2337100008775916241 with 11004 entries, sequence id 32535827, and size 149.3k for 540402979/size
2008-01-21 23:51:31,956 DEBUG org.apache.hadoop.hbase.HStore: Added 540402979/rank_total/6511354828803004572 with 0 entries, sequence id 32535827, and size 153.0 for 540402979/rank_total
2008-01-21 23:51:32,210 DEBUG org.apache.hadoop.hbase.HStore: Added 540402979/first_seen/116626538562002170 with 0 entries, sequence id 32535827, and size 153.0 for 540402979/first_seen
2008-01-21 23:51:40,084 DEBUG org.apache.hadoop.hbase.HStore: Added 540402979/anchor/5635668875929221096 with 189726 entries, sequence id 32535827, and size 3.8m for 540402979/anchor
2008-01-21 23:51:48,134 DEBUG org.apache.hadoop.hbase.HStore: Added 540402979/last_seen/5152520466951569025 with 189726 entries, sequence id 32535827, and size 2.6m for 540402979/last_seen
2008-01-21 23:51:48,400 DEBUG org.apache.hadoop.hbase.HStore: Added 540402979/source/7366562139108336733 with 0 entries, sequence id 32535827, and size 153.0 for 540402979/source
2008-01-21 23:51:49,075 DEBUG org.apache.hadoop.hbase.HStore: Added 540402979/stime/5227996581170475543 with 11004 entries, sequence id 32535827, and size 129.6k for 540402979/stime
2008-01-21 23:51:55,944 DEBUG org.apache.hadoop.hbase.HStore: Added 540402979/in_rank/7235419568042410782 with 189726 entries, sequence id 32535827, and size 2.6m for 540402979/in_rank
2008-01-21 23:51:56,217 DEBUG org.apache.hadoop.hbase.HStore: Added 540402979/rank_visual/5975562391477267376 with 0 entries, sequence id 32535827, and size 153.0 for 540402979/rank_visual
2008-01-21 23:51:56,217 DEBUG org.apache.hadoop.hbase.HRegion: Finished memcache flush for region webdata,,1200970130265 in 25405ms, sequenceid=32535827
{code}
"
HBASE-18088	"I just noticed the following warns.
{code}
[WARNING] Some problems were encountered while building the effective model for org.apache.hbase:hbase-resource-bundle:jar:2.0.0-SNAPSHOT
[WARNING] 'reporting.plugins.plugin.version' for org.scala-tools:maven-scala-plugin is missing. @ org.apache.hbase:hbase:2.0.0-SNAPSHOT, /home/chia7712/apache/hbase/pom.xml, line 3212, column 15
[WARNING]
[WARNING] Some problems were encountered while building the effective model for org.apache.hbase:hbase-server:jar:2.0.0-SNAPSHOT
[WARNING] 'reporting.plugins.plugin.version' for org.scala-tools:maven-scala-plugin is missing. @ org.apache.hbase:hbase:2.0.0-SNAPSHOT, /home/chia7712/apache/hbase/pom.xml, line 3212, column 15
[WARNING]
[WARNING] Some problems were encountered while building the effective model for org.apache.hbase:hbase-thrift:jar:2.0.0-SNAPSHOT
[WARNING] 'reporting.plugins.plugin.version' for org.scala-tools:maven-scala-plugin is missing. @ org.apache.hbase:hbase:2.0.0-SNAPSHOT, /home/chia7712/apache/hbase/pom.xml, line 3212, column 15
[WARNING]
[WARNING] Some problems were encountered while building the effective model for org.apache.hbase:hbase-shell:jar:2.0.0-SNAPSHOT
[WARNING] 'reporting.plugins.plugin.version' for org.scala-tools:maven-scala-plugin is missing. @ org.apache.hbase:hbase:2.0.0-SNAPSHOT, /home/chia7712/apache/hbase/pom.xml, line 3212, column 15
[WARNING]
[WARNING] Some problems were encountered while building the effective model for org.apache.hbase:hbase-protocol-shaded:jar:2.0.0-SNAPSHOT
[WARNING] 'reporting.plugins.plugin.version' for org.scala-tools:maven-scala-plugin is missing. @ org.apache.hbase:hbase:2.0.0-SNAPSHOT, /home/chia7712/apache/hbase/pom.xml, line 3212, column 15
{code}
We should add the version of the plugin to eliminate the warning.
{code:title=pom.xml#3212|borderStyle=solid}
      <plugin>
        <groupId>org.scala-tools</groupId>
        <artifactId>maven-scala-plugin</artifactId>
      </plugin>
{code}
"
HBASE-17591	"As consequence of HBASE-12577, `hbase.master.distributed.log.replay` is no longer default true. But in the documentation, it is still noted as default true as follows:

{quote}
To enable distributed log replay, set hbase.master.distributed.log.replay to true. This will be the default for HBase 0.99 (HBASE-10888).
{quote}"
HBASE-17843	"Junit test sometimes failed in TestRegionReplicaFailover.java, so I changed the testPrimaryRegionKill method  test timeout  to 240000ms, and add sleep 5000ms for verify result. 
error logs:
Tests run: 6, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 285.221 sec <<< FAILURE! - in org.apache.hadoop.hbase.regionserver.TestRegionReplicaFailover
testPrimaryRegionKill[0](org.apache.hadoop.hbase.regionserver.TestRegionReplicaFailover)  Time elapsed: 125.963 sec  <<< ERROR!
org.junit.runners.model.TestTimedOutException: test timed out after 120000 milliseconds
at java.lang.Object.wait(Native Method)
	at java.lang.Object.wait(Object.java:460)
	at java.util.concurrent.TimeUnit.timedWait(TimeUnit.java:348)
	at org.apache.hadoop.hbase.client.ResultBoundedCompletionService.pollForSpecificCompletedTask(ResultBoundedCompletionService.java:258)
	at org.apache.hadoop.hbase.client.ResultBoundedCompletionService.pollForFirstSuccessfullyCompletedTask(ResultBoundedCompletionService.java:214)
	at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.call(RpcRetryingCallerWithReadReplicas.java:209)
	at org.apache.hadoop.hbase.client.HTable.get(HTable.java:428)
	at org.apache.hadoop.hbase.client.HTable.get(HTable.java:392)
	at org.apache.hadoop.hbase.HBaseTestingUtility.verifyNumericRows(HBaseTestingUtility.java:2197)
	at org.apache.hadoop.hbase.regionserver.TestRegionReplicaFailover.verifyNumericRowsWithTimeout(TestRegionReplicaFailover.java:227)
	at org.apache.hadoop.hbase.regionserver.TestRegionReplicaFailover.testPrimaryRegionKill(TestRegionReplicaFailover.java:200)"
HBASE-16113	"Step 1: put a cell
Step 2: delete the cell
Step 3: Scan#setRaw(true).setMaxVersions(0)
Step 4: Result result = table.getScanner(scan).next()
Step 5: The result will contain a Cell of DELETE type
Is it a correct result ? The critical code is shown below:
{code:title=ScanWildcardColumnTracker.java|borderStyle=solid}
// May be we should check the verion first
  private MatchCode checkVersion(byte type, long timestamp) {
    if (!CellUtil.isDelete(type)) {
      currentCount++;
    }
    if (currentCount > maxVersions) {
      return ScanQueryMatcher.MatchCode.SEEK_NEXT_COL; // skip to next col
    }
    // keep the KV if required by minversions or it is not expired, yet
    if (currentCount <= minVersions || !isExpired(timestamp)) {
      setTSAndType(timestamp, type);
      return ScanQueryMatcher.MatchCode.INCLUDE;
    } else {
      return MatchCode.SEEK_NEXT_COL;
    }

  }
{code}
Comments?
Thanks"
HBASE-2010	"In hadoop projects the jar-test task is used to compile the test jars. 
In hbase this task is called compile-core-test. 

Shouldn't we have a jar-test task that depends on the compile-core-test?"
HBASE-10777	"Sometimes the embedded thrift server dies, but the regionserver is still alive. This make the regions living on that regionserver available via the thrift client. This JIRA is to restart the embedded thrift in such scenarios."
HBASE-11840	"Formally the HRegionServer's address was printed in the constructor even though the port was actually set only in initialize(). For example when the port supplied in the conf is 0, then a random open one is allocated upon socket initialization, and that's the one which needs to be printed."
HBASE-11931	Avoid copy/paste in RegionServerCoprocessorHost methods like we did with HBASE-11733  
HBASE-3926	The count help for example shows the call with no curly braces. We should make them all look like the one recommended on the main help screen. Just to keep it coherent.
HBASE-8095	"Failed for me while my machine was under load: Failed tests:   testBackoffLogic(org.apache.hadoop.hbase.client.TestSnapshotFromAdmin): Elapsed time:4119 is more than expected max:4050

Looks like it uses system time."
HBASE-8268	HBASE-4894 removed sbin from tarball for 0.92 release. I think the 'mess' Stack referred to in jira hasn't been taken care of in 0.94. So we should have HBASE-4894 fix in 0.94
HBASE-8562	"StoreFileScanners open readers to read the store file. However, these readers are not closed upon StoreFileScanner.close().

This should be closed at the end of the compaction."
HBASE-8623	"Got this running Jeffrey's fancy new IntegrationTestDataIngestWithChaosMonkey

{code}
2013-05-25 14:12:07,972 WARN  [Thread-4] util.ChaosMonkey: Exception occured during performing action: java.lang.IllegalArgumentException: n must be positive
	at java.util.Random.nextInt(Random.java:250)
	at org.apache.hadoop.hbase.util.ChaosMonkey.selectRandomItem(ChaosMonkey.java:595)
	at org.apache.hadoop.hbase.util.ChaosMonkey$RestartRandomRs.perform(ChaosMonkey.java:275)
	at org.apache.hadoop.hbase.util.ChaosMonkey$PeriodicRandomActionPolicy.runOneIteration(ChaosMonkey.java:576)
	at org.apache.hadoop.hbase.util.ChaosMonkey$PeriodicPolicy.run(ChaosMonkey.java:488)
	at java.lang.Thread.run(Thread.java:662)
{code}"
HBASE-5339	"Add support that you can combine some columns from the TSV with either a given separator, no separator, or with a custom row key generator class. Syntax could be:

{code}
-Dimporttsv.columns=HBASE_ROW_KEY_1,HBASE_ROW_KEY_2,cf1:col1,cf2:col3,HBASE_ROW_KEY_3
-Dimporttsv.rowkey.separator=""-""
{code}

Another option of course is using the custom mapper class and handle this there, but this also seems like a nice to have option, probably often covering the 80% this sort of thing is needed."
HBASE-8679	" boolean shouldFlush = true;
      if (request.hasIfOlderThanTs()) {
        shouldFlush = region.getLastFlushTime() < request.getIfOlderThanTs();
      }
If shouldFlush is false should add a log to say that flush did not happen for what reason. Better for debugging."
HBASE-10626	"since the hadoop-2.2.0 need a reverse look up about hostname and our dns doesn't support the reverse look up,  we configured the ""/etc/hosts"" of namenode, This lead a bug when we started hbase master in the same machine of namenode:
the ""HMaster"" would get a ""ServerName"" which consisted of hostname,port,ts and get another ""ServerName"" from zookeeper consisted of ip,port,ts, One regionserver will be recorded twice.
"
HBASE-11269	"Currently RegionPlacement use LOG.info to output the results of""print"" command. The results will be sent to stderr. This is not a common design for linux command because then it's hard to redirect the results to e.g. grep or less."
HBASE-11213	"At line 175:
{code}
    RestoreMetaChanges metaChanges = new RestoreMetaChanges(parentsMap);
{code}
Access to parentsMap should be synchronized."
HBASE-11184	"when i read Store close() method i found  the thread pool for closing store files  with only one Thread.
why initialize the thread pool with only one thread for closing store files?"
HBASE-8926	"Exporting default GC options in HBASE_OPTS from hbase-env.sh by default can interfere with some deployment scenarios. We probably shouldn't be doing that.

Noticed this when using scripts to set up EC2 test clusters for G1 GC. Having ""-XX:+UseConcMarkSweepGC"" on the command line in that case will cause the JVMs to fail to launch."
HBASE-11141	"{code}
      globalCache = newCache;
      mtime++;
{code}
mtime is a long. The above increment doesn't have protection against intervening update in another thread."
HBASE-9881	"Currently, there seems two call paths to determine the value of _hbase.root.logger_ in log4j.properties file.

1. hbase -> hbase-config.sh -> hbase-env.sh (set _HBASE_ROOT_LOGGER_ if any) -> hbase (set default value to HBASE_ROOT_LOGGER if no variable declared)

2. hbase-daemon.sh -> hbase-config.sh -> hbase-env.sh (set _HBASE_ROOT_LOGGER_ if any) -> hbase-daemon.sh (set default value to HBASE_ROOT_LOGGER if no variable declared)

We found an issue at call path#1, while using 'bin/hbase' the original +console output will redirect to the log file+ if the _HBASE_ROOT_LOGGER_ enabled in hbase-env.sh

for example
1. we use 'bin/hbase zkcli' to connect to zookeeper, will see following log output to the console...
{noformat}
# bin/hbase zkcli
Connecting to scottm-hbase-1.lab:2181
2013-11-04 08:33:39,855 INFO  [main] zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.5-1392090, built on 09/30/2012 17:52 GMT
2013-11-04 08:33:39,855 INFO  [main] zookeeper.ZooKeeper: Client environment:host.name=scottm-hbase-1.lab
2013-11-04 08:33:39,856 INFO  [main] zookeeper.ZooKeeper: Client environment:java.version=1.6.0_26
2013-11-04 08:33:39,856 INFO  [main] zookeeper.ZooKeeper: Client environment:java.vendor=Sun Microsystems Inc.
2013-11-04 08:33:39,856 INFO  [main] zookeeper.ZooKeeper: Client environment:java.home=/usr/java/jdk1.6.0_26/jre
2013-11-04 08:33:39,856 INFO  [main] zookeeper.ZooKeeper: Client environment:java.class.path=/opt/hbase/bin/../conf:/usr/java/jdk1.6.0_26/lib/tools.jar:/opt/hbase/bin/..:/opt/hbase/bin/../lib/activation-1.1.jar:/opt/hbase/bin/../lib/asm-3.1.jar:/opt/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/opt/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/opt/hbase/bin/../lib/commons-cli-1.2.jar:/opt/hbase/bin/../lib/commons-codec-1.7.jar:/opt/hbase/bin/../lib/commons-collections-3.2.1.jar:/opt/hbase/bin/../lib/commons-configuration-1.6.jar:/opt/hbase/bin/../lib/commons-digester-1.8.jar:/opt/hbase/bin/../lib/commons-el-1.0.jar:/opt/hbase/bin/../lib/commons-httpclient-3.1.jar:/opt/hbase/bin/../lib/commons-io-2.4.jar:/opt/hbase/bin/../lib/commons-lang-2.6.jar:/opt/hbase/bin/../lib/commons-logging-1.1.1.jar:/opt/hbase/bin/../lib/commons-math-2.2.jar:/opt/hbase/bin/../lib/commons-net-1.4.1.jar:/opt/hbase/bin/../lib/core-3.1.1.jar:/opt/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/opt/hbase/bin/../lib/guava-12.0.1.jar:/opt/hbase/bin/../lib/hadoop-core-1.2.1.jar:/opt/hbase/bin/../lib/hamcrest-core-1.3.jar:/opt/hbase/bin/../lib/hbase-client-0.97.0-hadoop1-SNAPSHOT-SNAPSHOT.jar:/opt/hbase/bin/../lib/hbase-common-0.97.0-hadoop1-SNAPSHOT-SNAPSHOT.jar:/opt/hbase/bin/../lib/hbase-common-0.97.0-hadoop1-SNAPSHOT-SNAPSHOT-tests.jar:/opt/hbase/bin/../lib/hbase-examples-0.97.0-hadoop1-SNAPSHOT-SNAPSHOT.jar:/opt/hbase/bin/../lib/hbase-hadoop1-compat-0.97.0-hadoop1-SNAPSHOT-SNAPSHOT.jar:/opt/hbase/bin/../lib/hbase-hadoop-compat-0.97.0-hadoop1-SNAPSHOT-SNAPSHOT.jar:/opt/hbase/bin/../lib/hbase-it-0.97.0-hadoop1-SNAPSHOT-SNAPSHOT.jar:/opt/hbase/bin/../lib/hbase-it-0.97.0-hadoop1-SNAPSHOT-SNAPSHOT-tests.jar:/opt/hbase/bin/../lib/hbase-prefix-tree-0.97.0-hadoop1-SNAPSHOT-SNAPSHOT.jar:/opt/hbase/bin/../lib/hbase-protocol-0.97.0-hadoop1-SNAPSHOT-SNAPSHOT.jar:/opt/hbase/bin/../lib/hbase-server-0.97.0-hadoop1-SNAPSHOT-SNAPSHOT.jar:/opt/hbase/bin/../lib/hbase-server-0.97.0-hadoop1-SNAPSHOT-SNAPSHOT-tests.jar:/opt/hbase/bin/../lib/hbase-shell-0.97.0-hadoop1-SNAPSHOT-SNAPSHOT.jar:/opt/hbase/bin/../lib/hbase-testing-util-0.97.0-hadoop1-SNAPSHOT-SNAPSHOT.jar:/opt/hbase/bin/../lib/hbase-thrift-0.97.0-hadoop1-SNAPSHOT-SNAPSHOT.jar:/opt/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/opt/hbase/bin/../lib/htrace-core-2.01.jar:/opt/hbase/bin/../lib/httpclient-4.1.3.jar:/opt/hbase/bin/../lib/httpcore-4.1.3.jar:/opt/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/opt/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/opt/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/opt/hbase/bin/../lib/jackson-xc-1.8.8.jar:/opt/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/opt/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/opt/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/opt/hbase/bin/../lib/jaxb-api-2.2.2.jar:/opt/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/opt/hbase/bin/../lib/jersey-core-1.8.jar:/opt/hbase/bin/../lib/jersey-json-1.8.jar:/opt/hbase/bin/../lib/jersey-server-1.8.jar:/opt/hbase/bin/../lib/jettison-1.3.1.jar:/opt/hbase/bin/../lib/jetty-6.1.26.jar:/opt/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/opt/hbase/bin/../lib/jetty-util-6.1.26.jar:/opt/hbase/bin/../lib/jruby-complete-1.6.8.jar:/opt/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/opt/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/opt/hbase/bin/../lib/jsr305-1.3.9.jar:/opt/hbase/bin/../lib/junit-4.11.jar:/opt/hbase/bin/../lib/libthrift-0.9.0.jar:/opt/hbase/bin/../lib/log4j-1.2.17.jar:/opt/hbase/bin/../lib/metrics-core-2.1.2.jar:/opt/hbase/bin/../lib/netty-3.6.6.Final.jar:/opt/hbase/bin/../lib/protobuf-java-2.5.0.jar:/opt/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/opt/hbase/bin/../lib/slf4j-api-1.6.4.jar:/opt/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/opt/hbase/bin/../lib/stax-api-1.0.1.jar:/opt/hbase/bin/../lib/xmlenc-0.52.jar:/opt/hbase/bin/../lib/zookeeper-3.4.5.jar:2013-11-04 08:33:39,858 INFO  [main] zookeeper.ZooKeeper: Client environment:java.library.path=/usr/java/jdk1.6.0_26/jre/lib/amd64/server:/usr/java/jdk1.6.0_26/jre/lib/amd64:/usr/java/jdk1.6.0_26/jre/../lib/amd64:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
2013-11-04 08:33:39,858 INFO  [main] zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
2013-11-04 08:33:39,858 INFO  [main] zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
2013-11-04 08:33:39,858 INFO  [main] zookeeper.ZooKeeper: Client environment:os.name=Linux
2013-11-04 08:33:39,859 INFO  [main] zookeeper.ZooKeeper: Client environment:os.arch=amd64
2013-11-04 08:33:39,859 INFO  [main] zookeeper.ZooKeeper: Client environment:os.version=2.6.18-164.el5xen
2013-11-04 08:33:39,859 INFO  [main] zookeeper.ZooKeeper: Client environment:user.name=root
2013-11-04 08:33:39,859 INFO  [main] zookeeper.ZooKeeper: Client environment:user.home=/root
2013-11-04 08:33:39,859 INFO  [main] zookeeper.ZooKeeper: Client environment:user.dir=/opt/hbase-0.97.0-hadoop1-SNAPSHOT-SNAPSHOT
2013-11-04 08:33:39,861 INFO  [main] zookeeper.ZooKeeper: Initiating client connection, connectString=scottm-hbase-1.lab:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@2efb56b1
2013-11-04 08:33:40,026 INFO  [main-SendThread(scottm-hbase-1.lab:2181)] zookeeper.ClientCnxn: Opening socket connection to server scottm-hbase-1.lab/10.1.145.175:2181. Will not attempt to authenticate using SASL (Unable to locate a login configuration)
2013-11-04 08:33:40,034 INFO  [main-SendThread(scottm-hbase-1.lab:2181)] zookeeper.ClientCnxn: Socket connection established to scottm-hbase-1.lab/10.1.145.175:2181, initiating session
Welcome to ZooKeeper!
JLine support is enabled
2013-11-04 08:33:40,310 INFO  [main-SendThread(scottm-hbase-1.lab:2181)] zookeeper.ClientCnxn: Session establishment complete on server scottm-hbase-1.lab/10.1.145.175:2181, sessionid = 0x14221eebeb70017, negotiated timeout = 30000

WATCHER::

WatchedEvent state:SyncConnected type:None path:null
[zk: scottm-hbase-1.lab:2181(CONNECTED) 0]
{noformat}

2. then we enable the _HBASE_ROOT_LOGGER_ in hbase-env.sh
{noformat}
129 # HBASE_ROOT_LOGGER=INFO,DRFA
{noformat}
remove pound sign to enable the _HBASE_ROOT_LOGGER_
{noformat}
129 HBASE_ROOT_LOGGER=INFO,DRFA
{noformat}

3. and re-issue the 'bin/hbase zkcli' again, we will see the logging msg disappear in console output
{noformat}
bin/hbase zkcli
Connecting to scottm-hbase-1.lab:2181
Welcome to ZooKeeper!
JLine support is enabled

WATCHER::

WatchedEvent state:SyncConnected type:None path:null
[zk: scottm-hbase-1.lab:2181(CONNECTED) 0]
{noformat}

The reason is that the _HBASE_ROOT_LOGGER_ in hbase-env.sh over-writes the _HBASE_ROOT_LOGGER_ in hbase shell file

put the _egrep_ results for _HBASE_ROOT_LOGGER_ for reference.
{code}
$ egrep -in --color ""HBASE_[a-zA-Z]+_LOGGER"" conf/*
conf/hbase-env.sh:127:# HBASE_ROOT_LOGGER to ""<DESIRED_LOG LEVEL>,DRFA"".
conf/hbase-env.sh:129:# HBASE_ROOT_LOGGER=INFO,DRFA

$ egrep -in --color ""HBASE_[a-zA-Z]+_LOGGER"" bin/*
bin/hbase:47:#   HBASE_ROOT_LOGGER The root appender. Default is INFO,console
bin/hbase:345:HBASE_OPTS=""$HBASE_OPTS -Dhbase.root.logger=${HBASE_ROOT_LOGGER:-INFO,console}""
bin/hbase:353:  HBASE_OPTS=""$HBASE_OPTS -Dhbase.security.logger=${HBASE_SECURITY_LOGGER:-INFO,RFAS}""
bin/hbase:355:  HBASE_OPTS=""$HBASE_OPTS -Dhbase.security.logger=${HBASE_SECURITY_LOGGER:-INFO,NullAppender}""
鈥?bin/hbase-daemon.sh:147:export HBASE_ROOT_LOGGER=${HBASE_ROOT_LOGGER:-""INFO,RFA""}
bin/hbase-daemon.sh:148:export HBASE_SECURITY_LOGGER=${HBASE_SECURITY_LOGGER:-""INFO,RFAS""}
{code}"
HBASE-474	"Instead of just compressing when we write to disk, maybe we should consider compressing any cell data that would be over a few KB so that it would mean less is going out over the wire. 

Additionally, if we were using record compression, possibly we could even leave it compressed and write it directly to disk. It'd be a way to distribute the compression overhead to clients."
HBASE-8047	Does anyone use this? Delete it or document it in book.
HBASE-4467	"When using an Hadoop tarball that has a library naming of ""hadoop-x.y.z-core"" as opposed to ""hadoop-core-x.y.z"" then the hbase script throws errors.

{noformat}
$ bin/start-hbase.sh 
ls: /projects/opensource/hadoop-0.20.2-append/hadoop-core*.jar: No such file or directory
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/util/PlatformName
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.util.PlatformName
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
ls: /projects/opensource/hadoop-0.20.2-append/hadoop-core*.jar: No such file or directory
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/util/PlatformName
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.util.PlatformName
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
localhost: starting zookeeper, logging to /projects/opensource/hbase-trunk-rw//logs/hbase-larsgeorge-zookeeper-de1-app-mbp-2.out
localhost: /projects/opensource/hadoop-0.20.2-append
localhost: ls: /projects/opensource/hadoop-0.20.2-append/hadoop-core*.jar: No such file or directory
localhost: Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/util/PlatformName
localhost: Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.util.PlatformName
localhost: 	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
localhost: 	at java.security.AccessController.doPrivileged(Native Method)
localhost: 	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
localhost: 	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
localhost: 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
localhost: 	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
starting master, logging to /projects/opensource/hbase-trunk-rw/bin/../logs/hbase-larsgeorge-master-de1-app-mbp-2.out
/projects/opensource/hadoop-0.20.2-append
ls: /projects/opensource/hadoop-0.20.2-append/hadoop-core*.jar: No such file or directory
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/util/PlatformName
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.util.PlatformName
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
localhost: starting regionserver, logging to /projects/opensource/hbase-trunk-rw//logs/hbase-larsgeorge-regionserver-de1-app-mbp-2.out
localhost: /projects/opensource/hadoop-0.20.2-append
localhost: ls: /projects/opensource/hadoop-0.20.2-append/hadoop-core*.jar: No such file or directory
localhost: Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/util/PlatformName
localhost: Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.util.PlatformName
localhost: 	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
localhost: 	at java.security.AccessController.doPrivileged(Native Method)
localhost: 	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
localhost: 	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
localhost: 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
localhost: 	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
{noformat}

The naming in this case:

{noformat}
$ ll /projects/opensource/hadoop-0.20.2-append/
total 14960
drwxr-xr-x@  26 larsgeorge  staff      884 Apr 13 09:09 .
drwxr-xr-x  114 larsgeorge  staff     3876 Sep 22 19:42 ..
-rw-r--r--@   1 larsgeorge  staff   348624 Apr 13 08:58 CHANGES.txt
-rw-r--r--@   1 larsgeorge  staff    13366 Apr 13 08:58 LICENSE.txt
-rw-r--r--@   1 larsgeorge  staff      101 Apr 13 08:58 NOTICE.txt
-rw-r--r--@   1 larsgeorge  staff     1366 Apr 13 08:58 README.txt
drwxr-xr-x@  17 larsgeorge  staff      578 Apr 13 08:58 bin
-rw-r--r--@   1 larsgeorge  staff    74035 Apr 13 08:58 build.xml
drwxr-xr-x@   4 larsgeorge  staff      136 Apr 13 08:58 c++
drwxr-xr-x   18 larsgeorge  staff      612 Aug  9 15:11 conf
drwxr-xr-x@  15 larsgeorge  staff      510 Apr 13 08:58 conf.original
drwxr-xr-x@  13 larsgeorge  staff      442 Apr 13 08:58 contrib
drwxr-xr-x@  63 larsgeorge  staff     2142 Apr 13 08:58 docs
-rw-r--r--@   1 larsgeorge  staff     6839 Apr 13 08:58 hadoop-0.20.2-ant.jar
-rw-r--r--    1 larsgeorge  staff  2707920 Apr 13 09:06 hadoop-0.20.2-core.jar
-rw-r--r--@   1 larsgeorge  staff  2689741 Apr 13 08:58 hadoop-0.20.2-core.jar.original
-rw-r--r--@   1 larsgeorge  staff   142466 Apr 13 08:58 hadoop-0.20.2-examples.jar
-rw-r--r--@   1 larsgeorge  staff  1563859 Apr 13 08:58 hadoop-0.20.2-test.jar
-rw-r--r--@   1 larsgeorge  staff    69940 Apr 13 08:58 hadoop-0.20.2-tools.jar
drwxr-xr-x@   6 larsgeorge  staff      204 Apr 13 08:58 ivy
-rw-r--r--@   1 larsgeorge  staff     8852 Apr 13 08:58 ivy.xml
drwxr-xr-x@  30 larsgeorge  staff     1020 Jul 13 10:20 lib
drwxr-xr-x@   3 larsgeorge  staff      102 Apr 13 08:58 librecordio
drwxr-xr-x    3 larsgeorge  staff      102 May 16 09:56 logs
drwxr-xr-x@  17 larsgeorge  staff      578 Apr 13 08:58 src
drwxr-xr-x@   8 larsgeorge  staff      272 Apr 13 08:58 webapps
{noformat}"
HBASE-8440	"As per Stack and J-D's comments:
ReplicationStateInterface should drop the interface part and ReplicationStateImpl should indicate that it is the Zookeeper implementation."
HBASE-3009	"We had an internal need for a method to update a single cell thousands of times per second without having a ""versions explosion"" problem, and found that the code can be somewhat easily modified for that by reusing Store.updateColumnValue and making it accept byte[]. I'm putting the patch here in case someone else finds it useful, and maybe it can eventually make its way into the source."
HBASE-7746	"Small nit. These jobs are examples, no used directly by any part of the system, nor exposed explicitly to the user. Move them to the examples module."
HBASE-4782	Adding a script to check if table descriptors for each region in META for a given table are consistent.  The script compares the table descriptors in META for all regions of a particular table to the table's descriptor (which is just the descriptor for the first region).
HBASE-5065	"When trying to build an 'HServerAddress' object with an unresolvable hostname:

e.g. new HServerAddress(""www.IAMUNREACHABLE.com:80"")

a call to 'getResolvedAddress' would cause the 'InetSocketAddress' c'tor to throw an IllegalArgumentException because it is called with a null 'hostname' parameter.
This happens because there is no null-check after the static 'getBindAddressInternal' method returns a null value when the hostname is unresolved.

This is a trivial bug because the code HServerAddress is expected to throw this kind of exception when this error occurs, but it is thrown ""for the wrong reason"". The method 'checkBindAddressCanBeResolved' should be the one throwing the exception (and give a slightly different reason). Because of this reason the method call itself becomes redundent as it will always succeed in the current flow, because the case it checks is already ""checked"" for by the previous ""getResolvedAddress"" method.

In short:
an IllegalArgumentException is thrown with reason: ""hostname can't be null"" from the InetSocketAddress c'tor
INSTEAD OF
an IllegalArgumentException with reason: ""Could not resolve the DNS name of [BADHOSTNAME]:[PORT]"" from HServerAddress's checkBindCanBeResolved method.

Stack trace:
java.lang.IllegalArgumentException: hostname can't be null
	at java.net.InetSocketAddress.<init>(InetSocketAddress.java:139) ~[na:1.7.0_02]
	at org.apache.hadoop.hbase.HServerAddress.getResolvedAddress(HServerAddress.java:108) ~[hbase-0.90.4.jar:0.90.4]
	at org.apache.hadoop.hbase.HServerAddress.<init>(HServerAddress.java:64) ~[hbase-0.90.4.jar:0.90.4]
	at org.apache.hadoop.hbase.zookeeper.RootRegionTracker.dataToHServerAddress(RootRegionTracker.java:82) ~[hbase-0.90.4.jar:0.90.4]
	at org.apache.hadoop.hbase.zookeeper.RootRegionTracker.waitRootRegionLocation(RootRegionTracker.java:73) ~[hbase-0.90.4.jar:0.90.4]
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:579) ~[hbase-0.90.4.jar:0.90.4]
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:559) ~[hbase-0.90.4.jar:0.90.4]
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:688) ~[hbase-0.90.4.jar:0.90.4]
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:590) ~[hbase-0.90.4.jar:0.90.4]
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:559) ~[hbase-0.90.4.jar:0.90.4]
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:688) ~[hbase-0.90.4.jar:0.90.4]
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:594) ~[hbase-0.90.4.jar:0.90.4]
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:559) ~[hbase-0.90.4.jar:0.90.4]
	at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:173) ~[hbase-0.90.4.jar:0.90.4]
	at org.apache.hadoop.hbase.client.HTable.<init>(HTable.java:147) ~[hbase-0.90.4.jar:0.90.4]

"
HBASE-4226	Just a simple style cleanup of HFileBlock.java.
HBASE-2813	"It seems we require SSH even for non-distributed mode.  Is it possible to not use SSH and thus not require it, just for the simple local box environment?  (rburton up on irc brought this up)"
HBASE-398	"This is a comparison table between a SQL and HQL.
IMO, i don't think these features(SQL) are perfectly fit with HQL and Hbase, but If you want for anything from here, please let me know using comment.

Thanks.

|| *SQL* || *HQL* ||
| *Select Statement* \\
SELECT ""column_name"" FROM ""table_name"" \\ | O \\
\\ |
| *Distinct* \\
SELECT DISTINCT ""column_name"" \\
FROM ""table_name"" \\ | {color:#cc0000}{*}X{*}{color} |
| *Where* \\
SELECT ""column_name"" \\
FROM ""table_name"" \\
WHERE ""condition"" \\ | {color:#cc0000}{*}X{*}{color} |
| *And / Or* \\
SELECT ""column_name"" \\
FROM ""table_name"" \\
WHERE ""simple condition"" \\
\{\[AND\|OR\] ""simple condition""\}\+ \\ | {color:#cc0000}{*}X{*}{color}\\ |
| *In* \\
SELECT ""column_name"" \\
FROM ""table_name"" \\
WHERE ""column_name"" IN ('value1', 'value2', ...) \\ | {color:#cc0000}{*}X{*}{color} |
| *Between* \\
SELECT ""column_name"" \\
FROM ""table_name"" \\
WHERE ""column_name"" BETWEEN 'value1' AND 'value2' \\ | {color:#cc0000}{*}X{*}{color} |
| *Like* \\
SELECT ""column_name"" \\
FROM ""table_name"" \\
WHERE ""column_name"" LIKE \{PATTERN\} \\ | {color:#cc0000}{*}X{*}{color}\\ |
| *Order By* \\
SELECT ""column_name"" \\
FROM ""table_name"" \\
\[WHERE ""condition""\] \\
ORDER BY ""column_name"" \[ASC, DESC\] \\ | {color:#cc0000}{*}X{*}{color} |
| *Count / Sum / Min / Max / Avg* \\
SELECT COUNT(""column_name"") \\
FROM ""table_name"" \\ | {color:#cc0000}{*}X{*}{color}\\ |
| *Group By* \\
SELECT ""column_name1"", SUM(""column_name2"") \\
FROM ""table_name"" \\
GROUP BY ""column_name1"" \\ | {color:#cc0000}{*}X{*}{color} |
| *Having* \\
SELECT ""column_name1"", SUM(""column_name2"") \\
FROM ""table_name"" \\
GROUP BY ""column_name1"" \\
HAVING (arithematic function condition) \\ | {color:#cc0000}{*}X{*}{color} \\ |
| *Create Table Statement* \\
CREATE TABLE ""table_name"" \\
(""column 1"" ""data_type_for_column_1"", \\
""column 2"" ""data_type_for_column_2"", \\
... ) \\ | O \\ |
| *Drop Table Statement* \\
DROP TABLE ""table_name"" \\ | O \\ |
| *Truncate Table Statement* \\
TRUNCATE TABLE ""table_name"" \\ | O \\ |
| *Insert Into Statement* \\
INSERT INTO ""table_name"" (""column1"", ""column2"", ...) \\
VALUES (""value1"", ""value2"", ...) \\ | O \\ |
| *Update Statement* \\
UPDATE ""table_name"" \\
SET ""column_1"" = \[new value\] \\
WHERE \{condition\} \\ | {color:#cc0000}{*}X{*}{color} \\ |
| *Delete From Statement* \\
DELETE FROM ""table_name"" \\
WHERE \{condition\} \\ | O \\ |

"
HBASE-1701	Set some status on rowcounter.
HBASE-429	"Can we get the hbase-site.xml out of there. Seems it should not be there. 

In particular this causes a problem with maven, as we have little control over classpath ordering, so our hbase-site.xml is behind the hbase jar."
HBASE-25	"Looking in logs, a regionserver went down because it could not contact the master after 60 seconds.  Watching logging, the HRS is repeatedly checking all 150 loaded regions over and over again w/ a pause of about 5 seconds between runs... then there is a suspicious 60+ second gap with no logging as though the regionserver had hung up on something:

{code}
2007-12-03 13:14:54,178 DEBUG hbase.HRegionServer - flushing region postlog,img151/60/plakatlepperduzy1hh7.jpg,1196614355635
2007-12-03 13:14:54,178 DEBUG hbase.HRegion - Not flushing cache for region postlog,img151/60/plakatlepperduzy1hh7.jpg,1196614355635: snapshotMemcaches() determined that there was nothing to do
2007-12-03 13:14:54,205 DEBUG hbase.HRegionServer - flushing region postlog,img247/230/seanpaul4li.jpg,1196615889965
2007-12-03 13:14:54,205 DEBUG hbase.HRegion - Not flushing cache for region postlog,img247/230/seanpaul4li.jpg,1196615889965: snapshotMemcaches() determined that there was nothing to do
2007-12-03 13:16:04,305 FATAL hbase.HRegionServer - unable to report to master for 67467 milliseconds - aborting server
2007-12-03 13:16:04,455 INFO  hbase.Leases - regionserver/0:0:0:0:0:0:0:0:60020 closing leases
2007-12-03 13:16:04,455 INFO  hbase.Leases$LeaseMonitor - regionserver/0:0:0:0:0:0:0:0:60020.leaseChecker exiting
{code}

Master seems to be running fine scanning its ~700 regions.  Then you see this in log, before the HRS shuts itself down.

{code}
2007-12-03 13:14:31,416 INFO  hbase.Leases - HMaster.leaseChecker lease expired 153260899/1532608992007-12-03 13:14:31,417 INFO  hbase.HMaster - XX.XX.XX.102:60020 lease expired
{code}

... and we go on to process shutdown."
HBASE-15309	"I'd memory leak issue in regionserver process, VM and RSS memory continiously increasing 64MB.
I'm using hbase-0.94.6 , MALLOC_ARENA_MAX=4 was set in hbase-env.sh
I've huge write load and frequent minor compaction, We've used GZip hfile compression.
Max java regionserver heap size is 32GB.


{noformat}
top - 14:28:30 up 201 days, 21:06,  3 users,  load average: 5.67, 3.72, 3.31
Tasks: 803 total,   1 running, 802 sleeping,   0 stopped,   0 zombie
Cpu(s):  8.3%us,  2.1%sy,  0.0%ni, 85.9%id,  3.5%wa,  0.0%hi,  0.1%si,  0.0%st
Mem:  65932340k total, 63961912k used,  1970428k free,  2394528k buffers
Swap: 29659132k total,    63532k used, 29595600k free,  1095268k cached

 PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
 57335 hbase     20   0 46.4g  44g 9296 S 98.2 70.9  13319:10 java
{noformat}

{noformat}
 [hbase@xxxxxx-hslave ~]$  pmap -x 57335 | sort -k 3 -nr | more
total kB        48695984 46765792 46756512
00007ff312460000 33171072 33169464 33169464 rwx--    [ anon ]
000000004010a000 1448552 1448552 1448552 rwx--    [ anon ]
00007ff2d1810000  612120  603124  603124 rwx--    [ anon ]
00007ff2fadff000  383364  383364  383364 rwx--    [ anon ]
00007ff0e8000000  131072  131072  131072 rwx--    [ anon ]
00007ff218000000  131048  131048  131048 rwx--    [ anon ]
00007ff128000000  131068  131048  131048 rwx--    [ anon ]
00007ff230000000  131044  131044  131044 rwx--    [ anon ]
00007ff000000000  131036  131036  131036 rwx--    [ anon ]
00007fefe0000000  131060  131036  131036 rwx--    [ anon ]
00007ff23c000000   65536   65536   65536 rwx--    [ anon ]
00007ff0a4000000   65536   65536   65536 rwx--    [ anon ]
00007ff054000000   65536   65536   65536 rwx--    [ anon ]
00007ff01c000000   65536   65536   65536 rwx--    [ anon ]
00007fefb4000000   65536   65536   65536 rwx--    [ anon ]
00007ff22c000000   65532   65532   65532 rwx--    [ anon ]
00007ff110000000   65532   65532   65532 rwx--    [ anon ]
00007ff10c000000   65532   65532   65532 rwx--    [ anon ]
00007ff0b8000000   65532   65532   65532 rwx--    [ anon ]
00007ff09c000000   65532   65532   65532 rwx--    [ anon ]
00007feff8000000   65532   65532   65532 rwx--    [ anon ]
00007ff250000000   65528   65528   65528 rwx--    [ anon ]
--More--
{noformat}

{noformat}
$pmap -x 57335 | awk '{print $3}' | awk '{ if($i<65536 && $i>64000) print $i}' | wc -l
146
{noformat}

Regionserver process has many anon pages, size varying from 64000 to 65536 KB that shown above."
HBASE-21176	"Faced with several inconsistency issues in HBase 2.0.1:
{code}
ERROR: Region \{ meta => null, hdfs => hdfs://master:50001/hbase/data/default/some_table/0646d0bee757d0fb0de1529475b5426f, deployed => hbase-region,16020,1536493017073;some_table,,1534195327532.0646d0bee757d0fb0de1529475b5426f., replicaId => 0 } not in META, but deployed on hbase-region,16020,1536493017073
...
ERROR: hbase:namespace has no state in meta
ERROR: table1 has no state in meta
ERROR: table2 has no state in meta
2018-09-09 21:40:04,155 INFO [main] util.HBaseFsck: Handling overlap merges in parallel. set hbasefsck.overlap.merge.parallel to false to run serially.
ERROR: There is a hole in the region chain between and . You need to create a new .regioninfo and region dir in hdfs to plug the hole.
ERROR: Found inconsistency in table test3
{code}

BUT in 2.0.x HBAse version options _-repair, -fix, -fixHdfsHoles, etc_ was deprecated.
How I can fix it without these options?

Thanks."
HBASE-21049	"Power off and restart(Hadoop and HBase), Master is initializing -聽Hbase ServerManager: but crash processing already in progress

command jps, HMaster聽and HRegionServer is live

聽

LOG:

core file size (blocks, -c) 0
data seg size (kbytes, -d) unlimited
scheduling priority (-e) 0
file size (blocks, -f) unlimited
pending signals (-i) 64091
max locked memory (kbytes, -l) 64
max memory size (kbytes, -m) unlimited
open files (-n) 1024
pipe size (512 bytes, -p) 8
POSIX message queues (bytes, -q) 819200
real-time priority (-r) 0
stack size (kbytes, -s) 8192
cpu time (seconds, -t) unlimited
max user processes (-u) 64091
virtual memory (kbytes, -v) unlimited
file locks (-x) unlimited
2018-08-14 17:25:00,173 INFO [main] master.HMaster: STARTING service HMaster
2018-08-14 17:25:00,174 INFO [main] util.VersionInfo: HBase 2.1.0
2018-08-14 17:25:00,174 INFO [main] util.VersionInfo: Source code repository revision=4531d1c947a25b28a9a994b60c791a112c12a2b4
2018-08-14 17:25:00,174 INFO [main] util.VersionInfo: Compiled by hbase on Wed Aug 1 11:25:59 2018
2018-08-14 17:25:00,174 INFO [main] util.VersionInfo: From source with checksum fc32566f7e030ff71458fbf6dc77bce9
2018-08-14 17:25:00,516 INFO [main] util.ServerCommandLine: hbase.tmp.dir: /tmp/hbase-root
2018-08-14 17:25:00,516 INFO [main] util.ServerCommandLine: hbase.rootdir: hdfs://192.168.101.114:9000/hbase
2018-08-14 17:25:00,516 INFO [main] util.ServerCommandLine: hbase.cluster.distributed: true
2018-08-14 17:25:00,516 INFO [main] util.ServerCommandLine: hbase.zookeeper.quorum: 192.168.101.114:2181
2018-08-14 17:25:00,517 INFO [main] util.ServerCommandLine: env:PATH=/opt/apache-phoenix-5.0.0-HBase-2.0-bin/bin:/opt/hbase-2.1.0/bin:/opt/hadoop-2.8.4/bin:/opt/jdk1.8.0_172/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin
2018-08-14 17:25:00,517 INFO [main] util.ServerCommandLine: env:HADOOP_CONF_DIR=/opt/hadoop-2.8.4/etc/hadoop
2018-08-14 17:25:00,517 INFO [main] util.ServerCommandLine: env:HISTCONTROL=ignoredups
2018-08-14 17:25:00,517 INFO [main] util.ServerCommandLine: env:JAVA_LIBRARY_PATH=/opt/hadoop-2.8.4/lib/native::/opt/hadoop-2.8.4/lib/native:
2018-08-14 17:25:00,517 INFO [main] util.ServerCommandLine: env:HBASE_REGIONSERVER_OPTS= -Xdebug -Xnoagent -Djava.compiler=NONE -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8071
2018-08-14 17:25:00,517 INFO [main] util.ServerCommandLine: env:HBASE_CONF_DIR=/opt/hbase-2.1.0/conf
2018-08-14 17:25:00,517 INFO [main] util.ServerCommandLine: env:HDFS_DATANODE_SECURE_USER=root
2018-08-14 17:25:00,517 INFO [main] util.ServerCommandLine: env:MAIL=/var/spool/mail/root
2018-08-14 17:25:00,517 INFO [main] util.ServerCommandLine: env:PHOENIX_HOME=/opt/apache-phoenix-5.0.0-HBase-2.0-bin
2018-08-14 17:25:00,517 INFO [main] util.ServerCommandLine: env:LD_LIBRARY_PATH=:/opt/hadoop-2.8.4/lib/native::/opt/hadoop-2.8.4/lib/native:
2018-08-14 17:25:00,517 INFO [main] util.ServerCommandLine: env:LOGNAME=root
2018-08-14 17:25:00,518 INFO [main] util.ServerCommandLine: env:HBASE_REST_OPTS=
2018-08-14 17:25:00,518 INFO [main] util.ServerCommandLine: env:PWD=/opt/hbase-2.1.0/bin
2018-08-14 17:25:00,518 INFO [main] util.ServerCommandLine: env:HADOOP_PREFIX=/opt/hadoop-2.8.4
2018-08-14 17:25:00,518 INFO [main] util.ServerCommandLine: env:HADOOP_INSTALL=/opt/hadoop-2.8.4
2018-08-14 17:25:00,518 INFO [main] util.ServerCommandLine: env:HBASE_ROOT_LOGGER=INFO,RFA
2018-08-14 17:25:00,518 INFO [main] util.ServerCommandLine: env:LESSOPEN=||/usr/bin/lesspipe.sh %s
2018-08-14 17:25:00,518 INFO [main] util.ServerCommandLine: env:SHELL=/bin/bash
2018-08-14 17:25:00,518 INFO [main] util.ServerCommandLine: env:SELINUX_USE_CURRENT_RANGE=
2018-08-14 17:25:00,518 INFO [main] util.ServerCommandLine: env:YARN_CONF_DIR=/opt/hadoop-2.8.4/etc/hadoop
2018-08-14 17:25:00,518 INFO [main] util.ServerCommandLine: env:HBASE_ENV_INIT=true
2018-08-14 17:25:00,518 INFO [main] util.ServerCommandLine: env:HADOOP_YARN_HOME=/opt/hadoop-2.8.4
2018-08-14 17:25:00,518 INFO [main] util.ServerCommandLine: env:HBASE_MASTER_OPTS= -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8070
2018-08-14 17:25:00,518 INFO [main] util.ServerCommandLine: env:HBASE_MANAGES_ZK=false
2018-08-14 17:25:00,518 INFO [main] util.ServerCommandLine: env:HADOOP_HOME=/opt/hadoop-2.8.4
2018-08-14 17:25:00,518 INFO [main] util.ServerCommandLine: env:HBASE_NICENESS=0
2018-08-14 17:25:00,518 INFO [main] util.ServerCommandLine: env:HBASE_OPTS= -XX:+UseConcMarkSweepGC -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8070 -Dhbase.log.dir=/opt/hbase-2.1.0/logs -Dhbase.log.file=hbase-root-master-hbase-114.log -Dhbase.home.dir=/opt/hbase-2.1.0 -Dhbase.id.str=root -Dhbase.root.logger=INFO,RFA -Djava.library.path=/opt/hadoop-2.8.4/lib/native::/opt/hadoop-2.8.4/lib/native: -Dhbase.security.logger=INFO,RFAS
2018-08-14 17:25:00,518 INFO [main] util.ServerCommandLine: env:HBASE_SECURITY_LOGGER=INFO,RFAS
2018-08-14 17:25:00,518 INFO [main] util.ServerCommandLine: env:HDFS_DATANODE_USER=root
2018-08-14 17:25:00,518 INFO [main] util.ServerCommandLine: 1:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.axv=01;35:*.anx=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.axa=01;36:*.oga=01;36:*.spx=01;36:*.xspf=01;36:
2018-08-14 17:25:00,518 INFO [main] util.ServerCommandLine: env:SHLVL=4
2018-08-14 17:25:00,518 INFO [main] util.ServerCommandLine: env:HBASE_LOGFILE=hbase-root-master-hbase-114.log
2018-08-14 17:25:00,519 INFO [main] util.ServerCommandLine: env:HISTSIZE=1000
2018-08-14 17:25:00,519 INFO [main] util.ServerCommandLine: env:JAVA_HOME=/opt/jdk1.8.0_172
2018-08-14 17:25:00,519 INFO [main] util.ServerCommandLine: env:TERM=xterm
2018-08-14 17:25:00,519 INFO [main] util.ServerCommandLine: env:LANG=zh_CN.UTF-8
2018-08-14 17:25:00,519 INFO [main] util.ServerCommandLine: env:XDG_SESSION_ID=1
2018-08-14 17:25:00,519 INFO [main] util.ServerCommandLine: env:SELINUX_LEVEL_REQUESTED=
2018-08-14 17:25:00,519 INFO [main] util.ServerCommandLine: env:HADOOP_LIBEXEC_DIR=/opt/hadoop-2.8.4/libexec
2018-08-14 17:25:00,519 INFO [main] util.ServerCommandLine: env:SELINUX_ROLE_REQUESTED=
2018-08-14 17:25:00,519 INFO [main] util.ServerCommandLine: env:HADOOP_HDFS_HOME=/opt/hadoop-2.8.4
2018-08-14 17:25:00,519 INFO [main] util.ServerCommandLine: env:HADOOP_MAPRED_HOME=/opt/hadoop-2.8.4
2018-08-14 17:25:00,519 INFO [main] util.ServerCommandLine: env:HADOOP_COMMON_HOME=/opt/hadoop-2.8.4
2018-08-14 17:25:00,519 INFO [main] util.ServerCommandLine: env:HADOOP_OPTS=-Djava.library.path=/opt/hadoop-2.8.4/lib:/opt/hadoop-2.8.4/lib/native
2018-08-14 17:25:00,519 INFO [main] util.ServerCommandLine: env:HBASE_IDENT_STRING=root
2018-08-14 17:25:00,519 INFO [main] util.ServerCommandLine: env:HBASE_ZNODE_FILE=/tmp/hbase-root-master.znode
2018-08-14 17:25:00,519 INFO [main] util.ServerCommandLine: env:SSH_TTY=/dev/pts/0
2018-08-14 17:25:00,519 INFO [main] util.ServerCommandLine: env:SSH_CLIENT=192.168.98.129 35604 22
2018-08-14 17:25:00,519 INFO [main] util.ServerCommandLine: env:HBASE_LOG_PREFIX=hbase-root-master-hbase-114
2018-08-14 17:25:00,519 INFO [main] util.ServerCommandLine: env:HBASE_LOG_DIR=/opt/hbase-2.1.0/logs
2018-08-14 17:25:00,519 INFO [main] util.ServerCommandLine: env:USER=root
2018-08-14 17:25:00,519 INFO [main] util.ServerCommandLine: dparty/commons-logging-1.2.jar:/opt/hbase-2.1.0/lib/client-facing-thirdparty/findbugs-annotations-1.3.9-1.jar:/opt/hbase-2.1.0/lib/client-facing-thirdparty/htrace-core4-4.2.0-incubating.jar:/opt/hbase-2.1.0/lib/client-facing-thirdparty/log4j-1.2.17.jar:/opt/hbase-2.1.0/lib/client-facing-thirdparty/phoenix-5.0.0-HBase-2.0-server.jar:/opt/hbase-2.1.0/lib/client-facing-thirdparty/phoenix-core-5.0.0-HBase-2.0.jar:/opt/hbase-2.1.0/lib/client-facing-thirdparty/slf4j-api-1.7.25.jar:/opt/hadoop-2.8.4/etc/hadoop:/opt/hadoop-2.8.4/share/hadoop/common/lib/*:/opt/hadoop-2.8.4/share/hadoop/common/*:/opt/hadoop-2.8.4/share/hadoop/hdfs:/opt/hadoop-2.8.4/share/hadoop/hdfs/lib/*:/opt/hadoop-2.8.4/share/hadoop/hdfs/*:/opt/hadoop-2.8.4/share/hadoop/yarn/lib/*:/opt/hadoop-2.8.4/share/hadoop/yarn/*:/opt/hadoop-2.8.4/share/hadoop/mapreduce/lib/*:/opt/hadoop-2.8.4/share/hadoop/mapreduce/*:/opt/hadoop-2.8.4/contrib/capacity-scheduler/*.jar:/opt/hbase-2.1.0/lib/client-facing-thirdparty/slf4j-log4j12-1.7.25.jar
2018-08-14 17:25:00,519 INFO [main] util.ServerCommandLine: env:HDFS_NAMENODE_USER=root
2018-08-14 17:25:00,519 INFO [main] util.ServerCommandLine: env:SSH_CONNECTION=192.168.98.129 35604 192.168.101.114 22
2018-08-14 17:25:00,519 INFO [main] util.ServerCommandLine: env:HBASE_AUTOSTART_FILE=/tmp/hbase-root-master.autostart
2018-08-14 17:25:00,519 INFO [main] util.ServerCommandLine: env:HOSTNAME=hbase-114
2018-08-14 17:25:00,520 INFO [main] util.ServerCommandLine: env:HADOOP_COMMON_LIB_NATIVE_DIR=/opt/hadoop-2.8.4/lib/native
2018-08-14 17:25:00,520 INFO [main] util.ServerCommandLine: env:XDG_RUNTIME_DIR=/run/user/0
2018-08-14 17:25:00,520 INFO [main] util.ServerCommandLine: env:HDFS_SECONDARYNAMENODE_USER=root
2018-08-14 17:25:00,520 INFO [main] util.ServerCommandLine: env:HBASE_THRIFT_OPTS=
2018-08-14 17:25:00,520 INFO [main] util.ServerCommandLine: env:HBASE_HOME=/opt/hbase-2.1.0
2018-08-14 17:25:00,520 INFO [main] util.ServerCommandLine: env:HOME=/root
2018-08-14 17:25:00,520 INFO [main] util.ServerCommandLine: env:MALLOC_ARENA_MAX=4
2018-08-14 17:25:00,521 INFO [main] util.ServerCommandLine: vmName=Java HotSpot(TM) 64-Bit Server VM, vmVendor=Oracle Corporation, vmVersion=25.172-b11
2018-08-14 17:25:00,521 INFO [main] util.ServerCommandLine: vmInputArguments=[-Dproc_master, -XX:OnOutOfMemoryError=kill -9 %p, -XX:+UseConcMarkSweepGC, -Xdebug, -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8070, -Dhbase.log.dir=/opt/hbase-2.1.0/logs, -Dhbase.log.file=hbase-root-master-hbase-114.log, -Dhbase.home.dir=/opt/hbase-2.1.0, -Dhbase.id.str=root, -Dhbase.root.logger=INFO,RFA, -Djava.library.path=/opt/hadoop-2.8.4/lib/native::/opt/hadoop-2.8.4/lib/native:, -Dhbase.security.logger=INFO,RFAS]
2018-08-14 17:25:00,886 INFO [main] metrics.MetricRegistries: Loaded MetricRegistries class org.apache.hadoop.hbase.metrics.impl.MetricRegistriesImpl
2018-08-14 17:25:01,258 INFO [main] regionserver.RSRpcServices: master/hbase-114:16000 server-side Connection retries=3
2018-08-14 17:25:01,278 INFO [main] ipc.RpcExecutor: Instantiated default.FPBQ.Fifo with queueClass=class java.util.concurrent.LinkedBlockingQueue; numCallQueues=3, maxQueueLength=300, handlerCount=30
2018-08-14 17:25:01,280 INFO [main] ipc.RpcExecutor: Instantiated priority.FPBQ.Fifo with queueClass=class java.util.concurrent.LinkedBlockingQueue; numCallQueues=2, maxQueueLength=300, handlerCount=20
2018-08-14 17:25:01,280 INFO [main] ipc.RpcExecutor: Instantiated replication.FPBQ.Fifo with queueClass=class java.util.concurrent.LinkedBlockingQueue; numCallQueues=1, maxQueueLength=300, handlerCount=3
2018-08-14 17:25:01,418 INFO [main] ipc.RpcServerFactory: Creating org.apache.hadoop.hbase.ipc.NettyRpcServer hosting hbase.pb.MasterService, hbase.pb.RegionServerStatusService, hbase.pb.LockService, hbase.pb.ClientService, hbase.pb.AdminService
2018-08-14 17:25:01,632 INFO [main] ipc.NettyRpcServer: Bind to /192.168.101.114:16000
2018-08-14 17:25:01,688 INFO [main] hfile.CacheConfig: Allocating onheap LruBlockCache size=1.55 GB, blockSize=64 KB
2018-08-14 17:25:01,694 INFO [main] hfile.CacheConfig: Created cacheConfig: blockCache=LruBlockCache\{blockCount=0, currentSize=1.16 MB, freeSize=1.55 GB, maxSize=1.55 GB, heapSize=1.16 MB, minSize=1.47 GB, minFactor=0.95, multiSize=752.80 MB, multiFactor=0.5, singleSize=376.40 MB, singleFactor=0.25}, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheDataCompressed=false, prefetchOnOpen=false
2018-08-14 17:25:01,695 INFO [main] hfile.CacheConfig: Created cacheConfig: blockCache=LruBlockCache\{blockCount=0, currentSize=1.16 MB, freeSize=1.55 GB, maxSize=1.55 GB, heapSize=1.16 MB, minSize=1.47 GB, minFactor=0.95, multiSize=752.80 MB, multiFactor=0.5, singleSize=376.40 MB, singleFactor=0.25}, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheDataCompressed=false, prefetchOnOpen=false
2018-08-14 17:25:02,160 INFO [main] fs.HFileSystem: Added intercepting call to namenode#getBlockLocations so can do block reordering using class org.apache.hadoop.hbase.fs.HFileSystem$ReorderWALBlocks
2018-08-14 17:25:02,163 INFO [main] fs.HFileSystem: Added intercepting call to namenode#getBlockLocations so can do block reordering using class org.apache.hadoop.hbase.fs.HFileSystem$ReorderWALBlocks
2018-08-14 17:25:02,233 INFO [main] zookeeper.RecoverableZooKeeper: Process identifier=master:16000 connecting to ZooKeeper ensemble=192.168.101.114:2181
2018-08-14 17:25:02,239 INFO [main] zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT
2018-08-14 17:25:02,239 INFO [main] zookeeper.ZooKeeper: Client environment:host.name=hbase-114
2018-08-14 17:25:02,239 INFO [main] zookeeper.ZooKeeper: Client environment:java.version=1.8.0_172
2018-08-14 17:25:02,239 INFO [main] zookeeper.ZooKeeper: Client environment:java.vendor=Oracle Corporation
2018-08-14 17:25:02,239 INFO [main] zookeeper.ZooKeeper: Client environment:java.home=/opt/jdk1.8.0_172/jre
2018-08-14 17:25:02,239 INFO [main] zookeeper.ZooKeeper: o-2.4.jar:/opt/hadoop-2.8.4/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop-2.8.4/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop-2.8.4/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.4.jar:/opt/hadoop-2.8.4/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.4.jar:/opt/hadoop-2.8.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar:/opt/hadoop-2.8.4/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.4.jar:/opt/hadoop-2.8.4/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.4.jar:/opt/hadoop-2.8.4/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.4.jar:/opt/hadoop-2.8.4/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.4.jar:/opt/hadoop-2.8.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.4-tests.jar:/opt/hadoop-2.8.4/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.4.jar:/opt/hadoop-2.8.4/contrib/capacity-scheduler/*.jar:/opt/hbase-2.1.0/lib/client-facing-thirdparty/slf4j-log4j12-1.7.25.jar
2018-08-14 17:25:02,239 INFO [main] zookeeper.ZooKeeper: Client environment:java.library.path=/opt/hadoop-2.8.4/lib/native::/opt/hadoop-2.8.4/lib/native:
2018-08-14 17:25:02,239 INFO [main] zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
2018-08-14 17:25:02,239 INFO [main] zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
2018-08-14 17:25:02,239 INFO [main] zookeeper.ZooKeeper: Client environment:os.name=Linux
2018-08-14 17:25:02,239 INFO [main] zookeeper.ZooKeeper: Client environment:os.arch=amd64
2018-08-14 17:25:02,239 INFO [main] zookeeper.ZooKeeper: Client environment:os.version=3.10.0-862.el7.x86_64
2018-08-14 17:25:02,239 INFO [main] zookeeper.ZooKeeper: Client environment:user.name=root
2018-08-14 17:25:02,239 INFO [main] zookeeper.ZooKeeper: Client environment:user.home=/root
2018-08-14 17:25:02,239 INFO [main] zookeeper.ZooKeeper: Client environment:user.dir=/opt/hbase-2.1.0/bin
2018-08-14 17:25:02,240 INFO [main] zookeeper.ZooKeeper: Initiating client connection, connectString=192.168.101.114:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.PendingWatcher@4ae2e781
2018-08-14 17:25:02,256 INFO [main-SendThread(192.168.101.114:2181)] zookeeper.ClientCnxn: Opening socket connection to server 192.168.101.114/192.168.101.114:2181. Will not attempt to authenticate using SASL (unknown error)
2018-08-14 17:25:02,264 INFO [main-SendThread(192.168.101.114:2181)] zookeeper.ClientCnxn: Socket connection established to 192.168.101.114/192.168.101.114:2181, initiating session
2018-08-14 17:25:02,282 INFO [main-SendThread(192.168.101.114:2181)] zookeeper.ClientCnxn: Session establishment complete on server 192.168.101.114/192.168.101.114:2181, sessionid = 0x10004dac1970000, negotiated timeout = 40000
2018-08-14 17:25:02,352 INFO [main] util.log: Logging initialized @2552ms
2018-08-14 17:25:02,413 INFO [main] http.HttpRequestLog: Http request log for http.requests.master is not defined
2018-08-14 17:25:02,426 INFO [main] http.HttpServer: Added global filter 'safety' (class=org.apache.hadoop.hbase.http.HttpServer$QuotingInputFilter)
2018-08-14 17:25:02,426 INFO [main] http.HttpServer: Added global filter 'clickjackingprevention' (class=org.apache.hadoop.hbase.http.ClickjackingPreventionFilter)
2018-08-14 17:25:02,428 INFO [main] http.HttpServer: Added filter static_user_filter (class=org.apache.hadoop.hbase.http.lib.StaticUserWebFilter$StaticUserFilter) to context master
2018-08-14 17:25:02,428 INFO [main] http.HttpServer: Added filter static_user_filter (class=org.apache.hadoop.hbase.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2018-08-14 17:25:02,428 INFO [main] http.HttpServer: Added filter static_user_filter (class=org.apache.hadoop.hbase.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2018-08-14 17:25:02,455 INFO [main] http.HttpServer: Jetty bound to port 16010
2018-08-14 17:25:02,456 INFO [main] server.Server: jetty-9.3.19.v20170502
2018-08-14 17:25:02,489 INFO [main] handler.ContextHandler: Started o.e.j.s.ServletContextHandler@49232c6f\{/logs,file:///opt/hbase-2.1.0/logs/,AVAILABLE}
2018-08-14 17:25:02,490 INFO [main] handler.ContextHandler: Started o.e.j.s.ServletContextHandler@279126f5\{/static,file:///opt/hbase-2.1.0/hbase-webapps/static/,AVAILABLE}
2018-08-14 17:25:02,582 INFO [main] handler.ContextHandler: Started o.e.j.w.WebAppContext@537b3b2e\{/,file:///opt/hbase-2.1.0/hbase-webapps/master/,AVAILABLE}{file:/opt/hbase-2.1.0/hbase-webapps/master}
2018-08-14 17:25:02,587 INFO [main] server.AbstractConnector: Started ServerConnector@550c973e\{HTTP/1.1,[http/1.1]}{0.0.0.0:16010}
2018-08-14 17:25:02,587 INFO [main] server.Server: Started @2787ms
2018-08-14 17:25:02,590 INFO [main] master.HMaster: hbase.rootdir=hdfs://192.168.101.114:9000/hbase, hbase.cluster.distributed=true
2018-08-14 17:25:02,606 INFO [Thread-14] master.HMaster: Adding backup master ZNode /hbase/backup-masters/hbase-114,16000,1534238700547
2018-08-14 17:25:02,685 INFO [Thread-14] master.ActiveMasterManager: Deleting ZNode for /hbase/backup-masters/hbase-114,16000,1534238700547 from backup master directory
2018-08-14 17:25:02,691 INFO [Thread-14] master.ActiveMasterManager: Registered as active master=hbase-114,16000,1534238700547
2018-08-14 17:25:02,697 INFO [Thread-14] regionserver.ChunkCreator: Allocating data MemStoreChunkPool with chunk size 2 MB, max count 713, initial count 0
2018-08-14 17:25:02,698 INFO [Thread-14] regionserver.ChunkCreator: Allocating index MemStoreChunkPool with chunk size 204.80 KB, max count 792, initial count 0
2018-08-14 17:25:02,992 INFO [Thread-14] fs.HFileSystem: Added intercepting call to namenode#getBlockLocations so can do block reordering using class org.apache.hadoop.hbase.fs.HFileSystem$ReorderWALBlocks
2018-08-14 17:25:03,001 INFO [Thread-14] coordination.SplitLogManagerCoordination: Found 0 orphan tasks and 0 rescan nodes
2018-08-14 17:25:03,094 INFO [Thread-14] zookeeper.ReadOnlyZKClient: Connect 0x66461af1 to 192.168.101.114:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms
2018-08-14 17:25:03,100 INFO [ReadOnlyZKClient-192.168.101.114:2181@0x66461af1] zookeeper.ZooKeeper: Initiating client connection, connectString=192.168.101.114:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$52/1619197561@6e451c19
2018-08-14 17:25:03,101 INFO [ReadOnlyZKClient-192.168.101.114:2181@0x66461af1-SendThread(192.168.101.114:2181)] zookeeper.ClientCnxn: Opening socket connection to server 192.168.101.114/192.168.101.114:2181. Will not attempt to authenticate using SASL (unknown error)
2018-08-14 17:25:03,101 INFO [ReadOnlyZKClient-192.168.101.114:2181@0x66461af1-SendThread(192.168.101.114:2181)] zookeeper.ClientCnxn: Socket connection established to 192.168.101.114/192.168.101.114:2181, initiating session
2018-08-14 17:25:03,104 INFO [ReadOnlyZKClient-192.168.101.114:2181@0x66461af1-SendThread(192.168.101.114:2181)] zookeeper.ClientCnxn: Session establishment complete on server 192.168.101.114/192.168.101.114:2181, sessionid = 0x10004dac1970001, negotiated timeout = 40000
2018-08-14 17:25:03,145 INFO [Thread-14] procedure2.ProcedureExecutor: Starting 16 core workers (bigger of cpus/4 or 16) with max (burst) worker count=160
2018-08-14 17:25:03,149 INFO [Thread-14] util.FSHDFSUtils: Recover lease on dfs file hdfs://192.168.101.114:9000/hbase/MasterProcWALs/pv2-00000000000000000004.log
2018-08-14 17:25:03,153 INFO [Thread-14] util.FSHDFSUtils: Recovered lease, attempt=0 on file=hdfs://192.168.101.114:9000/hbase/MasterProcWALs/pv2-00000000000000000004.log after 4ms
2018-08-14 17:25:03,188 WARN [Thread-14] util.CommonFSUtils: Your Hadoop installation does not include the StreamCapabilities class from HDFS-11644, so we will skip checking if any FSDataOutputStreams actually support hflush/hsync. If you are running on top of HDFS this probably just means you have an older version and this can be ignored. If you are running on top of an alternate FileSystem implementation you should manually verify that hflush and hsync are implemented; otherwise you risk data loss and hard to diagnose errors when our assumptions are violated.
2018-08-14 17:25:03,189 INFO [Thread-14] wal.WALProcedureStore: Rolled new Procedure Store WAL, id=5
2018-08-14 17:25:03,190 INFO [Thread-14] procedure2.ProcedureExecutor: Recovered WALProcedureStore lease in 42msec
2018-08-14 17:25:03,224 INFO [Thread-14] procedure2.ProcedureExecutor: Loaded WALProcedureStore in 33msec
2018-08-14 17:25:03,224 INFO [Thread-14] procedure2.RemoteProcedureDispatcher: Instantiated, coreThreads=128 (allowCoreThreadTimeOut=true), queueMaxSize=32, operationDelay=150
2018-08-14 17:25:03,261 WARN [Thread-14] master.ServerManager: Expiration of hbase-116,16020,1534237430655 but server not online
2018-08-14 17:25:03,261 INFO [Thread-14] master.ServerManager: Processing expiration of hbase-116,16020,1534237430655 on hbase-114,16000,1534238700547
2018-08-14 17:25:03,481 WARN [Thread-14] master.ServerManager: Expiration of hbase-115,16020,1534237425729 but server not online
2018-08-14 17:25:03,481 INFO [Thread-14] master.ServerManager: Processing expiration of hbase-115,16020,1534237425729 on hbase-114,16000,1534238700547
2018-08-14 17:25:03,622 INFO [Thread-14] balancer.BaseLoadBalancer: slop=0.001, tablesOnMaster=false, systemTablesOnMaster=false
2018-08-14 17:25:03,629 INFO [Thread-14] balancer.StochasticLoadBalancer: Loaded config; maxSteps=1000000, stepsPerRegion=800, maxRunningTime=30000, isByTable=false, etc.
2018-08-14 17:25:03,669 INFO [Thread-14] master.HMaster: Active/primary master=hbase-114,16000,1534238700547, sessionid=0x10004dac1970000, setting cluster-up flag (Was=false)
2018-08-14 17:25:03,771 INFO [PEWorker-4] procedure.ServerCrashProcedure: Start pid=12, state=RUNNABLE:SERVER_CRASH_START; ServerCrashProcedure server=hbase-115,16020,1534237425729, splitWal=true, meta=false
2018-08-14 17:25:03,772 INFO [Thread-14] procedure2.TimeoutExecutorThread: ADDED pid=-1, state=WAITING_TIMEOUT; org.apache.hadoop.hbase.procedure2.ProcedureExecutor$CompletedProcedureCleaner; timeout=30000, timestamp=1534238733772
2018-08-14 17:25:03,774 INFO [PEWorker-3] procedure.ServerCrashProcedure: Start pid=11, state=RUNNABLE:SERVER_CRASH_START; ServerCrashProcedure server=hbase-116,16020,1534237430655, splitWal=true, meta=false
2018-08-14 17:25:03,775 INFO [Thread-14] cleaner.CleanerChore: Cleaner pool size is 1
2018-08-14 17:25:03,776 INFO [Thread-14] zookeeper.RecoverableZooKeeper: Process identifier=replicationLogCleaner connecting to ZooKeeper ensemble=192.168.101.114:2181
2018-08-14 17:25:03,776 INFO [Thread-14] zookeeper.ZooKeeper: Initiating client connection, connectString=192.168.101.114:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.PendingWatcher@46bb7405
2018-08-14 17:25:03,777 INFO [Thread-14-SendThread(192.168.101.114:2181)] zookeeper.ClientCnxn: Opening socket connection to server 192.168.101.114/192.168.101.114:2181. Will not attempt to authenticate using SASL (unknown error)
2018-08-14 17:25:03,777 INFO [Thread-14-SendThread(192.168.101.114:2181)] zookeeper.ClientCnxn: Socket connection established to 192.168.101.114/192.168.101.114:2181, initiating session
2018-08-14 17:25:03,777 INFO [Thread-14] cleaner.LogCleaner: Creating OldWALs cleaners with size=2
2018-08-14 17:25:03,780 INFO [Thread-14-SendThread(192.168.101.114:2181)] zookeeper.ClientCnxn: Session establishment complete on server 192.168.101.114/192.168.101.114:2181, sessionid = 0x10004dac1970006, negotiated timeout = 40000
2018-08-14 17:25:03,967 INFO [RpcServer.default.FPBQ.Fifo.handler=28,queue=1,port=16000] master.ServerManager: Registering regionserver=hbase-116,16020,1534238701517
2018-08-14 17:25:03,967 INFO [RpcServer.default.FPBQ.Fifo.handler=29,queue=2,port=16000] master.ServerManager: Registering regionserver=hbase-115,16020,1534238702258
2018-08-14 17:25:04,022 INFO [RegionServerTracker-0] master.RegionServerTracker: RegionServer ephemeral node created, adding [hbase-116,16020,1534238701517]
2018-08-14 17:25:04,023 INFO [RegionServerTracker-0] master.RegionServerTracker: RegionServer ephemeral node created, adding [hbase-115,16020,1534238702258]
2018-08-14 17:25:33,877 INFO [WALProcedureStoreSyncThread] wal.ProcedureWALFile: Archiving hdfs://192.168.101.114:9000/hbase/MasterProcWALs/pv2-00000000000000000004.log to hdfs://192.168.101.114:9000/hbase/oldWALs/pv2-00000000000000000004.log
2018-08-14 17:26:59,875 WARN [qtp1304765785-87] servlet.ServletHandler: /master-status
org.apache.hadoop.hbase.PleaseHoldException: Master is initializing
 at org.apache.hadoop.hbase.master.HMaster.isInMaintenanceMode(HMaster.java:2890)
 at org.apache.hadoop.hbase.tmpl.master.MasterStatusTmplImpl.renderNoFlush(MasterStatusTmplImpl.java:277)
 at org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.renderNoFlush(MasterStatusTmpl.java:395)
 at org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.render(MasterStatusTmpl.java:386)
 at org.apache.hadoop.hbase.master.MasterStatusServlet.doGet(MasterStatusServlet.java:81)
 at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
 at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
 at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1772)
 at org.apache.hadoop.hbase.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:112)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
 at org.apache.hadoop.hbase.http.ClickjackingPreventionFilter.doFilter(ClickjackingPreventionFilter.java:48)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
 at org.apache.hadoop.hbase.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:1374)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
 at org.apache.hadoop.hbase.http.NoCacheFilter.doFilter(NoCacheFilter.java:49)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
 at org.apache.hadoop.hbase.http.NoCacheFilter.doFilter(NoCacheFilter.java:49)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)
 at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)
 at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)
 at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)
 at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)
 at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
 at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)
 at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)
 at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
 at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
 at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)
 at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
 at org.eclipse.jetty.server.Server.handle(Server.java:534)
 at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
 at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
 at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
 at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
 at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
 at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
 at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
 at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
 at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
 at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
 at java.lang.Thread.run(Thread.java:748)"
HBASE-17966	"tableExists can return false for cases where createTable fails. Obviously the logic implemented in these two methods are different. 
This is to make this implementation consistent"
HBASE-19499	"Probably this is the first of few issues found during some tests with RegionMover. After HBASE-13014 we ship the new RegionMover tool but it currently assumes that master will be hosting regions so it attempts to remove master from the list and that causes an issue similar to this:

{code}
17/12/12 11:01:06 WARN util.RegionMover: Could not remove master from list of RS
java.lang.Exception: Server host1.example.com:22001 is not in list of online servers(Offline/Incorrect)
	at org.apache.hadoop.hbase.util.RegionMover.stripServer(RegionMover.java:818)
	at org.apache.hadoop.hbase.util.RegionMover.stripMaster(RegionMover.java:757)
	at org.apache.hadoop.hbase.util.RegionMover.access$1800(RegionMover.java:78)
	at org.apache.hadoop.hbase.util.RegionMover$Unload.call(RegionMover.java:339)
	at org.apache.hadoop.hbase.util.RegionMover$Unload.call(RegionMover.java:314)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code}

Basicaly"
HBASE-19507	"1銆?
create   'abc',{NAME => 'cf', MOB_THRESHOLD => '10', IS_MOB => 'true'}
put 'abc','1','cf:a','xxxx1'
put 'abc','2','cf:a','xxxx2'
put 'abc','3','cf:a','xxxx3'
put 'abc','4','cf:a','yyyyyyyyyyyyy1'
put 'abc','5','cf:a','yyyyyyyyyyyyy2'
put 'abc','6','cf:a','yyyyyyyyyyyyy3'
  
hbase(main):011:0> scan 'abc'
ROW                                        COLUMN+CELL                                                                                                                 
 1                                         column=cf:a, timestamp=1513171753098, value=xxxx1                                                                           
 2                                         column=cf:a, timestamp=1513171753208, value=xxxx2                                                                           
 3                                         column=cf:a, timestamp=1513171753246, value=xxxx3                                                                           
 4                                         column=cf:a, timestamp=1513171753273, value=yyyyyyyyyyyyy1                                                                  
 5                                         column=cf:a, timestamp=1513171753301, value=yyyyyyyyyyyyy2                                                                  
 6                                         column=cf:a, timestamp=1513171754282, value=yyyyyyyyyyyyy3                                                                  

hbase(main):012:0> flush 'abc'
hbase(main):012:0> major_compact 'abc'
hbase(main):012:0> major_compact_mob 'abc'

2銆?
[See Hfile]:
hbase org.apache.hadoop.hbase.io.hfile.HFile -f /hbase/data/default/abc/a31b3146cba0d4569a7bf44e70e299c9/cf/22a432ba5c2c4802bedd947b99626f10 -p
K: 1/cf:a/1513172294864/Put/vlen=5/seqid=4 V: xxxx1
K: 2/cf:a/1513172294892/Put/vlen=5/seqid=5 V: xxxx2
K: 3/cf:a/1513172294914/Put/vlen=5/seqid=6 V: xxxx3
K: 4/cf:a/1513172294954/Put/vlen=76/seqid=7 V: \x00\x00\x00\x0Ed41d8cd98f00b204e9800998ecf8427e20171213ce022548c4c3498e864fda289b81e711 T[0]:  T[1]: abc
K: 5/cf:a/1513172294982/Put/vlen=76/seqid=8 V: \x00\x00\x00\x0Ed41d8cd98f00b204e9800998ecf8427e20171213ce022548c4c3498e864fda289b81e711 T[0]:  T[1]: abc
K: 6/cf:a/1513172296455/Put/vlen=76/seqid=9 V: \x00\x00\x00\x0Ed41d8cd98f00b204e9800998ecf8427e20171213ce022548c4c3498e864fda289b81e711 T[0]:  T[1]: abc
Scanned kv count -> 6

[See Mobfile]:
hbase org.apache.hadoop.hbase.io.hfile.HFile -f /hbase/mobdir/data/default/abc/07aab825b62dd9111831839cc9039df9/cf/d41d8cd98f00b204e9800998ecf8427e20171213bd8cfaf146684d4096ebf7994f050e96 -p
K: 4/cf:a/1513172924196/Put/vlen=14/seqid=7 V: yyyyyyyyyyyyy1
K: 5/cf:a/1513172924214/Put/vlen=14/seqid=8 V: yyyyyyyyyyyyy2
K: 6/cf:a/1513172925768/Put/vlen=14/seqid=9 V: yyyyyyyyyyyyy3

3銆?
alter 'abc',{NAME => 'cf', MOB_THRESHOLD => '10240' }
put 'abc','7','cf:a','zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz'
ROW                                        COLUMN+CELL                                                                                                                 
 1                                         column=cf:a, timestamp=1513172294864, value=xxxx1                                                                           
 2                                         column=cf:a, timestamp=1513172294892, value=xxxx2                                                                           
 3                                         column=cf:a, timestamp=1513172294914, value=xxxx3                                                                           
 4                                         column=cf:a, timestamp=1513172294954, value=yyyyyyyyyyyyy1                                                                  
 5                                         column=cf:a, timestamp=1513172294982, value=yyyyyyyyyyyyy2                                                                  
 6                                         column=cf:a, timestamp=1513172296455, value=yyyyyyyyyyyyy3                                                                  
 7                                         column=cf:a, timestamp=1513172691250, value=zzzzzzzzzzzzzzzzzzzzzz .....'(10304 Byte) 

hbase(main):012:0> flush 'abc'
hbase(main):013:0> major_compact_mob 'abc'

hbase(main):070:0> scan 'abc'
ROW                                        COLUMN+CELL                                                                                                                 
 1                                         column=cf:a, timestamp=1513172294864, value=xxxx1                                                                           
 2                                         column=cf:a, timestamp=1513172294892, value=xxxx2                                                                           
 3                                         column=cf:a, timestamp=1513172294914, value=xxxx3                                                                           
 4                                         column=cf:a, timestamp=1513172294954, value=\x00\x00\x00\x0Ed41d8cd98f00b204e9800998ecf8427e2017121320a7c40681254d6a8a396e9d
                                           b34529af                                                                                                                    
 5                                         column=cf:a, timestamp=1513172294982, value=\x00\x00\x00\x0Ed41d8cd98f00b204e9800998ecf8427e2017121320a7c40681254d6a8a396e9d
                                           b34529af                                                                                                                    
 6                                         column=cf:a, timestamp=1513172296455, value=\x00\x00\x00\x0Ed41d8cd98f00b204e9800998ecf8427e2017121320a7c40681254d6a8a396e9d
                                           b34529af                                                                                                                    
 7                                         column=cf:a, timestamp=1513172691250, value=\x00\x00(@d41d8cd98f00b204e9800998ecf8427e2017121320a7c40681254d6a8a396e9db34529
                                           af                                                                                                                          
7 row(s) in 0.0280 seconds

"
HBASE-17714	"We have a test in Phoenix where we introduce an artificial sleep of 2 times the RPC timeout in preScannerNext() hook of a co-processor. 

{code}
 public static class SleepingRegionObserver extends SimpleRegionObserver {
        public SleepingRegionObserver() {}
        
        @Override
        public boolean preScannerNext(final ObserverContext<RegionCoprocessorEnvironment> c,
                final InternalScanner s, final List<Result> results,
                final int limit, final boolean hasMore) throws IOException {
            try {
                if (SLEEP_NOW && c.getEnvironment().getRegion().getRegionInfo().getTable().getNameAsString().equals(TABLE_NAME)) {
                    Thread.sleep(RPC_TIMEOUT * 2);
                }
            } catch (InterruptedException e) {
                throw new IOException(e);
            }
            return super.preScannerNext(c, s, results, limit, hasMore);
        }
    }
{code}

This test was passing fine till 1.1.3 but started failing sometime before 1.1.9 with an OutOfOrderScannerException. See PHOENIX-3702. [~lhofhansl] mentioned that we have client heartbeats enabled and that should prevent us from running into issues like this. FYI, this test fails with 1.2.3 version of HBase too.

CC [~apurtell], [~jamestaylor]

"
HBASE-12433	"When modifying the coprocessor priority through the HBase shell, the order of the firing of the coprocessors wasn't changing. It probably would have with a cluster bounce, but if we can make it dynamic easily, that would be preferable."
HBASE-16212	"As described in https://issues.apache.org/jira/browse/HDFS-8659, the datanode is suffering from logging the same repeatedly. Adding log to DFSInputStream, it outputs as follows:

2016-07-10 21:31:42,147 INFO  [B.defaultRpcServer.handler=22,queue=1,port=16020] hdfs.DFSClient: DFSClient_NONMAPREDUCE_1984924661_1 seek DatanodeInfoWithStorage[10.130.1.29:50010,DS-086bc494-d862-470c-86e8-9cb7929985c6,DISK] for BP-360285305-10.130.1.11-1444619256876:blk_1109360829_35627143. pos: 111506876, targetPos: 111506843
 ...
As the pos of this input stream is larger than targetPos(the pos trying to seek), A new connection to the datanode will be created, the older one will be closed as a consequence. When the wrong seeking ops are large, the datanode's block scanner info message is spamming logs, as well as many connections to the same datanode will be created.

hadoop version: 2.7.1

"
HBASE-14442	"I created a Scan whose startRow and stopRow are the same with a region's startRow, then I found no map was built. 
The following is the source code of this condtion:
(startRow.length == 0 || keys.getSecond()[i].length == 0 ||
                    Bytes.compareTo(startRow, keys.getSecond()[i]) < 0) &&
                    (stopRow.length == 0 || Bytes.compareTo(stopRow,
                            keys.getFirst()[i]) > 0)

I think  a ""="" should be added."
HBASE-15483	"After setting hbase.security.authorization to be false, hbase does NOT do authority check for any operations by any users. Thus, any user, including read only user, has the authority to grant <user> <any permission>. The change to ACL record is lasted and will take effective after next authorization enabling. 

The conseqence is,
A readonly user can change an admin user to be a ""readonly"" user after a round of ""disable authorization"" and ""enable authorization""
Also,
A readonly user can change a ""readonly"" user to be an Admin after such a round of disable/enable.

It is expected that 
after authorization is disabled, the authorization related file, the ACL record, should not be open to users and not be changed. Otherwise, after the authorization next enablement, the changed ACL takes action and users get unexpected authority."
HBASE-19153	"The latest version of LruBolckCache, I found the code logic of cache too big bolcks is inconsistent with annotation.
If follow the notes, the code should look like this:

if (buf.heapSize() > maxBlockSize) {
      // If there are a lot of blocks that are too
      // big this can make the logs way too noisy.
      // So we log 2%
      if (stats.failInsert() % 50 != 0) {
        return;
      }
      LOG.warn(""Trying to cache too large a block ""
            + cacheKey.getHfileName() + "" @ ""
            + cacheKey.getOffset()
            + "" is "" + buf.heapSize()
            + "" which is larger than "" + maxBlockSize);
      
    }

"
