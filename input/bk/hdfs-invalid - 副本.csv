"Update hadoop.version in the trunk, which is causing compilation failure",HDFS-13952,"<hadoop.version>3.2.0-SNAPSHOT</hadoop.version> to

<hadoop.version>3.3.0-SNAPSHOT</hadoop.version>

聽

On trunk compilation failure:

[WARNING] Rule 0: org.apache.maven.plugins.enforcer.RequireProperty failed with message:
The hadoop.version property should be set and should be 3.3.0-SNAPSHOT."
DataNodeManager#getDataNodeStorageInfos not backward compatibility,HDFS-9513,"We is upgraded our new HDFS cluster to 2.7.1,but we YARN cluster is 2.2.0(8000+,it's too hard to upgrade as soon as HDFS cluster).
The compatible case happened  datasteamer do pipeline recovery, the NN need DN's storageInfo to update pipeline, and the storageIds is pair of pipleline's DN,but HDFS support storage type feature from 2.3.0 [HDFS-2832|https://issues.apache.org/jira/browse/HDFS-2832], older version not have storageId ,although the protobuf serialization make the protocol compatible,but the client  will throw remote exception as ArrayIndexOutOfBoundsException.
----
the exception stack is below:
{noformat}
2015-12-05 20:26:38,291 ERROR [Thread-4] org.apache.hadoop.hdfs.DFSClient: Failed to close file XXX
org.apache.hadoop.ipc.RemoteException(java.lang.ArrayIndexOutOfBoundsException): 0
	at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.getDatanodeStorageInfos(DatanodeManager.java:513)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.updatePipelineInternal(FSNamesystem.java:6439)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.updatePipeline(FSNamesystem.java:6404)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.updatePipeline(NameNodeRpcServer.java:892)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.updatePipeline(ClientNamenodeProtocolServerSideTranslatorPB.java:997)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1066)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)

	at org.apache.hadoop.ipc.Client.call(Client.java:1347)
	at org.apache.hadoop.ipc.Client.call(Client.java:1300)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.updatePipeline(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.updatePipeline(ClientNamenodeProtocolTranslatorPB.java:801)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.updatePipeline(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1047)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:823)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:475)
{noformat}
"
Ozone: mvn package compilation fails on HDFS-7240,HDFS-12515,"Creation of a package on ozone(HDFS-7240) fails
{{mvn clean package -Pdist -DskipTests -Dtar -Dmaven.javadoc.skip=true}}

{code}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project hadoop-mapreduce-examples: Compilation failure: Compilation failure: 
[ERROR] /Users/msingh/code/work/apache/cblock/ozone_review2/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraGen.java:[50,17] package org.slf4j does not exist
[ERROR] /Users/msingh/code/work/apache/cblock/ozone_review2/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraGen.java:[51,17] package org.slf4j does not exist
[ERROR] /Users/msingh/code/work/apache/cblock/ozone_review2/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraGen.java:[69,24] cannot find symbol
[ERROR]   symbol:   class Logger
[ERROR]   location: class org.apache.hadoop.examples.terasort.TeraGen
[ERROR] /Users/msingh/code/work/apache/cblock/ozone_review2/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/BaileyBorweinPlouffe.java:[53,17] package org.slf4j does not exist
[ERROR] /Users/msingh/code/work/apache/cblock/ozone_review2/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/BaileyBorweinPlouffe.java:[86,24] cannot find symbol
[ERROR]   symbol:   class Logger
[ERROR]   location: class org.apache.hadoop.examples.BaileyBorweinPlouffe
[ERROR] /Users/msingh/code/work/apache/cblock/ozone_review2/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/DBCountPageView.java:[51,17] package org.slf4j does not exist
[ERROR] /Users/msingh/code/work/apache/cblock/ozone_review2/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/DBCountPageView.java:[80,24] cannot find symbol
[ERROR]   symbol:   class Logger
[ERROR]   location: class org.apache.hadoop.examples.DBCountPageView
[ERROR] /Users/msingh/code/work/apache/cblock/ozone_review2/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/DistSum.java:[57,17] package org.slf4j does not exist
[ERROR] /Users/msingh/code/work/apache/cblock/ozone_review2/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/DistSum.java:[69,24] cannot find symbol
[ERROR]   symbol:   class Logger
[ERROR]   location: class org.apache.hadoop.examples.pi.DistSum
[ERROR] /Users/msingh/code/work/apache/cblock/ozone_review2/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/dancing/DancingLinks.java:[23,17] package org.slf4j does not exist
[ERROR] /Users/msingh/code/work/apache/cblock/ozone_review2/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/dancing/DancingLinks.java:[38,24] cannot find symbol
[ERROR]   symbol:   class Logger
[ERROR]   location: class org.apache.hadoop.examples.dancing.DancingLinks<ColumnName>
[ERROR] /Users/msingh/code/work/apache/cblock/ozone_review2/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraSort.java:[40,17] package org.slf4j does not exist
[ERROR] /Users/msingh/code/work/apache/cblock/ozone_review2/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraSort.java:[50,24] cannot find symbol
[ERROR]   symbol:   class Logger
[ERROR]   location: class org.apache.hadoop.examples.terasort.TeraSort
[ERROR] /Users/msingh/code/work/apache/cblock/ozone_review2/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraOutputFormat.java:[40,17] package org.slf4j does not exist
[ERROR] /Users/msingh/code/work/apache/cblock/ozone_review2/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraOutputFormat.java:[46,24] cannot find symbol
[ERROR]   symbol:   class Logger
[ERROR]   location: class org.apache.hadoop.examples.terasort.TeraOutputFormat
[ERROR] /Users/msingh/code/work/apache/cblock/ozone_review2/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraScheduler.java:[29,17] package org.slf4j does not exist
[ERROR] /Users/msingh/code/work/apache/cblock/ozone_review2/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraScheduler.java:[34,24] cannot find symbol
[ERROR]   symbol:   class Logger
[ERROR]   location: class org.apache.hadoop.examples.terasort.TeraScheduler
[ERROR] /Users/msingh/code/work/apache/cblock/ozone_review2/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraGen.java:[69,37] cannot find symbol
[ERROR]   symbol:   variable LoggerFactory
[ERROR]   location: class org.apache.hadoop.examples.terasort.TeraGen
[ERROR] /Users/msingh/code/work/apache/cblock/ozone_review2/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/BaileyBorweinPlouffe.java:[87,7] cannot find symbol
[ERROR]   symbol:   variable LoggerFactory
[ERROR]   location: class org.apache.hadoop.examples.BaileyBorweinPlouffe
[ERROR] /Users/msingh/code/work/apache/cblock/ozone_review2/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/DBCountPageView.java:[81,7] cannot find symbol
[ERROR]   symbol:   variable LoggerFactory
[ERROR]   location: class org.apache.hadoop.examples.DBCountPageView
[ERROR] /Users/msingh/code/work/apache/cblock/ozone_review2/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/pi/DistSum.java:[69,37] cannot find symbol
[ERROR]   symbol:   variable LoggerFactory
[ERROR]   location: class org.apache.hadoop.examples.pi.DistSum
[ERROR] /Users/msingh/code/work/apache/cblock/ozone_review2/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/dancing/DancingLinks.java:[38,37] cannot find symbol
[ERROR]   symbol:   variable LoggerFactory
[ERROR]   location: class org.apache.hadoop.examples.dancing.DancingLinks<ColumnName>
[ERROR] /Users/msingh/code/work/apache/cblock/ozone_review2/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraSort.java:[50,37] cannot find symbol
[ERROR]   symbol:   variable LoggerFactory
[ERROR]   location: class org.apache.hadoop.examples.terasort.TeraSort
[ERROR] /Users/msingh/code/work/apache/cblock/ozone_review2/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraOutputFormat.java:[47,7] cannot find symbol
[ERROR]   symbol:   variable LoggerFactory
[ERROR]   location: class org.apache.hadoop.examples.terasort.TeraOutputFormat
[ERROR] /Users/msingh/code/work/apache/cblock/ozone_review2/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/TeraScheduler.java:[35,7] cannot find symbol
[ERROR]   symbol:   variable LoggerFactory
[ERROR]   location: class org.apache.hadoop.examples.terasort.TeraScheduler
{code}"
Ozone: Ozone data placement is not even,HDFS-12461,"On a machine with 3 data disks, Ozone keeps on picking the same disk to place all containers. Looks like we have a bug in the round robin selection of disks.

Steps to Reproduce:
1. Install an Ozone cluster.
2. Make sure that datanodes have more than one disk.
3. Run corona few times, each run creates more containers.
4. Login into the data node.
5. Run a command like tree or ls -R /data or independently verify each location.


"
 Data node process crashes after kernel upgrade,HDFS-12029," We have seen that when Linux kernel is upgraded to address a specific CVE 
 ( https://access.redhat.com/security/vulnerabilities/stackguard ) it might cause a datanode crash.

We have observed this issue while upgrading from 3.10.0-514.6.2 to 3.10.0-514.21.2 versions of the kernel.

Original kernel fix is here -- https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=1be7107fbe18eed3e319a6c3e83c78254b693acb

Datanode fails with the following stack trace, 

{noformat}

# 
# A fatal error has been detected by the Java Runtime Environment: 
# 
# SIGBUS (0x7) at pc=0x00007f458d078b7c, pid=13214, tid=139936990349120 
# 
# JRE version: (8.0_40-b25) (build ) 
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.40-b25 mixed mode linux-amd64 compressed oops) 
# Problematic frame: 
# j java.lang.Object.<clinit>()V+0 
# 
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again 
# 
# An error report file with more information is saved as: 
# /tmp/hs_err_pid13214.log 
# 
# If you would like to submit a bug report, please visit: 
# http://bugreport.java.com/bugreport/crash.jsp 
# 
{noformat}

The root cause is a failure in jsvc. If we pass a greater than 1MB value as the stack size argument, this can be mitigated.  Something like:

{code}
exec ""$JSVC"" \
-Xss2m
org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter ""$@""
{code}

This JIRA tracks potential fixes for this problem. We don't have data on how this impacts other applications that run on datanode as this might impact datanodes memory usage.

"
"In failover state, HDFS commands fail on ""Retrying connect to server:<master1>""",HDFS-8662,"Steps (on a 3M cluster):
1. Poweroff master1.
2. Failover to master2.
3. On HDFS commands, there are messages indicating failed attempt to connect to master1, though namenode is active and running on master2.
{code}
infinity2:~ # su - hdfs -c ""hdfs haadmin -getServiceState nn2""
active
infinity2:~ # su hdfs -c ""hdfs dfs -ls /""
15/06/24 15:28:15 INFO ipc.Client: Retrying connect to server: infinity1.labs.teradata.com/39.0.24.1:8020. Already tried 0 time(s); retry policy is RetryPolicy[MultipleLinearRandomRetry[6x10000ms, 10x60000ms], TryOnceThenFail]
15/06/24 15:28:31 INFO ipc.Client: Retrying connect to server: infinity1.labs.teradata.com/39.0.24.1:8020. Already tried 1 time(s); retry policy is RetryPolicy[MultipleLinearRandomRetry[6x10000ms, 10x60000ms], TryOnceThenFail]
Found 7 items
drwxrwxrwx   - yarn   hadoop          0 2015-06-24 14:42 /app-logs
drwxr-xr-x   - hdfs   hdfs            0 2015-06-24 06:16 /apps
drwxr-xr-x   - hdfs   hdfs            0 2015-06-24 06:14 /hdp
drwxr-xr-x   - mapred hdfs            0 2015-06-24 06:15 /mapred
drwxrwxrwx   - mapred hadoop          0 2015-06-24 06:15 /mr-history
drwxrwxrwx   - hdfs   hdfs            0 2015-06-24 06:16 /tmp
drwxr-xr-x   - hdfs   hdfs            0 2015-06-24 06:17 /user
infinity2:~ # ps -fu hdfs
UID        PID  PPID  C STIME TTY          TIME CMD
hdfs     16318     1  0 06:25 ?        00:00:40 /opt/teradata/jvm64/jdk8/bin/java -Dproc_journalnode -Xmx4096m -Dhdp.version=2.3.0.0-2462 -Djava.net.prefe
hdfs     16859     1  0 06:26 ?        00:00:29 /opt/teradata/jvm64/jdk8/bin/java -Dproc_zkfc -Xmx4096m -Dhdp.version=2.3.0.0-2462 -Djava.net.preferIPv4St
hdfs     17791     1  0 06:26 ?        00:02:34 /opt/teradata/jvm64/jdk8/bin/java -Dproc_namenode -Xmx4096m -Dhdp.version=2.3.0.0-2462 -Djava.net.preferIP
infinity2:~ #
{code}
"
SNN crashed because edit log has gap after upgrade,HDFS-5553,"As HDFS-5550 depicted, journal nodes doesn't upgrade, so I change the VERSION manually according to NN's VERSION.
then , I do upgrade and get this exception. I also marked this as Blocker.

my steps as following:
It's a fresh cluster with hadoop-2.0.1 before upgrading.

0) install hadoop-2.2.0 hadoop package on all nodes.
1) stop-dfs.sh on active NN
2) disable HA in the core-site.xml and hdfs-site.xml on active NN and SNN
3) start-dfs.sh -upgrade -clusterId test-cluster on active NN(only one NN now.)
4) stop-dfs.sh after active NN started successfully.
5) enable HA in the core-site.xml and hdfs-site.xml on active NN and SNN
6) change all journal nodes' VERSION manually according to NN's VERSION
7) rm -f 'dfs.journalnode.edits.dir'/test-cluster/current/* (just keep VERSION here)
8) delete all data under 'dfs.namenode.name.dir' on SNN
9) scp -r 'dfs.namenode.name.dir' to SNN on active NN
10) start-dfs.sh




"
Journal Node is not upgrade during HDFS upgrade,HDFS-5550,"HDFS upgrade from 2.0.3 to 2.2.0,  but Journal node doesn't upgrade, a directly problem is VERSION file is old.

so SNN relay edit log through http://hostname:8480/getJournal?*** get 403, because VERSION is mismatched.

I marked this as Blocker, does that OK?

"
Exception in createBlockOutputStream java.io.EOFException: Premature EOF: no length prefix available,HDFS-8475,"Scenraio:
=========
write a file
corrupt block manually

Exception stack trace- 

2015-05-24 02:31:55.291 INFO [T-33716795] [org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer] Exception in createBlockOutputStream
java.io.EOFException: Premature EOF: no length prefix available
        at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:1492)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1155)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1088)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:514)
[5/24/15 2:31:55:291 UTC] 02027a3b DFSClient     I org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer createBlockOutputStream Exception in createBlockOutputStream
                                 java.io.EOFException: Premature EOF: no length prefix available
        at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:1492)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1155)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1088)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:514)

2015-05-24 02:31:55.291 INFO [T-33716795] [org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer] Abandoning BP-176676314-10.108.106.59-1402620296713:blk_1404621403_330880579
[5/24/15 2:31:55:291 UTC] 02027a3b DFSClient     I org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer nextBlockOutputStream Abandoning BP-176676314-10.108.106.59-1402620296713:blk_1404621403_330880579
2015-05-24 02:31:55.299 INFO [T-33716795] [org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer] Excluding datanode 10.108.106.59:50010
[5/24/15 2:31:55:299 UTC] 02027a3b DFSClient     I org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer nextBlockOutputStream Excluding datanode 10.108.106.59:50010
2015-05-24 02:31:55.300 WARNING [T-33716795] [org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer] DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /var/db/opera/files/B4889CCDA75F9751DDBB488E5AAB433E/BE4DAEF290B7136ED6EF3D4B157441A2/BE4DAEF290B7136ED6EF3D4B157441A2-4.pag could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1384)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2477)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:555)


[5/24/15 2:31:55:300 UTC] 02027a3b DFSClient     W org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer run DataStreamer Exception
                                 org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /var/db/opera/files/B4889CCDA75F9751DDBB488E5AAB433E/BE4DAEF290B7136ED6EF3D4B157441A2/BE4DAEF290B7136ED6EF3D4B157441A2-4.pag could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1384)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2477)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:555)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:387)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:59582)
        
		
2015-05-24 02:31:55.301 WARNING [T-880] [E-AA380B730CF751508DC9163BAC8E4D1D] [job:B94FEC9411E2C8563C842833D78142CF] [org.apache.hadoop.hdfs.DFSOutputStream] Error while syncing
org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /var/db/opera/files/B4889CCDA75F9751DDBB488E5AAB433E/BE4DAEF290B7136ED6EF3D4B157441A2/BE4DAEF290B7136ED6EF3D4B157441A2-4.pag could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1384)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2477)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:555)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:387)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:59582)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2048)
"
Webhdfs not working with shell command when kerberos security+https is enabled.,HDFS-9471,"*Client exception*
{code}
secure@host85:/opt/hdfsdata/HA/install/hadoop/namenode/bin> ./hdfs dfs -ls webhdfs://x.x.x.x:50070/test
15/11/25 18:46:55 ERROR web.WebHdfsFileSystem: Unable to get HomeDirectory from original File System
java.net.SocketException: Unexpected end of file from server
        at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:792)
{code}

*Exception in namenode log*
{code}
2015-11-26 11:03:18,231 WARN org.mortbay.log: EXCEPTION
javax.net.ssl.SSLException: Unrecognized SSL message, plaintext connection?
        at sun.security.ssl.InputRecord.handleUnknownRecord(InputRecord.java:710)
        at sun.security.ssl.InputRecord.read(InputRecord.java:527)
        at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:961)
        at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1363)
        at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1391)
        at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1375)
        at org.mortbay.jetty.security.SslSocketConnector$SslConnection.run(SslSocketConnector.java:708)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
{code}

This is because URL schema hard coded in {{WebHdfsFileSystem.getTransportScheme()}}.

{code}
 /**
   * return the underlying transport protocol (http / https).
   */
  protected String getTransportScheme() {
    return ""http"";
  }
{code}"
HDFS-2317 in trunk appears to have broken the build,HDFS-2329,"Trunk HDFS build fails with 

{noformat}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.3.2:compile (default-compile) on project hadoop-hdfs: Compilation failure
[ERROR] /home/evans/src/hadoop-git/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java:[209,22] type parameters of <T>T cannot be determined; no unique maximal instance exists for type variable T with upper bounds T,java.lang.Object
{noformat}

On my systems
{noformat}
$ java -version
java version ""1.6.0_22""
Java(TM) SE Runtime Environment (build 1.6.0_22-b04)
Java HotSpot(TM) Server VM (build 17.1-b03, mixed mode)
{noformat}

{noformat}
 $ svn info
Path: .
URL: http://svn.apache.org/repos/asf/hadoop/common/trunk
Repository Root: http://svn.apache.org/repos/asf
Repository UUID: 13f79535-47bb-0310-9956-ffa450edef68
Revision: 1170215
Node Kind: directory
Schedule: normal
Last Changed Author: szetszwo
Last Changed Rev: 1170085
Last Changed Date: 2011-09-13 03:34:27 -0500 (Tue, 13 Sep 2011)
{noformat}"
HDFS truncate may remove data from the supposedly read-only previous/ directory during an upgrade,HDFS-7646,"During a DataNode layout version upgrade, HDFS creates hardlinks.  These hardlinks allow the same block to be accessible from both the current/ and previous/ directories.  Rollback is possible by deleting the current/ directory and renaming previous/ to current/.

However, if the user truncates one of these hardlinked block files, it effectively eliminates the ability to roll back to the previous data.

We probably need to disable truncation-in-place during a DataNode upgrade."
HDFS compiling failure on trunk,HDFS-7288,"See https://builds.apache.org/job/PreCommit-YARN-Build/5545/artifact/patchprocess/trunkJavacWarnings.txt

{code}
[INFO] 40 errors 
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Apache Hadoop Main ................................ SUCCESS [  0.285 s]
[INFO] Apache Hadoop Project POM ......................... SUCCESS [  0.462 s]
[INFO] Apache Hadoop Annotations ......................... SUCCESS [  1.368 s]
[INFO] Apache Hadoop Project Dist POM .................... SUCCESS [  0.116 s]
[INFO] Apache Hadoop Assemblies .......................... SUCCESS [  0.108 s]
[INFO] Apache Hadoop Maven Plugins ....................... SUCCESS [  2.382 s]
[INFO] Apache Hadoop MiniKDC ............................. SUCCESS [  2.612 s]
[INFO] Apache Hadoop Auth ................................ SUCCESS [  3.365 s]
[INFO] Apache Hadoop Auth Examples ....................... SUCCESS [  1.018 s]
[INFO] Apache Hadoop Common .............................. SUCCESS [ 20.312 s]
[INFO] Apache Hadoop NFS ................................. SUCCESS [  3.382 s]
[INFO] Apache Hadoop KMS ................................. SUCCESS [  4.705 s]
[INFO] Apache Hadoop Common Project ...................... SUCCESS [  0.029 s]
[INFO] Apache Hadoop HDFS ................................ FAILURE [ 25.008 s]
[INFO] Apache Hadoop HttpFS .............................. SKIPPED
[INFO] Apache Hadoop HDFS BookKeeper Journal ............. SKIPPED
[INFO] Apache Hadoop HDFS-NFS ............................ SKIPPED
[INFO] Apache Hadoop HDFS Project ........................ SKIPPED
[INFO] hadoop-yarn ....................................... SKIPPED
[INFO] hadoop-yarn-api ................................... SKIPPED
[INFO] hadoop-yarn-common ................................ SKIPPED
[INFO] hadoop-yarn-server ................................ SKIPPED
[INFO] hadoop-yarn-server-common ......................... SKIPPED
[INFO] hadoop-yarn-server-nodemanager .................... SKIPPED
[INFO] hadoop-yarn-server-web-proxy ...................... SKIPPED
[INFO] hadoop-yarn-server-applicationhistoryservice ...... SKIPPED
[INFO] hadoop-yarn-server-resourcemanager ................ SKIPPED
[INFO] hadoop-yarn-server-tests .......................... SKIPPED
[INFO] hadoop-yarn-client ................................ SKIPPED
[INFO] hadoop-yarn-server-sharedcachemanager ............. SKIPPED
[INFO] hadoop-yarn-applications .......................... SKIPPED
[INFO] hadoop-yarn-applications-distributedshell ......... SKIPPED
[INFO] hadoop-yarn-applications-unmanaged-am-launcher .... SKIPPED
[INFO] hadoop-yarn-site .................................. SKIPPED
[INFO] hadoop-yarn-registry .............................. SKIPPED
[INFO] hadoop-yarn-project ............................... SKIPPED
[INFO] hadoop-mapreduce-client ........................... SKIPPED
[INFO] hadoop-mapreduce-client-core ...................... SKIPPED
[INFO] hadoop-mapreduce-client-common .................... SKIPPED
[INFO] hadoop-mapreduce-client-shuffle ................... SKIPPED
[INFO] hadoop-mapreduce-client-app ....................... SKIPPED
[INFO] hadoop-mapreduce-client-hs ........................ SKIPPED
[INFO] hadoop-mapreduce-client-jobclient ................. SKIPPED
[INFO] hadoop-mapreduce-client-hs-plugins ................ SKIPPED
[INFO] hadoop-mapreduce-client-nativetask ................ SKIPPED
[INFO] Apache Hadoop MapReduce Examples .................. SKIPPED
[INFO] hadoop-mapreduce .................................. SKIPPED
[INFO] Apache Hadoop MapReduce Streaming ................. SKIPPED
[INFO] Apache Hadoop Distributed Copy .................... SKIPPED
[INFO] Apache Hadoop Archives ............................ SKIPPED
[INFO] Apache Hadoop Rumen ............................... SKIPPED
[INFO] Apache Hadoop Gridmix ............................. SKIPPED
[INFO] Apache Hadoop Data Join ........................... SKIPPED
[INFO] Apache Hadoop Ant Tasks ........................... SKIPPED
[INFO] Apache Hadoop Extras .............................. SKIPPED
[INFO] Apache Hadoop Pipes ............................... SKIPPED
[INFO] Apache Hadoop OpenStack support ................... SKIPPED
[INFO] Apache Hadoop Amazon Web Services support ......... SKIPPED
[INFO] Apache Hadoop Azure support ....................... SKIPPED
[INFO] Apache Hadoop Client .............................. SKIPPED
[INFO] Apache Hadoop Mini-Cluster ........................ SKIPPED
[INFO] Apache Hadoop Scheduler Load Simulator ............ SKIPPED
[INFO] Apache Hadoop Tools Dist .......................... SKIPPED
[INFO] Apache Hadoop Tools ............................... SKIPPED
[INFO] Apache Hadoop Distribution ........................ SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 01:08 min
[INFO] Finished at: 2014-10-24T18:42:14+00:00
[INFO] Final Memory: 47M/813M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hadoop-hdfs: Compilation failure: Compilation failure:
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitShm.java:[39,15] sun.misc.Unsafe is Sun proprietary API and may be removed in a future release
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitShm.java:[53,23] sun.misc.Unsafe is Sun proprietary API and may be removed in a future release
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitShm.java:[55,17] sun.misc.Unsafe is Sun proprietary API and may be removed in a future release
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java:[726,16] cannot find symbol
[ERROR] symbol  : class TokenKindParam
[ERROR] location: class org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java:[728,16] cannot find symbol
[ERROR] symbol  : class TokenServiceParam
[ERROR] location: class org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java:[767,16] cannot find symbol
[ERROR] symbol  : class TokenKindParam
[ERROR] location: class org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java:[769,16] cannot find symbol
[ERROR] symbol  : class TokenServiceParam
[ERROR] location: class org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java:[806,12] cannot find symbol
[ERROR] symbol  : class TokenKindParam
[ERROR] location: class org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java:[807,12] cannot find symbol
[ERROR] symbol  : class TokenServiceParam
[ERROR] location: class org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DeprecatedUTF8.java:[38,56] [deprecation] org.apache.hadoop.io.UTF8 in org.apache.hadoop.io has been deprecated
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[32,31] [deprecation] org.apache.xml.serialize.OutputFormat in org.apache.xml.serialize has been deprecated
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[33,31] [deprecation] org.apache.xml.serialize.XMLSerializer in org.apache.xml.serialize has been deprecated
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java:[725,18] cannot find symbol
[ERROR] symbol  : variable TokenKindParam
[ERROR] location: class org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java:[725,53] cannot find symbol
[ERROR] symbol  : variable TokenKindParam
[ERROR] location: class org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java:[727,18] cannot find symbol
[ERROR] symbol  : variable TokenServiceParam
[ERROR] location: class org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java:[727,56] cannot find symbol
[ERROR] symbol  : variable TokenServiceParam
[ERROR] location: class org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java:[766,18] cannot find symbol
[ERROR] symbol  : variable TokenKindParam
[ERROR] location: class org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java:[766,53] cannot find symbol
[ERROR] symbol  : variable TokenKindParam
[ERROR] location: class org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java:[768,18] cannot find symbol
[ERROR] symbol  : variable TokenServiceParam
[ERROR] location: class org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java:[768,56] cannot find symbol
[ERROR] symbol  : variable TokenServiceParam
[ERROR] location: class org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java:[970,17] [deprecation] org.apache.hadoop.hdfs.server.namenode.FSImageFormat.Saver in org.apache.hadoop.hdfs.server.namenode.FSImageFormat has been deprecated
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java:[970,49] [deprecation] org.apache.hadoop.hdfs.server.namenode.FSImageFormat.Saver in org.apache.hadoop.hdfs.server.namenode.FSImageFormat has been deprecated
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitShm.java:[57,16] sun.misc.Unsafe is Sun proprietary API and may be removed in a future release
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/shortcircuit/ShortCircuitShm.java:[59,14] sun.misc.Unsafe is Sun proprietary API and may be removed in a future release
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java:[1857,8] [deprecation] setStorageUuid(java.lang.String) in org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageReportProto.Builder has been deprecated
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/PBHelper.java:[1866,33] [deprecation] getStorageUuid() in org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.StorageReportProto has been deprecated
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolClientSideTranslatorPB.java:[216,16] [deprecation] setStorageUuid(java.lang.String) in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto.Builder has been deprecated
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java:[419,35] [deprecation] getReplica(java.lang.String,long) in org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi has been deprecated
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java:[1114,14] [deprecation] getDefaultBlockSize() in org.apache.hadoop.fs.FileSystem has been deprecated
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/WebHdfsFileSystem.java:[1120,15] [deprecation] getDefaultReplication() in org.apache.hadoop.fs.FileSystem has been deprecated
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java:[159,14] [deprecation] getDefaultBlockSize() in org.apache.hadoop.fs.FileSystem has been deprecated
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java:[164,15] [deprecation] getDefaultReplication() in org.apache.hadoop.fs.FileSystem has been deprecated
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java:[412,33] [deprecation] primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet<org.apache.hadoop.fs.CreateFlag>,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options.ChecksumOpt) in org.apache.hadoop.fs.FileSystem has been deprecated
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java:[1068,26] [deprecation] getServerDefaults() in org.apache.hadoop.fs.FileSystem has been deprecated
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolServerSideTranslatorPB.java:[205,57] [deprecation] getStorageUuid() in org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.StorageReceivedDeletedBlocksProto has been deprecated
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocolPB/DatanodeProtocolServerSideTranslatorPB.java:[205,18] [deprecation] StorageReceivedDeletedBlocks(java.lang.String,org.apache.hadoop.hdfs.server.protocol.ReceivedDeletedBlockInfo[]) in org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks has been deprecated
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[55,4] [deprecation] org.apache.xml.serialize.OutputFormat in org.apache.xml.serialize has been deprecated
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[55,33] [deprecation] org.apache.xml.serialize.OutputFormat in org.apache.xml.serialize has been deprecated
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[59,4] [deprecation] org.apache.xml.serialize.XMLSerializer in org.apache.xml.serialize has been deprecated
[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-YARN-Build/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:[59,35] [deprecation] org.apache.xml.serialize.XMLSerializer in org.apache.xml.serialize has been deprecated
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hadoop-hdfs
{code}"
Site Scripting and Phishing Through Frames in browseDirectory.jsp,HDFS-4901,"It is possible to steal or manipulate customer session and cookies, which might be used to impersonate a legitimate user,
allowing the hacker to view or alter user records, and to perform transactions as that user.
e.g.
GET /browseDirectory.jsp? dir=%2Fhadoop'""/><script>alert(759)</script> &namenodeInfoPort=50070

Also;

Phishing Through Frames

Try:
GET /browseDirectory.jsp? dir=%2Fhadoop%27%22%3E%3Ciframe+src%3Dhttp%3A%2F%2Fdemo.testfire.net%2Fphishing.html%3E
&namenodeInfoPort=50070 HTTP/1.1
Cookie: JSESSIONID=qd9i8tuccuam1cme71swr9nfi
Accept-Language: en-US
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;


"
TestParallelRead times out consistently on Jenkins,HDFS-2498,During last several Jenkins builds TestParallelRead consistently fails. See Hadoop-Hdfs-22-branch for logs.
Provide a HDFS HA guide,HDFS-3354,We should add some _reasonable_ HDFS HA guide for hadoop-2.0.0-alpha since it's a *major* feature.
Test append and quotas ,HDFS-1515,There is no test coverage for quotas and append. Let's add a test to TestQuota that covers that quotas are updated correctly when appending to a file.
TestWriteConfigurationToDFS is timing out on trunk,HDFS-1576,"On a fresh checkout, TestWriteConfigurationToDFS, runs, errors out and then never returns, blocking all subsequent tests.  This is reproducible with -Dtestcase=
{noformat}
    [junit] Running org.apache.hadoop.hdfs.TestWriteConfigurationToDFS
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 60.023 sec
{noformat}
"
Namenode in infinite loop for removing/recovering lease.,HDFS-650,"2009-09-23 18:05:48,929 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Lease Monitor: Removing lease [Lease.  Holder: DFSClient_2121971893, pendingcreates: 1], sortedLeases.size()=: 1
2009-09-23 18:05:48,929 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Recovering lease=[Lease.  Holder: DFSClient_2121971893, pendingcreates: 1], src=/54_upload/GALGAME/<椋庡瓙銇紓娴佺摱@Sumisora+2Dgal>CANVAS3V1.ISO
2009-09-23 18:05:48,929 WARN org.apache.hadoop.hdfs.StateChange: DIR* NameSystem.internalReleaseCreate: attempt to release a create lock on /54_upload/GALGAME/<椋庡瓙銇紓娴佺摱@Sumisora+2Dgal>CANVAS3V1.ISO but file is already closed.
2009-09-23 18:05:48,929 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Lease Monitor: Removing lease [Lease.  Holder: DFSClient_2121971893, pendingcreates: 1], sortedLeases.size()=: 1
2009-09-23 18:05:48,929 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Recovering lease=[Lease.  Holder: DFSClient_2121971893, pendingcreates: 1], src=/54_upload/GALGAME/<椋庡瓙銇紓娴佺摱@Sumisora+2Dgal>CANVAS3V1.ISO
2009-09-23 18:05:48,929 WARN org.apache.hadoop.hdfs.StateChange: DIR* NameSystem.internalReleaseCreate: attempt to release a create lock on /54_upload/GALGAME/<椋庡瓙銇紓娴佺摱@Sumisora+2Dgal>CANVAS3V1.ISO but file is already closed.
2009-09-23 18:05:48,929 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Lease Monitor: Removing lease [Lease.  Holder: DFSClient_2121971893, pendingcreates: 1], sortedLeases.size()=: 1
2009-09-23 18:05:48,929 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Recovering lease=[Lease.  Holder: DFSClient_2121971893, pendingcreates: 1], src=/54_upload/GALGAME/<椋庡瓙銇紓娴佺摱@Sumisora+2Dgal>CANVAS3V1.ISO
2009-09-23 18:05:48,929 WARN org.apache.hadoop.hdfs.StateChange: DIR* NameSystem.internalReleaseCreate: attempt to release a create lock on /54_upload/GALGAME/<椋庡瓙銇紓娴佺摱@Sumisora+2Dgal>CANVAS3V1.ISO but file is already closed.
2009-09-23 18:05:48,929 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Lease Monitor: Removing lease [Lease.  Holder: DFSClient_2121971893, pendingcreates: 1], sortedLeases.size()=: 1
2009-09-23 18:05:48,929 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Recovering lease=[Lease.  Holder: DFSClient_2121971893, pendingcreates: 1], src=/54_upload/GALGAME/<椋庡瓙銇紓娴佺摱@Sumisora+2Dgal>CANVAS3V1.ISO
2009-09-23 18:05:48,929 WARN org.apache.hadoop.hdfs.StateChange: DIR* NameSystem.internalReleaseCreate: attempt to release a create lock on /54_upload/GALGAME/<椋庡瓙銇紓娴佺摱@Sumisora+2Dgal>CANVAS3V1.ISO but file is already closed.
2009-09-23 18:05:48,929 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Lease Monitor: Removing lease [Lease.  Holder: DFSClient_2121971893, pendingcreates: 1], sortedLeases.size()=: 1"
FSNameSystem#renewDelegationToken doesn't compile,HDFS-990,"The following code returns a boolean but a long (the new expiration time) is expected. Looks like HDFS-986 introduced this.

{code} 
 public long renewDelegationToken(Token<DelegationTokenIdentifier> token)
      throws InvalidToken, IOException {
    String renewer = UserGroupInformation.getCurrentUser().getShortUserName();
    return dtSecretManager.renewToken(token, renewer);
  }
{code}"
many tests in a patch verification process are failing without an error code,HDFS-794,"Many tests of a patch verification are failing silently. Hudson reports them as PASSED.
Because of this problem patches aren't properly verified since at least November 11th, 2009.
This problem can be observed both in 0.21 and 0.22."
DataNode breaching Xceiver Count,HDFS-13828,"We were observing the breach聽of the聽xceiver count 4096, On a particular set of nodes from 5 - 8 nodes in a 900 nodes cluster.
And we stopped the聽datanode聽services on those nodes and made to replicate across the cluster. After that also, we observed the same issue on a new set of nodes.

Q1: Why on a particular node, and also after decommissioning the node the data should be replicated across the cluster, But why again difference set of node?

Assumptions :
Reading a particular block/ data on that node might be the cause for this but it should be mitigated after the decommission聽but not why?聽So suspected that those MR jobs are triggered from Hive, so the query might be referring to the same block聽mulitple聽times聽 in different stages and creating this issue?

From Thread Dump :

Thread dump聽of datanode says that out of 4090+ xceiver threads created on that node nearly聽4000+ where belong to the same AppId of multiple mappers with state no operation.

聽

Any suggestions on this?

聽

聽

聽"
Ozone: Make hadoop-common ozone free,HDFS-13221,"From the voting thread comments from [~daryn]. 
{noformat}
Common
聽
Appear to be a number of superfluous changes.聽聽The conf servlet must not be
polluted with specific references and logic for ozone.聽聽We don鈥檛 create
dependencies from common to hdfs, mapred, yarn, hive, etc.聽聽Common must be
鈥渙zone free鈥? 
{noformat}
This JIRA is to make sure that notions of HDSL or Ozone abstractions have not leaked into hadoop-common; This JIRA will clean up the current instances. [~daryn] Thanks for pointing this out.

聽"
"HDFS File Not Removed Despite Successful ""Moved to .Trash"" Message",HDFS-12688,"Wrote a simple script to delete and create a file and ran it multiple times. However, some executions of the script randomly threw a FileAlreadyExists error while the others succeeded despite successful hdfs dfs -rm command. The script is as below, I have reproduced it in two different environments -- 

hdfs dfs -ls  /user/shriya/shell_test/
echo ""starting hdfs remove **************"" 
hdfs dfs -rm -r -f /user/shriya/shell_test/wordcountOutput
 echo ""hdfs compeleted!""
hdfs dfs -ls  /user/shriya/shell_test/
echo ""starting mapReduce*******************************""
mapred job -libjars /data/home/shriya/shell_test/hadoop-mapreduce-client-jobclient-2.7.1.jar -submit /data/home/shriya/shell_test/wordcountJob.xml

The message confirming successful move -- 

17/10/19 14:49:12 INFO fs.TrashPolicyDefault: Moved: 'hdfs://nameservice1/user/shriya/shell_test/wordcountOutput' to trash at: hdfs://nameservice1/user/shriya/.Trash/Current/user/shriya/shell_test/wordcountOutput1508438952728

The contents of subsequent -ls after -rm also showed that the file still existed)

The error I got when my MapReduce job tried to create the file -- 

17/10/19 14:50:00 WARN security.UserGroupInformation: PriviledgedActionException as:<REDACTED> (auth:KERBEROS) cause:org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://nameservice1/user/shriya/shell_test/wordcountOutput already exists
Exception in thread ""main"" org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://nameservice1/user/shriya/shell_test/wordcountOutput already exists
        at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)
        at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:272)
        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:143)
        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1307)
        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1304)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1304)
        at org.apache.hadoop.mapreduce.tools.CLI.run(CLI.java:315)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)
        at org.apache.hadoop.mapred.JobClient.main(JobClient.java:1277)"
"Remove a user defined EC Policy,the policy is not removed from the userPolicies map",HDFS-12088,"When user remove an user defined EC policy, it needs to remove the policy from the userPolicies Map but not only remove from the enabledPolicies Map.Otherwise, after remove the user defined EC policy, user can recover the EC policy by enable the same EC policy."
FullBlockReports retransmission delays NN startup time in large cluster.,HDFS-10365,"Whenever NN is restarted, it takes huge time for NN to come back to stable state. i.e. Last contact time remains more than 1 or 2 mins continuously for around 3 to 4 hours. This is mainly because most of the DN's getting timeout (60s) in blockReport (FBR) rpc call and then it keep sending FBR again."
hadoop-hdfs-native-client tests asking for JDK6 on OS X,HDFS-10483,Running the native tests in hadoop-hdfs-native-client causes a dialog box to pop up asking for JDK6 on my dev box.
NN may reject formerly dead DNs,HDFS-5773,"If the heartbeat monitor declares a node dead, it may never allow a DN to rejoin.  The NN will generate messages like ""Got blockReceivedDeleted message from unregistered or dead node"".

There appears to be a bug where the the isAlive flag is not set to true when a formerly known DN attempts to rejoin."
Transparent Encryption Fails to work with Yarn/MapReduce,HDFS-8485,"Running a simple MapReduce job that writes to a path configured as an encryption zone throws exception

11:26:26,343 INFO  [org.apache.hadoop.mapreduce.Job] (pool-14-thread-1) Task Id : attempt_1432740034176_0001_m_000000_2, Status : FAILED
11:26:26,346 ERROR [stderr] (pool-14-thread-1) Error: java.io.IOException: org.apache.hadoop.security.authentication.client.AuthenticationException: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)
11:26:26,346 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.crypto.key.kms.KMSClientProvider.createConnection(KMSClientProvider.java:424)
11:26:26,346 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.crypto.key.kms.KMSClientProvider.decryptEncryptedKey(KMSClientProvider.java:710)
11:26:26,346 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.decryptEncryptedKey(KeyProviderCryptoExtension.java:388)
11:26:26,346 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.hdfs.DFSClient.decryptEncryptedDataEncryptionKey(DFSClient.java:1358)
11:26:26,346 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:1457)
11:26:26,346 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:1442)
11:26:26,346 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:400)
11:26:26,346 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:393)
11:26:26,346 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
11:26:26,346 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:393)
11:26:26,347 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:337)
11:26:26,347 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)
11:26:26,347 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)
11:26:26,347 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:786)
11:26:26,347 ERROR [stderr] (pool-14-thread-1) 	at com.s3.ingestion.S3ImportMR$S3ImportMapper.map(S3ImportMR.java:112)
11:26:26,347 ERROR [stderr] (pool-14-thread-1) 	at com.s3.ingestion.S3ImportMR$S3ImportMapper.map(S3ImportMR.java:43)
11:26:26,347 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
11:26:26,347 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)
11:26:26,347 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
11:26:26,347 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
11:26:26,347 ERROR [stderr] (pool-14-thread-1) 	at java.security.AccessController.doPrivileged(Native Method)
11:26:26,347 ERROR [stderr] (pool-14-thread-1) 	at javax.security.auth.Subject.doAs(Subject.java:422)
11:26:26,347 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
11:26:26,348 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
11:26:26,348 ERROR [stderr] (pool-14-thread-1) Caused by: org.apache.hadoop.security.authentication.client.AuthenticationException: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)
11:26:26,348 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.doSpnegoSequence(KerberosAuthenticator.java:306)
11:26:26,348 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.authenticate(KerberosAuthenticator.java:196)
11:26:26,348 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.authenticate(DelegationTokenAuthenticator.java:127)
11:26:26,348 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.security.authentication.client.AuthenticatedURL.openConnection(AuthenticatedURL.java:216)
11:26:26,348 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL.openConnection(DelegationTokenAuthenticatedURL.java:322)
11:26:26,348 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.crypto.key.kms.KMSClientProvider$1.run(KMSClientProvider.java:418)
11:26:26,348 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.crypto.key.kms.KMSClientProvider$1.run(KMSClientProvider.java:413)
11:26:26,348 ERROR [stderr] (pool-14-thread-1) 	at java.security.AccessController.doPrivileged(Native Method)
11:26:26,348 ERROR [stderr] (pool-14-thread-1) 	at javax.security.auth.Subject.doAs(Subject.java:422)
11:26:26,348 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
11:26:26,348 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.crypto.key.kms.KMSClientProvider.createConnection(KMSClientProvider.java:413)
11:26:26,349 ERROR [stderr] (pool-14-thread-1) 	... 23 more
11:26:26,349 ERROR [stderr] (pool-14-thread-1) Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)
11:26:26,349 ERROR [stderr] (pool-14-thread-1) 	at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147)
11:26:26,349 ERROR [stderr] (pool-14-thread-1) 	at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:122)
11:26:26,349 ERROR [stderr] (pool-14-thread-1) 	at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187)
11:26:26,349 ERROR [stderr] (pool-14-thread-1) 	at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:224)
11:26:26,349 ERROR [stderr] (pool-14-thread-1) 	at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212)
11:26:26,349 ERROR [stderr] (pool-14-thread-1) 	at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179)
11:26:26,349 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator$1.run(KerberosAuthenticator.java:285)
11:26:26,349 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator$1.run(KerberosAuthenticator.java:261)
11:26:26,349 ERROR [stderr] (pool-14-thread-1) 	at java.security.AccessController.doPrivileged(Native Method)
11:26:26,349 ERROR [stderr] (pool-14-thread-1) 	at javax.security.auth.Subject.doAs(Subject.java:422)
11:26:26,349 ERROR [stderr] (pool-14-thread-1) 	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.doSpnegoSequence(KerberosAuthenticator.java:261)
11:26:26,350 ERROR [stderr] (pool-14-thread-1) 	... 33 more
11:26:26,350 ERROR [stderr] (pool-14-thread-1) "
Not balance block in RAM_DISK use lazy_persist policy  on datanode local copy,HDFS-8954,"I was try to use Heterogeneous Storage feature and I config RAM_DISK volume for each DN size of 20g , and set one directory policy of lazy_persist at HDFS , then command copyFromLocal to load a 79g file to the directory HDFS ,  it is have 593 block , and only 141 block in RAM_DISK , I use command ""df"" to look tmpfs usage it only my execute command DN fill enough the tmpfs , other DN tmpfs is empty !   

when I use other machine without datanode installed , it is worked fine all DN tmpfs is fill enough

Regards"
hadoop-hdfs build fails at module-javadocs,HDFS-6762,"Note; I never used JIRA before so sorry if I did it wrong

Maven build hangs for a long time at

main:
[INFO] Executed tasks
[INFO]
[INFO] --- maven-javadoc-plugin:2.8.1:jar (module-javadocs) @ hadoop-hdfs ---

Eventually the build fails

main:
[INFO] Executed tasks
[INFO]
[INFO] --- maven-javadoc-plugin:2.8.1:jar (module-javadocs) @ hadoop-hdfs ---
[INFO]
ExcludePrivateAnnotationsStandardDoclet
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] Apache Hadoop Main ................................ SUCCESS [3.133s]
[INFO] Apache Hadoop Project POM ......................... SUCCESS [3.134s]
[INFO] Apache Hadoop Annotations ......................... SUCCESS [7.367s]
[INFO] Apache Hadoop Assemblies .......................... SUCCESS [1.000s]
[INFO] Apache Hadoop Project Dist POM .................... SUCCESS [4.297s]
[INFO] Apache Hadoop Maven Plugins ....................... SUCCESS [4.673s]
[INFO] Apache Hadoop MiniKDC ............................. SUCCESS [3.471s]
[INFO] Apache Hadoop Auth ................................ SUCCESS [4.589s]
[INFO] Apache Hadoop Auth Examples ....................... SUCCESS [2.567s]
[INFO] Apache Hadoop Common .............................. SUCCESS [1:26.140s]
[INFO] Apache Hadoop NFS ................................. SUCCESS [25.938s]
[INFO] Apache Hadoop Common Project ...................... SUCCESS [0.070s]
[INFO] Apache Hadoop HDFS ................................ FAILURE [12:25.155s]
[INFO] Apache Hadoop HttpFS .............................. SKIPPED
[INFO] Apache Hadoop HDFS BookKeeper Journal ............. SKIPPED
[INFO] Apache Hadoop HDFS-NFS ............................ SKIPPED
[INFO] Apache Hadoop HDFS Project ........................ SKIPPED
[INFO] hadoop-yarn ....................................... SKIPPED
[INFO] hadoop-yarn-api ................................... SKIPPED
[INFO] hadoop-yarn-common ................................ SKIPPED
[INFO] hadoop-yarn-server ................................ SKIPPED
[INFO] hadoop-yarn-server-common ......................... SKIPPED
[INFO] hadoop-yarn-server-nodemanager .................... SKIPPED
[INFO] hadoop-yarn-server-web-proxy ...................... SKIPPED
[INFO] hadoop-yarn-server-applicationhistoryservice ...... SKIPPED
[INFO] hadoop-yarn-server-resourcemanager ................ SKIPPED
[INFO] hadoop-yarn-server-tests .......................... SKIPPED
[INFO] hadoop-yarn-client ................................ SKIPPED
[INFO] hadoop-yarn-applications .......................... SKIPPED
[INFO] hadoop-yarn-applications-distributedshell ......... SKIPPED
[INFO] hadoop-yarn-applications-unmanaged-am-launcher .... SKIPPED
[INFO] hadoop-yarn-site .................................. SKIPPED
[INFO] hadoop-yarn-project ............................... SKIPPED
[INFO] hadoop-mapreduce-client ........................... SKIPPED
[INFO] hadoop-mapreduce-client-core ...................... SKIPPED
[INFO] hadoop-mapreduce-client-common .................... SKIPPED
[INFO] hadoop-mapreduce-client-shuffle ................... SKIPPED
[INFO] hadoop-mapreduce-client-app ....................... SKIPPED
[INFO] hadoop-mapreduce-client-hs ........................ SKIPPED
[INFO] hadoop-mapreduce-client-jobclient ................. SKIPPED
[INFO] hadoop-mapreduce-client-hs-plugins ................ SKIPPED
[INFO] Apache Hadoop MapReduce Examples .................. SKIPPED
[INFO] hadoop-mapreduce .................................. SKIPPED
[INFO] Apache Hadoop MapReduce Streaming ................. SKIPPED
[INFO] Apache Hadoop Distributed Copy .................... SKIPPED
[INFO] Apache Hadoop Archives ............................ SKIPPED
[INFO] Apache Hadoop Rumen ............................... SKIPPED
[INFO] Apache Hadoop Gridmix ............................. SKIPPED
[INFO] Apache Hadoop Data Join ........................... SKIPPED
[INFO] Apache Hadoop Extras .............................. SKIPPED
[INFO] Apache Hadoop Pipes ............................... SKIPPED
[INFO] Apache Hadoop OpenStack support ................... SKIPPED
[INFO] Apache Hadoop Client .............................. SKIPPED
[INFO] Apache Hadoop Mini-Cluster ........................ SKIPPED
[INFO] Apache Hadoop Scheduler Load Simulator ............ SKIPPED
[INFO] Apache Hadoop Tools Dist .......................... SKIPPED
[INFO] Apache Hadoop Tools ............................... SKIPPED
[INFO] Apache Hadoop Distribution ........................ SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 14:55.690s
[INFO] Finished at: Mon Jul 28 13:52:42 UTC 2014
[INFO] Final Memory: 72M/241M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:2.8                     .1:jar (module-javadocs) on project hadoop-hdfs: MavenReportException: Error whi                     le creating archive:
[ERROR] Exit code: 137 - /var/hadoop-2.4.1-src/hadoop-hdfs-project/hadoop-hdfs/s                     rc/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.jav                     a:32: warning: OutputFormat is internal proprietary API and may be removed in a                      future release
[ERROR] import com.sun.org.apache.xml.internal.serialize.OutputFormat;
[ERROR] ^
[ERROR] /var/hadoop-2.4.1-src/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/                     apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java:33: warning: XM                     LSerializer is internal proprietary API and may be removed in a future release
[ERROR] import com.sun.org.apache.xml.internal.serialize.XMLSerializer;
[ERROR] ^
[ERROR] Killed
[ERROR]
[ERROR] Command line was: /usr/lib/jvm/java-7-oracle/jre/../bin/javadoc -J-Xmx51                     2m @options @packages
[ERROR]
[ERROR] Refer to the generated Javadoc files in '/var/hadoop-2.4.1-src/hadoop-hd                     fs-project/hadoop-hdfs/target' dir.
[ERROR] -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e swit                     ch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please rea                     d the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionE                     xception
[ERROR]
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hadoop-hdfs

*Install procedure on an empty ubuntu trusty tahr machine*

\# Java
echo ""deb http://ppa.launchpad.net/webupd8team/java/ubuntu precise main"" | tee -a /etc/apt/sources.list
echo ""deb-src http://ppa.launchpad.net/webupd8team/java/ubuntu precise main"" | tee -a /etc/apt/sources.list
apt-key adv --keyserver keyserver.ubuntu.com --recv-keys EEA14886
apt-get update
#Auto-accept license
echo debconf shared/accepted-oracle-license-v1-1 select true | debconf-set-selections
echo debconf shared/accepted-oracle-license-v1-1 seen true | debconf-set-selections
#Install (without prompts)
DEBIAN_FRONTEND=noninteractive apt-get install -y --force-yes oracle-java7-installer
DEBIAN_FRONTEND=noninteractive apt-get install -y --force-yes oracle-java7-set-default

\# Dependencies apt
apt-get -y --force-yes install autoconf automake libtool zlib1g-dev build-essential maven cmake

\# Dependencies tarball
cd /tmp
wget https://protobuf.googlecode.com/files/protobuf-2.5.0.tar.gz
tar -zxvf protobuf-2.5.0.tar.gz
rm protobuf-2.5.0.tar.gz
cd protobuf-2.5.0
./configure --prefix=/usr
make
make install
ldconfig

\# Hadoop
cd /var
wget http://apache.mirror1.spango.com/hadoop/common/current2/hadoop-2.4.1-src.tar.gz
tar -zxvf hadoop-2.4*-src.tar.gz
rm hadoop-2.4*-src.tar.gz
cd /var/hadoop-2.4*-src
mvn package -Pdist,native -DskipTests -Dtar"
ZKFC - transitionToActive is indefinitely waiting to complete fenceOldActive,HDFS-7739," *Scenario:* 

One of the cluster disk got full and ZKFC making tranisionToAcitve ,To fence old active node it needs to execute the command and wait for tge result, since disk got full, strempumper thread will be indefinitely waiting( Even after free the disk also, it will not come out)...

 *{color:blue}Please check the attached thread dump of ZKFC{color}* ..

 *{color:green}Better to maintain the timeout for stream-pumper thread{color}* .

{code}
protected void pump() throws IOException {
    InputStreamReader inputStreamReader = new InputStreamReader(stream);
    BufferedReader br = new BufferedReader(inputStreamReader);
    String line = null;
    while ((line = br.readLine()) != null) {
      if (type == StreamType.STDOUT) {
        log.info(logPrefix + "": "" + line);
      } else {
        log.warn(logPrefix + "": "" + line);          
      }
{code}
"
Block recovery with closeFile flag true can race with blockReport. Due to this blocks are getting marked as corrupt.,HDFS-3122,"*Block Report* can *race* with *Block Recovery* with closeFile flag true.

 Block report generated just before block recovery at DN side and due to N/W problems, block report got delayed to NN. 
After this, recovery success and generation stamp modifies to new one. 
And primary DN invokes the commitBlockSynchronization and block got updated in NN side. Also block got marked as complete, since the closeFile flag was true. Updated with new genstamp.

Now blockReport started processing at NN side. This particular block from RBW (when it generated the BR at DN), and file was completed at NN side.
Finally block will be marked as corrupt because of genstamp mismatch.

{code}
 case RWR:
      if (!storedBlock.isComplete()) {
        return null; // not corrupt
      } else if (storedBlock.getGenerationStamp() != iblk.getGenerationStamp()) {
        return new BlockToMarkCorrupt(storedBlock,
            ""reported "" + reportedState + "" replica with genstamp "" +
            iblk.getGenerationStamp() + "" does not match COMPLETE block's "" +
            ""genstamp in block map "" + storedBlock.getGenerationStamp());
      } else { // COMPLETE block, same genstamp
{code}




"
Fetching tokens from insecure cluster throws NPEs,HDFS-2473,"Retrieving tokens from an insecure cluster throws an NPE.  This prevents copies from insecure to secure clusters.  The problem is in {{DFSClient.getDelegationToken()}}.  It attempts to set the service and log the token w/o a null check.

FetchDT will also throw exceptions when fetching tokens from an insecure cluster."
HDFS cannot be browsed from web UI while in safe mode,HDFS-1574,"As of HDFS-984, the NN does not issue delegation tokens while in safe mode (since it would require writing to the edit log). But the browsedfscontent servlet relies on getting a delegation token before redirecting to a random DN to browse the FS. Thus, the ""browse the filesystem"" link does not work while the NN is in safe mode."
Ignoring IOExceptions on close,HDFS-43,"Currently in HDFS there are a lot of calls to IOUtils.closeStream that are from finally blocks. I'm worried that this can lead to data corruption in the file system. Take the first instance in DataNode.copyBlock: it writes the block and then calls closeStream on the output stream. If there is an error at the end of the file that is detected in the close, it will be *completely* ignored. Note that logging the error is not enough, the error should be thrown so that the client knows the failure happened.

{code}
   try {
     file1.write(...);
     file2.write(...);
   } finally {
      IOUtils.closeStream(file);
  }
{code}

is *bad*. It must be rewritten as:

{code}
   try {
     file1.write(...);
     file2.write(...);
     file1.close(...);
     file2.close(...);
   } catch (IOException ie) {
     IOUtils.closeStream(file1);
     IOUtils.closeStream(file2);
     throw ie;
   }
{code}

I also think that IOUtils.closeStream should be renamed IOUtils.cleanupFailedStream or something to make it clear it can only be used after the write operation has failed and is being cleaned up."
timeout when writing dfs file causes infinite loop when closing the file,HDFS-148,"If, when writing to a dfs file, I get a timeout exception:

06/11/29 11:16:05 WARN fs.DFSClient: Error while writing.
java.net.SocketTimeoutException: timed out waiting for rpc response
       at org.apache.hadoop.ipc.Client.call(Client.java:469)
       at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:164)
       at org.apache.hadoop.dfs.$Proxy0.reportWrittenBlock(Unknown Source)
       at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.internalClose(DFSClient.java:1220)
       at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.endBlock(DFSClient.java:1175)
       at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.flush(DFSClient.java:1121)
       at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.write(DFSClient.java:1103)
       at org.apache.hadoop.examples.NNBench2.createWrite(NNBench2.java:107)
       at org.apache.hadoop.examples.NNBench2.main(NNBench2.java:247)

then the close() operation on the file appears to go into an infinite loop of retrying:

06/11/29 13:11:19 INFO fs.DFSClient: Could not complete file, retrying...
06/11/29 13:11:20 INFO fs.DFSClient: Could not complete file, retrying...
06/11/29 13:11:21 INFO fs.DFSClient: Could not complete file, retrying...
06/11/29 13:11:23 INFO fs.DFSClient: Could not complete file, retrying...
06/11/29 13:11:24 INFO fs.DFSClient: Could not complete file, retrying...
..."
DFSClient mkdirs applies default umask to the specified permission,HDFS-3903,"Per Romain on HDFS-3491: This is still broken. Octal is correctly accepted but not applied when during a MKDIR:

{noformat}
romain@runreal:~/projects/hue$ curl -X PUT ""http://localhost:14000/webhdfs/v1/tmp/test-perm-httpfs?permission=01777&op=MKDIRS&user.name=hue&doas=hue""
{""boolean"":true}
romain@runreal:~/projects/hue$ curl ""http://localhost:14000/webhdfs/v1/tmp/test-perm-httpfs?op=GETFILESTATUS&user.name=hue&doas=hue""
{""FileStatus"":{""pathSuffix"":"""",""type"":""DIRECTORY"",""length"":0,""owner"":""hue"",""group"":""supergroup"",""permission"":""755"",""accessTime"":0,""modificationTime"":1345658456869,""blockSize"":0,""replication"":0}}
{noformat}
"
bad connection to fs since a datanode ,HDFS-1040,"I have installed the hadoop  version = 0.19.2 and how I trying execute command ./hadoop fs -ls since a datanode and 
show Bad connection to FS. command aborted.

How I execute command  jps in the datanode show

root@ubuntu:~# jps
4401 TaskTracker
4504 Jps
4259 DataNode

How I execute command  jps in the namenode show

8548 DataNode
8706 SecondaryNameNode
9115 Jps
8406 NameNode
8928 TaskTracker
8783 JobTracker

thanks a lot for fast answer

"
RBF: create mount point with RANDOM policy and with 2 Nameservices doesn't work properly ,HDFS-13817,"{{Scenario:-}} 

# Create a mount point with RANDOM policy and with 2 Nameservices .
# List the target mount path of the Global path.

Actual Output: 
=========== 
{{ls: `/apps5': No such file or directory}}

Expected Output: 
=============

{{if the files are availabel list those files or if it's emtpy it will disply nothing}}

{noformat} 
bin> ./hdfs dfsrouteradmin -add /apps5 hacluster,ns2 /tmp10 -order RANDOM -owner securedn -group hadoop
Successfully added mount point /apps5
bin> ./hdfs dfs -ls /apps5
ls: `/apps5': No such file or directory
bin> ./hdfs dfs -ls /apps3
Found 2 items
drwxrwxrwx   - user group 0 2018-08-09 19:55 /apps3/apps1
-rw-r--r--   3   - user group  4 2018-08-10 11:55 /apps3/ttt
 {noformat}

{{please refer the bellow image for mount inofrmation}}

{{/apps3 tagged with HASH policy}}
{{/apps5 tagged with RANDOM policy}}

{noformat}
/bin> ./hdfs dfsrouteradmin -ls

Mount Table Entries:
Source                    Destinations              Owner                     Group                     Mode                      Quota/Usage

/apps3                    hacluster->/tmp3,ns2->/tmp4 securedn                  users                     rwxr-xr-x                 [NsQuota: -/-, SsQuota: -/-]

/apps5                    hacluster->/tmp5,ns2->/tmp5 securedn                  users                     rwxr-xr-x                 [NsQuota: -/-, SsQuota: -/-]

{noformat}"
Unable to change the state of DN  to maintenance using dfs.hosts.maintenance,HDFS-14014,"聽

hdfs-site.xml configurations :

<property>
 <name>dfs.namenode.maintenance.replication.min</name>
 <value>1</value>
 </property>
 <property>
 <name>dfs.namenode.hosts.provider.classname</name>
 <value>org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager</value>
 </property>
 <property>
 <name>dfs.hosts.maintenance</name>
 <value>/opt/lifeline2/install/hadoop/namenode/etc/hadoop/maintenance</value>
 </property>
</configuration>

聽

maintenance file :

{ ""hostName"": ""vm1"", ""port"": 50076, ""adminState"": ""IN_MAINTENANCE"", ""maintenanceExpireTimeInMS"" : 1540204025000}

Command :聽

/hadoop/namenode/bin # ./hdfs dfsadmin -refreshNodes
2018-10-22 17:45:54,286 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Refresh nodes failed for vm1:65110
Refresh nodes failed for vm2:65110
refreshNodes: 2 exceptions [org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): (No such file or directory)
 at java.io.FileInputStream.open0(Native Method)
 at java.io.FileInputStream.open(FileInputStream.java:195)
 at java.io.FileInputStream.<init>(FileInputStream.java:138)
 at java.io.FileInputStream.<init>(FileInputStream.java:93)
 at org.apache.hadoop.hdfs.util.CombinedHostsFileReader.readFile(CombinedHostsFileReader.java:75)
 at org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager.refresh(CombinedHostFileManager.java:215)
 at org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager.refresh(CombinedHostFileManager.java:210)
 at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.refreshHostsReader(DatanodeManager.java:1195)
 at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.refreshNodes(DatanodeManager.java:1177)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.refreshNodes(FSNamesystem.java:4488)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.refreshNodes(NameNodeRpcServer.java:1270)
 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.refreshNodes(ClientNamenodeProtocolServerSideTranslatorPB.java:913)
 at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
, org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): (No such file or directory)
 at java.io.FileInputStream.open0(Native Method)
 at java.io.FileInputStream.open(FileInputStream.java:195)
 at java.io.FileInputStream.<init>(FileInputStream.java:138)
 at java.io.FileInputStream.<init>(FileInputStream.java:93)
 at org.apache.hadoop.hdfs.util.CombinedHostsFileReader.readFile(CombinedHostsFileReader.java:75)
 at org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager.refresh(CombinedHostFileManager.java:215)
 at org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager.refresh(CombinedHostFileManager.java:210)
 at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.refreshHostsReader(DatanodeManager.java:1195)
 at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.refreshNodes(DatanodeManager.java:1177)
 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.refreshNodes(FSNamesystem.java:4488)
 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.refreshNodes(NameNodeRpcServer.java:1270)
 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.refreshNodes(ClientNamenodeProtocolServerSideTranslatorPB.java:913)
 at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)

聽

聽"
WebHDFS: Uploading a file again with the same naming convention,HDFS-14062,"*PROBLEM STATEMENT:*

If we want to re-upload a file with the same name, to HDFS using WebHDFS APIs, WebHDFS APIs does not allow, giving error:
{code:java}
""exception"":""FileAlreadyExistsException"",""javaClassName"":""org.apache.hadoop.fs.FileAlreadyExistsException""

{code}

But from HDFS command we can force upload(overwrite) a file having same name:

{code:java}
hdfs dfs -put -f /tmp/file1.txt /user/ambari-test {code}
聽

Can we enable this feature via WebHDFS APIs also?

聽

*STEPS TO REPRODUCE:*

1. Create a directory in HDFS using WebHDFS API:
{code:java}
 # curl -iL -X PUT ""http://<NAMENODE_IP>:<PORT>/webhdfs/v1/user/admin/Test?op=MKDIRS&user.name=admin""{code}

2. Upload a file called /tmp/file1.txt:
{code:java}
 # curl -iL -X PUT -T ""/tmp/file1.txt"" ""http://<NAMENODE_IP>:<PORT>/webhdfs/v1/user/admin/Test/file1.txt?op=CREATE&user.name=admin"" {code}

3. Now edit this file and then try uploading it back:
{code}
 # curl -iL -X PUT -T ""/tmp/file1.txt"" ""http://<NAMENODE_IP>:<PORT>/webhdfs/v1/user/admin/Test/file1.txt?op=CREATE&user.name=admin"" {code}

4. We get the following error:
{code:java}
HTTP/1.1 100 Continue

HTTP/1.1 403 Forbidden
 Content-Type: application/json; charset=utf-8
 Content-Length: 1465
 Connection: close

{""RemoteException"":\{""exception"":""FileAlreadyExistsException"",""javaClassName"":""org.apache.hadoop.fs.FileAlreadyExistsException"",""message"":""/user/admin/Test/file1.txt for client 172.26.123.95 already exists\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2815)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2702)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2586)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:736)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:409)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2347)\n""}}
{code}

聽"
Add diag info in RetryInvocationHandler,HDFS-14012,"RetryInvocationHandler does the following logging:
{code:java}
} else { 
  LOG.warn(""A failover has occurred since the start of this method"" + "" invocation attempt.""); 
}{code}
Would be helpful to report the method name, and call stack in this message.

Thanks."
"After Unset the EC policy for a directory, Still inside the directory files having the EC Policy",HDFS-12782,"Scenario:

Set the EC policy for Dir
Write a file and check the EC policy for that file
Unset the EC policy for the above Dir
Check the policy for the file.

Actual Output:
==============
Still having the EC policy for a file

Expected Output:
================
Inside the Dir all files release the EC policy when we do unset the top level Dir.
"
java.lang.OutOfMemoryError: unable to create new native thread,HDFS-12936,"I configure the max user processes 65535 with any user ,and the datanode memory is 8G.
 When a log of data was been writeen,the datanode was been shutdown.
 But I can see the memory use only < 1000M.
 Please to see the attachment. !Datanode Memory.png!

*DataNode shutdown error log:*
{code:java}
2017-12-17 23:58:14,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1437036909-192.168.17.36-1509097205664:blk_1074725940_987917, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2017-12-17 23:58:31,425 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode is out of memory. Will retry in 30 seconds.
java.lang.OutOfMemoryError: unable to create new native thread
	at java.lang.Thread.start0(Native Method)
	at java.lang.Thread.start(Thread.java:714)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:154)
	at java.lang.Thread.run(Thread.java:745)
2017-12-17 23:59:01,426 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode is out of memory. Will retry in 30 seconds.
java.lang.OutOfMemoryError: unable to create new native thread
	at java.lang.Thread.start0(Native Method)
	at java.lang.Thread.start(Thread.java:714)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:154)
	at java.lang.Thread.run(Thread.java:745)
2017-12-17 23:59:05,520 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode is out of memory. Will retry in 30 seconds.
java.lang.OutOfMemoryError: unable to create new native thread
	at java.lang.Thread.start0(Native Method)
	at java.lang.Thread.start(Thread.java:714)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:154)
	at java.lang.Thread.run(Thread.java:745)
2017-12-17 23:59:31,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1437036909-192.168.17.36-1509097205664:blk_1074725951_987928 src: /192.168.17.54:40478 dest: /192.168.17.48:50010

{code}"
While there be mass Replication Queue Initializer thread,HDFS-13874,"I see dn register pipeline if it is a new dn, then will tick a thread named ""Replication Queue Initializer thread"".
You can see DatanodeManager#checkIfClusterIsNowMultiRack
"
Incorrect path is passed to checkPermission during authorization of file under a snapshot (specifically under a subdir) after original subdir is deleted,HDFS-13205,"Steps to reproduce the issue.

+As 'hdfs' superuser+ 
 鈥?Create a folder (/hdptest/test) with 700 permissions and (聽/hdptest/test/mydir) with 755.

--HDFS Ranger policy is defined 聽with RWX for user ""test"" on /hdptest/test/ recursively.

聽--Allow snapshot on the directory聽聽/hdptest/test/mydir:聽
{code:java}
#su - test
[test@node1 ~]$ hdfs dfs -ls /hdptest/test/mydir
[test@node1 ~]$ hdfs dfs -mkdir /hdptest/test/mydir/test
[test@node1 ~]$ hdfs dfs -put /etc/passwd /hdptest/test/mydir/test
[test@node1 ~]$ hdfs lsSnapshottableDir
drwxr-xr-x 0 test hdfs 0 2018-01-25 14:22 1 65536 /hdptest/test/mydir
聽
{code}
聽

-->Create Snapshot 聽
{code:java}
[test@node1 ~]$ hdfs dfs -createSnapshot /hdptest/test/mydir
Created snapshot /hdptest/test/mydir/.snapshot/s20180125-135430.953
{code}
聽-->Verifying that snapshot directory has the current files from directory and verify the file is accessible 聽.snapshot path:聽聽
{code:java}
[test@node1 ~]$ hdfs dfs -ls -R /hdptest/test/mydir/.snapshot/s20180125-135430.953
drwxr-xr-x聽聽 - test hdfs聽聽聽聽聽聽聽聽聽 0 2018-01-25 13:53 /hdptest/test/mydir/.snapshot/s20180125-135430.953/test
-rw-r--r--聽聽 3 test hdfs聽聽聽聽聽聽 3227 2018-01-25 13:53 /hdptest/test/mydir/.snapshot/s20180125-135430.953/test/passwd
[test@node1 ~]$ hdfs dfs -cat /hdptest/test/mydir/.snapshot/s20180125-135430.953/test/passwd | tail
livytest:x:1015:496::/home/livytest:/bin/bash
ehdpzepp:x:1016:496::/home/ehdpzepp:/bin/bash
zepptest:x:1017:496::/home/zepptest:/bin/bash
{code}
聽-->Remove the file from main directory and verified that file is still accessible:
{code:java}
[test@node1 ~]$ hdfs dfs -rm /hdptest/test/mydir/test/passwd
18/01/25 13:55:06 INFO fs.TrashPolicyDefault: Moved: 'hdfs://rangerSME/hdptest/test/mydir/test/passwd' to trash at: hdfs://rangerSME/user/test/.Trash/Current/hdptest/test/mydir/test/passwd
[test@node1 ~]$ hdfs dfs -cat /hdptest/test/mydir/.snapshot/s20180125-135430.953/test/passwd | tail
livytest:x:1015:496::/home/livytest:/bin/bash
{code}
聽-->Remove the parent directory of the file which was deleted, now accessing the same file under .snapshot dir fails with permission denied error
{code:java}
[test@node1 ~]$ hdfs dfs -rm -r /hdptest/test/mydir/test
18/01/25 13:55:25 INFO fs.TrashPolicyDefault: Moved: 'hdfs://rangerSME/hdptest/test/mydir/test' to trash at: hdfs://rangerSME/user/test/.Trash/Current/hdptest/test/mydir/test1516888525269
[test@node1 ~]$ hdfs dfs -cat /hdptest/test/mydir/.snapshot/s20180125-135430.953/test/passwd | tail
cat: Permission denied: user=test, access=EXECUTE, inode=""/hdptest/test/mydir/.snapshot/s20180125-135430.953/test/passwd"":hdfs:hdfs:drwxr-x---
聽
{code}
聽Ranger policies are not honored in this case for .snapshot directories/files after main directory is deleted under聽snapshotable聽directory.

聽Workaround is to provide execute permission at HDFS level for the parent folder聽
{code:java}
#su - hdfs
#hdfs dfs -chmod 701聽/hdptest/test
{code}"
Httpfs does not support custom authentication,HDFS-13680,"Currently Httpfs Authentication Filter does not support聽any custom authentication: the Authentication Handler can only be聽PseudoAuthenticationHandler or聽KerberosDelegationTokenAuthenticationHandler.

We should allow other authentication handlers to manage custom authentication."
Ozone: SCM: ContainerStateManager#updateContainerState updates incorrect AllocatedBytes to container info.,HDFS-12962,"While updating container state through {{ContainerStateManager#updateContainerState}}, AllocatedBytes of {{ContainerStateManager}} should be used, not the one from {{ContainerMapping}}."
"Ozone: generate optional, version specific documentation during the build",HDFS-12734,"HDFS-12664 susggested a new way to include documentation in the KSM web ui.

This patch modifies the build lifecycle to automatically generate the documentation *if* hugo is on the PATH. If hugo is not there  the documentation won't be generated and it won't be displayed (see HDFS-12661)

To test: Apply this patch on top of HDFS-12664 do a full build and check the KSM webui."
Ozone: ContainerID has incorrect package name,HDFS-13394,{{ContainerID}}聽class's package name and the directory structure where the class is present doesn't match.
IBRs from dead DNs go into infinite loop,HDFS-8675,"If the DN sends an IBR after the NN declares it dead, the NN returns an IOE of unregistered or dead.  The DN catches the IOE, ignores it, and infinitely loops spamming the NN with retries."
FileInputStream not closed when read the checksum header from the meta file,HDFS-13247,"When i read the聽computeChecksum() of聽FsDatasetImpl聽class in datanode,聽I found the following code not closed fis after聽聽read the checksum header from the meta file,聽According to the comments of function聽readDataChecksum()聽 we must close inputStream聽 by the caller.聽Thanks.
{code:java|title=FsDatasetImpl.java|borderStyle=solid}
DataChecksum checksum;
try (FileInputStream fis =
 srcReplica.getFileIoProvider().getFileInputStream(
 srcReplica.getVolume(), srcMeta)) {
 checksum = BlockMetadataHeader.readDataChecksum(
 fis, DFSUtilClient.getIoFileBufferSize(conf), srcMeta);
}

final byte[] data = new byte[1 << 16];
final byte[] crcs = new byte[checksum.getChecksumSize(data.length)];

{code}"
APPEND API call is different in HTTPFS and NameNode REST,HDFS-12654,"The APPEND REST API call behaves differently in the NameNode REST and the HTTPFS codes. The NameNode version creates the target file the new data being appended to if it does not exist at the time of the call issued. The HTTPFS version assumes the target file exists when APPEND is called and can append only the new data but does not create the target file it doesn't exist.

The two implementations should be standardized, preferably the HTTPFS version should be modified to execute an implicit CREATE if the target file does not exist."
Default HDFS as Azure WASB tries rebalancing datanode data to HDFS (0% capacity) and fails,HDFS-13139,"Configuring聽Azure WASB storage as a default HDFS location which means that Hadoop HDFS capacity will be 0. I have default replication as 1 but now when I am trying to decommission a node, datanode tries to rebalance some 28KB of data to another available datanode. However, our HDFS has 0 capacity and therefore, decommissioning fails with below given error:
{code:java}
New node(s) could not be removed from the cluster. Reason Trying to move '28672' bytes worth of data to nodes with '0' bytes of capacity is not allowed{code}
Getting the information on cluster shows that default local HDFS is still used for some KB space which is getting rebalanced whereas available capacity is 0:
{code:java}
""CapacityRemaining"" : 0,
 ""CapacityTotal"" : 0,
 ""CapacityUsed"" : 131072,
 ""DeadNodes"" : ""{}"",
 ""DecomNodes"" : ""{}"",
 ""HeapMemoryMax"" : 1060372480,
 ""HeapMemoryUsed"" : 147668152,
 ""NonDfsUsedSpace"" : 0,
 ""NonHeapMemoryMax"" : -1,
 ""NonHeapMemoryUsed"" : 75319744,
 ""PercentRemaining"" : 0.0,
 ""PercentUsed"" : 100.0,
 ""Safemode"" : """",
 ""StartTime"" : 1518241019502,
 ""TotalFiles"" : 1,
 ""UpgradeFinalized"" : true,{code}"
Make hadoop proxy user changes reconfigurable in Datanode,HDFS-13105,Currently any changes to add/delete a new proxy user requires DN restart requiring a downtime. This jira proposes to make the changes in proxy/user configuration reconfiguration via that ReconfigurationProtocol so that the changes can take effect without a DN restart. For details please refer https://hadoop.apache.org/docs/r2.8.0/hadoop-project-dist/hadoop-common/Superusers.html.
Make hadoop proxy user changes reconfigurable in Namenode,HDFS-13104,Currently any changes to add/delete a new proxy user requires NN restart requiring a downtime. This jira proposes to make the changes in proxy/user configuration reconfiguration via that ReconfigurationProtocol so that the changes can take effect without a NN restart. For details please refer https://hadoop.apache.org/docs/r2.8.0/hadoop-project-dist/hadoop-common/Superusers.html.
The error message is not friendly when set a path with the policy not enabled,HDFS-12087,"First user add a policy by -addPolicies command but not enabled, then user set a path with this policy. The error message displayed as below:
{color:#707070}RemoteException: Policy 'XOR-2-1-128k' does not match any enabled erasure coding policies: []. The set of enabled erasure coding policies can be configured at 'dfs.namenode.ec.policies.enabled'.{color}

The policy 'XOR-2-1-128k' is added by user but not be enabled.The error message is not promot user to enable the policy first.I think the error message may be better as below:
{color:#707070}RemoteException: Policy 'XOR-2-1-128k' does not match any enabled erasure coding policies: []. The set of enabled erasure coding policies can be configured at 'dfs.namenode.ec.policies.enabled' or enable the policy by '-enablePolicy' EC command before.{color}"
User with no permission on file is able to run getfacl for that file,HDFS-13038,"Currently any user with EXECUTE permission can run getfacl on a file or directory.聽This Jira adds a check for READ access of user on the inode path.聽
{code:java}
[root@host ~]$ hdfs dfs -copyFromLocal /etc/a.txt /tmp
[root@host ~]$ hdfs dfs -setfacl -m user:abc:--- /tmp/a.txt
{code}
Since user abc does not have read permission on the file 'cat' command throws Permission Denied error but getfacl executes normally.
{code:java}
[abc@host ~]$ hdfs dfs -cat /tmp/a.txt
cat: Permission denied: user=abc, access=READ, inode=""/tmp/a.txt"":abc:hdfs:-rw-r--r-- 
[abc@host ~]$ hdfs dfs -getfacl /tmp/a.txt 
# file: /tmp/a.txt 
# owner:root 
# group: hdfs 
user::rw- 
user:abc:--- 
group::r-- 
mask::r-- 
other::r--
{code}"
Ozone: Mini cluster can't start up on Windows after HDFS-12159,HDFS-12331,"ozone mini cluster can't start up on Windows after HDFS-12159. 
The error log:
{noformat}
java.net.URISyntaxException: Illegal character in opaque part at index 2: D:\work-project\hadoop\hadoop-hdfs-project\hadoop-hdfs\target\test\data\dfs\data\dn0_data-1\3d3d5718-4219-4ec3-a9c5-e594801a1430
	at java.net.URI$Parser.fail(URI.java:2848)
	at java.net.URI$Parser.checkChars(URI.java:3021)
	at java.net.URI$Parser.parse(URI.java:3058)
	at java.net.URI.<init>(URI.java:588)
	at org.apache.ratis.util.FileUtils.stringAsURI(FileUtils.java:133)
	at org.apache.ratis.server.storage.RaftStorage.<init>(RaftStorage.java:49)
	at org.apache.ratis.server.impl.ServerState.<init>(ServerState.java:85)
	at org.apache.ratis.server.impl.RaftServerImpl.<init>(RaftServerImpl.java:94)
	at org.apache.ratis.server.impl.RaftServerProxy.initImpl(RaftServerProxy.java:67)
	at org.apache.ratis.server.impl.RaftServerProxy.<init>(RaftServerProxy.java:62)
	at org.apache.ratis.server.impl.ServerImplUtils.newRaftServer(ServerImplUtils.java:43)
	at org.apache.ratis.server.impl.ServerImplUtils.newRaftServer(ServerImplUtils.java:35)
	at org.apache.ratis.server.RaftServer$Builder.build(RaftServer.java:70)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.<init>(XceiverServerRatis.java:68)
	at org.apache.hadoop.ozone.container.common.transport.server.ratis.XceiverServerRatis.newXceiverServerRatis(XceiverServerRatis.java:130)
	at org.apache.hadoop.ozone.container.ozoneimpl.OzoneContainer.<init>(OzoneContainer.java:113)
	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.<init>(DatanodeStateMachine.java:76)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.bpRegistrationSucceeded(DataNode.java:1592)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.registrationSucceeded(BPOfferService.java:409)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.register(BPServiceActor.java:783)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:286)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:816)
{noformat}
The root cause is that RatiServer instance was newly created in {{OzoneContainer}} after HDFS-12159 but it can't recognize the path under Windows."
javadoc: error - class file for org.apache.http.annotation.ThreadSafe not found,HDFS-12527,"{code}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:2.10.4:jar (module-javadocs) on project hadoop-hdfs-client: MavenReportException: Error while generating Javadoc: 
[ERROR] Exit code: 1 - /Users/szetszwo/hadoop/t2/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/util/StripedBlockUtil.java:694: warning - Tag @link: reference not found: StripingCell
[ERROR] javadoc: error - class file for org.apache.http.annotation.ThreadSafe not found
[ERROR] 
[ERROR] Command line was: /Library/Java/JavaVirtualMachines/jdk1.8.0_144.jdk/Contents/Home/jre/../bin/javadoc -J-Xmx768m @options @packages
[ERROR] 
[ERROR] Refer to the generated Javadoc files in '/Users/szetszwo/hadoop/t2/hadoop-hdfs-project/hadoop-hdfs-client/target/api' dir.
{code}
To reproduce the error above, run
{code}
mvn package -Pdist -DskipTests -DskipDocs -Dtar
{code}"
Inotify should support erasure coding policy op as replica meta change,HDFS-12413,Currently HDFS Inotify already supports meta change like replica for a file. We should also support erasure coding policy setting/unsetting for a file similarly.
Invalid value configured for dfs.datanode.failed.volumes.tolerated cause the datanode exit,HDFS-10269,"The datanode start failed and exited when I reused configured for dfs.datanode.failed.volumes.tolerated as 5 from my another cluster but actually the new cluster only have one datadir path. And this leaded the Invalid volume failure config value and threw {{DiskErrorException}}, so the datanode shutdown. The info is below:
{code}
2016-04-07 09:34:45,358 WARN org.apache.hadoop.hdfs.server.common.Storage: Failed to add storage for block pool: BP-1239160341-xx.xx.xx.xx-1459929303126 : BlockPoolSliceStorage.recoverTransitionRead: attempt to load an used block storage: /home/data/hdfs/data/current/BP-1239160341-xx.xx.xx.xx-1459929303126
2016-04-07 09:34:45,358 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to /xx.xx.xx.xx:9000. Exiting.
java.io.IOException: All specified directories are failed to load.
        at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:477)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
        at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
        at java.lang.Thread.run(Thread.java:745)
2016-04-07 09:34:45,358 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to /xx.xx.xx.xx:9000. Exiting.
org.apache.hadoop.util.DiskChecker$DiskErrorException: Invalid volume failure  config value: 5
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:281)
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:34)
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.newInstance(FsDatasetFactory.java:30)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1374)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)
        at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:316)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)
        at java.lang.Thread.run(Thread.java:745)
2016-04-07 09:34:45,358 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to /xx.xx.xx.xx:9000
2016-04-07 09:34:45,359 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to /xx.xx.xx.xx:9000
2016-04-07 09:34:45,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)
2016-04-07 09:34:47,460 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2016-04-07 09:34:47,462 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2016-04-07 09:34:47,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG:
{code}

IMO, this will let users feel bad because I only configured a value incorrectly. Instead of, we can give a warn info for this and reset this value to the default value. It will be a better way for this case."
What is the correct way of retrying when failure occurs during writing,HDFS-12326,"I'm using hdfs client for golang https://github.com/colinmarc/hdfs to write to the hdfs. And I'm using hadoop 2.7.3
When the number of files concurrently being opened is larger, for example 200. I'll always get the 'broken pipe' error.

So I want to retry to continue writing. What is the correct way of retrying? Because https://github.com/colinmarc/hdfs hasn't been able to recover the stream status when an error occurs duing writing, so I have to reopen and get a new stream. So I tried the following steps:
1 close the current stream
2 Append the file to get a new stream

But when I close the stream, I got the error ""updateBlockForPipeline call failed with ERROR_APPLICATION (java.io.IOException""
and it seems the namenode complains:

{code:java}
2017-08-20 03:22:55,598 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.updateBlockForPipeline from 192.168.0.39:46827 Call#50183 Retry#-1
java.io.IOException: BP-1152809458-192.168.0.39-1502261411064:blk_1073825071_111401 does not exist or is not under Constructionblk_1073825071_111401{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d61914ba-df64-467b-bb75-272875e5e865:NORMAL:192.168.0.39:50010|RBW], ReplicaUC[[DISK]DS-1314debe-ab08-4001-ab9a-8e234f28f87c:NORMAL:192.168.0.38:50010|RBW]]}
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkUCBlock(FSNamesystem.java:6241)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.updateBlockForPipeline(FSNamesystem.java:6309)
    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.updateBlockForPipeline(NameNodeRpcServer.java:806)
    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.updateBlockForPipeline(ClientNamenodeProtocolServerSideTranslatorPB.java:955)
    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:422)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
2017-08-20 03:22:56,333 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073825071_111401{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d61914ba-df64-467b-bb75-272875e5e865:NORMAL:192.168.0.39:50010|RBW], ReplicaUC[[DISK]DS-1314debe-ab08-4001-ab9a-8e234f28f87c:NORMAL:192.168.0.38:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/am/scan_task/2017-08-20/192.168.0.38_audience_f/user-bak010-20170820030804.log
{code}

when I Appended to get a new stream, I got the error 'append call failed with ERROR_APPLICATION (org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException)', and the corresponding error in namenode is:

{code:java}
2017-08-20 03:22:56,335 WARN org.apache.hadoop.hdfs.StateChange: DIR* NameSystem.append: Failed to APPEND_FILE /user/am/scan_task/2017-08-20/192.168.0.38_audience_f/user-bak010-20170820030804.log for go-hdfs-OAfvZiSUM2Eu894p on 192.168.0.39 because go-hdfs-OAfvZiSUM2Eu894p is already the current lease holder.
2017-08-20 03:22:56,335 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.append from 192.168.0.39:46827 Call#50186 Retry#-1: org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: Failed to APPEND_FILE /user/am/scan_task/2017-08-20/192.168.0.38_audience_f/user-bak010-20170820030804.log for go-hdfs-OAfvZiSUM2Eu894p on 192.168.0.39 because go-hdfs-OAfvZiSUM2Eu894p is already the current lease holder.
{code}

Could you please suggest the correct way of retrying of the client side when write fails?"
SnapshotDiffReport should detect open files in HDFS Snapshots,HDFS-11220,"*Problem:*

1. When there are files being written and when HDFS Snapshots are taken in parallel, Snapshots do capture all these files, but these being written files in Snapshots do not have the point-in-time file length captured. Most of the times, these open files will have a length of 0, or the last block boundary size.

2. Only at the time of File close or any other meta data modification operation on these files, HDFS reconciles the file length and records the modification in the last taken Snapshot. All the previously taken Snapshots continue to have those open Files with no modification recorded. So, all those previous snapshots end up using the final modification record in the next available snapshot. So, after the file close, file lengths in all those snapshots will end up same.

Assume File1 is opened for write and a total of 1MB written to it. While the writes are happening, snapshots are taken in parallel.

{noformat}
|---Time---T1-----------T2-------------T3----------------T4------>
|-----------------------Snap1----------Snap2-------------Snap3--->
|---File1.open---write---------write-----------close------------->
{noformat}

Then at time,
T2:
Snap1.File1.length = 0

T3:
Snap1.File1.length = 0
Snap2.File1.length = 0

<File1 write completed and closed>

T4:
Snap1.File1.length = 1MB
Snap2.File1.length = 1MB
Snap3.File1.length = 1MB

So, Snapshot Diff Report running against any of above snapshots will not detect any delta changes in the open files. 

*Proposal:*

1. HDFS Snapshots can stash open file details in the snapshot record. 
2. NameNode might not have the accurate byte level length visibility on the open files, Snapshots might not have the accurate point-in-time length captured. So, SnapshotDiffReport can have an option to detect open files and always show {{M}} flag for the open files, if the files are available on both the snapshots it is running against with. 

{noformat}
hdfs snapshotDiff -includeOpenFiles <snapDir> <snapName> <snapName>
{noformat}"
Add option to skip open files during HDFS Snapshots,HDFS-11218,"*Problem:* 

When there are files being written and when HDFS Snapshots are taken in parallel,  Snapshots do capture all these files, but these being written files in Snapshots do not have the point-in-time file length captured.

At the time of File close or any other meta data modification operation on that file which was previously open, HDFS reconciles the file length and records the modification in the last taken Snapshot. All the previously taken Snapshots continue to have the same open File with no modification recorded. So, all those previous snapshots end up using the final modification record in the next available snapshot.

*Proposal:*

HDFS Snapshot Design goal was to have O(M) space usage for Snapshots, where M is the number file modifications. So, it would very expensive to record modifications for all the open files in all the snapshots. For applications that do not want to capture incomplete / partial being written binary files in the snapshots, it would be preferable to have an extra option to skip open files. This way, they don't have to worry about restoring inconsistent files from the snapshots. 

{noformat}
hdfs dfs -createSnapshot -skipOpenFiles <snapDir> <snapName>
{noformat}"
INode#getSnapshotINode() should get INodeAttributes from INodeAttributesProvider for the current INode,HDFS-12201,"Problem: When an external INodeAttributesProvider is enabled, SnapshotDiff is not detecting changes in files when the external ACL/XAttr attributes change. 

{{FileWithSnapshotFeature#changedBetweenSnapshots()}} when trying to detect changes in snapshots for the given file, does meta data comparison which takes in the attributes retrieved from {{INode#getSnapshotINode()}}

{{INodeFile}}
{noformat}
  @Override
  public INodeFileAttributes getSnapshotINode(final int snapshotId) {
    FileWithSnapshotFeature sf = this.getFileWithSnapshotFeature();
    if (sf != null) {
      return sf.getDiffs().getSnapshotINode(snapshotId, this);
    } else {
      return this;
    }
  }
{noformat}

{{AbstractINodeDiffList#getSnapshotINode}}
{noformat}
  public A getSnapshotINode(final int snapshotId, final A currentINode) {
    final D diff = getDiffById(snapshotId);
    final A inode = diff == null? null: diff.getSnapshotINode();
    return inode == null? currentINode: inode;
  }
{noformat}

But, INodeFile, INodeDirectory #getSnapshotINode() returns the current INode's local INodeAttributes if there is anything available for the given snapshot id. When there is an INodeAttributesProvider configured, attributes provided by the external provider could be different from the local. But, getSnapshotINode() always returns the local attributes without retrieving them from attributes provider. "
"""fs"" java.net.UnknownHostException when HA NameNode is used",HDFS-12109,"After setting up an HA NameNode configuration, the following invocation of ""fs"" fails:

[hadoop@namenode01 ~]$ /usr/local/hadoop/bin/hdfs dfs -ls /
-ls: java.net.UnknownHostException: saccluster

It works if properties are defined as per below:

/usr/local/hadoop/bin/hdfs dfs -Ddfs.nameservices=saccluster -Ddfs.client.failover.proxy.provider.saccluster=org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider -Ddfs.ha.namenodes.saccluster=namenode01,namenode02 -Ddfs.namenode.rpc-address.saccluster.namenode01=namenode01:8020 -Ddfs.namenode.rpc-address.saccluster.namenode02=namenode02:8020 -ls /

These properties are defined in /usr/local/hadoop/etc/hadoop/hdfs-site.xml as per below:

    <property>
        <name>dfs.nameservices</name>
        <value>saccluster</value>
    </property>
    <property>
        <name>dfs.ha.namenodes.saccluster</name>
        <value>namenode01,namenode02</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.saccluster.namenode01</name>
        <value>namenode01:8020</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.saccluster.namenode02</name>
        <value>namenode02:8020</value>
    </property>
        <property>
        <name>dfs.namenode.http-address.saccluster.namenode01</name>
        <value>namenode01:50070</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.saccluster.namenode02</name>
        <value>namenode02:50070</value>
    </property>
        <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://namenode01:8485;namenode02:8485;datanode01:8485/saccluster</value>
    </property>
    <property>
        <name>dfs.client.failover.proxy.provider.mycluster</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>

In /usr/local/hadoop/etc/hadoop/core-site.xml the default FS is defined as per below:

    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://saccluster</value>
    </property>

In /usr/local/hadoop/etc/hadoop/hadoop-env.sh the following export is defined:

export HADOOP_CONF_DIR=""/usr/local/hadoop/etc/hadoop""

Is ""fs"" trying to read these properties from somewhere else, such as a separate client configuration file?

Apologies if I am missing something obvious here."
getNonDfsUsed return 0 if reserved bigger than actualNonDfsUsed,HDFS-11752,"{code}
public long getNonDfsUsed() throws IOException {
    long actualNonDfsUsed = getActualNonDfsUsed();
    if (actualNonDfsUsed < reserved) {
      return 0L;
    }
    return actualNonDfsUsed - reserved;
  }
{code}

The code block above is the function to caculate nonDfsUsed, but in fact it will let the result to be 0L out of expect. Such as this following situation:

du.reserved  = 50G
Disk Capacity = 2048G
Disk Available = 2000G
Dfs used = 30G

usage.getUsed() = dirFile.getTotalSpace() - dirFile.getFreeSpace()
                            = 2048G - 2000G
                            = 48G
getActualNonDfsUsed  =  usage.getUsed() - getDfsUsed()
                                      =  48G - 30G
                                      = 18G
18G < 50G, so the function `getNonDfsUsed` actualNonDfsUsed < reserved, and the NonDfsUsed will return 0, is that logic make sense?"
access standy namenode slow,HDFS-9473,"access standy namenode slow

we have a hadoop cluster with 200 nodes. And use ha namenodes.
nn1 hadoop109  active
nn2 hadoop110  standby

after we switchover nms,  
hadoop110(nn2) is active
hadoop109(nn1) is standy

when we access hdfs://hadoop109:8020 ( hdfs dfs -ls hdfs://hadoop109:8020),  that get the response sometimes fast, sometimes slow.

I tuned the rpc parameters about dfs.namenode.handler.count and dfs.namenode.service.handler.count . the value from 105 to 150 ( >=20*ln(datanodes) ).
But the problem still occured.

Does someone have the same problem, and could give some suggestion ?


take a look the debug log....


15/11/27 19:37:46 DEBUG util.Shell: setsid exited with exit code 0
15/11/27 19:37:46 DEBUG conf.Configuration: parsing URL jar:file:/usr/local/hadoop-2.4.0/share/hadoop/common/hadoop-common-2.4.0.jar!/core-default.xml
15/11/27 19:37:46 DEBUG conf.Configuration: parsing input stream sun.net.www.protocol.jar.JarURLConnection$JarURLInputStream@720c653f
15/11/27 19:37:46 DEBUG conf.Configuration: parsing URL file:/usr/local/hadoop-2.4.0/etc/hadoop/core-site.xml
15/11/27 19:37:46 DEBUG conf.Configuration: parsing input stream java.io.BufferedInputStream@7038ce7b
15/11/27 19:37:47 DEBUG security.Groups:  Creating new Groups object
15/11/27 19:37:47 DEBUG util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...
15/11/27 19:37:47 DEBUG util.NativeCodeLoader: Loaded the native-hadoop library
15/11/27 19:37:47 DEBUG security.JniBasedUnixGroupsMapping: Using JniBasedUnixGroupsMapping for Group resolution
15/11/27 19:37:47 DEBUG security.JniBasedUnixGroupsMappingWithFallback: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMapping
15/11/27 19:37:47 DEBUG security.Groups: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
15/11/27 19:37:47 DEBUG security.UserGroupInformation: hadoop login
15/11/27 19:37:47 DEBUG security.UserGroupInformation: hadoop login commit
15/11/27 19:37:47 DEBUG security.UserGroupInformation: using local user:UnixPrincipal: hdfs
15/11/27 19:37:47 DEBUG security.UserGroupInformation: UGI loginUser:hdfs (auth:SIMPLE)
15/11/27 19:37:47 DEBUG hdfs.BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false
15/11/27 19:37:47 DEBUG hdfs.BlockReaderLocal: dfs.client.read.shortcircuit = true
15/11/27 19:37:47 DEBUG hdfs.BlockReaderLocal: dfs.client.domain.socket.data.traffic = false
15/11/27 19:37:47 DEBUG hdfs.BlockReaderLocal: dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn_socket
15/11/27 19:37:47 DEBUG retry.RetryUtils: multipleLinearRandomRetry = null
15/11/27 19:37:48 DEBUG ipc.Server: rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcRequestWrapper, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@739820e5
15/11/27 19:37:48 DEBUG ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@42988fee
15/11/27 19:37:49 DEBUG unix.DomainSocketWatcher: org.apache.hadoop.net.unix.DomainSocketWatcher$1@7116778b: starting with interruptCheckPeriodMs = 60000
15/11/27 19:37:49 DEBUG hdfs.BlockReaderLocal: The short-circuit local reads feature is enabled.
15/11/27 19:37:49 DEBUG ipc.Client: The ping interval is 60000 ms.
15/11/27 19:37:49 DEBUG ipc.Client: Connecting to hadoop109:8020
15/11/27 19:37:49 DEBUG ipc.Client: IPC Client (320922331) connection to hadoop109:8020 from hdfs: starting, having connections 1

 .........#  response 4 mins #...........
15/11/27 19:37:49 DEBUG ipc.Client: IPC Client (320922331) connection to hadoop109:8020 from hdfs sending #0   
15/11/27 19:41:16 DEBUG ipc.Client: IPC Client (320922331) connection to hadoop109:8020 from hdfs got value #0
......................


ls: Operation category READ is not supported in state standby
15/11/27 19:41:16 DEBUG ipc.Client: stopping client from cache: org.apache.hadoop.ipc.Client@42988fee
15/11/27 19:41:16 DEBUG ipc.Client: removing client from cache: org.apache.hadoop.ipc.Client@42988fee
15/11/27 19:41:16 DEBUG ipc.Client: stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@42988fee
15/11/27 19:41:16 DEBUG ipc.Client: Stopping client
15/11/27 19:41:16 DEBUG ipc.Client: IPC Client (320922331) connection to hadoop109:8020 from hdfs: closed
15/11/27 19:41:16 DEBUG ipc.Client: IPC Client (320922331) connection to hadoop109:8020 from hdfs: stopped, remaining connections 0
"
"Why when using the hdfs nfs gateway, a file which is smaller than one block size required a block",HDFS-11917,"I use the linux shell to put the file into the hdfs throuth the hdfs nfs gateway. I found that if the file which size is smaller than one block(128M), it will still takes one block(128M) of hdfs storage by this way. But after a few minitues the excess storage will be released.
e.g:If I put the file(60M) into the hdfs throuth the hdfs nfs gateway, it will takes one block(128M) at first. After a few minitues the excess storage(68M) will
be released. The file only use 60M hdfs storage at last.
Why is will be this?"
distcp interrupt does not kill hadoop job,HDFS-11599,"keyboard interrupt for example leaves the hadoop job & copy still running, is this intended behavior?"
Ozone : need to refactor StorageContainerLocationProtocolServerSideTranslatorPB,HDFS-11857,"Currently, StorageContainerLocationProtocolServerSideTranslatorPB has two protocol impls:
{{StorageContainerLocationProtocol impl}}
{{ScmBlockLocationProtocol blockImpl}}.
the class provides container-related services by invoking {{impl}}, and block-related services by invoking {{blockImpl}}. Namely, on server side, the implementation makes a distinguish between ""container protocol"" and ""block protocol"". 

An issue is that, currently, everywhere except for the server side is viewing ""container protocol"" and ""block protocol"" as different. More specifically, StorageContainerLocationProtocol.proto still includes both container operation and block operation in itself alone. As a result of this difference, it is difficult to implement certain APIs  (e.g. putKey) properly from client side.

This JIRA merges ""block protocol"" back to ""container protocol"" in StorageContainerLocationProtocolServerSideTranslatorPB, to unblock the implementation of other APIs for client side. 

Please note that, in the long run, separating these two protocols does seem to be the right way. This JIRA is only a temporary solution to unblock developing other APIs. Will need to revisit these protocols in the future.

Thanks [~xyao] for the offline discussion.
"
Ozone:SCM: Add support for getContainer in SCM,HDFS-11746,Adds support for getContainer in SCM. With this change we will be able to get the container pipeline from SCM using containerId.
CacheAdmin operations not supported with viewfs,HDFS-5567,"On a federated cluster with viewfs configured, we'll run into the following error when using CacheAdmin commands:

{code}
bash-4.1$ hdfs cacheadmin -listPools
Exception in thread ""main"" java.lang.IllegalArgumentException: FileSystem viewfs://cluster3/ is not an HDFS file system
	at org.apache.hadoop.hdfs.tools.CacheAdmin.getDFS(CacheAdmin.java:96)
	at org.apache.hadoop.hdfs.tools.CacheAdmin.access$100(CacheAdmin.java:50)
	at org.apache.hadoop.hdfs.tools.CacheAdmin$ListCachePoolsCommand.run(CacheAdmin.java:748)
	at org.apache.hadoop.hdfs.tools.CacheAdmin.run(CacheAdmin.java:84)
	at org.apache.hadoop.hdfs.tools.CacheAdmin.main(CacheAdmin.java:89)
bash-4.1$
{code}

"
getErasureCodingPolicyByName should throw a checked exception,HDFS-11683,"getErasureCodingPolicyByName throws a HadoopIllegalArgumentException, which is a RuntimeException. This is basically only used by CLI tools and client code, we should throw a different exception in the NameNode."
Erasure Coding: Support Parity Blocks placement onto same nodes hosting Data Blocks when DataNodes are insufficient,HDFS-11552,"Currently, {{DFSStripedOutputStream}} verifies if the allocated block locations are at least numDataBlocks length. That is, for the EC Policy RS-6-3-64K, though the total needed DNs for a full EC Block Group is 9, Clients will be able to successfully create a DFSStripedOutputStream with just 6 DNs. Moreover, the output stream thus created with less DNs will totally ignore writing Parity Blocks.

{code}
[Thread-5] WARN  hdfs.DFSOutputStream (DFSStripedOutputStream.java:allocateNewBlock(497)) - Failed to get block location for parity block, index=6
[Thread-5] WARN  hdfs.DFSOutputStream (DFSStripedOutputStream.java:allocateNewBlock(497)) - Failed to get block location for parity block, index=7
[Thread-5] WARN  hdfs.DFSOutputStream (DFSStripedOutputStream.java:allocateNewBlock(497)) - Failed to get block location for parity block, index=8
{code}

So, upon file stream close we get the following warning message (though not accurate) when the parity blocks are not yet written out.
{code}
INFO  namenode.FSNamesystem (FSNamesystem.java:checkBlocksComplete(2726)) - BLOCK* blk_-9223372036854775792_1002 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 6) in file /ec/test1
INFO  hdfs.StateChange (FSNamesystem.java:completeFile(2679)) - DIR* completeFile: /ec/test1 is closed by DFSClient_NONMAPREDUCE_-1900076771_17
WARN  hdfs.DFSOutputStream (DFSStripedOutputStream.java:logCorruptBlocks(1117)) - Block group <1> has 3 corrupt blocks. It's at high risk of losing data.
{code}

I am not sure if there are any practical limitations in placing more blocks of a Block Group onto the same node. At least, we can allow parity blocks co-exist with data blocks, whenever there are insufficient DNs in the cluster. Later, upon addition of more DataNodes, the Block Placement Policy can detect the improper placement for such BlockGroups and can tigger EC reconstruction. "
Datanode registration process fails in hadoop 2.6 ,HDFS-7810,"When a new DN is added to the cluster, the registration process fails. The following are the steps followed.

- Install and start a new DN
- Add entry for the DN in the NN {{/etc/hosts}} file

DN log shows that the registration process failed

- Tried to restart DN with the same result

Since all the DNs have multiple NW interface, we are using the following {{hdfs-site.xml}} property, instead of listing all the {{dfs.datanode.xx.address}} properties.

{code:xml}
  <property>
    <name>dfs.datanode.dns.interface</name>
    <value>eth2</value>
  </property>
{code}

- Restarting the NN resolves the issue with registration which is not desired. 
- Adding the following {{dfs.datanode.xx.address}} properties seem to resolve DN registration process without NN restart. But this is a different behavior compared to *hadoop 2.2*. Is there a reason for the change?

{code:xml}
  <property>
    <name>dfs.datanode.address</name>
    <value>192.168.0.12:50010</value>
  </property>

  <property>
    <name>dfs.datanode.ipc.address</name>
    <value>192.168.0.12:50020</value>
  </property>

  <property>
    <name>dfs.datanode.http.address</name>
    <value>192.168.0.12:50075</value>
  </property>
{code}

*NN Log Error Entry*
{quote}
2015-02-17 12:21:53,583 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 8020, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.registerDatanode from 192.168.100.13:37516 Call#1027 Retry#0 
org.apache.hadoop.hdfs.server.protocol.DisallowedDatanodeException: Datanode denied communication with namenode because hostname cannot be resolved (ip=192.168.100.13, hostname=192.168.100.13): DatanodeRegistration(0.0.0.0, datanodeUuid=bd23eb3c-a5b9-43e4-ad23-1683346564ac, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-02099252-fbca-4bf2-b466-9a0ed67e53a3;nsid=2048643132;c=0) 
at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.registerDatanode(DatanodeManager.java:887) 
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.registerDatanode(FSNamesystem.java:5002) 
at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.registerDatanode(NameNodeRpcServer.java:1065) 
at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.registerDatanode(DatanodeProtocolServerSideTranslatorPB.java:92) 
at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:26378) 
at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619) 
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962) 
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039) 
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035) 
at java.security.AccessController.doPrivileged(Native Method) 
at javax.security.auth.Subject.doAs(Subject.java:415) 
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) 
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033) 
2015-02-17 12:21:58,607 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: Unresolved datanode registration: hostname cannot be resolved (ip=192.168.100.13, hostname=192.168.100.13) 
{quote}

*DN Log Error Entry*
{quote}
2015-02-17 12:21:02,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1782713777-10.0.100.11-1424188575377 (Datanode Uuid null) service to f-bcpc-vm1/192.168.100.11:8020 beginning handshake with NN 
2015-02-17 12:21:03,006 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool BP-1782713777-10.0.100.11-1424188575377 (Datanode Uuid null) service to f-bcpc-vm1/192.168.100.11:8020 Datanode denied communication with namenode because hostname cannot be resolved (ip=192.168.100.13, hostname=192.168.100.13): DatanodeRegistration(0.0.0.0, datanodeUuid=bd23eb3c-a5b9-43e4-ad23-1683346564ac, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-02099252-fbca-4bf2-b466-9a0ed67e53a3;nsid=2048643132;c=0) 
at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.registerDatanode(DatanodeManager.java:887) 
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.registerDatanode(FSNamesystem.java:5002) 
at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.registerDatanode(NameNodeRpcServer.java:1065) 
at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.registerDatanode(DatanodeProtocolServerSideTranslatorPB.java:92) 
at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:26378) 
at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619) 
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962) 
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039) 
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035) 
at java.security.AccessController.doPrivileged(Native Method) 
at javax.security.auth.Subject.doAs(Subject.java:415) 
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) 
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)
{quote}"
TestTrashWithEncryptionZones UT fails with NPE,HDFS-11452,"TestTrashWithSecureEncryptionZones UT fails with NPE
{code}
Stacktrace

java.lang.NullPointerException: null
	at org.apache.hadoop.hdfs.DFSTestUtil.createKey(DFSTestUtil.java:1581)
	at org.apache.hadoop.hdfs.DFSTestUtil.createKey(DFSTestUtil.java:1562)
	at org.apache.hadoop.hdfs.TestTrashWithSecureEncryptionZones.init(TestTrashWithSecureEncryptionZones.java:221)
{code}"
HDFS federation configuration should be documented,HDFS-1956,"HDFS-1689 didn't document any of the new configuration options it introduced. 
These should be in a ""Federation user guide"", or at the very least in Javadoc."
Document XOR policy XOR-2-1-64k support in HDFS,HDFS-11183,A new EC policy XOR-2-1-64k is added in HDFS. This task is to update HDFS related documents about how to use this policy
NetworkTopology#chooseRandom may run into a dead loop due to race condition,HDFS-11507,"{{NetworkTopology#chooseRandom()}} works as:
1. counts the number of available nodes as {{availableNodes}},
2. checks how many nodes are excluded, deduct from {{availableNodes}}
3. if {{availableNodes}} still > 0, then there are nodes available.
4. keep looping to find that node

But now imagine, in the meantime, the actually available nodes got removed in step 3 or step 4, and all remaining nodes are excluded nodes. Then, although there are no more nodes actually available, the code would still run as {{availableNodes}} > 0, and then it would keep getting excluded node and loop forever, as 
{{if (excludedNodes == null || !excludedNodes.contains(ret))}} 
will always be false.

We may fix this by expanding the while loop to also include the {{availableNodes}} calculation. Such that we re-calculate {{availableNodes}} every time it fails to find an available node."
Namenode Startup Failing When we add Jcarder.jar in class Path,HDFS-8132," *{color:blue}Namenode while Startup Args{color}*   ( Just added the jcarder args)

exec /home/hdfs/jdk1.7.0_72/bin/java -Dproc_namenode -Xmx1000m -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/opt/ClusterSetup/Hadoop2.7/install/hadoop/namenode/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/opt/ClusterSetup/Hadoop2.7/install/hadoop/namenode -Dhadoop.id.str=hdfs -Dhadoop.root.logger=INFO,console -Djava.library.path=/opt/ClusterSetup/Hadoop2.7/install/hadoop/namenode/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender {color:red}-javaagent:/opt/Jcarder/jcarder.jar=outputdir=/opt/Jcarder/Output/nn-jcarder{color} -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.hdfs.server.namenode.NameNode
Setting outputdir to /opt/Jcarder/Output/nn-jcarder
Starting JCarder (2.0.0/6) agent
Opening for writing: /opt/Jcarder/Output/nn-jcarder/jcarder_events.db
Opening for writing: /opt/Jcarder/Output/nn-jcarder/jcarder_contexts.db
Not instrumenting standard library classes (AWT, Swing, etc.)
JCarder agent initialized

 *{color:red}ERROR{color}* 

{noformat}
Exception in thread ""main"" java.lang.VerifyError: Expecting a stackmap frame at branch target 21
Exception Details:
  Location:
    org/apache/hadoop/hdfs/server/namenode/NameNode.createHAState(Lorg/apache/hadoop/hdfs/server/common/HdfsServerConstants$StartupOption;)Lorg/apache/hadoop/hdfs/server/namenode/ha/HAState; @4: ifeq
  Reason:
    Expected stackmap frame at this location.
  Bytecode:
    0000000: 2ab4 02d2 9900 112b b203 08a5 000a 2bb2
    0000010: 030b a600 07b2 030d b0b2 030f b0       

	at java.lang.Class.getDeclaredMethods0(Native Method)
	at java.lang.Class.privateGetDeclaredMethods(Class.java:2615)
	at java.lang.Class.getMethod0(Class.java:2856)
	at java.lang.Class.getMethod(Class.java:1668)
	at sun.launcher.LauncherHelper.getMainMethod(LauncherHelper.java:494)
	at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:486)
{noformat}"
HDFS fsck command shows health as corrupt for '/',HDFS-11413,"I have open source hadoop version 2.7.3 cluster (2 Masters + 3 Slaves) installed on AWS EC2 instances. I am using the cluster to integrate it with Kafka Connect. 

The setup of cluster was done last month and setup of kafka connect was completed last fortnight. Since then, we were able to operate the kafka topic records on our HDFS and do various operations.

Since last afternoon, I find that any kafka topic is not getting committed to the cluster. When I tried to open the older files, I started getting below error. When I copy a new file to the cluster from local, it comes and gets opened but after some time, again starts showing similar IOException:

==========================================================
17/02/14 07:57:55 INFO hdfs.DFSClient: No node available for BP-1831277630-10.16.37.124-1484306078618:blk_1073793876_55013 file=/test/inputdata/derby.log
17/02/14 07:57:55 INFO hdfs.DFSClient: Could not obtain BP-1831277630-10.16.37.124-1484306078618:blk_1073793876_55013 from any node: java.io.IOException: No live nodes contain block BP-1831277630-10.16.37.124-1484306078618:blk_1073793876_55013 after checking nodes = [], ignoredNodes = null No live nodes contain current block Block locations: Dead nodes: . Will get new block locations from namenode and retry...
17/02/14 07:57:55 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 499.3472970548959 msec.
17/02/14 07:57:55 INFO hdfs.DFSClient: No node available for BP-1831277630-10.16.37.124-1484306078618:blk_1073793876_55013 file=/test/inputdata/derby.log
17/02/14 07:57:55 INFO hdfs.DFSClient: Could not obtain BP-1831277630-10.16.37.124-1484306078618:blk_1073793876_55013 from any node: java.io.IOException: No live nodes contain block BP-1831277630-10.16.37.124-1484306078618:blk_1073793876_55013 after checking nodes = [], ignoredNodes = null No live nodes contain current block Block locations: Dead nodes: . Will get new block locations from namenode and retry...
17/02/14 07:57:55 WARN hdfs.DFSClient: DFS chooseDataNode: got # 2 IOException, will wait for 4988.873277172643 msec.
17/02/14 07:58:00 INFO hdfs.DFSClient: No node available for BP-1831277630-10.16.37.124-1484306078618:blk_1073793876_55013 file=/test/inputdata/derby.log
17/02/14 07:58:00 INFO hdfs.DFSClient: Could not obtain BP-1831277630-10.16.37.124-1484306078618:blk_1073793876_55013 from any node: java.io.IOException: No live nodes contain block BP-1831277630-10.16.37.124-1484306078618:blk_1073793876_55013 after checking nodes = [], ignoredNodes = null No live nodes contain current block Block locations: Dead nodes: . Will get new block locations from namenode and retry...
17/02/14 07:58:00 WARN hdfs.DFSClient: DFS chooseDataNode: got # 3 IOException, will wait for 8598.311122824263 msec.
17/02/14 07:58:09 WARN hdfs.DFSClient: Could not obtain block: BP-1831277630-10.16.37.124-1484306078618:blk_1073793876_55013 file=/test/inputdata/derby.log No live nodes contain current block Block locations: Dead nodes: . Throwing a BlockMissingException
17/02/14 07:58:09 WARN hdfs.DFSClient: Could not obtain block: BP-1831277630-10.16.37.124-1484306078618:blk_1073793876_55013 file=/test/inputdata/derby.log No live nodes contain current block Block locations: Dead nodes: . Throwing a BlockMissingException
17/02/14 07:58:09 WARN hdfs.DFSClient: DFS Read
org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1831277630-10.16.37.124-1484306078618:blk_1073793876_55013 file=/test/inputdata/derby.log
        at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:983)
        at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:642)
        at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:882)
        at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
        at java.io.DataInputStream.read(DataInputStream.java:100)
        at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:85)
        at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:59)
        at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:119)
        at org.apache.hadoop.fs.shell.Display$Cat.printToStdout(Display.java:107)
        at org.apache.hadoop.fs.shell.Display$Cat.processPath(Display.java:102)
        at org.apache.hadoop.fs.shell.Command.processPaths(Command.java:317)
        at org.apache.hadoop.fs.shell.Command.processPathArgument(Command.java:289)
        at org.apache.hadoop.fs.shell.Command.processArgument(Command.java:271)
        at org.apache.hadoop.fs.shell.Command.processArguments(Command.java:255)
        at org.apache.hadoop.fs.shell.Command.processRawArguments(Command.java:201)
        at org.apache.hadoop.fs.shell.Command.run(Command.java:165)
        at org.apache.hadoop.fs.FsShell.run(FsShell.java:287)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)
        at org.apache.hadoop.fs.FsShell.main(FsShell.java:340)
cat: Could not obtain block: BP-1831277630-10.16.37.124-1484306078618:blk_1073793876_55013 file=/test/inputdata/derby.log
==========================================================

When I do : hdfs fsck / , I get:
==========================================================
 Total size:    667782677 B
 Total dirs:    406
 Total files:   44485
 Total symlinks:                0
 Total blocks (validated):      43767 (avg. block size 15257 B)
  ********************************
  UNDER MIN REPL'D BLOCKS:      43766 (99.99772 %)
  dfs.namenode.replication.min: 1
  CORRUPT FILES:        43766
  MISSING BLOCKS:       43766
  MISSING SIZE:         667781648 B
  CORRUPT BLOCKS:       43766
  ********************************
 Minimally replicated blocks:   1 (0.0022848265 %)
 Over-replicated blocks:        0 (0.0 %)
 Under-replicated blocks:       0 (0.0 %)
 Mis-replicated blocks:         0 (0.0 %)
 Default replication factor:    3
 Average block replication:     6.8544796E-5
 Corrupt blocks:                43766
 Missing replicas:              0 (0.0 %)
 Number of data-nodes:          3
 Number of racks:               1
FSCK ended at Tue Feb 14 07:59:10 UTC 2017 in 932 milliseconds


The filesystem under path '/' is CORRUPT
==========================================================

That means, all my files got corrupted somehow. 

I want to recover my HDFS and fix the corrupt health status. Also, I would like to understand, how such an issue occurred suddenly and how to prevent it in future?

Many thanks,
Nishant Verma "
Erasure Coding: Expose refreshECSchemas command to reload predefined schemas,HDFS-8125,"This is to expose {{refreshECSchemas}} command to administrators. When invoking this command it will reload predefined schemas from configuration file and dynamically update the schema definitions maintained in Namenode.

Note: For more details please refer the [discussion|https://issues.apache.org/jira/browse/HDFS-7866?focusedCommentId=14489387&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14489387] with [~drankye]"
TestHdfsConfigFields always fails in trunk,HDFS-11343,"TestHdfsConfigFields always fails in trunk. The stack info(https://builds.apache.org/job/PreCommit-HDFS-Build/18172/testReport/)
{code}
java.lang.AssertionError: interface org.apache.hadoop.hdfs.client.HdfsClientConfigKeys interface org.apache.hadoop.hdfs.client.HdfsClientConfigKeys$StripedRead class org.apache.hadoop.hdfs.DFSConfigKeys has 1 variables missing in hdfs-default.xml
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)

org.apache.hadoop.conf.TestConfigurationFieldsBase.testCompareConfigurationClassAgainstXml(TestConfigurationFieldsBase.java:549)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMetho
{code}"
Allow to configure the system default EC schema,HDFS-8095,"As suggested by [~umamaheswararao] and [~vinayrpet] in HDFS-8074, we may desire allowing to configure the system default EC schema, so in any deployment a cluster admin may be able to define their own system default one. In the discussion, we have two approaches to configure the system default schema: 1) predefine it in the {{ecschema-def.xml}} file, making sure it's not changed; 2) configure the key parameter values as properties in {{core-site.xml}}. Open this for future consideration in case it's forgotten."
 Erasure Coding: Postpone the recovery work for a configurable time period,HDFS-9826,"Currently NameNode prepares recovering when finding an under replicated  block group. This is inefficient and reduces resources for other operations. It would be better to postpone the recovery work for a period of time if only one internal block is corrupted considering points shown by papers such as \[1\]\[2\]:
1.	Transient errors in which no data are lost account for more than 90% of data center failures, owing to network partitions, software problems, or non-disk hardware faults.
2.	Although erasure codes tolerate multiple simultaneous failures, single failures represent 99.75% of recoveries.

Different clusters may have different status, so we should allow user to configure the time for postponing the recoveries. Proper configuration will reduce a large proportion of unnecessary recoveries. When finding multiple internal blocks corrupted in a block group, we prepare the recovery work immediately because it鈥檚 very rare and we don鈥檛 want to increase the risk of losing data.

[1] Availability in globally distributed storage systems
http://static.usenix.org/events/osdi10/tech/full_papers/Ford.pdf
[2] Rethinking erasure codes for cloud file systems: minimizing I/O for recovery and degraded reads
http://static.usenix.org/events/fast/tech/full_papers/Khan.pdf
"
Specify file encryption attributes at create time,HDFS-6555,We need to create a Crypto Blob for passing around crypto info. 
Implement equals and hashcode in FsVolumeSpi implementations,HDFS-11277,Certain of the implementations of FsVolumeSpi say for eg:- FsVolumeImpl can implement equals and hashcode. This is to avoid object identity check during disk check scheduling in ThrottledAsyncChecker and instead use other means of checking whether a diskcheck is already in progress or not for  FsVolumeImpl object.
"build hadoop in windows 7/10 64 bit using Visual C++ Build Tools Standalone compiler, dotnet 4.5",HDFS-11200,"configure maven , cmake to build the new version hadoop 2.7.4 to be packaged using Visual C++ Build Tools Standalone compiler, libraries, dotnet 4.5 (http://landinghub.visualstudio.com/visual-cpp-build-tools)"
Fix findbugs issue with BlockPoolSlice#validateIntegrityAndSetLength,HDFS-11205,This ticket is opened to fix the follow up findbugs issue introduced by HDFS-10930. 
failure to build hadoop 2.7.1 in windows 7 64 bit,HDFS-11199,"In windows 7 64 bit, I build hadoop version 2.7.1 
i installed all needed software, 
for c compiler i use the c++ compiler of windows sdk 7.1 (visual studio 2010 isn't installed)

I run from Windows SDK 7.1 Command Prompt with release x64

but the build failed with errors

 The C compiler identification is unknown
 -- The CXX compiler identification is unknown
 CMake Error in CMakeLists.txt:
   No CMAKE_C_COMPILER could be found.

  
 CMake Error in CMakeLists.txt:
 No CMAKE_CXX_COMPILER could be found.

The following is the console output 




[INFO] ------------------------------------------------------------------------
[INFO] Building Apache Hadoop HDFS 2.7.1
[INFO] ------------------------------------------------------------------------
[INFO]
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-os) @ hadoop-hdfs ---
....
....

main:
[INFO] Executed tasks
[INFO]
[INFO] --- maven-antrun-plugin:1.7:run (make) @ hadoop-hdfs ---
[INFO] Executing tasks

main:
     [exec] -- The C compiler identification is unknown
     [exec] -- The CXX compiler identification is unknown
     [exec] CMake Error in CMakeLists.txt:
     [exec]   No CMAKE_C_COMPILER could be found.
     [exec]
     [exec]
     [exec]
     [exec] CMake Error in CMakeLists.txt:
     [exec]   No CMAKE_CXX_COMPILER could be found.
     [exec]
     [exec]
     [exec]
     [exec] -- Configuring incomplete, errors occurred!
     [exec] See also ""E:/hadoop-2.7.1-src/hadoop-hdfs-project/hadoop-hdfs/target
/native/CMakeFiles/CMakeOutput.log"".
     [exec] See also ""E:/hadoop-2.7.1-src/hadoop-hdfs-project/hadoop-hdfs/target
/native/CMakeFiles/CMakeError.log"".
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] Apache Hadoop Main ................................. SUCCESS [  2.995 s]
[INFO] Apache Hadoop Project POM .......................... SUCCESS [  4.477 s]
[INFO] Apache Hadoop Annotations .......................... SUCCESS [  4.696 s]
[INFO] Apache Hadoop Assemblies ........................... SUCCESS [  0.250 s]
[INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [  3.759 s]
[INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [  3.775 s]
[INFO] Apache Hadoop MiniKDC .............................. SUCCESS [  3.354 s]
[INFO] Apache Hadoop Auth ................................. SUCCESS [  4.056 s]
[INFO] Apache Hadoop Auth Examples ........................ SUCCESS [  3.807 s]
[INFO] Apache Hadoop Common ............................... SUCCESS [02:09 min]
[INFO] Apache Hadoop NFS .................................. SUCCESS [ 12.776 s]
[INFO] Apache Hadoop KMS .................................. SUCCESS [ 15.304 s]
[INFO] Apache Hadoop Common Project ....................... SUCCESS [  0.031 s]
[INFO] Apache Hadoop HDFS ................................. FAILURE [ 42.105 s]
[INFO] Apache Hadoop HttpFS ............................... SKIPPED
[INFO] Apache Hadoop HDFS BookKeeper Journal .............. SKIPPED
.....
.....


[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 03:55 min
[INFO] Finished at: 2016-12-03T14:30:39+02:00
[INFO] Final Memory: 83M/494M
[INFO] ------------------------------------------------------------------------
.....
....



"
VolumeScanner should report the latest generation stamp of a bad replica,HDFS-11155,"HDFS-10512 fixed a race condition that caused VolumeScanner to terminate abruptly when a corrupt replica, which is being updated, is detected. However, when such a corrupt replica is detected, VolumeScanner still reports the old replica generation stamp to the NN. NN then directs DN to remove the older replica. Because the generation stamp is updated, DN can not find it, so corrupt replica remains corrupt.

NN's log shows something similar to the following:
{quote}
2016-11-17 21:08:05,350 INFO BlockStateChange: BLOCK NameSystem.addToCorruptReplicasMap: blk_1077571736 added as corrupt on 192.168.168.58:50010 by /192.168.168.58  because client machine reported it
2016-11-17 21:08:05,350 INFO BlockStateChange: BLOCK* invalidateBlock: blk_1077571736_3991953(stored=blk_1077571736_3992018) on 192.168.168.58:50010
{quote}
The DN's log has these:

{noformat}
2016-11-17 21:08:04,815 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Appending to FinalizedReplica, blk_1077571736_3991953, FINALIZED
  getNumBytes()     = 39061752
  getBytesOnDisk()  = 39061752
  getVisibleLength()= 39061752
  getVolume()       = /data/3/dfs/dn/current
  getBlockFile()    = /data/3/dfs/dn/current/BP-1092022411-192.168.168.55-1474407949037/current/finalized/subdir58/subdir112/blk_1077571736

2016-11-17 21:08:09,158 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Failed to delete replica blk_1077571736_3991953: ReplicaInfo not found.
{noformat}"
Accelerate TestDiskBalancerCommand using static shared MiniDFSCluster,HDFS-11089,"It takes 50+ seconds to run the test suite. Similar to HDFS-11079, static shared MiniDFSCluster will be used to accelerate the run."
Need a new Result state for DiskBalancerWorkStatus to indicate the final Plan step errors and stuck rebalancing,HDFS-10904,"* A DiskBalancer {{NodePlan}} might include a Single {{MoveStep}} or a list of MoveSteps to perform the requested disk balancing operation.

* {{DiskBalancerWorkStatus}} tracks the current disk balancing operation status for the {{Plan}} just submitted. 

* {{DiskBalancerWorkStatus#Result}} has following states and the state machine movement for the {{currentResult}} state doesn't seem to be a driven totally from disk balancing operation. Especially, the state movement to DONE is happening only upon QueryResult, which can be improved. {code}
  /** Various result values. **/
  public enum Result {
    NO_PLAN(0),
    PLAN_UNDER_PROGRESS(1),
    PLAN_DONE(2),
    PLAN_CANCELLED(3);

DiskBalancer
cancelPlan(String)
        this.currentResult = Result.PLAN_CANCELLED;
DiskBalancer(String, Configuration, BlockMover)
    this.currentResult = Result.NO_PLAN;
queryWorkStatus()
        this.currentResult = Result.PLAN_DONE;
shutdown()
      this.currentResult = Result.NO_PLAN;
        this.currentResult = Result.PLAN_CANCELLED;
submitPlan(String, long, String, String, boolean)
      this.currentResult = Result.PLAN_UNDER_PROGRESS;
{code}


* More importantly, when the final {{MoveStep}} of the {{NodePlan}} fails, the currentResult state is stuck in {{PLAN_UNDER_PROGRESS}} forever. User querying the status will assume the operation is in progress when in reality its not making any progress.  User can also run {{Query}} command with _verbose_ option which then will display more details about the operation which includes details about errors encountered.
**  Query Output: {code}
Plan File:  <_file_path_>
Plan ID: <_plan_hash_>
Result: PLAN_UNDER_PROGRESS
{code}

** {code}
""sourcePath"" : ""/data/disk2/hdfs/dn"",
  ""destPath"" : ""/data/disk3/hdfs/dn"",
  ""workItem"" :
    .. .. ..
    ""errorCount"" : 0,
    ""errMsg"" : null,
    .. .. 
    ""maxDiskErrors"" : 5,
    .. .. ..
{code}
** But, user has to decipher these details to make out that the disk balancing operation is stuck as the top level Result still says {{PLAN_UNDER_PROGRESS}}. So, we want the DiskBalancer differentiate between the in-progress operation and the stuck or final error operations.
"
Add a metric to expose the timestamp of the oldest under-replicated block,HDFS-6682,"In the following case, the data in the HDFS is lost and a client needs to put the same file again.
# A Client puts a file to HDFS
# A DataNode crashes before replicating a block of the file to other DataNodes

I propose a metric to expose the timestamp of the oldest under-replicated/corrupt block. That way client can know what file to retain for the re-try."
Kms server UNAUTHENTICATED,HDFS-10428,"UNAUTHENTICATED RemoteHost:${ip} Method:OPTIONS URL:http://kms-server/kms/v1/?op=GETDELEGATIONTOKEN&renewer=yarn ErrorMsg:'Authentication required'
"
"Erasure Code(6:3) can not work, using get fs shell when 2 datanode down",HDFS-10697,"I can put file into EC folder(6:3), and get the file successfully.
But when I shutdown one datanode, I still can get the file, using hdfs dfs -get.
But when I shutdown two datanodes, when i use hdfs dfs -get, I get following error:
get: 4 missing blocks, the stripe is: Offset=786432, length=65536, fetchedChunksNum=5, missingChunksNum=4; locatedBlocks is: LocatedBlocks{"
MAX_DIR_ITEMS should not be hard coded since RPC buff size is configurable ,HDFS-10593,"In HDFS, ""dfs.namenode.fs-limits.max-directory-items"" was introduced in HDFS-6102 to restrict max items of single directory, and the value of it can not be larger than the value of MAX_DIR_ITEMS. Since ""ipc.maximum.data.length"" was added in HADOOP-9676 and documented in HADOOP-13039 to make maximum RPC buffer size configurable, it's not proper to hard code the value of MAX_DIR_ITEMS in {{FSDirectory}}."
FileNotFoundException: ID mismatch,HDFS-4654,"Mu cluster was build from source code trunk r1463074.

I got an exception as follows when I put a file to the HDFS.

13/04/01 09:33:45 WARN retry.RetryInvocationHandler: Exception while invoking addBlock of class ClientNamenodeProtocolTranslatorPB. Trying to fail over immediately.
13/04/01 09:33:45 WARN hdfs.DFSClient: DataStreamer Exception
java.io.FileNotFoundException: ID mismatch. Request id and saved id: 1073 , 1050
    at org.apache.hadoop.hdfs.server.namenode.INodeId.checkId(INodeId.java:51)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2501)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2298)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2212)
    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:498)
    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:356)
    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:40979)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:526)
    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1018)
    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1818)
    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1814)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:415)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1489)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1812)


please reproduce as :

hdfs dfs -put test.data  /user/data/test.data
after this command start to run, then kill active name node process.


I have only three nodes(A,B,C) for test
A and B are name nodes.
B and C are data nodes.
ZK deployed on A, B and C.

A, B and C are all journal nodes.

Thanks."
Standy NameNode is entering into Safemode even after HDFS-2914 due to resources low,HDFS-3526,"Scenario:
=========
Start ANN SNN with One DN
Make 100% disk full for SNN
Now restart SNN..

Here SNN is going safemode..But it shouldnot happen according to HDFS-2914"
TestCacheDirectives#testCreateAndModifyPools fails,HDFS-5838,"testCreateAndModifyPools generates an assertion fail when it runs after testBasicPoolOperations.

{noformat}
Running org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 5.045 sec <<< FAILURE! - in org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives
test(org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives)  Time elapsed: 4.649 sec  <<< FAILURE!
java.lang.AssertionError: expected no cache pools after deleting pool
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.assertTrue(Assert.java:43)
	at org.junit.Assert.assertFalse(Assert.java:68)
	at org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.testCreateAndModifyPools(TestCacheDirectives.java:334)
	at org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.test(TestCacheDirectives.java:160)


Results :

Failed tests: 
  TestCacheDirectives.test:160->testCreateAndModifyPools:334 expected no cache pools after deleting pool
{noformat}"
4 unit tests in TestFcHdfsSymlink failed on trunk,HDFS-4653,"4 Failed Tests:
org.apache.hadoop.fs.TestFcHdfsSymlink.testCreateWithPartQualPathFails
org.apache.hadoop.fs.TestFcHdfsSymlink.testCreateLinkUsingRelPaths
org.apache.hadoop.fs.TestFcHdfsSymlink.testCreateLinkUsingAbsPaths
org.apache.hadoop.fs.TestFcHdfsSymlink.testCreateLinkUsingFullyQualPaths
Details in: https://builds.apache.org/job/PreCommit-HDFS-Build/4166/"
Webhdfs should recover from dead DNs,HDFS-6221,"We've repeatedly observed the jetty acceptor thread silently dying in the DNs.  The webhdfs servlet may also ""disappear"" and jetty returns non-json 404s.

One approach to make webhdfs more resilient to bad DNs is dfsclient-like fetching of block locations to directly access the DNs instead of relying on a NN redirect that may repeatedly send the client to the same faulty DN(s)."
"If persistBlocks enabled in Namenode, hflush call can avoid persisting blocks with fsync",HDFS-4526,"If persists blocks enabled/in HA, evenry getAdditionalBlock will persist the blocks.
So, if user calls Hflush, then client can simply ensure data flushed to DNs and need not worry about fsync. 
Since we can not depend upon client side configuration about persistBlocks, we can just make a fsync call to NN and check whether persistBlocks enabled. If it is enabled, then simply return as it would have already persisted. So, that below peice of code execution can be avoided for every blocks and hflush calls

from fsync:
{code}
 writeLock();
    try {
      checkOperation(OperationCategory.WRITE);
      if (isInSafeMode()) {
        throw new SafeModeException(""Cannot fsync file "" + src, safeMode);
      }
      INodeFileUnderConstruction pendingFile  = checkLease(src, clientName);
      dir.persistBlocks(src, pendingFile);
    } finally {
      writeUnlock();
    }
    getEditLog().logSync();
{code}"
FSPermission check is incorrect,HDFS-5670,"FSPermission check is incorrect after update in the trunk recently.
I submitted MR job using root, but the whole output directory must be owned by root, otherwise, it throws Exception:
{code}
[root@10 ~]# hadoop fs -ls /
Found 1 items
drwxr-xr-x   - hadoop supergroup          0 2013-12-15 10:04 /user
[root@10 ~]# 
[root@10 ~]# hadoop fs -ls /user
Found 1 items
drwxr-xr-x   - root root          0 2013-12-15 10:04 /user/root
{code}

{code}
[root@10 ~]# hadoop jar airui.jar  /input /user/root/
Exception in thread ""main"" org.apache.hadoop.security.AccessControlException: Permission denied: user=root, access=WRITE, inode=""/user"":hadoop:supergroup:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:234)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:214)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:161)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:5410)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal(FSNamesystem.java:3236)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInt(FSNamesystem.java:3190)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:3174)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:708)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:514)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:605)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:932)
{code}
"
getFileStatus and getFileLinkStatus do not properly handle intermediate symlinks in a path,HDFS-5026,"Imagine a filesystem with the following:

{noformat}
/dir
/dir/file
/dirLink -> /dir
{noformat}

When calling getFileStatus or getFileLinkStatus on {{/dirLink/file}}, the returned FileStatus incorrectly has the resolved path of {{/dir/file}}. This differs from expectations and the Unix {{stat(1)}} command, which would return {{/dirLink/file}}."
Fix findbug warnings in raid,HDFS-3563,MAPREDUCE-3868 re-enabled raid but introduced 31 new findbugs warnings.  Those warnings should be fixed or appropriate items placed in an exclude file.
Remove the assumption that symlink is inside HDFS namespace and resolvable,HDFS-4771,"Currently HDFS always resolves symlink when setting certain file attributes, such as setPermission and setTime. And thus the client can't set some file attributes of the symlink itself.

Two major problems with current symlink support:
 1. HDFS assumes the link is inside its own namespace. This is a problem when HDFS is integrated into client's file system namespace.
2. Suppose the linked target is inside HFDS, HDFS doesn't really check whether the target saved in the link file is a valid path. Even the target was valid, it could become invalid as the namespace changes.

For example, create a symlink /user/brandon/iamlink.lnk and it has the content(target) as ""/invalid/path"". The file /user/brandon/iamlink.lnk can't be deleted since HDFS can't resolve it.

"
Namenode.getAddress() should return namenode rpc-address,HDFS-5227,"Currently, {{Namenode.getAddress(Configuration conf)}} will return default file system address as its result. The correct behavior should be returning config value of ""dfs.namenode.rpc-address"" if it presents in the configurations. Otherwise namenode will fail to start if the default file system is configured to another file system other than the one running in the cluster. We have a similar issue in 1.0 code base. The JIRA is HDFS-4320. The previous JIRA is closed and we cannot open it. Thus create a new one to track the issue in trunk."
Namenode https web UI starts up on wrong port by default,HDFS-5271,"When the NameNode is started with hadoop.ssl.enabled, the web UI listens on the wrong port by default (50070). The configuration suggests it should be port 50470:

{code}
  public static final int     DFS_NAMENODE_HTTPS_PORT_DEFAULT = 50470;
{code}"
Generate block ID sequentially cannot work with QJM HA,HDFS-4967,"There are two name nodes, one is active, another acts as standby name node. QJM Ha  configured.

After HDFS-4645 committed in the trunk, then the following error showed during name node start:


{code}
2013-07-09 11:28:45,394 FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Exception in namenode join
java.lang.IllegalStateException: Cannot skip to less than the current value (=1073741824), where newValue=0
        at org.apache.hadoop.util.SequentialNumber.skipTo(SequentialNumber.java:58)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setLastAllocatedBlockId(FSNamesystem.java:5124)
        at org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader.load(FSImageFormat.java:278)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:809)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:798)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile(FSImage.java:653)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:623)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:260)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:719)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:552)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:401)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:435)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:607)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:592)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1172)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1238)
2013-07-09 11:28:45,397 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
{code}





"
Cannot upgrade the layout version with -49 from -48,HDFS-5686,"When we upgraded the layout version from -48 to -49, the following exception output. 

{code}
2013-12-19 13:02:28,143 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Starting upgrade of image directory /hadoop/data1/dfs/name.
   old LV = -48; old CTime = 0.
   new LV = -49; new CTime = 1387425748143
2013-12-19 13:02:28,160 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Saving image file /hadoop/data1/dfs/name/current/fsimage.ckpt_0000000000000004837 using no compression
2013-12-19 13:02:28,191 ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: Unable to save image for /hadoop/data1/dfs/name
java.lang.NoSuchMethodError: org.apache.hadoop.hdfs.server.namenode.CacheManager.saveState(Ljava/io/DataOutput;Ljava/lang/String;)V
        at org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Saver.save(FSImageFormat.java:1037)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.saveFSImage(FSImage.java:854)
        at org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.run(FSImage.java:883)
        at java.lang.Thread.run(Thread.java:662)
2013-12-19 13:02:28,193 ERROR org.apache.hadoop.hdfs.server.common.Storage: Error reported on storage directory Storage Directory /hadoop/data1/dfs/name
{code}"
DFSClient more robust if the namenode is busy doing GC,HDFS-350,"In the current code, if the client (writer) encounters an RPC error while fetching a new block id from the namenode, it does not retry. It throws an exception to the application. This becomes especially bad if the namenode is in the middle of a GC and does not respond in time. The reason the client throws an exception is because it does not know whether the namenode successfully allocated a block for this file.

One possible enhancement would be to make the client retry the addBlock RPC if needed. The client can send the block list that it currently has. The namenode can match the block list send by the client with what it has in its own metadata and then send back a new blockid (or a previously allocated blockid that the client had not yet received because the earlier RPC timedout). This will make the client more robust!

This works even when we support Appends because the namenode will *always* verify that the client has the lease for the file in question."
TestHFlushInterrupted verifies interrupt state incorrectly,HDFS-10185,"In unit test {{TestHFlush#testHFlushInterrupted}}, there were some places verifying interrupt state incorrectly. As follow:
{code}
      Thread.currentThread().interrupt();
      try {
        stm.hflush();
        // If we made it past the hflush(), then that means that the ack made it back
       // from the pipeline before we got to the wait() call. In that case we should
        // still have interrupted status.
        assertTrue(Thread.interrupted());
      } catch (InterruptedIOException ie) {
        System.out.println(""Got expected exception during flush"");
      }
{code}
When stm do the {{hflush}} operation, it will throw interrupted exception and the {{assertTrue(Thread.interrupted())}} will not be execute. And if you put this before the {{hflush}}, this method will clear interrupted state and the expected exception will not be throw. The similar problem also appears after in stm.close.
So we should use a way to get state without clearing interrupted state like {{Thread.currentThread().isInterrupted()}}."
NPE running NNThroughputBenchmark,HDFS-1613,"After the NNThroughputBenchmark completes, I see the following NPEs on the console:

java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.namenode.BlocksMap.size(BlocksMap.java:178)
        at org.apache.hadoop.hdfs.server.namenode.BlockManager.getTotalBlocks(BlockManager.java:1538)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlocksTotal(FSNamesystem.java:4201)
        at org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMetrics.doUpdates(FSNamesystemMetrics.java:109)
"
Remove JNI code from libwebhdfs (C client library),HDFS-3917,The current implementation of libwebhdfs (C client library) uses JNI for loading NameNode configuration and implementing hdfsCopy/hdfsMove. We need to implement the same functionalities in libwebhdfs without using JNI.
 BlockManager.getUnderReplicatedBlocksCount() is not giving correct count if namenode in safe mode.,HDFS-8296,"{{underReplicatedBlocksCount}} update by the {{updateState()}} API.

{code}
 void updateState() {
    pendingReplicationBlocksCount = pendingReplications.size();
    underReplicatedBlocksCount = neededReplications.size();
    corruptReplicaBlocksCount = corruptReplicas.size();
  }
 {code}

 but this will not call when NN in safe mode. This is happening because ""computeDatanodeWork()"" we will return 0 if NN in safe mode 

 {code}

  int computeDatanodeWork() {
   .........
    if (namesystem.isInSafeMode()) {
      return 0;
    }
    ....................
    ....................
    this.updateState();
    ....................
    ....................
  }
 {code}"
Number of maximum Acl entries on a File/Folder should be made user configurable than hardcoding .,HDFS-7447,"By default on creating a folder1 will have 6 acl entries . On top of that assigning acl  on a folder1 exceeds 32 , then unable to assign acls for a group/user to folder1. 
{noformat}
2014-11-20 18:55:06,553 ERROR [qtp1279235236-17 - /rolexml/role/modrole] Error occured while setting permissions for Resource:[ hdfs://hacluster/folder1 ] and Error message is : Invalid ACL: ACL has 33 entries, which exceeds maximum of 32.
        at org.apache.hadoop.hdfs.server.namenode.AclTransformation.buildAndValidateAcl(AclTransformation.java:274)
        at org.apache.hadoop.hdfs.server.namenode.AclTransformation.mergeAclEntries(AclTransformation.java:181)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedModifyAclEntries(FSDirectory.java:2771)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.modifyAclEntries(FSDirectory.java:2757)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.modifyAclEntries(FSNamesystem.java:7734)
{noformat}

Here value 32 is hardcoded  , which can be made user configurable. 

{noformat}
private static List buildAndValidateAcl(ArrayList aclBuilder)
        throws AclException
    {
        if(aclBuilder.size() > 32)
            throw new AclException((new StringBuilder()).append(""Invalid ACL: ACL has "").append(aclBuilder.size()).append("" entries, which exceeds maximum of "").append(32).append(""."").toString());
:
:
}
{noformat}"
Handle SafeModeException in ReportBadBlockAction#reportTo,HDFS-9540,"BPServiceActor#processQueueMessages() tries to execute the ReportBadBlockAction#reportTo(..) and on any exception, it will add back to queue. 

For StandbyExceptoin, this caused HDFS-7916, that a request kept being added back to the queue while it should not. 

HDFS-7916 solution treats all exceptions wrapped by RemoteException the same, including StandbyException. That is, when RemoteException is caught, the request is not added back to the queue.

This solved the StandbyException issue reported in HDFS-7916, but the side effect is, it does not add the request back to the queue for SafeModeException wrapper by RemoteException, which appears to be incorrect.

Thanks [~vinayrpet] and [~kihwal] for the discussion in HDFS-7916.
"
Rename webhdfs HTTP param 'delegation' to 'delegationtoken',HDFS-2593,"to be consistent with other params names and to be more clear for users on what it is.

webhdfs spec doc should be updated as well."
HA: Optimize stale block processing by triggering block reports immediately on failover,HDFS-2851,"After Balancer runs, usedSpace is not balancing correctly.

{code}
java.util.concurrent.TimeoutException: Cluster failed to reached expected values of totalSpace (current: 1500, expected: 1500), or usedSpace (current: 390, expected: 300), in more than 20000 msec.
	at org.apache.hadoop.hdfs.server.balancer.TestBalancer.waitForHeartBeat(TestBalancer.java:233)
	at org.apache.hadoop.hdfs.server.balancer.TestBalancerWithHANameNodes.testBalancerWithHANameNodes(TestBalancerWithHANameNodes.java:99)
{code}"
Default block placement policy causes TestReplaceDataNodeOnFailure to fail intermittently,HDFS-9361,"TestReplaceDatanodeOnFailure sometimes fail (See HDFS-6101).
(For background information, the test case set up a cluster with three data nodes, add two more data nodes, remove one data nodes, and verify that clients can correctly recover from the failure and set up three replicas)

I traced down and found that some times a client only set up a pipeline with only two data nodes, which is one less than configured in the test case, even though the test case configures to always replace failed nodes.

Digging into the log, I saw:
{noformat}
2015-11-02 12:07:38,634 [IPC Server handler 8 on 50673] WARN  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(355)) - Failed to place enough replicas, still in nee
d of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException: [
Node /rack0/127.0.0.1:32931 [
  Datanode 127.0.0.1:32931 is not chosen since the rack has too many chosen nodes .
]
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:723)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:624)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:429)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:342)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:220)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:105)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:120)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1727)
        at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:299)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2457)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:796)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:500)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:637)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:976)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2305)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2301)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1669)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2299)
{noformat}

So from the log, it seems the policy causes the pipeline selection to give up on the data node.
I wonder whether this is appropriate or not. If the load factor exceeds certain threshold, but the file is insufficient of replicas, should it accept it as is, or should it attempt to acquire more replicas? 

I am filing this JIRA for discussion. I am very unfamiliar with block placement, so I may be wrong about my hypothesis.

(Edit: I turned on DEBUG option for Log4j, and changed the logging message a bit to make it show the stack trace)"
chmod impact user's effective ACL,HDFS-8419,"I set a directory's ACL to assign rwx permission to user h_user1. Later, I used chmod to change the group permission to r-x. I understand chmod of an acl enabled file would only change the permission mask. The abnormal thing is that the operation will change the h_user1's effective ACL from rwx to r-x.

Following are ACLs before any operaton:
-----------------------------------------
\# file: /grptest
\# owner: hdfs_tst_admin
\# group: supergroup
user::rwx
user:h_user1:rwx
group::r-x
mask::rwx
other::---
-----------------------------------------

Following are ACLs after ""chmod 750 /grptest""
-----------------------------------------
\# file: /grptest
\# owner: hdfs_tst_admin
\# group: supergroup
user::rwx
user:h_user1:rwx	#effective:r-x
group::r-x
mask::r-x
other::---
-----------------------------------------

I'm wondering if this behavior is by design.  If not, I'd like to fix the issue. Thank you."
hadoop fsck does not correctly check for corrupt blocks for a file,HDFS-8126,"hadoop fsck does not correctly check for corrupt blocks for a file until we try to read that file.

Test steps (Followed on Cloudera CDH5.1 single node VM and Hortonworks HDP2.2 single node VM ) : 
1. Uploaded a files ""test.txt"" to /user/abc/test.txt on HDFS
2. Ran ""hadoop fsck  /user/abc/test.txt -files -blocks "" command to check file integrity and retrieve block id.
3. Search for the block file location  at linux filesystem level.
4. Manually edit the block file.
5. Re-run the fsck command ""hadoop fsck /user/abc/test.txt"".
6. At this stage , FSCK still shows that files in HEALTHY state.
7. Waited for more than 30 sec to re-run FSCK test and still shows healthy state.
8. Try to read file ""hadoop fs -cat /user/abc/test.txt"" . Thsi command failes with an error of mis-match in checksum (as expected).
9. re-run FSCK. Now FSCK show that 1 block is corrupt.
10. Manually edit the file and restore to previous state.
11. Try to cat file. It works.
12. Run FSCK test. Still fails
"
Name node is using the write-ahead log improperly,HDFS-1137,"The Name node is doing the write-ahead log (WAL) (aka edit log) improperly. Usually when using WAL, changes are written to the log before they are applied to the state. Currently the Namenode does the WAL after applying the change. This means that read may see changes before they are durable. A client may read information and the server fail before the information is written to the WAL, which results in the client reading state that disappears. To fix the Namenode should write changes before (aka ahead of) applying the change."
"When block count for a volume exceeds dfs.blockreport.split.threshold, block report causes exception",HDFS-8574,"This piece of code in {{org.apache.hadoop.hdfs.server.datanode.BPServiceActor.blockReport()}}

{code}
// Send one block report per message.
        for (int r = 0; r < reports.length; r++) {
          StorageBlockReport singleReport[] = { reports[r] };
          DatanodeCommand cmd = bpNamenode.blockReport(
              bpRegistration, bpos.getBlockPoolId(), singleReport,
              new BlockReportContext(reports.length, r, reportId));
          numReportsSent++;
          numRPCs++;
          if (cmd != null) {
            cmds.add(cmd);
          }
{code}

when a single volume contains many blocks, i.e more than the threshold, it is trying to send the entire blockreport in one RPC, causing exception
{code}
java.lang.IllegalStateException: com.google.protobuf.InvalidProtocolBufferException: Protocol message was too large.  May be malicious.  Use CodedInputStream.setSizeLimit() to increase the size limit.
        at org.apache.hadoop.hdfs.protocol.BlockListAsLongs$BufferDecoder$1.next(BlockListAsLongs.java:369)
        at org.apache.hadoop.hdfs.protocol.BlockListAsLongs$BufferDecoder$1.next(BlockListAsLongs.java:347)
        at org.apache.hadoop.hdfs.protocol.BlockListAsLongs$BufferDecoder.getBlockListAsLongs(BlockListAsLongs.java:325)
        at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.blockReport(DatanodeProtocolClientSideTranslatorPB.java:190)
        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.blockReport(BPServiceActor.java:473)
{code}"
org.apache.hadoop.hdfs.TestDatanodeBlockScanner Fails Intermittently,HDFS-2881,org.apache.hadoop.hdfs.TestDatanodeBlockScanner fails intermittently durring test-patch.
rename dfs.namenode.replication.min to dfs.replication.min,HDFS-9199,dfs.namenode.replication.min should be dfs.replication.min to match the other dfs.replication config knobs.
Ec files can't be deleted into Trash because of that Trash isn't EC zone.,HDFS-8373,"When EC files were deleted, they would be moved into {{Trash}} directory. But, EC files can only be placed under EC zone. So, EC files which have been deleted can not be moved to {{Trash}} directory.

Problem could be solved by creating a EC zone(floder) inside {{Trash}} to contain deleted EC files.
"
Simplify embedding libhdfspp into other projects,HDFS-9186,"I'd like to add a script to the root libhdfspp directory that can prune anything that libhdfspp doesn't need to compile out of the hadoop source tree.  

This way the project is a lot smaller if it's going to be included in a third-party directory of another project.  The directory structure, aside from missing directories, is preserved so modifications can be diffed against a fresh checkout of the source."
query method for what kind of safe mode the Namenode is in,HDFS-1726,"If we could differentiate between ""startup safemode"" vs other safemode, it would be easier to do startup optimizations like HDFS-1295.  Looking at FSNamesystem, this can be queried, but not with a single query, and the semantics are not reliable under future changes.  Also, the FSNamesystem code itself, internally, uses more than one way to test for manual safe mode.

Proposal is to create a status field and query method in FSNamesystem with enum values
    {NOT_IN_SAFEMODE, SAFEMODE_STARTUP, SAFEMODE_EXTENSION, SAFEMODE_MANUAL}
If in the future we add automatic fallback to safe mode, we would add value SAFEMODE_AUTOMATIC.

This change will make it easier to do startup optimizations, and will also allow making the safemode management code in FSNamesystem simpler and more consistent.
"
updatePipeline RPC call should only update the GS of the block,HDFS-9050,"The only usage of the call is in {{DataStreamer#updatePipeline}}, where {{newBlock}} differs from current {{block}} only in GS.

Basically the RPC call is not supposed to update the {{poolID}}, {{ID}}, or {{numBytes}} of the block on NN."
About Replication,HDFS-9102,"Hi I want to know that how many replications i can place in data centers
eg: we cant place more than 1 replica per node  and  more than 2 replicas pr rack. Like that how many replicas we can place in a data center.
"
Erasure Coding: cache ErasureCodingZone,HDFS-8594,"scenario 1:
We have 100m files in a EC zone. Everytime we open a file, we need to get ECSchema(need get ECZone first). So getting EC zone is frequent.
scenario 2:
We have a EC zone ""/d1"". We have a file in ""/d1/d2/d3/.../dN"". We have to search from xAttrs from dN, dN-1, ..., d3, d2, d1, until we find a EC zone from d1's xAttr.

It's better we cache the EC zone, like EncryptionZoneManager#encryptionZones"
Not able to build with 'mvn compile -Pnative',HDFS-8886,"I am running into a problem where i am not able to compile the native parts of hadoop-hdfs project. the problem is that it is not finding MakeFile in 
${project.build.dir}/native."
Allow bypassing some minor exceptions while loading editlog,HDFS-8877,"Usually when users hit editlog corruption like HDFS-7587, before upgrading to a new version with the bug fix, a customized jar has to be provided by developers first to bypass the exception while loading edits. The whole process is usually painful.

If we can confirm the corruption/exception is a known issue and can be ignored after upgrading to the newer version, it may be helpful to have the capability for users/developers to specify certain types/numbers of exceptions that can be bypassed while loading edits."
isGoodBlockCandidate() in Balancer is not handling properly if replica factor >3,HDFS-3619,"Let's assume:
1. replica factor = 4
2. source node in rack 1 has 1st replica, 2nd and 3rd replica are in rack 2, 4th replica in rack3 and target node is in rack3. 
So, It should be good for balancer to move replica from source node to target node but will return ""false"" in isGoodBlockCandidate(). I think we can fix it by simply making judgement that at least one replica node (other than source) is on the different rack of target node."
Balancer fails to balance blocks between aboveAvgUtilized and belowAvgUtilized datanodes.,HDFS-3411,"Scaenario:
replication set to 1.
1. Start 1NN and IDN
2. pump 1GB of data.
3. Start one more DN
4. Run balancer with threshold 1.

Now DN1 is added into aboveAvgUtilizedDatanodes and DN2 into belowAvgUtilizedDatanodes. Hence overLoadedBytes and underLoadedBytes will be equal to 0. Resulting in bytesLeftToMove equal to 0. Thus balancer will exit without balancing the blocks."
"DateFormat.getDateTimeInstance() is very expensive, we can cache it to improve performance",HDFS-1676,"In the file:

./hadoop-0.21.0/hdfs/src/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java  line:1520

In the while loop, DateFormat.getDateTimeInstance()is called in each iteration. We can cache the result by moving it outside the loop or adding a class member.

This is similar to the Apache bug https://issues.apache.org/bugzilla/show_bug.cgi?id=48778 
"
Append failed due to unreleased lease from previous appender with quota exceeded exception,HDFS-8531,"Append operation fails if
1. Set SpaceQuota to 3G for /user/hrt_qa/heterogenous
2. Copy 1GB file to /user/hrt_qa/heterogenous
3. Run 'hdfs dfs -appendToFile' to append 1GB file to the already copied 1GB file in /user/hrt_qa/heterogenous
4. Append fails with Quota exceed message
5. Increase the Quota to 5G for /user/hrt_qa/heterogenous
6. Run the Append message again and the following error message comes.
{code}
appendToFile: Failed to APPEND_FILE /user/hrt_qa/quotaPerHerterogenousStorage/1GBFile_1/part-m-00000 for DFSClient_NONMAPREDUCE_1431576997_1 on 172.31.37.190 because this file lease is currently owned by DFSClient_NONMAPREDUCE_-231994503_1 on 172.31.37.190
{code}
7. Wait for a while (lease soft limit -- 1 min) and retry append it succeeds after the previous lease expire. 

This seems to relate to HDFS-7587
""
When a client was trying to append to the file, the remaining space quota was very small. This caused a failure in prepareFileForWrite(), but after the inode was already converted for writing and a lease added. Since these were not undone when the quota violation was detected, the file was left in under-construction with an active lease without edit logging OP_ADD.
A subsequent append() eventually caused a lease recovery after the soft limit period. This resulted in commitBlockSynchronization(), which closed the file with OP_CLOSE being logged.""

The fix in HDFS-7587 addressed the edit log corruption issue but does not handle the problem within soft limit period where other clients can't append.
"
"fs.copyoLocalFile(src, dst);will have NPE and can not run successful in windows",HDFS-8503,"======Test Code=======
//copy file from HDFS to local
fs.copyToLocalFile(dst, new Path(copiedlocalfile));
======ERROR Exception======
java.lang.NullPointerException
	at java.lang.ProcessBuilder.start(Unknown Source)
	at org.apache.hadoop.util.Shell.runCom"
[umbrella] Data striping support in HDFS client,HDFS-7545,Data striping is a commonly used data layout with critical benefits in the context of erasure coding. This JIRA aims to extend HDFS client to work with striped blocks.
"Audit logging should be added for Rolling Upgrade ( Rollback , Downgrade )",HDFS-7882,"Currently we have audit logs for Rolling upgrade: start and finalize .
{noformat}
2015-03-04 17:18:16,119 | INFO  | IPC Server handler 5 on 9000 | allowed=true   ugi=Rex (auth:SIMPLE)   ip=/XXXXXXXXXXXXXX      cmd=startRollingUpgrade src=null        dst=null        perm=null       proto=rpc | org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8104)
2015-03-04 17:24:04,615 | INFO  | IPC Server handler 7 on 9000 | allowed=true   ugi=Rex (auth:SIMPLE)   ip=/XXXXXXXXXXXXXX      cmd=finalizeRollingUpgrade      src=null        dst=null        perm=null      proto=rpc | org.apache.hadoop.hdfs.server.namenode.FSNamesystem$DefaultAuditLogger.logAuditMessage(FSNamesystem.java:8104)
{noformat}
For rollback and downgrade also audit logs should be added. 
"
Mover is success even when space exceeds storage quota.,HDFS-8465,"
*Steps :*
1. Create directory /dir 
2. Set its storage policy to HOT --
hdfs storagepolicies -setStoragePolicy -path /dir -policy HOT

3. Insert files of total size 10,000B  into /dir.
4. Set above path ""/dir"" ARCHIVE type quota to 5,000B --
hdfs dfsadmin -setSpaceQuota 5000 -storageType ARCHIVE /dir
{code}
hdfs dfs -count -v -q -h -t  /dir
   DISK_QUOTA    REM_DISK_QUOTA     SSD_QUOTA     REM_SSD_QUOTA ARCHIVE_QUOTA REM_ARCHIVE_QUOTA PATHNAME
         none               inf          none               inf         4.9 K             4.9 K /dir
{code}
5. Now change policy of '/dir' to COLD
6. Execute Mover command

*Observations:*
1. Mover is successful moving all 10,000B to ARCHIVE datapath.

2. Count command displays negative value '-59.4K'--
{code}
hdfs dfs -count -v -q -h -t  /dir
   DISK_QUOTA    REM_DISK_QUOTA     SSD_QUOTA     REM_SSD_QUOTA ARCHIVE_QUOTA REM_ARCHIVE_QUOTA PATHNAME
         none               inf          none               inf         4.9 K           -59.4 K /dir
{code}
*Expected:*
Mover should not be successful as ARCHIVE quota is only 5,000B.
Negative value should not be displayed for quota output."
[HDFS-Quota]Quota is getting updated after storage policy is modified even before mover command is executed.,HDFS-8467,"a. create a directory 
{code}
./hdfs dfs -mkdir /d1
{code}
b. Set storage policy HOT on /d1
{code}
./hdfs storagepolicies -setStoragePolicy -path /d1 -policy HOT
{code}

c. Set space quota to disk on /d1
{code}
  ./hdfs dfsadmin -setSpaceQuota 10000 -storageType DISK /d1
{code}

{code} 
./hdfs dfs -count -v -q -h -t  /d1
   DISK_QUOTA    REM_DISK_QUOTA     SSD_QUOTA     REM_SSD_QUOTA ARCHIVE_QUOTA REM_ARCHIVE_QUOTA PATHNAME
        9.8 K             9.8 K          none               inf          none               inf /d1
{code}

d. Insert 2 file each of 1000B
{code}
./hdfs dfs -count -v -q -h -t  /d1
   DISK_QUOTA    REM_DISK_QUOTA     SSD_QUOTA     REM_SSD_QUOTA ARCHIVE_QUOTA REM_ARCHIVE_QUOTA PATHNAME
        9.8 K             3.9 K          none               inf          none               inf /d1
{code}

e. Set ARCHIVE quota on /d1
{code}
./hdfs dfsadmin -setSpaceQuota 10000 -storageType ARCHIVE /d1
./hdfs dfs -count -v -q -h -t  /d1
   DISK_QUOTA    REM_DISK_QUOTA     SSD_QUOTA     REM_SSD_QUOTA ARCHIVE_QUOTA REM_ARCHIVE_QUOTA PATHNAME
        9.8 K             3.9 K          none               inf         9.8 K             9.8 K /d1
{code}

f. Change storagepilicy to COLD
{code}
./hdfs storagepolicies -setStoragePolicy -path /d1 -policy COLD
{code}

g. Check REM_ARCHIVE_QUOTA Value
{code}
./hdfs dfs -count -v -q -h -t  /d1
   DISK_QUOTA    REM_DISK_QUOTA     SSD_QUOTA     REM_SSD_QUOTA ARCHIVE_QUOTA REM_ARCHIVE_QUOTA PATHNAME
        9.8 K             9.8 K          none               inf         9.8 K             3.9 K /d1
{code}

Here even when 'Mover' command is not run, quota of REM_ARCHIVE_QUOTA is reduced and REM_DISK_QUOTA is increased.

Expected : After Mover is success quota values has to be changed."
createErasureCodingZone should check whether cellSize is available,HDFS-8473,"createErasureCodingZone should check whether cellSize is available, otherwise, when user create file under this EC zone may throw HadoopIllegalArgumentException."
Delay in scanning blocks at DN side when there are huge number of blocks,HDFS-3512,"Block scanner maintains the full list of blocks at DN side in a map and there is no differentiation between the blocks which are already scanned and the ones not scanend. For every check (ie every 5 secs) it will pick one block and scan. There are chances that it chooses a block which is already scanned which leads to further delay in scanning of blcoks which are yet to be scanned.

"
When a HDFS client fails to read a block (due to server failure) the namenode should log this,HDFS-348,Right now only client debugging info is available.  The fact that the client node needed to execute a failure mitigation strategy should be logged centrally so we can do analysis.
Able to move encryption zone to Trash,HDFS-8040,"Users can remove encryption directory using the FsShell remove commands without -skipTrash option.
{code}
/usr/hdp/current/hadoop-hdfs-client/bin/hdfs dfs -D ""fs.trash.interval=60"" -rm -r /user/hrt_qa/encryptionZone_1
2015-04-01 19:19:46,510|beaver.machine|INFO|654|140309507495680|MainThread|15/04/01 19:19:46 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 360 minutes, Emptier interval = 0 minutes.
2015-04-01 19:19:46,534|beaver.machine|INFO|654|140309507495680|MainThread|Moved: 'hdfs://sumana-dal-secure-4.novalocal:8020/user/hrt_qa/encryptionZone_1' to trash at: hdfs://sumana-dal-secure-4.novalocal:8020/user/hrt_qa/.Trash/Current
2015-04-01 19:19:46,863|test_TDE_trash|INFO|654|140309507495680|MainThread|Checking if the encryption zone is in Trash or not
2015-04-01 19:19:46,864|beaver.machine|INFO|654|140309507495680|MainThread|RUNNING: /usr/hdp/current/hadoop-client/bin/hadoop dfs -ls -R /user/hrt_qa/.Trash/Current
2015-04-01 19:19:46,892|beaver.machine|INFO|654|140309507495680|MainThread|DEPRECATED: Use of this script to execute hdfs command is deprecated.
2015-04-01 19:19:46,893|beaver.machine|INFO|654|140309507495680|MainThread|Instead use the hdfs command for it.
2015-04-01 19:19:46,893|beaver.machine|INFO|654|140309507495680|MainThread|
2015-04-01 19:19:50,289|beaver.machine|INFO|654|140309507495680|MainThread|drwx------   - hrt_qa hrt_qa          0 2015-04-01 19:19 /user/hrt_qa/.Trash/Current/user
2015-04-01 19:19:50,292|beaver.machine|INFO|654|140309507495680|MainThread|drwx------   - hrt_qa hrt_qa          0 2015-04-01 19:19 /user/hrt_qa/.Trash/Current/user/hrt_qa
2015-04-01 19:19:50,296|beaver.machine|INFO|654|140309507495680|MainThread|drwxr-xr-x   - hrt_qa hrt_qa          0 2015-04-01 19:19  /user/hrt_qa/.Trash/Current/user/hrt_qa/encryptionZone_1 
2015-04-01 19:19:50,326|beaver.machine|INFO|654|140309507495680|MainThread|-rw-r--r--   3 hrt_qa hrt_qa       3273 2015-04-01 19:19 /user/hrt_qa/.Trash/Current/user/hrt_qa/encryptionZone_1/file_to_get.txt
{code}"
directory size is shown as 0B when browsing directory,HDFS-7504,"No matter using hadoop dfs -ls or using web brower to look at the directory information, the size is always shown as 0B. The correct value should be the sum of its files and sub-directories."
how to set building information,HDFS-7432,"after i build hadoop from sorce coude, when i start namenode, i saw ""STARTUP_MSG: Unknown -r Unknown"" in its log file. this is url and revision information.
how could i set it when building"
HDFS Space Quota not working as expected,HDFS-7658,"I am implementing hdfs quota in a cdh4.6 cluster .Hdfs name quota has been working properly.But the Hdfs Space quota has not been working as expected.i.e,

I set the space quota of 500MB for a directory say /test-space-quota.

Then i put a file of 10 Mb into /test-space-quota which worked .Now the space available is 480 MB ( 500 - 10*2) where 2 is rep factor.

Then i put a file of 50Mb into /test-space-quota which worked too as expected. Now the space available is 380 MB (480 - 50*2)

""I am checking the quota left from the command hadoop fs -count -q /test-space-quota""

Then i tried to put a file of 100 Mb . It should since it will just consume 200 Mb of space with replication. But when i put that i got an error 

""DataStreamer Exception
org.apache.hadoop.hdfs.protocol.DSQuotaExceededException: The DiskSpace quota of /test is exceeded: quota = 524288000 B = 500 MB but diskspace consumed = 662700032 B = 632 MB""

But the quota says

hadoop fs -count -q /test-space-quota
        none             inf       524288000       398458880            1            2           62914560 /test-space-quota

Could you please help on this?"
Safemode admin command should log the audit logs.,HDFS-7972,"hdfs dfsadmin -safemode enter
hdfs dfsadmin -safemode leave"
XmlEditsVisitor should not use classes in com.sun.org.apache.xml.internal.serialize,HDFS-3668,"These classes are not public library.  It also generates a lot of ""Access restriction"" javac warnings."
Encryption Key created in Java Key Store after Namenode start unavailable for EZ Creation ,HDFS-7256,"Hit an error on ""RemoteException: Key ezkey1 doesn't exist."" when creating EZ with a Key created after NN starts.

Briefly check the code and found that the KeyProivder is loaded by FSN only at the NN start. My work around is to restart the NN which triggers the reload of Key Provider. Is this expected?

Repro Steps:

Create a new Key after NN and KMS starts
hadoop/bin/hadoop key create ezkey1 -size 256 -provider jceks://file/home/hadoop/kms.keystore

List Keys
hadoop@SaturnVm:~/deploy$ hadoop/bin/hadoop key list -provider jceks://file/home/hadoop/kms.keystore -metadata
Listing keys for KeyProvider: jceks://file/home/hadoop/kms.keystore
ezkey1 : cipher: AES/CTR/NoPadding, length: 256, description: null, created: Thu Oct 16 18:51:30 EDT 2014, version: 1, attributes: null
key2 : cipher: AES/CTR/NoPadding, length: 128, description: null, created: Tue Oct 14 19:44:09 EDT 2014, version: 1, attributes: null
key1 : cipher: AES/CTR/NoPadding, length: 128, description: null, created: Tue Oct 14 17:52:36 EDT 2014, version: 1, attributes: null

Create Encryption Zone
hadoop/bin/hdfs dfs -mkdir /Ez1
hadoop@SaturnVm:~/deploy$ hadoop/bin/hdfs crypto -createZone -keyName ezkey1 -path /Ez1
RemoteException: Key ezkey1 doesn't exist.
"
DFS rollback broken,HDFS-3028,"I ran an upgrade to test a patch which updates the data format, but when I then tried to rollback, the DNs don't seem to respond properly."
Semi-harmless race between block reports and block invalidation,HDFS-2282,"In the 0.20.203 codebase, block reports are not synchronized in any way against mutations to the actual file structure on disk. If a file is removed from a directory while the block report is scanning that directory, it will be mistakenly reported as existing with a length of 0, since File.length() on a non-existent file returns 0.

This results in an error being logged on the DataNode when the NN sends it a second block deletion request for the already-deleted block. I believe it to be harmless, but the error message can concern users.

This was fixed in the 0.20 code line in HDFS-2379.  This jira remains open to track the port to 0.24."
hadoop will terminate Web service process when a hadoop mapreduce task is finished.,HDFS-1488,"1. In the myeclipse 8.5 enviroment, I create a new Map/Reduce project named wordcount project!
2. create class including ""public void main(string[] args)"" named Wordcount
3.copy the hadoop wordcount exampler code from the hadoop folder to ""wordcount project "".
4. in the main() method, I add a jetty server and start it .the codes is showed as follows!
5.when I build and run it, i find jetty server will be terminate after hadoop task finishs.
6.I check hadoop jobtracker logs showing as follows.
======================================================
 logs
2010-11-05 16:47:41,968 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9001: readAndProcess threw exception java.io.IOException: connection was forcibly closed .Count of bytes read: 0
java.io.IOException: connection was forcibly closed
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:25)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:233)
	at sun.nio.ch.IOUtil.read(IOUtil.java:206)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:236)
	at org.apache.hadoop.ipc.Server.channelRead(Server.java:1214)
	at org.apache.hadoop.ipc.Server.access$16(Server.java:1210)
	at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:801)
	at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:419)
	at org.apache.hadoop.ipc.Server$Listener.run(Server.java:328)
==============================================
codes:
	public static void main(String[] args) throws Exception {

		Handler handler = new AbstractHandler() {

			@Override
			public void handle(String target, HttpServletRequest request,
					HttpServletResponse response, int dispatch)
					throws IOException, ServletException {
				// TODO Auto-generated method stub
				response.setContentType(""text/html"");
				response.setStatus(HttpServletResponse.SC_OK);
				response.getWriter().println(""<h1>------start-------</h1>"");
				// ---------------------------------------
				// ---------------------------------------
				response.getWriter().println(""<h1>------end1-------</h1>"");
				((Request) request).setHandled(true);
				// request.getRequestDispatcher(""/WebRoot/result.jsp"").forward(request,
				// response);
			}
		};

		// 寮€鍚疛etty鏈嶅姟
		Server server = new Server(8086);
		server.setHandler(handler);
		server.start();
		// server.join();
		
		

		SimpleDateFormat tempDate = new SimpleDateFormat(""yyyy_MM_dd""
				+ ""_hh_mm_ss"");
		String datetime = tempDate.format(new java.util.Date());

		String out4 = ""out"" + datetime;
		args = new String[] { ""in"", out4 };
		Configuration conf = new Configuration();
		String[] otherArgs = new GenericOptionsParser(conf, args)
				.getRemainingArgs();
		if (otherArgs.length != 2) {
			System.err.println(""Usage: wordcount <in> <out>"");
			System.exit(2);
		}
		Job job = new Job(conf, ""word count"");
		job.setJarByClass(WordCount.class);
		job.setMapperClass(TokenizerMapper.class);
		job.setCombinerClass(IntSumReducer.class);
		job.setReducerClass(IntSumReducer.class);
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
		FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));
		System.exit(job.waitForCompletion(true) ? 0 : 1);
	}"
Quota checks fail for small files and quotas,HDFS-1026,"If a directory has a quota less than blockSize * numReplicas then you can't add a file to it, even if the file size is less than the quota. This is because FSDirectory#addBlock updates the count assuming at least one block is written in full. We don't know how much of the block will be written when addBlock is called and supporting such small quotas is not important so perhaps we should document this and log an error message instead of making small (blockSize * numReplicas) quotas work.

{code}
// check quota limits and updated space consumed
updateCount(inodes, inodes.length-1, 0, fileINode.getPreferredBlockSize()*fileINode.getReplication(), true);
{code}

You can reproduce with the following commands:
{code}
$ dd if=/dev/zero of=temp bs=1000 count=64
$ hadoop fs -mkdir /user/eli/dir
$ hdfs dfsadmin -setSpaceQuota 191M /user/eli/dir
$ hadoop fs -put temp /user/eli/dir  # Causes DSQuotaExceededException
{code}"
HADOOP_*_HOME environment variables no longer work for tar ball distributions,HDFS-2045,"It used to be that you could do the following:

# Run `ant bin-package' in your hadoop-common checkout.
# Set HADOOP_COMMON_HOME to the built directory of hadoop-common.
# Run `ant bin-package' in your hadoop-hdfs checkout.
# Set HADOOP_HDFS_HOME to the built directory of hadoop-hdfs.
# Set PATH to have HADOOP_HDFS_HOME/bin and HADOOP_COMMON_HOME/bin on it.
# Run `hdfs'.
\\
\\
As of HDFS-1963, this no longer works since hdfs-config.sh is looking in HADOOP_COMMON_HOME/bin/ for hadoop-config.sh, but it's being placed in HADOOP_COMMON_HOME/libexec."
bin/hdfs should print a message when an invalid command is specified,HDFS-2930,"hdfs will currently give a NoClassDefFoundError stacktrace if there's a typo specifying the command.

{noformat}
hadoop-0.24.0-SNAPSHOT $ ./bin/hdfs dfadmin
Exception in thread ""main"" java.lang.NoClassDefFoundError: dfadmin
Caused by: java.lang.ClassNotFoundException: dfadmin
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:307)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
{noformat}"
DataXceiver could leak FileDescriptor,HDFS-7570,"DataXceiver doesn't close inputstream all the time, There could be FD leakage and overtime cause FDs exceed limit.

{code}
finally {
      if (LOG.isDebugEnabled()) {
        LOG.debug(datanode.getDisplayName() + "":Number of active connections is: ""
            + datanode.getXceiverCount());
      }
      updateCurrentThreadName(""Cleaning up"");
      if (peer != null) {
        dataXceiverServer.closePeer(peer);
        IOUtils.closeStream(in);
      }
    }
{code}"
Implement an Apache Commons VFS Driver for HDFS,HDFS-1213,"We have an open source ETL tool (Kettle) which uses VFS for many input/output steps/jobs.  We would like to be able to read/write HDFS from Kettle using VFS.  
 
I haven't been able to find anything out there other than ""it would be nice.""
 
I had some time a few weeks ago to begin writing a VFS driver for HDFS and we (Pentaho) would like to be able to contribute this driver.  I believe it supports all the major file/folder operations and I have written unit tests for all of these operations.  The code is currently checked into an open Pentaho SVN repository under the Apache 2.0 license.  There are some current limitations, such as a lack of authentication (kerberos), which appears to be coming in 0.22.0, however, the driver supports username/password, but I just can't use them yet.

I will be attaching the code for the driver once the case is created.  The project does not modify existing hadoop/hdfs source.

Our JIRA case can be found at http://jira.pentaho.com/browse/PDI-4146"
how to enable namenode GC logging,HDFS-7534,"Hey Team,

Could you please let me know how to enable GC logging for hadoop namenode. ?

I tried below method :

modified HADOOP_NAMENODE_OPTS in /etc/hadoop/conf/hadoop-env.sh as below and restarted namenode (HA is configured on cdh 4.7.0 )

*before modification :*

{code}
export HADOOP_NAMENODE_OPTS=""-Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,RFAS} -Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender} $HADOOP_NAMENODE_OPTS"";
{code}

*After modification:*

{code}
export HADOOP_NAMENODE_OPTS=""-Xloggc:%HADOOP_LOG_DIR%/gc-namenode.log -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,RFAS} -Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender} $HADOOP_NAMENODE_OPTS"";
{code}

this method is not working :( 

could you please help me on this.


Thanks,
Kuldeep
"
Replace ACLException with AccessControlException,HDFS-7477,"Currently many functions logs audit log during failures only when {{AccessControlException}} is thrown, thus no audit logs are logged if {{AclException}} is thrown when the ACLs deny the access."
Fix TestRollingUpgrade,HDFS-5960,"{{TestRollingUpgrade}} fails when restarting the NN because {{OP_ROLLING_UPGRADE_START}} and {{OP_ROLLING_UPGRADE_FINALIZE}} are not expected.

The fix is to start/finalize rolling upgrade when the corresponding edit log op is seen."
Namenode HA failover happens very frequently from active to standby,HDFS-7451,"We have two namenode having HA enabled. From last couple of days we are observing that the failover happens very frequently from active to standby mode. Below is the log details of the active namenode during failover happens. Is there any fix to get rid of this issue?

Namenode logs:

{code}
2014-11-25 22:24:02,020 WARN org.apache.hadoop.ipc.Server: IPC Server Responder, call org.apache.hadoop.hdfs.protocol.Clie
ntProtocol.getListing from 10.2.16.214:40751: output error
2014-11-25 22:24:02,020 INFO org.apache.hadoop.ipc.Server: IPC Server handler 23 on 8020 caught an exception
java.nio.channels.ClosedChannelException
        at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:265)
        at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:474)
        at org.apache.hadoop.ipc.Server.channelWrite(Server.java:2195)
        at org.apache.hadoop.ipc.Server.access$2000(Server.java:110)
        at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:979)
        at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1045)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1798)







2014-11-25 22:24:10,631 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /sda/dfs/namenode/current/edits_inprogress_0000000001643676954 -> /sda/dfs/namenode/current/edits_0000000001643676954-0000000001643677390
2014-11-25 22:24:10,631 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Closing
java.lang.Exception
        at org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel.close(IPCLoggerChannel.java:182)
        at org.apache.hadoop.hdfs.qjournal.client.AsyncLoggerSet.close(AsyncLoggerSet.java:102)
        at org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.close(QuorumJournalManager.java:446)
        at org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalAndStream.close(JournalSet.java:107)
        at org.apache.hadoop.hdfs.server.namenode.JournalSet$4.apply(JournalSet.java:222)
        at org.apache.hadoop.hdfs.server.namenode.JournalSet.mapJournalsAndReportErrors(JournalSet.java:347)
        at org.apache.hadoop.hdfs.server.namenode.JournalSet.close(JournalSet.java:219)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.close(FSEditLog.java:308)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.stopActiveServices(FSNamesystem.java:939)
        at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.stopActiveServices(NameNode.java:1365)
        at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.exitState(ActiveState.java:70)
        at org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:61)
        at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.setState(ActiveState.java:52)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToStandby(NameNode.java:1278)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToStandby(NameNodeRpcServer.java:1046)
        at org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToStandby(HAServiceProtocolServerSideTranslatorPB.java:119)
        at org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceProtocolService$2.callBlockingMethod(HAServiceProtocolProtos.java:3635)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:453)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1002)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1752)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1748)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)


2014-11-25 22:24:10,632 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for standby state
2014-11-25 22:24:10,633 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Will roll logs on active node at dc1-had03-m002.dc01.revsci.net/10.2.16.92:8020 every 120 seconds.
2014-11-25 22:24:10,634 INFO org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer: Starting standby checkpoint thread...
Checkpointing active NN at dc1-had03-m002.dc01.revsci.net:50070
Serving checkpoints at dc1-had03-m001.dc01.revsci.net/10.2.16.91:50070
{code}

zkfc logs:
{code}
2014-11-25 22:24:12,192 INFO org.apache.zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x449b8
ce9a110255, likely server has closed socket, closing socket connection and attempting reconnect
2014-11-25 22:24:12,293 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session disconnected. Entering neutral mode...
2014-11-25 22:24:12,950 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server dc1-had03-zook06.dc01.re
vsci.net/10.2.16.205:2181. Will not attempt to authenticate using SASL (unknown error)
2014-11-25 22:24:12,951 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to dc1-had03-zook06.dc01.revsc
i.net/10.2.16.205:2181, initiating session
2014-11-25 22:24:12,952 INFO org.apache.zookeeper.ClientCnxn: Unable to reconnect to ZooKeeper service, session 0x449b8ce9
a110255 has expired, closing socket connection
2014-11-25 22:24:12,952 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session expired. Entering neutral mode and rejoini
ng...
2014-11-25 22:24:12,952 INFO org.apache.hadoop.ha.ActiveStandbyElector: Trying to re-establish ZK session
2014-11-25 22:24:12,952 INFO org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=dc1-had03-zook01.
dc01.revsci.net:2181,dc1-had03-zook02.dc01.revsci.net:2181,dc1-had03-zook03.dc01.revsci.net:2181,dc1-had03-zook04.dc01.rev
sci.net:2181,dc1-had03-zook05.dc01.revsci.net:2181,dc1-had03-zook06.dc01.revsci.net:2181 sessionTimeout=5000 watcher=org.a
pache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef@7f042529
2014-11-25 22:24:12,954 INFO org.apache.zookeeper.ClientCnxn: Opening socket connection to server dc1-had03-zook01.dc01.revsci.net/10.2.16.200:2181. Will not attempt to authenticate using SASL (unknown error)
2014-11-25 22:24:12,954 INFO org.apache.zookeeper.ClientCnxn: Socket connection established to dc1-had03-zook01.dc01.revsci.net/10.2.16.200:2181, initiating session
2014-11-25 22:24:13,172 INFO org.apache.zookeeper.ClientCnxn: Session establishment complete on server dc1-had03-zook01.dc01.revsci.net/10.2.16.200:2181, sessionid = 0x149a7f9b6d60263, negotiated timeout = 5000
2014-11-25 22:24:13,173 INFO org.apache.hadoop.ha.ActiveStandbyElector: Session connected.
2014-11-25 22:24:13,173 WARN org.apache.hadoop.ha.ActiveStandbyElector: Ignoring stale result from old client with sessionId 0x449b8ce9a110255
2014-11-25 22:24:13,173 INFO org.apache.zookeeper.ClientCnxn: EventThread shut down
2014-11-25 22:24:13,389 INFO org.apache.hadoop.ha.ZKFailoverController: ZK Election indicated that NameNode at dc1-had03-m001.dc01.revsci.net/10.2.16.91:8020 should become standby
2014-11-25 22:24:13,391 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at dc1-had03-m001.dc01.revsci.net/10.2.16.91:8020 to standby state
{code}

-laxman"
Unable to create encryption zone for viewfs:// after namenode federation is enabled,HDFS-7241,"After configuring namenode federation for the cluster, I also enabled client mount table and viewfs as default URI. The hdfs crypto commands now failed with below error:
# hdfs crypto -createZone -keyName key1 -path /user/test
IllegalArgumentException: FileSystem viewfs://cluster18/ is not an HDFS file system
This blocks the whole encryption at-rest feature as no zone could be defined"
Browsing a block on the wrong datanode through web UI has unintuitive results,HDFS-1579,"If you navigate through the web UI to view a file, and then the replica you're looking at is removed, the ""Got error in response to OP_READ_BLOCK"" message is printed in the content of the textarea that usually displays the text. This is confusing to users.

Instead we should display the error message in a different format and direct the user to choose a different replica from the list of links at the bottom of the page. Or, alternatively, do an HTTP redirect to a different replica. We should also take care of the case where the generation stamp has a mismatch."
Web UI: Browse file system from data node will fail with secured HDFS ,HDFS-6049,"On a secured HDFS, the webUI for the Active Namenode will display, but when you select a Live Datanode, the browser displays an error and does not display datanode information. It does list a link to ""Logs"" but when you click on that you get an unauthorized error message.

When user click on the ""live nodes"" link on the namenode webui, it goes to ""dfsnodelist.jsp"" to show the list of datanodes;
on top half of the ""dfsnodelist.jsp"" page, you can choose to ""browse the filesystem"", which direct user to the link ""http://*:50070/nn_browsedfscontent.jsp"", and this ""nn_browsedfscontent.jsp"" will redirect user to a random datanode ""http://:1006/browseDirectory.jsp?namenodeInfoPort=50070&dir=/&delegation=****&nnaddr=192.168.2.201:8020""; note here the ""delegation"" is added by namenode to the above link; so in this link, user can browse the filesystem;
On the bottom half of the ""dfsnodelist.jsp"" page, a list of nodes are shown; when you click on one data node, the page goes to ""http://**:1006/browseDirectory.jsp?namenodeInfoPort=50070&dir=%2F&nnaddr=192.168.2.201:8020"", without the ""delegation"" part, so in this link, user will not be able to browse the content of the filesystem, and an error message is shown."
HDFS-1109 broke filesystem browsing,HDFS-1530,"Since HDFS-1109 changed the way filename is passed in HFTP the web browsing of the filesystem doesn't work.
Need to fix the way the links are generated for the web UI."
XSS and or content injection in hdfs,HDFS-6845,"Following up from email

""...
I was auditing the latest stable version of hdfs - 2.4.1 (as made
available from
http://mirror.nexcess.net/apache/hadoop/common/hadoop-2.4.1/hadoop-2.4.1-src.tar.gz
), I noticed an interesting XSS filter.  Ok, sure.  But what intrigued me
was where I didn't find any attempt to validate or sanitize.

Within DatanodeJSPHelper.java - line 108, nnAddr is assigned the value
from the raw parameter NAMENODE_ADDRESS.     On line 120, printgotoform is
called with the raw value.  Then then called JspHelper.java's
printGotoForm method - Line 452.  Then on line 468, the unvalidated or
sanitized value is printed to the html page.  Worst case, reflected XSS. 
Better case, content injection.

Similarily,  DatanodeJSPHelper.java's line 102 tokenString variable looks
plausible but I am not certain if an incorrect token will cause the
business logic to fail before the malicious input it displayed
(JspHelper.java - line 465.)
...""

These are not the only XSS / Content injection points but should give an easy idea to find the others.
"
Add 'Time Since Delared Dead' to namenode dead datanodes web page,HDFS-1764,"I am filing this jira for Andrew. :)

Currently on the dead nodes page of a namenode, we only list the dead datanode's hostnames. In addition I would like to list the duration since the node was declared as dead, for example in the same format as the ""Decommissioning Nodes"" page when it lists ""Time Since Decommissioning Started"".

In our Hadoop clusters if a node has only been dead for a few minutes, our monitoring is likely to bring the node back without us needing to do anything about it. This proposed functionality will help administrators identify which nodes need manual attention and which nodes are likely to be fixed by our monitoring. If the node has been dead for many hours, it merits a closer look.

This seems like useful functionality for open source as well."
HDFS NN and DN JSP pages do not check for script injection.,HDFS-6684,"Datanode's browseDirectory.jsp is not filtering script injection, able to inject a script with dir parameter using dir=/hadoop'\""/><script>alert(759)</script>.

NameNode's dfsnodelist.sjp is not filtering script injection either. Able to set the sorter/order parameter to ""DSC%20onMouseOver=alert(959)//""."
Add a message on the old web UI that indicates the old UI is deprecated,HDFS-6321,HDFS-6252 has removed the jsp ui from trunk. We should add a message in the old web ui to indicate that the ui has been deprecated and ask the user to move towards the new web ui. 
hdfsConnect segment fault where namenode not connected,HDFS-6254,"When namenode is not started, the libhdfs client will cause segment fault while connecting."
"Using the command setrep to set the replication factor more than the number of datanodes with the -w parameter, the command gets in a infinite loop.",HDFS-6223,"Using the command setrep to set the replication factor more than the number of datanodes with the -w parameter gets in a infinite loop. When the -w parameter is there, the command gets in the following code to wait for the replication factor to be met before exiting. But if the number of datanodes is less than the desired replication factor, the exiting condition is never met. 

A proposed fix is to add a timeout parameter, so the command will wait for a certain amount of time or number of tries before exiting. 

  for(boolean done = false; !done; ) {
        BlockLocation[] locations = fs.getFileBlockLocations(status, 0, len);
        int i = 0;
        for(; i < locations.length && 
          locations[i].getHosts().length == rep; i++)
          if (!printWarning && locations[i].getHosts().length > rep) {
            System.out.println(""\nWARNING: the waiting time may be long for ""
                + ""DECREASING the number of replication."");
            printWarning = true;
          }
        done = i == locations.length;

        if (!done) {
          System.out.print(""."");
          System.out.flush();
          try {Thread.sleep(10000);} catch (InterruptedException e) {}
        }
      }
"
Fix potential issue of cache refresh interval,HDFS-6786,"In {{CacheReplicationMonitor}}, following code is try to check whether needs to rescan every interval ms, if rescan takes n ms, then subtract n ms for the interval. But if the delta <=0, it breaks and start rescan, there will be potential issue: if user set the interval to a small value or rescan finished after a while exceeding the interval, then rescan will happen in loop. Furthermore, {{delta <= 0}} trigger the rescan should not be the intention, since if needs rescan, {{needsRescan}} will be set.
{code}
         while (true) {
            if (shutdown) {
              LOG.info(""Shutting down CacheReplicationMonitor"");
              return;
            }
            if (needsRescan) {
              LOG.info(""Rescanning because of pending operations"");
              break;
            }
            long delta = (startTimeMs + intervalMs) - curTimeMs;
            if (delta <= 0) {
              LOG.info(""Rescanning after "" + (curTimeMs - startTimeMs) +
                  "" milliseconds"");
              break;
            }
            doRescan.await(delta, TimeUnit.MILLISECONDS);
            curTimeMs = Time.monotonicNow();
          }
{code}"
Infinite WAITING on dead s3n connection.,HDFS-1441,"I am trying to read an image from a s3n URL as follows:

final FileSystem fileSystem;
fileSystem = FileSystem.get(new URI(filePath), config);
final FSDataInputStream fileData = fileSystem.open(new Path(filePath));
baseImage = ImageIO.read(fileData);
fileData.close();

When the ImageIO.read fails, i catch the exception and retry. It works flawlessly on the first four retries, but on the fifth retry, the fileSystem.open command block infinitely.
Here's the stack trace:


""Thread-83"" prio=10 tid=0x00007f9740182000 nid=0x7cbc in Object.wait() [0x0000000040bbf000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00007f974a822438> (a org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$ConnectionPool)
	at org.apache.commons.httpclient.MultiThreadedHttpConnectionManager.doGetConnection(MultiThreadedHttpConnectionManager.java:509)
	- locked <0x00007f974a822438> (a org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$ConnectionPool)
	at org.apache.commons.httpclient.MultiThreadedHttpConnectionManager.getConnectionWithTimeout(MultiThreadedHttpConnectionManager.java:394)
	at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:152)
	at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:396)
	at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:324)
	at org.jets3t.service.impl.rest.httpclient.RestS3Service.performRequest(RestS3Service.java:357)
	at org.jets3t.service.impl.rest.httpclient.RestS3Service.performRestHead(RestS3Service.java:652)
	at org.jets3t.service.impl.rest.httpclient.RestS3Service.getObjectImpl(RestS3Service.java:1556)
	at org.jets3t.service.impl.rest.httpclient.RestS3Service.getObjectDetailsImpl(RestS3Service.java:1492)
	at org.jets3t.service.S3Service.getObjectDetails(S3Service.java:1793)
	at org.jets3t.service.S3Service.getObjectDetails(S3Service.java:1225)
	at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.retrieveMetadata(Jets3tNativeFileSystemStore.java:111)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
	at org.apache.hadoop.fs.s3native.$Proxy1.retrieveMetadata(Unknown Source)
	at org.apache.hadoop.fs.s3native.NativeS3FileSystem.getFileStatus(NativeS3FileSystem.java:355)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:690)
	at org.apache.hadoop.fs.s3native.NativeS3FileSystem.open(NativeS3FileSystem.java:476)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:398)
	at com.spratpix.mapreduce.MapUrl2ImageData.loadImageAndStoreTo(MapUrl2ImageData.java:226)

I suppose this is because S3 is throttling connections to the same file, but the s3n client cannot handle that correctly."
Node is marked decommissioned if it becomes dead when it is being decommissioned,HDFS-6626,"Not sure if it is by design. But it isn't intuitive. The scenario is like this, you try to decommission a node; when the node is being decommissioned, the node becomes dead from NN's point of view; right after that NN will mark this node decommissioned. On the webUI, administrators will consider the decommission has completed successfully. That is because when there is no block left for the DN, decommission is considered done.

{noformat}
BlockManager.java
  boolean isReplicationInProgress(DatanodeDescriptor srcNode) {
    boolean status = false;
...
    final Iterator<? extends Block> it = srcNode.getBlockIterator();
    while(it.hasNext()) {
...
// set status if there is block under replication
    }
...
    return status;
}
{noformat}

The question is whether we should mark the dead node as decommission completed (the current behavior), or mark the dead node ""decommission aborted"". From administrators' point of view, when they are doing decomm, they want to know the status of decomm and the health of those decomm-in-progress nodes. If they can detect decommission failure earlier, they might be able to take actions earlier; for example if the TOR switch has issues during decomm, administrators will be able to quickly find out a bunch of ""decommission aborted"" nodes from the same rack. People can still find this information by doing the join between decomm node list and recent dead node list on the webUI; just not as convenient.

Suggestions?"
Generation stamp value should be validated when creating a Block,HDFS-279,"In hdfs, generation stamps < GenerationStamp.FIRST_VALID_STAMP are reserved values but not valid generation stamps.  Incorrect uses of the reserved values may cause unexpected behavior.  We should  validate generation stamp when creating a Block."
hadoop fs -test -d should return different codes for difference cases,HDFS-179,"
Currently, hadoop fs -test -d path  returns 0 for the case where path is a directory and -1 for all the other cases such as execution errors, path does not exists, and path is a file.
It is better to use different return codes to differentiate these cases.

Similar issues also apply to hadoop fs -test -e|z

  "
high rate of task failures because of bad or full datanodes,HDFS-130,"With 0.17 we notice a fast rate of task failures because of the same bad data nodes being reported repeatedly as badFirstLink. We never saw this in 0.16.

After running less than 20,000 map tasks, more than 2,500 of them reported a single certain datanode as badFirstLink, with typical exception of the form:

08/09/09 14:41:14 INFO dfs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: 189000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/xxx.yyy.zzz.ttt:38788 remote=/xxx.yyy.zzz.ttt:50010]
08/09/09 14:41:14 INFO dfs.DFSClient: Abandoning block blk_-3650954811734254315
08/09/09 14:41:14 INFO dfs.DFSClient: Waiting to find target node: xxx.yyy.zzz.ttt:50010
08/09/09 14:44:29 INFO dfs.DFSClient: Exception in createBlockOutputStream java.net.SocketTimeoutException: 189000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/xxx.yyy.zzz.ttt:39014 remote=/xxx.yyy.zzz.ttt:50010]
08/09/09 14:44:29 INFO dfs.DFSClient: Abandoning block blk_8665387817606483066
08/09/09 14:44:29 INFO dfs.DFSClient: Waiting to find target node: xxx.yyy.zzz.ttt:50010
08/09/09 14:47:35 INFO dfs.DFSClient: Exception in createBlockOutputStream java.io.IOException: Bad connect ack with firstBadLink ip.bad.data.node:50010
08/09/09 14:47:35 INFO dfs.DFSClient: Abandoning block blk_8475261758012143524
08/09/09 14:47:35 INFO dfs.DFSClient: Waiting to find target node: xxx.yyy.zzz.ttt:50010
08/09/09 14:50:42 INFO dfs.DFSClient: Exception in createBlockOutputStream java.io.IOException: Bad connect ack with firstBadLink ip.bad.data.node:50010
08/09/09 14:50:42 INFO dfs.DFSClient: Abandoning block blk_4847638219960634858
08/09/09 14:50:42 INFO dfs.DFSClient: Waiting to find target node: xxx.yyy.zzz.ttt:50010
08/09/09 14:50:48 WARN dfs.DFSClient: DataStreamer Exception: java.io.IOException: Unable to create new block.
08/09/09 14:50:48 WARN dfs.DFSClient: Error Recovery for block blk_4847638219960634858 bad datanode[2]
Exception in thread ""main"" java.io.IOException: Could not get block locations. Aborting...

With several such bad datanodes the probability of jobs failing goes up a lot.
"
DFS client should throw version mismatch errors in case of a changed functionality,HDFS-45,"I started a hadoop cluster, built with the (latest) trunk  and tried doing _dfs -put_ with the (dfs) clients from the (older/stale) trunk. The client went ahead and tried to upload the data onto the cluster and failed with the following error
{noformat}
-bash-3.00$ ./bin/hadoop dfs -put file file
08/08/22 05:11:06 INFO hdfs.DFSClient: Exception in createBlockOutputStream java.io.IOException: Could not read from stream
08/08/22 05:11:06 INFO hdfs.DFSClient: Abandoning block blk_5748330682182803489_1002
08/08/22 05:11:12 INFO hdfs.DFSClient: Exception in createBlockOutputStream java.io.IOException: Could not read from stream
08/08/22 05:11:12 INFO hdfs.DFSClient: Abandoning block blk_7482082538144151768_1002
08/08/22 05:11:18 INFO hdfs.DFSClient: Exception in createBlockOutputStream java.io.IOException: Could not read from stream
08/08/22 05:11:18 INFO hdfs.DFSClient: Abandoning block blk_-3132217232090937466_1002
08/08/22 05:11:24 INFO hdfs.DFSClient: Exception in createBlockOutputStream java.io.IOException: Could not read from stream
08/08/22 05:11:24 INFO hdfs.DFSClient: Abandoning block blk_-6473055472384366978_1002
08/08/22 05:11:30 WARN hdfs.DFSClient: DataStreamer Exception: java.io.IOException: Unable to create new block.
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2504)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:1810)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:1990)

put: Filesystem closed
{noformat}

It would be better if somehow the client detects that its not *made* for this master and softly/simply bail out.
----
Note that I *did not* do it on purpose but forgot to replace the older installation with the newer one."
Hadoop JMX stats are not refreshed,HDFS-6688,"Even when the HDFS datanode process is stopped the JMX attribute Hadoop.NameNode.FSNamesystemState.NumLiveDataNodes/NumDeadDataNodes attribute values doesn't change. Also Hadoop.NameNode.NameNodeInfo.Attributes.LiveNodes shows the stopped datanode details. If these attributes reflect the actual changes in the datanode, they can be used to monitor the health of the HDFS cluster which currently can't be used."
[ HDFS- File Concat ] Concat will fail when target file is having one block which is not full ,HDFS-6641,"sually we can't ensure lastblock alwaysfull...please let me know purpose of following check..

    long blockSize = trgInode.getPreferredBlockSize();

    // check the end block to be full
    final BlockInfo last = trgInode.getLastBlock();
    if(blockSize != last.getNumBytes()) {
      throw new HadoopIllegalArgumentException(""The last block in "" + target
          + "" is not full; last block size = "" + last.getNumBytes()
          + "" but file block size = "" + blockSize);
    }

If it is issue, I'll file jira.

Following is the trace..

exception in thread ""main"" org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.HadoopIllegalArgumentException): The last block in /Test.txt is not full; last block size = 14 but file block size = 134217728
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.concatInternal(FSNamesystem.java:1887)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.concatInt(FSNamesystem.java:1833)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.concat(FSNamesystem.java:1795)
at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.concat(NameNodeRpcServer.java:704)
at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.concat(ClientNamenodeProtocolServerSideTranslatorPB.java:512)
at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
"
Setting Extended ACLs recursively for  another user belonging to the same group  is not working,HDFS-6654,"{noformat}
1.Setting Extended ACL recursively for  a user belonging to the same group  is not working
{noformat}

Step 1: Created a Dir1 with User1
	          ./hdfs dfs -rm -R /Dir1
Step 2: Changed the permission (600) for Dir1 recursively
	         ./hdfs dfs -chmod -R 600 /Dir1
Step 3: setfacls is executed to give read and write permissions to User2 which belongs to the same group as User1
	         ./hdfs dfs -setfacl -R -m user:User2:rw- /Dir1

	         ./hdfs dfs -getfacl -R /Dir1
                         No GC_PROFILE is given. Defaults to medium.
                       # file: /Dir1
                       # owner: User1
                       # group: supergroup
                       user::rw-
                       user:User2:rw-
                       group::---
                       mask::rw-
                       other::---
Step 4: Now unable to write a File to Dir1 from User2

           ./hdfs dfs -put hadoop /Dir1/1
No GC_PROFILE is given. Defaults to medium.
put: Permission denied: user=User2, access=EXECUTE, inode=""/Dir1"":User1:supergroup:drw------

{noformat}
   2. Fetching filesystem name , when one of the disk configured for NN dir becomes full returns a value ""null"".
{noformat}
2014-07-08 09:23:43,020 WARN org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker: Space available on volume 'null' is 101060608, which is below the configured reserved amount 104857600
2014-07-08 09:23:43,020 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: NameNode low on available disk space. Already in safe mode.
2014-07-08 09:23:43,166 WARN org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker: Space available on volume 'null' is 101060608, which is below the configured reserved amount 104857600

 
"
Quota exceed exception creates file of size 0,HDFS-172,"Empty file of size 0 is created when QuotaExceed exception occurs while copying a file. This file is created with the same name of which file copy is tried .
I.E if operation 
Hadoop fs -copyFromLocal testFile1 /testDir   
Fails due to quota exceed exception then testFile1 of size 0 is created in testDir on HDFS.


Steps to verify 

1) Create testDir and apply space quota of 16kb
2) Copy file say testFile of size greater than 16kb from local file system
3) You should see QuotaException error 
4) testFile of size 0 is created in testDir which is not expected ."
"IVs need to be created with a counter, not a SRNG",HDFS-6547,"IVs should be created using a persistent counter, not a SRNG."
Multihoming brokenness in HDFS,HDFS-1379,"We have a setup where - because we only have a very few machines (4 x 16 core) we're looking at co-locating namenodes and datanodes. We also have front-end and back-end networks. Set-up is something like:

* machine1
** front-end 10.18.80.80
** back-end 192.168.24.40
* machine2
** front-end 10.18.80.82
** back-end 192.168.24.41
* machine3
** front-end 10.18.80.84
** back-end 192.168.24.42
* machine4
** front-end 10.18.80.86
** back-end 192.168.24.43

On each, the property *slave.host.name* is configured with the 192. address, (the *.dns.interface settings don't actually seem to help, but that's a separate problem), and the *dfs.datanode.address* is bound to the 192.168.24.x address on :50010, similarly the *dfs.datanode.ipc.address* is bound there.

In order to get efficient use of our machines, we bring up a namenode on one of them (this then rsyncs the latest namenode fsimage etc) by bringing up a VIP on each side (we use the 10.18.80.x side for monitoring, rather than actual hadoop comms), and binding the namenode to that - on the inside this is 192.168.24.19.

The namenode now knows about 4 datanodes - 192.168.24.40/1/2/3. These datanodes know how they're bound. However, when the datanode is telling an external hdfs client where to store the blocks, it gives out 192.168.24.19:50010 as one of the addresses (despite the datanode not being bound there) - because that's where the datanode->namenode RPC comes from.

This is wrong because if you've bound the datanode explicitly (using *dfs.datanode.address*) then that's should be the only address the namenode can give out (it's reasonable, given your comms model not to support NAT between clients and data slaves). If you bind it to * then your normal rules for slave.host.name, dfs.datanode.dns.interface etc should take precedence.

This may already be fixed in later releases than 0.20.1 - but if it isn't it should probably be - you explicitly allow binding of the datanode addresses - it's unreasonable to expect that comms to the datanode will always come from those addresses - especially in multi-homed environments (and separating traffic out by network - especially when dealing with large volumes of data) is useful."
Add an XAttr to specify the cipher mode,HDFS-6508,"We should specify the cipher mode in the xattrs for compatibility sake. Crypto changes over time and we need to prepare for that.
"
add unit tests for encryption zones api and cli,HDFS-6523,"Add unit tests for EZ apis and CLI. This work will be done after HDFS-6386, HDFS-638, and HDFS-6516 are completed."
User settable xAttr to stop HDFS admins from reading/chowning a file,HDFS-6393,"A user should be able to set an xAttr on any file in HDFS to stop an HDFS admin user from reading the file. The blacklist for chown/chgrp would also enforced.

This will stop an HDFS admin from gaining access to job token files and getting HDFS DelegationTokens that would allow him/her to read an encrypted file."
 chown/chgrp users/groups blacklist for encrypted files,HDFS-6390,"A blacklist of users and groups that stops an admin from changing the owner/group of the file for encrypted files and directories. This blacklist would typically contain the regular users used by admins.
"
Maintain a list of all the Encryption Zones in the file system,HDFS-6457,"We need to maintain a list of all encryption zones in the file system so that we can ask questions about what EZ a path belongs to, if any, and let the admin know all the EZs in the system.

[~andrew.wang] Why not just have a sorted structure with pointers to all the roots of the EZs? We can populate it during metadata loading on startup, and keep it updated during runtime."
Failure to create FinalizeCommand,HDFS-6577,"I visually see a bug in NameNodeRpcServer.java, 
{code}
 @Override // DatanodeProtocol
  public DatanodeCommand blockReport(DatanodeRegistration nodeReg,
      String poolId, StorageBlockReport[] reports) throws IOException {
    verifyRequest(nodeReg);
    if(blockStateChangeLog.isDebugEnabled()) {
      blockStateChangeLog.debug(""*BLOCK* NameNode.blockReport: ""
           + ""from "" + nodeReg + "", reports.length="" + reports.length);
    }
    final BlockManager bm = namesystem.getBlockManager();
    boolean noStaleStorages = false;
    for(StorageBlockReport r : reports) {
      final BlockListAsLongs blocks = new BlockListAsLongs(r.getBlocks());
      noStaleStorages = bm.processReport(nodeReg, r.getStorage(), poolId, blocks);
      metrics.incrStorageBlockReportOps();
    }

    if (nn.getFSImage().isUpgradeFinalized() &&
        !nn.isStandbyState() &&
        noStaleStorages) {
      return new FinalizeCommand(poolId);
    }

    return null;
  }
{code}

noStaleStorage is initialized to false, then in the for loop {{ for(StorageBlockReport r : reports) {}} it could be changed to true. 

The problem is, since it's a loop of all reports, the value returned from the last blockreport (the last iteration) will set it to the final value. Even if it was set to true in the middle of a loop, the final iteration could overwrite it to false. Similarly, if it was set to false in the middle, and the final iter could overwrite it to true.



"
Cannot download file via webhdfs when wildcard is enabled,HDFS-6336,"With wildcard is enabled, issuing a webhdfs command like
{code}
http://yjztvm2.private:50070/webhdfs/v1/tmp?op=OPEN
{code}
would give
{code}
http://yjztvm3.private:50075/webhdfs/v1/tmp?op=OPEN&namenoderpcaddress=0.0.0.0:8020&offset=0

{""RemoteException"":{""exception"":""ConnectException"",""javaClassName"":""java.net.ConnectException"",""message"":""Call From yjztvm3.private/192.168.142.230 to 0.0.0.0:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused""}}
{code}

"
TestRBWBlockInvalidation.testBlockInvalidationWhenRBWReplicaMissedInDN failed,HDFS-4051,"TestRBWBlockInvalidation.testBlockInvalidationWhenRBWReplicaMissedInDN failed with the following on a recent trunk job. Has been seen before in HDFS-3391 as well on branch-2.

{noformat}
java.lang.AssertionError: There should be 1 replica in the corruptReplicasMap expected:<1> but was:<0>
	at org.junit.Assert.fail(Assert.java:91)
	at org.junit.Assert.failNotEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:126)
	at org.junit.Assert.assertEquals(Assert.java:470)
	at org.apache.hadoop.hdfs.server.blockmanagement.TestRBWBlockInvalidation.testBlockInvalidationWhenRBWReplicaMissedInDN(TestRBWBlockInvalidation.java:99)
{noformat}"
DFSClient should handle all nodes in a pipeline failed.,HDFS-951,"processDatanodeError-> setupPipelineForAppendOrRecovery  will set streamerClosed to be true if all nodes in the pipeline failed in the past, and just return.
Back to run() in data streammer,  the logic 
 if (streamerClosed || hasError || dataQueue.size() == 0 || !clientRunning) {
                continue;
  }
will just let set closed=true in closeInternal().

And DataOutputStream will not get a chance to clean up. The DataOutputStream will throw exception or return null for following write/close.
It will leave the file in writing in incomplete state."
TruncateBlock does not update in-memory information correctly,HDFS-1336,"- Component: data node
 
- Version: 0.20-append
 
- Summary: we found a case that when a block is truncated during updateBlock,
the length on the ongoingCreates is not updated, hence leading to failed append.

 
- Setup:
# disks / datanode = 3
# failures "
File write fails after data node goes down,HDFS-108,"If a data node goes down while a file is being written do HDFS, the write fails with the following errors:
{noformat} 
09/04/20 17:15:39 INFO dfs.DFSClient: Exception in createBlockOutputStream java.io.IOException:
Bad connect ack with firstBadLink 192.168.0.66:50010
09/04/20 17:15:39 INFO dfs.DFSClient: Abandoning block blk_-6792221430152215651_1003
09/04/20 17:15:45 INFO dfs.DFSClient: Exception in createBlockOutputStream java.io.IOException:
Bad connect ack with firstBadLink 192.168.0.66:50010
09/04/20 17:15:45 INFO dfs.DFSClient: Abandoning block blk_-1056044503329698571_1003
09/04/20 17:15:51 INFO dfs.DFSClient: Exception in createBlockOutputStream java.io.IOException:
Bad connect ack with firstBadLink 192.168.0.66:50010
09/04/20 17:15:51 INFO dfs.DFSClient: Abandoning block blk_-1144491637577072681_1003
09/04/20 17:15:57 INFO dfs.DFSClient: Exception in createBlockOutputStream java.io.IOException:
Bad connect ack with firstBadLink 192.168.0.66:50010
09/04/20 17:15:57 INFO dfs.DFSClient: Abandoning block blk_6574618270268421892_1003
09/04/20 17:16:03 WARN dfs.DFSClient: DataStreamer Exception: java.io.IOException:
Unable to create new block.
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2387)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.access$1800(DFSClient.java:1746)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:1924)
09/04/20 17:16:03 WARN dfs.DFSClient: Error Recovery for block blk_6574618270268421892_1003 bad datanode[1]
{noformat} 

The tests were done with the following configuration:
* Hadoop version 0.18.3
* 3 data nodes with replication count of 2
* 1 GB file write
* 1 data node taken down during write

This issue seems to be caused by the fact that there is a delay between the time a data node goes down and the time it is marked as dead by the name node. This delay is unavoidable, but the name node should not keep allocating new blocks to data nodes that are known to be down by the client. Even by adjusting {{heartbeat.recheck.interval}}, there is still a window during which this issue can occur.

One possible fix would be to allow clients to exclude known bad data nodes when allocating new blocks. See {{failed_write.patch}} for an example."
decommissioned nodes report not consistent / clear,HDFS-1290,"after i add the list of decom nodes to exclude list and -refreshNodes.
In the WebUI the decom/excluded nodes show up in both the live node list and the dead node list.

when I do -report from the command line.
""""""
  Datanodes available: 14 (20 total, 6 dead)
""""""
The problem here is that is only 14 nodes total including the 6 added to the exclude list. 

Now, in the node level status for each of the nodes, the excluded nodes say 
""""""""
 Decommission Status : Normal
""""""""
 But, all the nodes say the same thing. I think if it said something like ""in-progress"", it would be more informative. 
note. one thing distinguishing these excluded nodes is that they all report 0 or 100% for all the values in -report.

Cause, at this point i know from https://issues.apache.org/jira/browse/HDFS-1125 that one may have to restart the cluster to completely remove the nodes.
But, i have no clue when i should restart.

Ultimately, whats needed is some indication to when the decomission is complete so that all references to the excluded nodes ( from excludes, slaves ) and restart the cluster.

"
Datanode throwing UnregisteredDatanodeException -- expects itself to serve storage!,HDFS-1106,"We run a large Hadoop cluster used by many different universities.  When some DataNodes went down recently, they came back up and then generated this error
message in their datanode logs:

2010-04-22 16:58:37,314 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.protocol.UnregisteredDatanodeException: Data node vm-10-160-4-109:50010 is attempting to report storage ID DS-1884904520-10.160.4.109-50010-1255720271773. Node 10.160.4.109:50010 is expected to serve this storage.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDatanode(FSNamesystem.java:3972)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.verifyNodeRegistration(FSNamesystem.java:3937)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.registerDatanode(FSNamesystem.java:2052)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.register(NameNode.java:735)
	at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:966)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:962)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:960)
	at org.apache.hadoop.ipc.Client.call(Client.java:740)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
	at $Proxy4.register(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.register(DataNode.java:544)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.runDatanodeDaemon(DataNode.java:1230)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1273)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:1394)


Note it is correctly expecting itself to serve the data, but throwing an UnregisteredDatanodeException for some reason.  This is causing these datanodes to remain ""dead"" to the namenode.  Does anyone know why this is occuring and what we can do to fix it?"
improve handling of datanode timeouts,HDFS-2420,"If a datanode ever times out on a heart beat, it gets marked dead permanently. I am finding that on AWS this is a periodic occurrence, i.e., datanodes time out although the datanode process is still alive. The current solution to this is to kill and restart each such process independently. 

It would be good if there were more retry logic (e.g., blacklisting the nodes but try heartbeats for a longer period before determining they are apparently dead). It would also be good if refreshNodes would check and attempt to recover timed out data nodes.
"
"Datanode is marked dead, but datanode process is alive and verifying blocks",HDFS-1766,"We have a datanode marked dead in the namenode, and it is not taking any traffic. But it is verifying blocks continuously, so the DataNode process is definitely not dead. Jstack shows that the main thread and the offerService thread are gone but the JVM stuck at waiting for other threads to die. It seems to me that the offerService thread has died abnormally, for example, by a runtime exception and it did not shut down other threads before exiting."
java.io.IOException: File /user/root/lwr/test31.txt could only be replicated to 0 nodes instead of minReplication (=1).  There are 3 datanode(s) running and 3 node(s) are excluded in this operation.,HDFS-3333,"log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).
log4j:WARN Please initialize the log4j system properly.
java.io.IOException: File /user/root/lwr/test31.txt could only be replicated to 0 nodes instead of minReplication (=1).  There are 3 datanode(s) running and 3 node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1259)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1916)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:472)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:292)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:42602)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:428)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:905)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1688)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1684)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1205)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1682)
i:4284

	at org.apache.hadoop.ipc.Client.call(Client.java:1159)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:185)
	at $Proxy9.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:84)
	at $Proxy9.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:295)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1097)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:973)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:455)







testcase:


import java.io.IOException;
import java.net.URI;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hdfs.DFSConfigKeys;
import org.apache.hadoop.hdfs.DistributedFileSystem;
public class Write1 {

	/**
	 * @param args
	 * @throws Exception 
	 */
	public static void main(String[] args) throws Exception {
		//System.out.println(""main"");
		String hdfsFile=""/user/root/lwr/test31.txt"";
    	byte writeBuff[] = new byte [1024 * 1024];
    	int i=0;
    	DistributedFileSystem dfs = new DistributedFileSystem();
    	Configuration conf=new Configuration();
    	//conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, 512);
    	//conf.setLong(DFSConfigKeys.DFS_REPLICATION_KEY, 2);
        // conf.setInt(""dfs.replication"", 3);
        conf.setLong(""dfs.blocksize"", 512);
    	dfs.initialize(URI.create(""hdfs://10.18.40.154:9000""), conf);
    	//dfs.delete(new Path(hdfsFile));
	    //appendFile(dfs,hdfsFile,1024 * 1024,true);
    	try
    	{
    	FSDataOutputStream out1=dfs.create(new Path(hdfsFile));
    	
   	    for(i=0;i<100000;i++)
   	    {
    	 out1.write(writeBuff, 0, 512);
    	}
        out1.hsync();
        out1.close();
        /*
	    FSDataOutputStream out=dfs.append(new Path(hdfsFile),4096);
		out.write(writeBuff, 0, 512 * 1024);
		out.hsync();
		out.close();
		*/
    	}catch (IOException e)
    	{    	   
    	   System.out.println(""i:"" + i);
    	   e.printStackTrace();

    	}
    	finally
    	{   
    		
    		System.out.println(""i:"" + i);
    	    System.out.println(""end!"");
    	 
    	}

    }
	

}
"
LeaseExpiredException on NameNode if file is moved while being created.,HDFS-3534,"If a file (big_file.txt size=512MB) being created (or uploaded) on hdfs, and a rename (fs -mv) of that file is done. Then following exception occurs:-

{noformat}
12/06/13 08:56:42 WARN hdfs.DFSClient: DataStreamer Exception: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /user/mitesh/temp/big_file.txt File does not exist. [Lease.  Holder: DFSClient_-2105467303, pendingcreates: 1]
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1604)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1595)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1511)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:685)
        at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:563)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1388)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1384)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1082)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1382)

        at org.apache.hadoop.ipc.Client.call(Client.java:1066)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)
        at $Proxy6.addBlock(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
        at $Proxy6.addBlock(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.locateFollowingBlock(DFSClient.java:3324)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:3188)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2300(DFSClient.java:2406)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2646)

12/06/13 08:56:42 WARN hdfs.DFSClient: Error Recovery for block blk_-5525713112321593595_679317395 bad datanode[0] nodes == null
12/06/13 08:56:42 WARN hdfs.DFSClient: Could not get block locations. Source file ""/user/mitesh/temp/big_file.txt"" - Aborting...
...
{noformat}


Whereas this issue is not seen on *Hadoop 0.23*.


I have used following shell script to simulate the issue.
{code:title=run_parallely.sh}
#!/bin/bash
hadoop=""hadoop""
filename=big_file.txt
dest=/user/mitesh/temp/$filename
dest2=/user/mitesh/xyz/$filename

## Clean up
hadoop fs -rm -skipTrash $dest
hadoop fs -rm -skipTrash $dest2

## Copy big_file.txt onto hdfs
hadoop fs -put $filename $dest > cmd1.log 2>&1 &

## sleep until entry is created, hoping copying is not finished
until $(hadoop fs -test -e $dest)
do
    sleep 1
done

## Now move
hadoop fs -mv $dest $dest2 > cmd2.log 2>&1 &
{code}"
Namenode blockMap not updated when datanode invalidates a block on heart beat,HDFS-39,"Here are related logs.  dn_10_251_107_213 deleted the block immediately after it received it (because of over-replication).  However, the information is not reported to namenode and name node ask this DN to invalidate the block again, causing the error ""Unexpected error trying to delete block blk_-1201258788540100829. BlockInfo not found in volumeMap."". 


dn_10_251_107_213 | 080722 145554 2497 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1201258788540100829 src: /10.251.67.214:54770 dest: /10.251.67.214:50010
dn_10_251_193_205 | 080722 145554 2468 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1201258788540100829 src: /10.251.107.213:36718 dest: /10.251.107.213:50010
dn_10_251_67_214 | 080722 145554 2306 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1201258788540100829 src: /10.251.67.214:60599 dest: /10.251.67.214:50010
nn_10_251_210_161 | 080722 145554 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_200807221347_0001_m_000092_1/part-00092. blk_-1201258788540100829
dn_10_251_107_213 | 080722 145656 2498 INFO dfs.DataNode$PacketResponder: Received block blk_-1201258788540100829 of size 29767698 from /10.251.67.214
dn_10_251_193_205 | 080722 145656 2469 INFO dfs.DataNode$PacketResponder: Received block blk_-1201258788540100829 of size 29767698 from /10.251.107.213
dn_10_251_67_214 | 080722 145656 2307 INFO dfs.DataNode$PacketResponder: Received block blk_-1201258788540100829 of size 29767698 from /10.251.67.214
dn_10_251_107_213 | 080722 145656 2498 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-1201258788540100829 terminating
dn_10_251_193_205 | 080722 145656 2469 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-1201258788540100829 terminating
dn_10_251_67_214 | 080722 145656 2307 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-1201258788540100829 terminating
nn_10_251_210_161 | 080722 145656 19 INFO dfs.FSNamesystem: BLOCK* ask 10.251.67.214:50010 to replicate blk_-1201258788540100829 to datanode(s) 10.251.107.49:50010
nn_10_251_210_161 | 080722 145656 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.67.214:50010 is added to blk_-1201258788540100829 size 29767698
nn_10_251_210_161 | 080722 145656 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.107.213:50010 is added to blk_-1201258788540100829 size 29767698
nn_10_251_210_161 | 080722 145656 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.193.205:50010 is added to blk_-1201258788540100829 size 29767698
dn_10_251_107_49 | 080722 145658 2484 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1201258788540100829 src: /10.251.67.214:42237 dest: /10.251.67.214:50010
dn_10_251_67_214 | 080722 145658 18 INFO dfs.DataNode: 10.251.67.214:50010 Starting thread to transfer block blk_-1201258788540100829 to 10.251.107.49:50010
dn_10_251_107_49 | 080722 145705 2484 INFO dfs.DataNode$DataXceiver: Received block blk_-1201258788540100829 src: /10.251.67.214:42237 dest: /10.251.67.214:50010 of size 29767698
dn_10_251_67_214 | 080722 145705 2342 INFO dfs.DataNode$DataTransfer: 10.251.67.214:50010:Transmitted block blk_-1201258788540100829 to /10.251.107.49:50010
nn_10_251_210_161 | 080722 145705 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.107.49:50010 is added to blk_-1201258788540100829 size 29767698
dn_10_251_107_213 | 080722 145710 19 INFO dfs.FSDataset: Deleting block blk_-1201258788540100829 file /mnt/hadoop/dfs/data/current/subdir11/blk_-1201258788540100829
nn_10_251_210_161 | 080722 150220 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-1201258788540100829 is added to invalidSet of 10.251.67.214:50010
nn_10_251_210_161 | 080722 150220 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-1201258788540100829 is added to invalidSet of 10.251.107.213:50010
nn_10_251_210_161 | 080722 150220 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-1201258788540100829 is added to invalidSet of 10.251.193.205:50010
nn_10_251_210_161 | 080722 150220 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: blk_-1201258788540100829 is added to invalidSet of 10.251.107.49:50010
dn_10_251_193_205 | 080722 150224 19 INFO dfs.FSDataset: Deleting block blk_-1201258788540100829 file /mnt/hadoop/dfs/data/current/subdir51/blk_-1201258788540100829
dn_10_251_107_49 | 080722 150226 19 INFO dfs.FSDataset: Deleting block blk_-1201258788540100829 file /mnt/hadoop/dfs/data/current/subdir24/blk_-1201258788540100829
dn_10_251_107_213 | 080722 150228 19 WARN dfs.FSDataset: Unexpected error trying to delete block blk_-1201258788540100829. BlockInfo not found in volumeMap.
dn_10_251_67_214 | 080722 150253 18 INFO dfs.FSDataset: Deleting block blk_-1201258788540100829 file /mnt/hadoop/dfs/data/current/subdir43/blk_-1201258788540100829"
Corrupted blocks value is not updating in jmx(FSNamesystemMBean),HDFS-5927,"Steps - 

1)- Write data to HDFS.

2)- Corrupt some blocks.

3)- Check whether Corrupted blocks count is updating in jmx


Observed output - JMX is showing always count zero where fsck report had shown 27 Corrupted blocks are there.



/** Returns number of blocks with corrupt replicas */
  @Metric({""CorruptBlocks"", ""Number of blocks with corrupt replicas""})
  public long getCorruptReplicaBlocks() {
    return blockManager.getCorruptReplicaBlocksCount();
  }"
Access time of HDFS directories stays at 1969-12-31,HDFS-5749,"Modify FsShell so that ""fs -lsr"" can show access time in addition to modification time, the access time stays at 1969-12-31. This means the access time is not set up initially. Filing this jira to fix this issue.


"
TestHASafeMode#testBlocksAddedWhenStandbyIsDown fails intermittently on Branch2,HDFS-5836,"This is an intermittent failure. Seems like a JDK7 issue. I did some initial research and found that the test always fails when it runs the following tests in order.
{noformat}
	  testBlocksRemovedBeforeStandbyRestart
	  testSafeBlockTracking
	  testBlocksAddedWhileStandbyIsDown
{noformat}

The test fails with the following error

{noformat}
Running org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 20.115 sec <<< FAILURE!
test(org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode)  Time elapsed: 19802 sec  <<< FAILURE!
java.lang.AssertionError: Bad safemode status: 'Safe mode is ON. The reported blocks 21 has reached the threshold 0.9990 of total blocks 21. The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically in 28 seconds.'
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.assertTrue(Assert.java:43)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode.assertSafeMode(TestHASafeMode.java:493)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode.testBlocksAddedWhileStandbyIsDown(TestHASafeMode.java:660)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode.test(TestHASafeMode.java:120)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:242)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:137)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)


Results :

Failed tests:   test(org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode): Bad safemode status: 'Safe mode is ON. The reported blocks 21 has reached the threshold 0.9990 of total blocks 21. The number of live datanodes 3 has reached the minimum number 0. Safe mode will be turned off automatically in 28 seconds.'
{noformat}"
org.apache.hadoop.dfs.LeaseExpiredException during dfs write,HDFS-198,"Many long running cpu intensive map tasks failed due to org.apache.hadoop.dfs.LeaseExpiredException.
See [a comment below|https://issues.apache.org/jira/browse/HDFS-198?focusedCommentId=12910298&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12910298] for the exceptions from the log:

"
hadoop 1.2.1 spengo HTTP web console access issue,HDFS-5108,"Hi Good Morning,

1) i created kerberos DB, realm and able to test properly
   
   added valid principals, key tab files generated using kadmin, signature created using udev/random

   I replaced latest jce libs from oracle to support sha1-96...

   $ kinit
   $ klist

2) i followed this link and configured appropriate

     http://hadoop.apache.org/docs/stable/HttpAuthentication.html


core-site.xml

<!-- HTTP web-consoles Authentication -->
  <property>
    <name>hadoop.http.filter.initializers</name>
    <value>org.apache.hadoop.security.AuthenticationFilterInitializer</value>
  </property>

  <property>
    <name>hadoop.http.authentication.type</name>
    <value>kerberos</value>
  </property>

  <property>
    <name>hadoop.http.authentication.token.validity</name>
    <value>36000</value>
  </property>

  <property>
    <name>hadoop.http.authentication.signature.secret.file</name>
    <value>/opt/software/hadoop-1.2.1/conf/security/http-secret-file</value>
  </property>

  <property>
    <name>hadoop.http.authentication.cookie.domain</name>
    <value></value>
  </property>

  <property>
    <name>hadoop.http.authentication.simple.anonymous.allowed</name>
    <value>false</value>
  </property>

  <property>
    <name>hadoop.http.authentication.kerberos.principal</name>
    <value>HTTP/localhost@NARAYANA.LOCAL</value>
  </property>

  <property>
    <name>hadoop.http.authentication.kerberos.keytab</name>
    <value>/opt/software/hadoop-1.2.1/conf/security/mergedKT.keytab</value>
  </property>
</configuration>

3)I have tested kerberos spengo http to namenode, jobnode on 

   single cluster environment but failed to access web consoles
   On browser: about:config then added negotiate-uri to localhost

   On browser : http://localhost:50070 

   Result: on browser....  index.html 401 error

4) curl -v -u hadoopA --negotiate http://localhost:50070 - works well

"
Skip failing commons tests on Windows,HDFS-4871,"This is a temporary fix proposed to get CI working. We will skip the following failing tests on Windows:

# -TestChRootedFs- - HADOOP-8957-
# -TestFSMainOperationsLocalFileSystem- - HADOOP-8957
# -TestFcCreateMkdirLocalFs- - HADOOP-8957
# -TestFcMainOperationsLocalFs- - HADOOP-8957
# -TestFcPermissionsLocalFs- - HADOOP-8957
# TestLocalFSFileContextSymlink - HADOOP-9527
# -TestLocalFileSystem- - HADOOP-9131
# -TestShellCommandFencer- - HADOOP-9526
# -TestSocketIOWithTimeout- - HADOOP-8982
# -TestViewFsLocalFs- - HADOOP-8957 and HADOOP-8958
# -TestViewFsTrash- - HADOOP-8957 and HADOOP-8958
# -TestViewFsWithAuthorityLocalFs- - HADOOP-8957 and HADOOP-8958

The tests will be re-enabled as we fix each. JIRAs for remaining failing tests to follow soon."
passing -Dfs.trash.interval to command line is not respected.,HDFS-4889,"Ran ""hadoop dfs -Dfs.trash.interval=0 -rm /user/<username>/README
DEPRECATED: Use of this script to execute hdfs command is deprecated.
Instead use the hdfs command for it.

Moved: 'hdfs://<host:port>/user/<username>/README' to trash at: hdfs://<host:port>/user/<username>/.Trash/Current

Expected that the file doesnt go to Trash and gets deleted directly."
DistributedFileSystem#listStatus() throws FileNotFoundException when directory doesn't exist,HDFS-5081,"I was running HBase trunk test suite against hadoop 2.1.1-SNAPSHOT (same behavior with hadoop 2.0.5-ALPHA)

One test failed due to:
{code}
org.apache.hadoop.hbase.catalog.TestMetaMigrationConvertingToPB  Time elapsed: 1,594,938.629 sec  <<< ERROR!
java.io.FileNotFoundException: File hdfs://localhost:61300/user/tyu/hbase/.archive does not exist.
  at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:656)
  at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:92)
  at org.apache.hadoop.hdfs.DistributedFileSystem$14.doCall(DistributedFileSystem.java:714)
  at org.apache.hadoop.hdfs.DistributedFileSystem$14.doCall(DistributedFileSystem.java:710)
  at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:78)
  at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:710)
  at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1478)
  at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1518)
  at org.apache.hadoop.hbase.util.FSUtils.getLocalTableDirs(FSUtils.java:1317)
  at org.apache.hadoop.hbase.migration.NamespaceUpgrade.migrateTables(NamespaceUpgrade.java:114)
  at org.apache.hadoop.hbase.migration.NamespaceUpgrade.upgradeTableDirs(NamespaceUpgrade.java:87)
  at org.apache.hadoop.hbase.migration.NamespaceUpgrade.run(NamespaceUpgrade.java:206)
  at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
  at org.apache.hadoop.hbase.catalog.TestMetaMigrationConvertingToPB.setUpBeforeClass(TestMetaMigrationConvertingToPB.java:128)
{code}
TestMetaMigrationConvertToPB.tgz was generated from filesystem image produced by previous release of HBase.
TestMetaMigrationConvertingToPB, activating NamespaceUpgrade, would upgrade it to current release of HBase.

The test is at hbase-server/src/test/java/org/apache/hadoop/hbase/catalog/TestMetaMigrationConvertingToPB.java under HBase trunk."
Hang when add/remove a datanode into/from a 2 datanode cluster,HDFS-5046,"1. Install a Hadoop 1.1.1 cluster, with 2 datanodes: dn1 and dn2. And, in hdfs-site.xml, set the 'dfs.replication' to 2
2. Add node dn3 into the cluster as a new datanode, and did not change the 'dfs.replication' value in hdfs-site.xml and keep it as 2
note: step 2 passed
3. Decommission dn3 from the cluster
Expected result: dn3 could be decommissioned successfully
Actual result:
a). decommission progress hangs and the status always be 'Waiting DataNode status: Decommissioned'. But, if I execute 'hadoop dfs -setrep -R 2 /', the decommission continues and will be completed finally.
b). However, if the initial cluster includes >= 3 datanodes, this issue won't be encountered when add/remove another datanode. For example, if I setup a cluster with 3 datanodes, and then I can successfully add the 4th datanode into it, and then also can successfully remove the 4th datanode from the cluster."
Snapshot of Being Written Files,HDFS-3960,"Here is a design question: Suppose there is a being written file when a snapshot is being taken.  What should the length of the file be shown in the snapshot?  In other words, how to determine the length of being written file when a snapshot is being taken?"
Serialize ipcPort in DatanodeID instead of DatanodeRegistration and DatanodeInfo,HDFS-282,"The field DatanodeID.ipcPort is currently serialized in DatanodeRegistration and DatanodeInfo.  Once HADOOP-2797 (remove the codes for handling old layout ) is committed, DatanodeID.ipcPort should be serialized in DatanodeID."
HDFS tests have multiple failures on Windows due to file locking conflict,HDFS-4276,"Multiple HDFS tests fail on Windows due to ""The process cannot access the file because another process has locked a portion of the file""."
freeze/seal a hdfs file,HDFS-4689,"I would like to describe the problem scenario at first, that is in our hbase cluster:
1. rs1 loses its zookeeper lock, and hmaster realizes that
2. hmaster assigns the regions of rs1 to rs2
3. rs2 renames the hlog of rs1, and begins to replay the log
4. but at the meantime, rs1 is still running, and the client still writes data to rs1
5. in this scenario, the data written after rs2 renamed rs1's hlog will be lost

The root cause of the problem is: 
As we all know, when we open a hdfs file for write, the file meta is only updated when a block is finished or when the file is closed. But the client thinks that the data is successfully written when it receives ack from datanode. Under this premise, after a file is renamed, the client is not required to update the meta immediately, so the client will not realize about the renaming, and will keep writing to the block, and will write successfully until the block is finished or the file is closed. The data written during this time will certainly be lost.

The basic idea about how to solve this is to add a freeze/seal semantics for a file, when a file is frozen/sealed, the client can't write any data to it, but it can be renamed or deleted.

If we can freeze/seal a file, the scenario at the beginning will like this:
1. rs1 loses its zookeeper lock, and hmaster realizes that
2. hmaster freezes/seals the hlog of rs1
3. hmaster assigns the regions of rs1 to rs2
4. rs2 renames the hlog of rs1, and begins to replay the log
5. after rs2 successfully replayed the log, the log file is deleted
6. in this scenario, after hmaster freezed/sealed the hlog file of rs1, rs1 can't write any data to it even if it is still running, this can guarantee no data will be lost

I hope I've described the problem clearly. Is there anyone has already worked on this feature? And any idea about this will be very appreciated."
java.net.SocketTimeoutException: 480000 millis timeout while waiting for channel to be ready for write exceptions were cast when trying to read file via StreamFile.,HDFS-693,"To exclude the case of network problem, I found the count of  dataXceiver is about 30.  Also, I could see the output of netstate -a | grep 50075 has many TIME_WAIT status when this happened.

partial log in attachment. 

"
DataNode is not responding After throwing java.lang.OutOfMemoryError: Direct buffer memory,HDFS-3600,"Scenario:
=========
Started NN with four DN's
written client program such that it will keep on write,append and read dta with 10 thread.
After 4 hours ,got OOME.Then DN listed under Dead it's not sending any heartbeats but GC is happening.
 *GC OPTS configured for DN* 
-Xms3G -Xmx4G -XX:NewSize=256M -XX:MaxNewSize=512M -XX:PermSize=128M -XX:MaxPermSize=128M -XX:CMSFullGCsBeforeCompaction=1 -XX:MaxDirectMemorySize=1G -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -XX:CMSInitiatingOccupancyFraction=65 -Xloggc:/home/install/hadoop/datanode/logs/datanode-root-gc.log -XX:+PrintGCDetails -XX:+DisableExplicitGC
 
 *CPU usage for DN* 
{noformat}
Tasks:   1 total,   0 running,   1 sleeping,   0 stopped,   0 zombie
Cpu(s):  0.1%us,  0.0%sy,  0.0%ni, 99.8%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st
Mem:     15955M total,    15806M used,      148M free,      436M buffers
Swap:    12284M total,        9M used,    12274M free,    11422M cached

  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND                              
 7431 root      20   0 6291m 2.6g  13m S    0 16.4  55:20.65 java  
{noformat}
 *JAVA Version* 
{noformat}
sun.boot.library.path = /root/nodesetup/java/jdk1.6.0_31/jre/lib/amd64
java version ""1.6.0_31""
Java(TM) SE Runtime Environment (build 1.6.0_31-b04)
Java HotSpot(TM) 64-Bit Server VM (build 20.6-b01, mixed mode)
{noformat}"
"WebHDFS have a ""not-enabled"" return code instead of using file not found  ",HDFS-3670,"WebHDFS (and HttpFs if it doesn't already) should have a return code indicating they are not enabled. Currently you get eg a file not found if webhdfs is not enabled which isn't intuitive.  

{code}
hadoop-3.0.0-SNAPSHOT $ ./bin/hadoop fs -get webhdfs://localhost:50070/user/eli/temp.json
get: Unexpected HTTP response: code=404 != 200, op=GETFILESTATUS, message=Not Found
{code}
"
Implement Background deletion,HDFS-1143,"Right now if you try to delete massive number of files from the namenode it will freeze (sometimes for minutes). Most of the time is spent going through the blocks map and invalidating all the blocks.
This can probably be improved by having a background GC process. The deletion will basically just remove the inode being deleted and then give the subtree that was just deleted to the background thread running cleanup.
This way the namenode becomes available for the clients soon after deletion, and all the heavy operations are done in the background.

Thoughts?"
HFDS CLI error tests fail with Avro RPC,HDFS-1076,"Some HDFS command-line tests (TestHDFSCLI) fail when using AvroRpcEngine because the error string does not match.  Calling getMessage() on a remote exception thrown by WritableRpcEngine produces a string that contains the exception name followed by its getMessage(), while exceptions thrown by AvroRpcEngine contain just the getMessage() string of the original exception."
"webhdfs setpermission,settimes,setowner api does not return any content upon success",HDFS-2431,"Request URI http://NN:50070/webhdfs/tmp/webhdfs_data/file_change_perm.txt?op=SETPERMISSION&permission=500
Request Method: PUT
Status Line: HTTP/1.1 200 OK
Response Content: <none>

We should return some success content."
webhdfs rest api's should send the content-length header consistently on all  calls,HDFS-2475,webhdfs rest api's on some calls return the content-length header in some calls but not on all. I think if possible we should return content-length header whenever possible
TestFileCreation occasionally fails because of an exception in DataStreamer.,HDFS-535,"One of test cases, namely {{testFsCloseAfterClusterShutdown()}}, of {{TestFileCreation}} fails occasionally."
start-all.sh namenode createSocketAddr,HDFS-2515,"2011-10-28 10:52:00,083 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.lang.IllegalArgumentException: Does not contain a valid host:port authority: file:///
at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:184)
at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)
at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)
at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:262)
at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:497)
at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1268)
at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1277)
"
Expose status of DNs from the NN programmatically,HDFS-2997,"Users who would like monitor the NN's view of DN health currently have two options: either parse the NN web UI, or call the (private API) DFSClient#datanodeReport. We should expose this in a nicer way, e.g. through metrics."
hdfs corruption appended to blocks is not detected by fs commands or fsck,HDFS-2848,"Courtesy Pat White [~patwhitey2007]
{quote}
Appears that there is a regression in corrupt block detection by both fsck and fs cmds like 'cat'. Testcases for
pre-block and block-overwrite corruption of all replicas is correctly reporting errors however post-block corruption is
not, fsck on the filesystem reports it's Healthy and 'cat' returns without error. Looking at the DN blocks themselves,
they clearly contain the injected corruption pattern.
{quote}"
FSDataset.invalidate() throws IOException trying to delete a block which is not in the data-node blockMap,HDFS-75,"Here is what I get in the data-node log. This is happening regularly, but not all the time.
{code}
07/08/21 15:22:20 INFO dfs.DataNode: Deleting block blk_5351209297984813226 file /hadoop-data/ndfs/data/current/blk_5351209297984813226
07/08/21 15:22:20 INFO dfs.DataNode: Deleting block blk_8240763237624393278 file /hadoop-data/ndfs/data/current/blk_8240763237624393278
07/08/21 15:22:21 WARN dfs.DataNode: Unexpected error trying to delete block blk_5351209297984813226. Block not found in blockMap. 
07/08/21 15:22:21 WARN dfs.DataNode: Unexpected error trying to delete block blk_8240763237624393278. Block not found in blockMap. 
07/08/21 15:22:21 WARN dfs.DataNode: java.io.IOException: Error in deleting blocks.
	at org.apache.hadoop.dfs.FSDataset.invalidate(FSDataset.java:714)
	at org.apache.hadoop.dfs.DataNode.processCommand(DataNode.java:590)
	at org.apache.hadoop.dfs.DataNode.offerService(DataNode.java:493)
	at org.apache.hadoop.dfs.DataNode.run(DataNode.java:1310)
	at java.lang.Thread.run(Thread.java:595)
{code}

The data-node successfully removed two blocks and then one second later received another request 
from the name-node to remove the same two blocks.
# I don't  see why the data-node should throw an exception in this case at all. It is a clear warning.
   Even if it does it should use the same description as in the warning message.
# Name-node calculates blocks to remove for a data-node in two cases: a heartbeat and a block-report.
   I suspect that this is what happened here. The 2 blocks were first requested to be removed in reply to
   the heartbeat and the in reply to the block report (or vice versa). 
   The question is why name-node returns the same blocks twice?"
Why TreeSet is used when collecting block information FSDataSet::getBlockReport,HDFS-1287,"As a return value we are converting this to array and returning and in name node also we are iterating ... so can we use list onstead of set. (As the block ids are unique, there may not be duplicates)"
ConfiguredFailoverProxyProvider should be moved to common.,HDFS-2806,"ConfiguredFailoverProxyProvider is a generally useful utility and it seems useful to keep it generic and move to common.
"
Generalize the HAServiceProtocol interface,HDFS-2354,"This JIRA intends to revisit the patches committed for HADOOP-7455 and HDFS-1974 & to provide more generic interfaces which allows alternative HA implementations to co-exist complying with HAServiceProtocol.

Some of the considerations are
1) Support life cycle methods (start*() and stop() APIs) in HAServiceProtocol
2) Support custom states in HAServiceProtocol
3) As per the patch submitted for HDFS-1974, Namenode implements HAService interface. This needs to be reconsidered.

I will elaborate on these points, in the form of comments below."
Warm standby does not read the in_progress edit log ,HDFS-2756,Warm standby does not read the in_progress edit log. This could result in standby taking a long time to become the primary during a failover scenario.
HDFS build failing in 0.22,HDFS-2787,"On a fresh install of Hadoop 0.22.0, I can get common to build fine, but when I try to run ""ant eclipse"" or ""ant jar"" on hdfs I get this:

[ivy:resolve] 	  http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/0.22.0-SNAPSHOT/hadoop-common-0.22.0-SNAPSHOT.jar
[ivy:resolve] 		::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve] 		::          UNRESOLVED DEPENDENCIES         ::
[ivy:resolve] 		::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve] 		:: org.apache.hadoop#hadoop-common;0.22.0-SNAPSHOT: not found
[ivy:resolve] 		::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve] 
[ivy:resolve] :: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS

BUILD FAILED
/home/hadoop/Desktop/hadoop-0.22.0/hdfs/build.xml:1814: impossible to resolve dependencies:
	resolve failed - see output for details


From my own investigation of the repo, I can see there is no ""0.22.0-SNAPSHOT"" folder,
there is instead a ""0.22.0"" folder, and inside of it there is hadoop-common-0.22.0.jar (without the -SNAPSHOT)."
NameNode.complete(..) may throw NullPointerException,HDFS-626,"We observe it in one of our clusters
{noformat}
2009-09-09 19:28:37,530 INFO org.apache.hadoop.ipc.Server: IPC Server handler 11 on 8020, call
complete(/path/to/a/file, DFSClient_attempt_200909091810_0062_r_000000_0) from xx.xx.xx.xx:xxxxx:
error: java.io.IOException: java.lang.NullPointerException
{noformat}"
dead datanodes because of OutOfMemoryError,HDFS-47,"We see more dead datanodes than in previous releases. The common exception is found in the out file:

Exception in thread ""org.apache.hadoop.dfs.DataBlockScanner@18166e5"" java.lang.OutOfMemoryError: Java heap space
Exception in thread ""DataNode: [dfs.data.dir-value]"" java.lang.OutOfMemoryError: Java heap space

"
/tmp/hadoop-${user}/dfs/tmp/tmp/client-${long}.tmp is not cleanup correctly,HDFS-67,"Diretory ""/tmp/hadoop-${user}/dfs/tmp/tmp"" is being filled with those kinfd of files: client-226966559287638337420857.tmp

I tried to look at the code and found:
h3. DFSClient.java
src/java/org/apache/hadoop/dfs/DFSClient.java
{code:java}
private void closeBackupStream() throws IOException {...}

/* Similar to closeBackupStream(). Theoritically deleting a file
 * twice could result in deleting a file that we should not.
 */
private void deleteBackupFile() {...}

private File newBackupFile() throws IOException {
String name = ""tmp"" + File.separator +
                     ""client-"" + Math.abs(r.nextLong());
File result = dirAllocator.createTmpFileForWrite(name,
                                                       2 * blockSize,
                                                       conf);
return result;
}
{code}

h3. LocalDirAllocator
src/java/org/apache/hadoop/fs/LocalDirAllocator.java#AllocatorPerContext.java
{code:java}
/** Creates a file on the local FS. Pass size as -1 if not known apriori. We
 *  round-robin over the set of disks (via the configured dirs) and return
 *  a file on the first path which has enough space. The file is guaranteed
 *  to go away when the JVM exits.
 */
public File createTmpFileForWrite(String pathStr, long size,
        Configuration conf) throws IOException {

// find an appropriate directory
Path path = getLocalPathForWrite(pathStr, size, conf);
File dir = new File(path.getParent().toUri().getPath());
String prefix = path.getName();

// create a temp file on this directory
File result = File.createTempFile(prefix, null, dir);
result.deleteOnExit();
return result;
}
{code}


First it seems to me it's a bit of a mess here I don't know if it's DFSClient.java#deleteBackupFile() or LocalDirAllocator#createTmpFileForWrite() {deleteOnExit(); ) who is call ... or both. Why not keep it dry and delete it only once.

But the most important is the ""deleteOnExit();"" since it mean if it is never restarted it will never delete files :("
handle return value of globStatus() to be uniform.,HDFS-103,"Some places in code does not expect null value from globStatus(Path path), they expect path. These have to be fixed to handle null to be uniform."
recentInvalidateSets in FSNamesystem is not required ,HDFS-82,"See HADOOP-2576 for more background. 

When a file is deleted, blocks are first placed in recentInvalidateSets and then later computeDatanodeWork moves it to 'invalidateSet' for each datanode. 

I could not see why a block is placed in this intermediate set. I think it is confusing as well.. for example, -metasave prints blocks from only one list. Unless we read very carefully its not easy to figure out that there are two lists. My proposal is to keep only one of them."
"if hadoop.tmp.dir is under your dfs.data.dir, HDFS will silently wipe out your ""name"" directory",HDFS-92,"I used a hadoop-site.xml conf file like:

  <property>
    <name>dfs.data.dir</name>
    <value>/data01/hadoop</value>
    <description>Dirs to store data on.</description>
  </property>
  <property>
    <name>hadoop.tmp.dir</name>
    <value>/data01/hadoop/tmp</value>
    <description>A base for other temporary directories.</description>
  </property>

This file will format the namenode properly.  Upon startup with the bin/start-dfs.sh script, however, the /data01/hadoop/tmp/dfs/name directory is silently wiped out.  This foobars the namenode, but only after the next DFS stop/start cycle.  (see output below)

This is obviously a configuration error first and foremost, but the fact that hadoop silently corrupts itself makes it tricky to track down.

[hid191]$ bin/hadoop namenode -format
08/04/04 18:41:43 INFO dfs.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = hid191.dev01.corp.metaweb.com/127.0.0.1
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 0.16.2
STARTUP_MSG:   build = http://svn.apache.org/repos/asf/hadoop/core/branches/branch-0.16 -r 642481; compiled by 'hadoopqa' on Sat Mar 29 01:59:04 UTC 2008
************************************************************/
08/04/04 18:41:43 INFO fs.FSNamesystem: fsOwner=zenkat,users
08/04/04 18:41:43 INFO fs.FSNamesystem: supergroup=supergroup
08/04/04 18:41:43 INFO fs.FSNamesystem: isPermissionEnabled=true
08/04/04 18:41:43 INFO dfs.Storage: Storage directory /data01/hadoop/tmp/dfs/name has been successfully formatted.
08/04/04 18:41:43 INFO dfs.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at hid191.dev01.corp.metaweb.com/127.0.0.1
************************************************************/
[hid191]$ ls /data01/hadoop/tmp/dfs/name
current  image
[hid191]$ bin/start-dfs.sh 
starting namenode, logging to /data01/hadoop/logs/hadoop-zenkat-namenode-hid191.out
localhost: starting datanode, logging to /data01/hadoop/logs/hadoop-zenkat-datanode-hid191.out
localhost: starting secondarynamenode, logging to /data01/hadoop/logs/hadoop-zenkat-secondarynamenode-hid191.out
[hid191]$ ls /data01/hadoop/tmp/dfs/name
ls: cannot access /data01/hadoop/tmp/dfs/name: No such file or directory


"
A Datanode's datadir could have lots of blocks in the top-level directory,HDFS-57,"When a datanode restarts, it moves all the blocks from the datadir's tmp directory into the top-level of the datadir. It does not move these blocks into subdirectories of the datadir."
delete on dfs hung,HDFS-64,"I had a case where the JobTracker was trying to delete some files, as part of Garbage Collect for a job, in a dfs directory. The thread hung and this is the trace:

Thread 19 (IPC Server handler 5 on 57344):
  State: WAITING
  Blocked count: 137022
  Waited count: 336004
  Waiting on org.apache.hadoop.ipc.Client$Call@eb6238
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.Object.wait(Object.java:485)
    org.apache.hadoop.ipc.Client.call(Client.java:683)
    org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:216)
    org.apache.hadoop.dfs.$Proxy4.delete(Unknown Source)
    sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    java.lang.reflect.Method.invoke(Method.java:597)
    org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
    org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
    org.apache.hadoop.dfs.$Proxy4.delete(Unknown Source)
    org.apache.hadoop.dfs.DFSClient.delete(DFSClient.java:515)
    org.apache.hadoop.dfs.DistributedFileSystem.delete(DistributedFileSystem.java:170)
    org.apache.hadoop.fs.FileUtil.fullyDelete(FileUtil.java:118)
    org.apache.hadoop.fs.FileUtil.fullyDelete(FileUtil.java:114)
    org.apache.hadoop.mapred.JobInProgress.garbageCollect(JobInProgress.java:1635)
    org.apache.hadoop.mapred.JobInProgress.isJobComplete(JobInProgress.java:1387)
    org.apache.hadoop.mapred.JobInProgress.completedTask(JobInProgress.java:1348)
    org.apache.hadoop.mapred.JobInProgress.updateTaskStatus(JobInProgress.java:565)
    org.apache.hadoop.mapred.JobTracker.updateTaskStatuses(JobTracker.java:2032)

and it hung for an enormously long amount of time ~1 hour. 

Not sure whether these will help:

I saw this message in the NameNode log around the time the delete was issued by the JobTracker
2008-05-07 09:55:57,375 WARN org.apache.hadoop.dfs.StateChange: DIR* FSDirectory.unprotectedDelete: failed to remove /mapredsystem/ddas/mapredsystem/10091.{running.machine.com}/job_200805070458_0004 because it does not exist

I also checked that the directory in question was actually there (and the job couldn't have run without this directory being there)."
No recovery when trying to replicate on marginal datanode,HDFS-59,"We have been uploading a lot of data to hdfs, running about 400 scripts in parallel calling hadoop's command line utility in distributed fashion. Many of them started to hang when copying large files (>120GB), repeating the following messages without end:

07/10/05 15:44:25 INFO fs.DFSClient: Could not complete file, retrying...
07/10/05 15:44:26 INFO fs.DFSClient: Could not complete file, retrying...
07/10/05 15:44:26 INFO fs.DFSClient: Could not complete file, retrying...
07/10/05 15:44:27 INFO fs.DFSClient: Could not complete file, retrying...
07/10/05 15:44:27 INFO fs.DFSClient: Could not complete file, retrying...
07/10/05 15:44:28 INFO fs.DFSClient: Could not complete file, retrying...

In the namenode log I eventually found repeated messages like:

2007-10-05 14:40:08,063 WARN org.apache.hadoop.fs.FSNamesystem: PendingReplicationMonitor timed out block blk_3124504920241431462
2007-10-05 14:40:11,876 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.pendingTransfer: ask <IP4>50010 to replicate blk_3124504920241431462 to datanode(s) <IP4_1>:50010
2007-10-05 14:45:08,069 WARN org.apache.hadoop.fs.FSNamesystem: PendingReplicationMonitor timed out block blk_8533614499490422104
2007-10-05 14:45:08,070 WARN org.apache.hadoop.fs.FSNamesystem: PendingReplicationMonitor timed out block blk_7741954594593177224
2007-10-05 14:45:13,973 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.pendingTransfer: ask <IP4>:50010 to replicate blk_7741954594593177224 to datanode(s) <IP4_2>:50010
2007-10-05 14:45:13,973 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.pendingTransfer: ask <IP4>:50010 to replicate blk_8533614499490422104 to datanode(s) <IP4_3>50010

I could not ssh to the  node with IpAdress <IP4>, but seemingly the datanode server still sent heartbeats. After rebooting the node it  was okay again and a few files and a few clients recovered, but not all.
I restarted these clients and they completed this time (before noticing the marginal node we restarted the clients twice without success).

I would conclude that the existence of the marginal node must have caused loss of blocks, at least in the tracking mechanism, in addition to eternal retries.

In summary, dfs should be able to handle datanodes with good heartbeat but otherwise failing to do their job. This should include datanodes that have a high rate of socket connection timeouts.


"
"""hadoop dfs -put"" does not return nonzero status on failure",HDFS-12,"I'm attempting to put a file on DFS with the ""hadoop dfs -put"" command.  The put is failing, probably because my cluster is still being initialized, but the command is still returning a status of 0.  

If there was a meaningful error status, I'd be able to handle the situation (in my case, waiting and putting again works).

The output is telling me there is a NotReplicatedYetException; it's a new cluster and the nodes are still being initialized.

Here's the beginning of the output; it tries a few times, but eventually gives up.

executing: source ~/.bash_profile; hadoop dfs -put ./vectorfile input/vectorfile
08/08/21 13:06:00 WARN fs.FileSystem: ""ip-10-251-195-162.ec2.internal:50001"" is a deprecated filesystem name. Use ""hdfs://ip-10-251-195-162.ec2.internal:50001/"" instead.
08/08/21 13:06:00 WARN fs.FileSystem: ""ip-10-251-195-162.ec2.internal:50001"" is a deprecated filesystem name. Use ""hdfs://ip-10-251-195-162.ec2.internal:50001/"" instead.
08/08/21 13:06:00 WARN fs.FileSystem: ""ip-10-251-195-162.ec2.internal:50001"" is a deprecated filesystem name. Use ""hdfs://ip-10-251-195-162.ec2.internal:50001/"" instead.
08/08/21 13:06:00 WARN fs.FileSystem: ""ip-10-251-195-162.ec2.internal:50001"" is a deprecated filesystem name. Use ""hdfs://ip-10-251-195-162.ec2.internal:50001/"" instead.
08/08/21 13:06:01 INFO dfs.DFSClient: org.apache.hadoop.ipc.RemoteException: java.io.IOException: File /user/root/input/vectorfile could only be replicated to 0 nodes, instead of 1
	at org.apache.hadoop.dfs.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1117)
	at org.apache.hadoop.dfs.NameNode.addBlock(NameNode.java:330)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:452)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:888)

	at org.apache.hadoop.ipc.Client.call(Client.java:715)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:216)
	at org.apache.hadoop.dfs.$Proxy0.addBlock(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
	at org.apache.hadoop.dfs.$Proxy0.addBlock(Unknown Source)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.locateFollowingBlock(DFSClient.java:2440)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2323)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.access$1800(DFSClient.java:1735)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:1912)

08/08/21 13:06:01 WARN dfs.DFSClient: NotReplicatedYetException sleeping /user/root/input/vectorfile retries left 4"
Help information of refreshNodes does not show how to decomission nodes,HDFS-22,"The help information does not indicate how to decommission nodes.

It only describes two scenarios:
* to stop nodes if not in dfs.hosts
* stop decommissioning if node is decommissioning and in both dfs.hosts and dfs.host.exclude

but omits this one:
* starting decommissioning if node is in service and  in both dfs.hosts and dfs.host.exclude

It would better describe as ""Each entry defined in dfs.hosts and also
in dfs.host.exclude is start decommissioning and start block replication
if it is in service, or is stopped from decommissioning if it has already
been marked for decommission."""
fsck <path> -delete doesn't report failures,HDFS-20,"When I have safemode on and do fsck / -delete, it legitimately fails on the first delete. However, the fsck stops and does not report the failure."
processIOError() may cause infinite loop.,HDFS-3,"In {{FSEditLog}} method {{logEdit()}} calls {{processIOError()}}, which calls {{incrementCheckpointTime()}}, which in turn again calls {{logEdit()}}."
TestHDFSCLI test failures,HDFS-1372,"A number of the TestHDFSCLI tests fail on trunk. Lots of messages in the log about turning safe mode off makes me think the NN entered safemode unexpectedly and refused any of the operations that add or remove files.

{noformat}
10/09/03 01:22:54 INFO cli.CLITestHelper: Detailed results:
10/09/03 01:22:54 INFO cli.CLITestHelper: ----------------------------------

10/09/03 01:22:54 INFO cli.CLITestHelper: -------------------------------------------
10/09/03 01:22:54 INFO cli.CLITestHelper:                     Test ID: [31]
10/09/03 01:22:54 INFO cli.CLITestHelper:            Test Description: [du: file using absolute path]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -put file:/home/eli/src/hdfs1/build/test/cache//data15bytes /data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -du /data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:            Cleanup Commands: [-fs NAMENODE -rm /data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [TokenComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [fail]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [Found 1 items]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [du: Cannot access /data15bytes: No such file or directory.
]
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [RegexpComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [fail]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [^15( |\t)*hdfs://localhost[.a-z]*:[0-9]*/data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [du: Cannot access /data15bytes: No such file or directory.
]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper: -------------------------------------------
10/09/03 01:22:54 INFO cli.CLITestHelper:                     Test ID: [32]
10/09/03 01:22:54 INFO cli.CLITestHelper:            Test Description: [du: file using relative path]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -put file:/home/eli/src/hdfs1/build/test/cache//data15bytes data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -du data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:            Cleanup Commands: [-fs NAMENODE -rmr /user]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [TokenComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [fail]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [Found 1 items]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [du: Cannot access data15bytes: No such file or directory.
]
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [RegexpComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [fail]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [^15( |\t)*hdfs://localhost[.a-z]*:[0-9]*/user/[a-z]*/data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [du: Cannot access data15bytes: No such file or directory.
]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper: -------------------------------------------
10/09/03 01:22:54 INFO cli.CLITestHelper:                     Test ID: [33]
10/09/03 01:22:54 INFO cli.CLITestHelper:            Test Description: [du: files using globbing]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -put file:/home/eli/src/hdfs1/build/test/cache//data15bytes data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -put file:/home/eli/src/hdfs1/build/test/cache//data30bytes data30bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -put file:/home/eli/src/hdfs1/build/test/cache//data60bytes data60bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -put file:/home/eli/src/hdfs1/build/test/cache//data120bytes data120bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -du data*]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:            Cleanup Commands: [-fs NAMENODE -rmr /user]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [TokenComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [fail]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [Found 4 items]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 2 items
120  hdfs://localhost:34212/user/eli/data120bytes
60   hdfs://localhost:34212/user/eli/data60bytes
]
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [RegexpComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [fail]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [^15( |\t)*hdfs://localhost[.a-z]*:[0-9]*/user/[a-z]*/data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 2 items
120  hdfs://localhost:34212/user/eli/data120bytes
60   hdfs://localhost:34212/user/eli/data60bytes
]
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [RegexpComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [fail]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [^30( |\t)*hdfs://localhost[.a-z]*:[0-9]*/user/[a-z]*/data30bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 2 items
120  hdfs://localhost:34212/user/eli/data120bytes
60   hdfs://localhost:34212/user/eli/data60bytes
]
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [RegexpComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [pass]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [^60( |\t)*hdfs://localhost[.a-z]*:[0-9]*/user/[a-z]*/data60bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 2 items
120  hdfs://localhost:34212/user/eli/data120bytes
60   hdfs://localhost:34212/user/eli/data60bytes
]
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [RegexpComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [pass]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [^120( |\t)*hdfs://localhost[.a-z]*:[0-9]*/user/[a-z]*/data120bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 2 items
120  hdfs://localhost:34212/user/eli/data120bytes
60   hdfs://localhost:34212/user/eli/data60bytes
]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper: -------------------------------------------
10/09/03 01:22:54 INFO cli.CLITestHelper:                     Test ID: [34]
10/09/03 01:22:54 INFO cli.CLITestHelper:            Test Description: [du: directory using absolute path]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -mkdir /dir0]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -put file:/home/eli/src/hdfs1/build/test/cache//data15bytes /dir0/data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -du /dir0]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:            Cleanup Commands: [-fs NAMENODE -rmr /dir0]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [TokenComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [fail]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [Found 1 items]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 0 items
]
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [RegexpComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [fail]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [^15( |\t)*hdfs://localhost[.a-z]*:[0-9]*/dir0/data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 0 items
]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper: -------------------------------------------
10/09/03 01:22:54 INFO cli.CLITestHelper:                     Test ID: [35]
10/09/03 01:22:54 INFO cli.CLITestHelper:            Test Description: [du: directory using relative path]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -mkdir dir0]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -put file:/home/eli/src/hdfs1/build/test/cache//data15bytes dir0/data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -du dir0]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:            Cleanup Commands: [-fs NAMENODE -rmr /user]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [TokenComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [fail]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [Found 1 items]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 0 items
]
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [RegexpComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [fail]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [^15( |\t)*hdfs://localhost[.a-z]*:[0-9]*/user/[a-z]*/dir0/data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 0 items
]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper: -------------------------------------------
10/09/03 01:22:54 INFO cli.CLITestHelper:                     Test ID: [36]
10/09/03 01:22:54 INFO cli.CLITestHelper:            Test Description: [du: directory using globbing]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -mkdir /dir0]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -put file:/home/eli/src/hdfs1/build/test/cache//data15bytes /dir0/data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -put file:/home/eli/src/hdfs1/build/test/cache//data30bytes /dir0/data30bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -put file:/home/eli/src/hdfs1/build/test/cache//data60bytes /dir0/data60bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -put file:/home/eli/src/hdfs1/build/test/cache//data120bytes /dir0/data120bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -du /dir0/*]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:            Cleanup Commands: [-fs NAMENODE -rmr /dir0]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [TokenComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [fail]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [Found 4 items]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 2 items
120  hdfs://localhost:34212/dir0/data120bytes
60   hdfs://localhost:34212/dir0/data60bytes
]
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [RegexpComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [fail]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [^15( |\t)*hdfs://localhost[.a-z]*:[0-9]*/dir0/data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 2 items
120  hdfs://localhost:34212/dir0/data120bytes
60   hdfs://localhost:34212/dir0/data60bytes
]
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [RegexpComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [fail]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [^30( |\t)*hdfs://localhost[.a-z]*:[0-9]*/dir0/data30bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 2 items
120  hdfs://localhost:34212/dir0/data120bytes
60   hdfs://localhost:34212/dir0/data60bytes
]
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [RegexpComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [pass]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [^60( |\t)*hdfs://localhost[.a-z]*:[0-9]*/dir0/data60bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 2 items
120  hdfs://localhost:34212/dir0/data120bytes
60   hdfs://localhost:34212/dir0/data60bytes
]
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [RegexpComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [pass]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [^120( |\t)*hdfs://localhost[.a-z]*:[0-9]*/dir0/data120bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 2 items
120  hdfs://localhost:34212/dir0/data120bytes
60   hdfs://localhost:34212/dir0/data60bytes
]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper: -------------------------------------------
10/09/03 01:22:54 INFO cli.CLITestHelper:                     Test ID: [37]
10/09/03 01:22:54 INFO cli.CLITestHelper:            Test Description: [du: Test for hdfs:// path - file]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -put file:/home/eli/src/hdfs1/build/test/cache//data15bytes hdfs:///data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -du hdfs:///data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:            Cleanup Commands: [-fs NAMENODE -rm hdfs:///data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [TokenComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [fail]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [Found 1 items]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [du: Cannot access hdfs:///data15bytes: No such file or directory.
]
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [RegexpComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [fail]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [^15( |\t)*hdfs://localhost[.a-z]*:[0-9]*/data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [du: Cannot access hdfs:///data15bytes: No such file or directory.
]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper: -------------------------------------------
10/09/03 01:22:54 INFO cli.CLITestHelper:                     Test ID: [38]
10/09/03 01:22:54 INFO cli.CLITestHelper:            Test Description: [du: Test for hdfs:// path - files using globbing]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -put file:/home/eli/src/hdfs1/build/test/cache//data15bytes hdfs:///data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -put file:/home/eli/src/hdfs1/build/test/cache//data30bytes hdfs:///data30bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -put file:/home/eli/src/hdfs1/build/test/cache//data60bytes hdfs:///data60bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -put file:/home/eli/src/hdfs1/build/test/cache//data120bytes hdfs:///data120bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -du hdfs:///data*]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:            Cleanup Commands: [-fs NAMENODE -rmr hdfs:///*]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [TokenComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [fail]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [Found 4 items]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 2 items
120  hdfs://localhost:34212/data120bytes
60   hdfs://localhost:34212/data60bytes
]
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [RegexpComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [fail]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [^15( |\t)*hdfs://localhost[.a-z]*:[0-9]*/data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 2 items
120  hdfs://localhost:34212/data120bytes
60   hdfs://localhost:34212/data60bytes
]
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [RegexpComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [fail]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [^30( |\t)*hdfs://localhost[.a-z]*:[0-9]*/data30bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 2 items
120  hdfs://localhost:34212/data120bytes
60   hdfs://localhost:34212/data60bytes
]
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [RegexpComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [pass]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [^60( |\t)*hdfs://localhost[.a-z]*:[0-9]*/data60bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 2 items
120  hdfs://localhost:34212/data120bytes
60   hdfs://localhost:34212/data60bytes
]
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [RegexpComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [pass]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [^120( |\t)*hdfs://localhost[.a-z]*:[0-9]*/data120bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 2 items
120  hdfs://localhost:34212/data120bytes
60   hdfs://localhost:34212/data60bytes
]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper: -------------------------------------------
10/09/03 01:22:54 INFO cli.CLITestHelper:                     Test ID: [39]
10/09/03 01:22:54 INFO cli.CLITestHelper:            Test Description: [du: Test for hdfs:// path - directory]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -mkdir hdfs:///dir0]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -put file:/home/eli/src/hdfs1/build/test/cache//data15bytes hdfs:///dir0/data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -du hdfs:///dir0]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:            Cleanup Commands: [-fs NAMENODE -rmr hdfs:///dir0]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [TokenComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [fail]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [Found 1 items]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 0 items
]
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [RegexpComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [fail]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [^15( |\t)*hdfs://localhost[.a-z]*:[0-9]*/dir0/data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 0 items
]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper: -------------------------------------------
10/09/03 01:22:54 INFO cli.CLITestHelper:                     Test ID: [40]
10/09/03 01:22:54 INFO cli.CLITestHelper:            Test Description: [duh: Test for hdfs:// path - directory]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -mkdir hdfs:///dir0]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -put file:/home/eli/src/hdfs1/build/test/cache//data15bytes hdfs:///dir0/data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -put file:/home/eli/src/hdfs1/build/test/cache//data1k hdfs:///dir0/data1k]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -du -h hdfs:///dir0]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:            Cleanup Commands: [-fs NAMENODE -rmr hdfs:///dir0]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [TokenComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [fail]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [Found 2 items]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 1 items
1.0k  hdfs://localhost:34212/dir0/data1k
]
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [RegexpComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [fail]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [^15( |\t)*hdfs://localhost[.a-z]*:[0-9]*/dir0/data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 1 items
1.0k  hdfs://localhost:34212/dir0/data1k
]
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [RegexpComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [pass]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [^1.0k( |\t)*hdfs://localhost[.a-z]*:[0-9]*/dir0/data1k]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 1 items
1.0k  hdfs://localhost:34212/dir0/data1k
]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper: -------------------------------------------
10/09/03 01:22:54 INFO cli.CLITestHelper:                     Test ID: [41]
10/09/03 01:22:54 INFO cli.CLITestHelper:            Test Description: [du: Test for hdfs:// path - directory using globbing]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -mkdir hdfs:///dir0]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -put file:/home/eli/src/hdfs1/build/test/cache//data15bytes hdfs:///dir0/data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -put file:/home/eli/src/hdfs1/build/test/cache//data30bytes hdfs:///dir0/data30bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -put file:/home/eli/src/hdfs1/build/test/cache//data60bytes hdfs:///dir0/data60bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -put file:/home/eli/src/hdfs1/build/test/cache//data120bytes hdfs:///dir0/data120bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -du hdfs:///dir0/*]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:            Cleanup Commands: [-fs NAMENODE -rmr hdfs:///dir0]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [TokenComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [fail]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [Found 4 items]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 2 items
120  hdfs://localhost:34212/dir0/data120bytes
60   hdfs://localhost:34212/dir0/data60bytes
]
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [RegexpComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [fail]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [^15( |\t)*hdfs://localhost[.a-z]*:[0-9]*/dir0/data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 2 items
120  hdfs://localhost:34212/dir0/data120bytes
60   hdfs://localhost:34212/dir0/data60bytes
]
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [RegexpComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [fail]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [^30( |\t)*hdfs://localhost[.a-z]*:[0-9]*/dir0/data30bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 2 items
120  hdfs://localhost:34212/dir0/data120bytes
60   hdfs://localhost:34212/dir0/data60bytes
]
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [RegexpComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [pass]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [^60( |\t)*hdfs://localhost[.a-z]*:[0-9]*/dir0/data60bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 2 items
120  hdfs://localhost:34212/dir0/data120bytes
60   hdfs://localhost:34212/dir0/data60bytes
]
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [RegexpComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [pass]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [^120( |\t)*hdfs://localhost[.a-z]*:[0-9]*/dir0/data120bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 2 items
120  hdfs://localhost:34212/dir0/data120bytes
60   hdfs://localhost:34212/dir0/data60bytes
]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper: -------------------------------------------
10/09/03 01:22:54 INFO cli.CLITestHelper:                     Test ID: [42]
10/09/03 01:22:54 INFO cli.CLITestHelper:            Test Description: [du: Test for Namenode's path - file]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -put file:/home/eli/src/hdfs1/build/test/cache//data15bytes NAMENODE/data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -du NAMENODE/data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:            Cleanup Commands: [-fs NAMENODE -rm NAMENODE/data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [TokenComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [fail]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [Found 1 items]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [du: Cannot access hdfs://localhost:34212/data15bytes: No such file or directory.
]
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [RegexpComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [fail]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [^15( |\t)*hdfs://localhost[.a-z]*:[0-9]*/data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [du: Cannot access hdfs://localhost:34212/data15bytes: No such file or directory.
]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper: -------------------------------------------
10/09/03 01:22:54 INFO cli.CLITestHelper:                     Test ID: [43]
10/09/03 01:22:54 INFO cli.CLITestHelper:            Test Description: [du: Test for Namenode's path - files using globbing]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -put file:/home/eli/src/hdfs1/build/test/cache//data15bytes NAMENODE/data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -put file:/home/eli/src/hdfs1/build/test/cache//data30bytes NAMENODE/data30bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -put file:/home/eli/src/hdfs1/build/test/cache//data60bytes NAMENODE/data60bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -put file:/home/eli/src/hdfs1/build/test/cache//data120bytes NAMENODE/data120bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Test Commands: [-fs NAMENODE -du NAMENODE/data*]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:            Cleanup Commands: [-fs NAMENODE -rmr NAMENODE/*]
10/09/03 01:22:54 INFO cli.CLITestHelper: 
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [TokenComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [fail]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [Found 4 items]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 2 items
120  hdfs://localhost:34212/data120bytes
60   hdfs://localhost:34212/data60bytes
]
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [RegexpComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [fail]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [^15( |\t)*hdfs://localhost[.a-z]*:[0-9]*/data15bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 2 items
120  hdfs://localhost:34212/data120bytes
60   hdfs://localhost:34212/data60bytes
]
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [RegexpComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [fail]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [^30( |\t)*hdfs://localhost[.a-z]*:[0-9]*/data30bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 2 items
120  hdfs://localhost:34212/data120bytes
60   hdfs://localhost:34212/data60bytes
]
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [RegexpComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [pass]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [^60( |\t)*hdfs://localhost[.a-z]*:[0-9]*/data60bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 2 items
120  hdfs://localhost:34212/data120bytes
60   hdfs://localhost:34212/data60bytes
]
10/09/03 01:22:54 INFO cli.CLITestHelper:                  Comparator: [RegexpComparator]
10/09/03 01:22:54 INFO cli.CLITestHelper:          Comparision result:   [pass]
10/09/03 01:22:54 INFO cli.CLITestHelper:             Expected output:   [^120( |\t)*hdfs://localhost[.a-z]*:[0-9]*/data120bytes]
10/09/03 01:22:54 INFO cli.CLITestHelper:               Actual output:   [Found 2 items
120  hd"
TestBlockRecovery#testZeroLenReplicas fails,HDFS-1374,"{noformat}
Testcase: testZeroLenReplicas took 10.577 sec
        Caused an ERROR

Wanted but not invoked:
datanodeProtocol.commitBlockSynchronization(
    blk_1000_2000,
    3000,
    0,
    true,
    true,
    []
);
-> at org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery.testZeroLenReplicas(TestBlockRecovery.java:402)

However, there were other interactions with this mock:
-> at org.apache.hadoop.hdfs.server.datanode.DataNode.handshake(DataNode.java:519)

Wanted but not invoked:
datanodeProtocol.commitBlockSynchronization(
    blk_1000_2000,
    3000,
    0,
    true,
    true,
    []
);
-> at org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery.testZeroLenReplicas(TestBlockRecovery.java:402)

However, there were other interactions with this mock:
-> at org.apache.hadoop.hdfs.server.datanode.DataNode.handshake(DataNode.java:519)

        at org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery.testZeroLenReplicas(TestBlockRecovery.java:402)
{noformat}"
Distcp not working due to NoClassDefFoundError: org/apache/hadoop/tools/DistCp,HDFS-2370,"$HADOOP_COMMON_HOME/bin/hadoop --config $HADOOP_CONF_DIR  distcp
hdfs://<NN1>:8020/tmp/input 
hdfs://<NN1>:8020/tmp/output
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/tools/DistCp
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.tools.DistCp
        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:307)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
Could not find the main class: org.apache.hadoop.tools.DistCp.  Program will exit.

it used to work before, do we need to export any environment variable? does distcp changed its place?"
hdfs unit test failures on trunk,HDFS-2278,"Following unit tests fail on hdfs trunk
org.apache.hadoop.hdfs.TestDfsOverAvroRpc.testWorkingDirectory
org.apache.hadoop.hdfs.server.blockmanagement.TestHost2NodesMap.testGetDatanodeByHost  
org.apache.hadoop.hdfs.server.blockmanagement.TestHost2NodesMap.testGetDatanodeByName 
org.apache.hadoop.hdfs.server.datanode.TestReplicasMap.testGet 
org.apache.hadoop.hdfs.tools.offlineEditsViewer.TestOfflineEditsViewer.testStored 
Console output: https://builds.apache.org/job/Hadoop-Hdfs-trunk/lastCompletedBuild/consoleFull"
TestNodeCount is failing with NPE,HDFS-1740,"As seen at least one TestNodeCount is failing on current trunk (0.23) with NPE:
{noformat}
Error Message:
null

Stack Trace:
java.lang.NullPointerException
       at org.apache.hadoop.hdfs.server.namenode.BlockManager.countNodes(BlockManager.java:1431)
       at org.apache.hadoop.hdfs.server.namenode.TestNodeCount.__CLR3_0_29bdgm6s8c(TestNodeCount.java:90)
       at org.apache.hadoop.hdfs.server.namenode.TestNodeCount.testNodeCount(TestNodeCount.java:40)
{noformat}"
har://hftp-<hostname>:<port>/ or har://hftp-<hostname>/ does not seems to work,HDFS-2457,"$ hadoop  jar $HADOOP_HOME/hadoop-examples.jar sort  har://hftp-<Namenode hostname>:50070/tmp/ARCHIVE.har/random/part-00000 /tmp/out

Running on 15 nodes to sort from
har://hftp-<Namenode hostname>:50070/tmp/ARCHIVE.har/random/part-00000 into
hdfs://<Namenode hostname>/tmp/out with 27 reduces.
Job started: Sat Oct 15 02:04:44 UTC 2011
11/10/15 02:04:44 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 43 for hadoopqa on
<Namenode hostname>:8020
11/10/15 02:04:44 INFO security.TokenCache: Got dt for
hdfs://<Namenode hostname>/user/hadoopqa/.staging/job_201110142346_0038;uri=<Namenode hostname>:8020;t.service=<Namenode hostname>:8020
11/10/15 02:04:45 INFO mapred.JobClient: Cleaning up the staging area
hdfs://<Namenode hostname>/user/hadoopqa/.staging/job_201110142346_0038
java.io.IOException: Can't seek!
        at org.apache.hadoop.hdfs.HftpFileSystem$3.seek(HftpFileSystem.java:359)
        at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:37)
        at org.apache.hadoop.fs.HarFileSystem$HarMetaData.parseMetaData(HarFileSystem.java:1055)
        at org.apache.hadoop.fs.HarFileSystem$HarMetaData.access$000(HarFileSystem.java:966)
        at org.apache.hadoop.fs.HarFileSystem.initialize(HarFileSystem.java:137)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1328)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:241)
        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:91)
        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:79)
        at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:160)
        at org.apache.hadoop.mapred.SequenceFileInputFormat.listStatus(SequenceFileInputFormat.java:40)
        at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:208)
        at org.apache.hadoop.mapred.JobClient.writeOldSplits(JobClient.java:981)
        at org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:973)
        at org.apache.hadoop.mapred.JobClient.access$600(JobClient.java:172)
        at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:889)
        at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:842)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)
        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:842)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:816)
        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1253)
        at org.apache.hadoop.examples.Sort.run(Sort.java:176)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.examples.Sort.main(Sort.java:187)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)
        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)
        at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:64)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)


$ hadoop  jar $HADOOP_HOME/hadoop-examples.jar sort  har://hftp-<Namenode hostname>/tmp/ARCHIVE.har/random/part-00000 /tmp/out

Also gives the same error .

Is it expected ? "
Additional QA tasks for Edit Log branch,HDFS-1896,"As we close out tasks in the HDFS-1073 branch, there are a few places where I've noticed that we lack some test coverage. Creating this ticket just as a place to jot down some notes on things that we ought to make sure are tested, preferably by automated (unit) tests."
Recurring failure of TestMissingBlocksAlert on branch-0.22,HDFS-2013,This has been failing on Hudson for the last two builds and fails on my local box as well.
add Kerberos HTTP SPNEGO authentication support to Hadoop JT/NN/DN/TT web-consoles,HDFS-1604,This JIRA is for the HDFS portion of HADOOP-7119
Use new Trash Emptier methods,HDFS-2150,"This issue is for HADOOP-7460 changes which are required in HDFS.

Since, we are implementing the pluggable trash policy in Common, changes required in HDFS to use that policy."
 Periodically move blocks from full nodes to those with space,HDFS-339,"Continuance of Hadoop-386. The patch to that issue makes it possible to redistribute blocks (change replication up, wait for replication to succeed, then lower replication again). However, this requires a lot more space, is not automatic, and doesn't respect a reasonable I/O limit. I have actually had MapReduce jobs fail from block missing execptions after having recently changed the replication level (from 3 to 4, with no underreplications to start with) because the datanodes were too slow responding to requests while performing the necessary replications.

A good fix to this problem would be a low-priority thread on the NameNode that schedules low-priority replications of blocks on over-full machines, followed by the removal of the extra replications. It might be worth having a specific prototocol for asking for these low-priority copies to happen in the datanodes, so that they continue to service (and be available to service) normal block requests."
ClusterTestDFS fails,HDFS-112,"
The dfs unit tests, from the ant target 'cluster' have been failing. (ClusterTestDFSNamespaceLogging, ClusterTestDFS). I don't know if anyone but me cares about these tests, but I do. I would like to write better tests for dns. I think we all need that.

They have been partially broken since  ""test.dfs.same.host.targets.allowed"" went away and replication ceased for these tests. 

They got really broken when NameNode stopped automatically formatting itself .

Since they seem to be ignored, I took the liberty of changing how they work.

The main thing is, you must put this into your hosts file:

127.0.0.1       localhost0
127.0.0.1       localhost1
127.0.0.1       localhost2
127.0.0.1       localhost3
127.0.0.1       localhost4
127.0.0.1       localhost5
127.0.0.1       localhost6
127.0.0.1       localhost7
127.0.0.1       localhost8
127.0.0.1       localhost9
127.0.0.1       localhost10
127.0.0.1       localhost11
127.0.0.1       localhost12
127.0.0.1       localhost13
127.0.0.1       localhost14
127.0.0.1       localhost15

This way you can start DataNodes, and TaskTrackers (up to 16 of them) with unique hostnames.

Also, I changed all the places that used to call InetAddress.getLocalHost().getHostName() to get it from a new method in Configuration (this issue is the same as http://issues.apache.org/jira/browse/HADOOP-197 ).
"
restartDFS causing a hang when running on Windows. Makes other test cases fail.,HDFS-2151,"When running all the namenode testcases on Windows via Eclipse, testRestartDFS runs first, however after printing out ""Shutting down the Mini HDFS Cluster"", it proceeds to hang, leading to timeout, and causing other tests to get an error saying ""Cannot remove directory X"".

Not to mention the test itself takes over 10 minutes to run.

This may strictly be a Windows environment error."
"No error message returned when we use ""hdfs dfs -copyFromLocal"" in error ",HDFS-2125,"while using ""dfs -copyFromLocal"" without dest path the error message is inadequate.
For instance:

$hdfs dfs -copyFromLocal /work/tmp/xg
copyFromLocal: `.': File exists

However, more useful message is expected."
"Use ""dfsadmin -setQuota <N> <directory>...<directory>""  without faults reported ",HDFS-2113,"use ""dfsadmin -setQuota <N> <directory"" to set the name quota to be N for the directory whether the directory has been set or not, without faults reported if the directory would immediately exceed the quota ."
Read lock must be released before acquiring a write lock,HDFS-2103,"In FSNamesystem.getBlockLocationsUpdateTimes function, we have the following code:

{code}
    for (int attempt = 0; attempt < 2; attempt++) {
      if (attempt == 0) { // first attempt is with readlock
        readLock();
      }  else { // second attempt is with  write lock
        writeLock(); // writelock is needed to set accesstime
      }

      ...

      if (attempt == 0) {
         continue;
      }
{code}

In the above code, readLock is acquired in attempt 0 and if the execution enters in the continue block, then it tries to acquire writeLock before releasing the readLock.
 "
TestJspHelper.testGetUgi fails,HDFS-1285,"{{TestJspHelper.testGetUgi}} fails during Hudson builds.
[See here|http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/423/testReport/junit/org.apache.hadoop.hdfs.server.common/TestJspHelper/testGetUgi/]"
HDFS Hudson trunk build is broken,HDFS-1050,See http://hudson.zones.apache.org/hudson/view/Hadoop/job/Hadoop-Hdfs-trunk/ - it's been broken since the end of January. Looks like a problem in the HDFS proxy contrib build.
HDFS forrest doc out-dated,HDFS-1388,"The following are out-dated:

HDFS Architecture:
- ""HDFS does not yet implement user quotas or access permissions.""
- ""...rebalancing schemes are not yet implemented.""
- ""Work is in progress to support periodic checkpointing in the near future.""

HDFS User Guide:
- ""Unlike a traditional fsck utility for native file systems, this command does not correct the errors it detects."""
NullPointerException in INode prevent Namenode from starting,HDFS-121,"After a headnode went down due to a kernel panic, it was restarted.  When trying to restart the Hadoop process we encountered the following NullPointerException.  It seems this should be handled more gracefully allowing the Name Node to come up and function while either deleting or ignoring the problematic INodes.

2008-07-09 14:30:11,458 INFO org.apache.hadoop.fs.FSNamesystem: isPermissionEnabled=true
2008-07-09 14:30:12,713 ERROR org.apache.hadoop.dfs.NameNode: java.lang.NullPointerException
	at org.apache.hadoop.dfs.INodeDirectory.getExistingPathINodes(INode.java:408)
	at org.apache.hadoop.dfs.INodeDirectory.getNode(INode.java:357)
	at org.apache.hadoop.dfs.INodeDirectory.getNode(INode.java:365)
	at org.apache.hadoop.dfs.FSDirectory.unprotectedDelete(FSDirectory.java:458)
	at org.apache.hadoop.dfs.FSEditLog.loadFSEdits(FSEditLog.java:537)
	at org.apache.hadoop.dfs.FSImage.loadFSEdits(FSImage.java:756)
	at org.apache.hadoop.dfs.FSImage.loadFSImage(FSImage.java:639)
	at org.apache.hadoop.dfs.FSImage.recoverTransitionRead(FSImage.java:222)
	at org.apache.hadoop.dfs.FSDirectory.loadFSImage(FSDirectory.java:79)
	at org.apache.hadoop.dfs.FSNamesystem.initialize(FSNamesystem.java:254)
	at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:235)
	at org.apache.hadoop.dfs.NameNode.initialize(NameNode.java:131)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:176)
	at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:162)
	at org.apache.hadoop.dfs.NameNode.createNameNode(NameNode.java:846)
	at org.apache.hadoop.dfs.NameNode.main(NameNode.java:855)
"
RPC on Datanode blocked forever.,HDFS-27,"
We recently noticed a number of datanodes got stuck. The main thread that sends heartbeats and block reports is blocked in select() in side blockReport() RPC.  I will add a stack trace in the next comment.

I am not sure why select was blocked forever since there is no connection open to NameNode. In fact, NN was restarted in between. It could be some JDK bug or a Hadoop bug.
"
Corrupted blocks leading to job failures,HDFS-387,"On one of our clusters we ended up with 11 singly-replicated corrupted blocks (checksum errors) such that jobs were failing because of no live blocks available.

fsck reports the system as healthy, although it is not.

I argue that fsck should have an option to check whether under-replicated blocks are okay.

Even better, the namenode should automatically check under-replicated blocks with repeated replication failures for corruption and list them somewhere on the GUI. And for checksum errors, there should be an option to undo the corruption and recompute the checksums.

Question: Is it at all probable that two or more replications of a block have checksum errors? If not, then we could reduce the checking to singly-replicated blocks."
hdfs jar-test ant target fails with the latest commons jar's from the common trunk,HDFS-623,"    [javac]    <somelocation>/src/test/hdfs/org/apache/hadoop/hdfs/server/namenode/TestReplicationPolicy.java:67: incompatible types
    [javac] found   : org.apache.hadoop.hdfs.server.namenode.ReplicationTargetChooser
    [javac] required: org.apache.hadoop.hdfs.server.namenode.BlockPlacementPolicy
    [javac]     replicator = fsNamesystem.blockManager.replicator;
    [javac]                                           ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] 5 errors
"
Port HDFS-101 to branch 0.20,HDFS-937,Port HDFS-101 to branch 0.20 after HDFS-872 was resolved.
ArrayIndexOutOfBoundsException throwed from BlockLocation ,HDFS-1549,"BlockLocation object created through the default constructor  has a hosts array with its length of zero.  It will apparently throw an ArrayIndexOutOfBoundsException when reading fields from DataInput if not resized the array length.

Exception in thread ""IPC Client (47) connection to nn151/192.168.201.151:9020 from zhoumin"" java.lang.ArrayIndexOutOfBoundsException: 0
        at org.apache.hadoop.fs.BlockLocation.readFields(BlockLocation.java:177)
        at org.apache.hadoop.fs.LocatedFileStatus.readFields(LocatedFileStatus.java:85)
        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:237)
        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:171)
        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:219)
        at org.apache.hadoop.io.ObjectWritable.readFields(ObjectWritable.java:66)
        at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:509)
        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:439)"
An invalidated block should be removed from the blockMap,HDFS-37,"Currently when a namenode schedules to delete an over-replicated block, the replica to be deleted does not get removed the block map immediately. Instead it gets removed when the next block report to comes in. This causes three problems: 
1. getBlockLocations may return locations that do not contain the block;
2. Over-replication due to unsuccessful deletion can not be detected as described in HADOOP-4477.
3. The number of blocks shown on dfs Web UI does not get updated on a source node when a large number of blocks have been moved from the source node to a target node, for example, when running a balancer."
getDatanodeReport can be moved to NameNodeProtocol from ClientProtocol,HDFS-1155,"Right now getDatanodeReport is being used in only two places in the HDFS code: the Balancer and DFSAdmin. And it is the only reason for these classes to use DFSClient. If we would move the method definition (or copy for now deprecating the old location) to the NameNode protocol DFSAdmin and Balancer will not rely on DFSClient anymore and will be cleaner. This will also help the Balancer to run cleaner against HDFS-599 changes.

Thoughts?"
DFSClient cpu overhead is too high,HDFS-375,"
When we do dfs throughput test using hadoop dfs -cat, we have observed that the client side cpu usage is very high, 3 to five times that of a data node serving the file.
Before 0.18, the data node cpu usage was equally high, and this problem is fixed since 0.18. However, the client side problem still exists.

"
Namenode unable to start due to truncated fstime,HDFS-1220,"- Summary: updating fstime file on disk is not atomic, so it is possible that
if a crash happens in the middle, next time when NameNode reboots, it will
read stale fstime, hence unable to start successfully.
 
- Details:
Basically, this involve 3 steps:
1"
NameNode unable to start due to stale edits log after a crash,HDFS-1221,"- Summary: 
If a crash happens during FSEditLog.createEditLogFile(), the
edits log file on disk may be stale. During next reboot, NameNode 
will get an exception when parsing the edits file, because of stale data, 
leading to unsuccessful reboot.
Note: Th"
TestRefreshUserMappings fails w/o security enabled,HDFS-1375,"
{noformat}
------------- Standard Error -----------------
refreshUserToGroupsMappings: Kerberos service principal name isn't configured properly (should have 3 parts): 
auth for userL1 failed
auth for userL2 succeeded
refreshSuperUserGroupsConfiguration: Kerberos service principal name isn't configured properly (should have 3 parts): 
------------- ---------------- ---------------

Testcase: testGroupMappingRefresh took 9.352 sec
        FAILED
Should be different group: eli1 and eli1
junit.framework.AssertionFailedError: Should be different group: eli1 and eli1
        at org.apache.hadoop.security.TestRefreshUserMappings.testGroupMappingRefresh(TestRefreshUserMappings.java:122)

Testcase: testRefreshSuperUserGroupsConfiguration took 2.334 sec
        FAILED
second auth for user2 should've failed 
junit.framework.AssertionFailedError: second auth for user2 should've failed 
        at org.apache.hadoop.security.TestRefreshUserMappings.testRefreshSuperUserGroupsConfiguration(TestRefreshUserMappings.java:199)
{noformat}"
Lease recovery doesn't reassign lease when triggered by append(),HDFS-1142,"If a soft lease has expired and another writer calls append(), it triggers lease recovery but doesn't reassign the lease to a new owner. Therefore, the old writer can continue to allocate new blocks, try to steal back the lease, etc. This is for the testRecoveryOnBlockBoundary case of HDFS-1139"
Allow MAX_XCEIVER_COUNT to be configured,HDFS-340,"Currently DataXcieverServer.java hardcodes MAX_XCEIVER_COUNT to 256. 

I ran a randomwriter with 4k maps and output replication set to 40 - all write pipelines failed since each DataNode hit the above limit and refused connections.

Should we consider making it configurable?"
"Add logs when delegation token is created, renewed or cancelled",HDFS-1025,"Info logs should be added when delegation token is created, renewed or cancelled.
"
NameNode startup fails if edit log terminates prematurely,HDFS-87,"I ran out of space on the device that stores the edit log, resulting in an edit log that is truncated mid transaction.

Ideally, the NameNode should start up, in SafeMode or the like, whenever this happens. Right now, you get this stack trace:

2006-12-12 15:33:57,212 ERROR org.apache.hadoop.dfs.NameNode: java.io.EOFExcepti
on
        at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:310)
        at org.apache.hadoop.io.UTF8.readFields(UTF8.java:104)
        at org.apache.hadoop.dfs.FSEditLog.loadFSEdits(FSEditLog.java:227)
        at org.apache.hadoop.dfs.FSImage.loadFSImage(FSImage.java:191)
        at org.apache.hadoop.dfs.FSDirectory.loadFSImage(FSDirectory.java:320)
        at org.apache.hadoop.dfs.FSNamesystem.<init>(FSNamesystem.java:226)
        at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:146)
        at org.apache.hadoop.dfs.NameNode.<init>(NameNode.java:138)
        at org.apache.hadoop.dfs.NameNode.main(NameNode.java:589)

"
"DFSClient getting stuck intermittently giving ""DFSClient: Could not complete file   filename retrying..."" message",HDFS-38,"While some streaming job with hadoop 0.18 and hadoop trunk, hadoop commands getting stuck intermittently and keeps on printing message ""DFSClient: <filename>Could not complete file   retrying...""  in indefinite loop . 
I have not faced this problem in hadoop 0.17,2.

On one occasion after copying to hdfs second command ""hadoop fs - put file.txt input/in.txt"" got stuck in hadoop 0.18 and hadoop trunk version
Following is the message -:
[
08/07/24 05:03:58 INFO hdfs.DFSClient: Could not complete file / user/userid/input/rin.txt  retrying...
08/07/24 05:03:59 INFO hdfs.DFSClient: Could not complete file / user/userid/input/rin.txt   retrying...
08/07/24 05:03:59 INFO hdfs.DFSClient: Could not complete file / user/userid/input/rin.txt  retrying...
]
On second occasion command -: ""hadoop jar hadoop-0.19.0-dev-examples.jar randomtextwriter  rwout"" got stuck with following -:
[
08/07/24 08:39:06 INFO hdfs.DFSClient: Could not complete file /mparedsystem/userid/mapredsystem/clusterid/ job_200807240828_0028/job.xml retrying...
08/07/24 08:39:06 INFO hdfs.DFSClient: Could not complete file /mparedsystem/userid/mapredsystem/clusterid/ job_200807240828_0028/job.xml retrying...
08/07/24 08:39:06 INFO hdfs.DFSClient: Could not complete file /mparedsystem/userid/mapredsystem/clusterid/ job_200807240828_0028/job.xml retrying...
08/07/24 08:39:07 INFO hdfs.DFSClient: Could not complete file /mparedsystem/userid/mapredsystem/clusterid/ job_200807240828_0028/job.xml retrying...
]

This is intermittent and there is no specific pattern.  I do not have namenode logs as I was hod for cluster allocation/deallocation with saving logs.

"
Improve fuse_dfs error when server != fs.default.name,HDFS-847,Giving fuse_dfs a server name other than an exact match of  fs.default.name results in cryptic errors. Let's make this scenario result in a better error message indicating what's up.
Fix hdfs debug command document in HDFSCommands.md,HDFS-8537,"From HDFSCommands.md, the *dfs* should be *debug*

{code}
Usage: `hdfs dfs verify [-meta <metadata-file>] [-block <block-file>]`
Usage: `hdfs dfs recoverLease [-path <path>] [-retries <num-retries>]`
{code}"
HDFS debug command is missed from top level help message,HDFS-8536,"HDFS top level help message missed debug command. This JIRA is opened to add it to hdfs top level command help. 

{code}
HW11217:hadoop xyao$ hdfs
Usage: hdfs [--config confdir] [--daemon (start|stop|status)]
           [--loglevel loglevel] COMMAND
       where COMMAND is one of:
  balancer             run a cluster balancing utility
  cacheadmin           configure the HDFS cache
  classpath            prints the class path needed to get the
                       Hadoop jar and the required libraries
  crypto               configure HDFS encryption zones
  datanode             run a DFS datanode
  dfs                  run a filesystem command on the file system
  dfsadmin             run a DFS admin client
  fetchdt              fetch a delegation token from the NameNode
  fsck                 run a DFS filesystem checking utility
  getconf              get config values from configuration
  groups               get the groups which users belong to
  haadmin              run a DFS HA admin client
  jmxget               get JMX exported values from NameNode or DataNode.
  journalnode          run the DFS journalnode
  lsSnapshottableDir   list all snapshottable dirs owned by the current user
                               Use -help to see options
  mover                run a utility to move block replicas across
                       storage types
  namenode             run the DFS namenode
                               Use -format to initialize the DFS filesystem
  nfs3                 run an NFS version 3 gateway
  oev                  apply the offline edits viewer to an edits file
  oiv                  apply the offline fsimage viewer to an fsimage
  oiv_legacy           apply the offline fsimage viewer to a legacy fsimage
  portmap              run a portmap service
  secondarynamenode    run the DFS secondary namenode
  snapshotDiff         diff two snapshots of a directory or diff the
                       current directory contents with a snapshot
  storagepolicies      list/get/set block storage policies
  version              print the version
  zkfc                 run the ZK Failover Controller daemon

Most commands print help when invoked w/o parameters.
{code}

{code}

HW11217:hadoop xyao$ hdfs debug
Usage: hdfs debug <command> [arguments]

verify [-meta <metadata-file>] [-block <block-file>]
recoverLease [-path <path>] [-retries <num-retries>]

{code}"
"Ozone: TestKeys, TestKeysRatis and TestOzoneShell are failing because of read timeout",HDFS-13047,"The tests are failing because of the following error.

{code}
org.apache.hadoop.ozone.web.client.OzoneRestClientException: Failed to putKey: keyName=01e8b923-5876-4d5e-8adc-4214caf33f64, file=/testptch/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/2/TestKeys/f8f75b8d-f15b-482f-afa0-babe1d6c4bf6
	at org.apache.hadoop.ozone.web.client.OzoneBucket.putKey(OzoneBucket.java:253)
	at org.apache.hadoop.ozone.web.client.TestKeys$PutHelper.putKey(TestKeys.java:218)
	at org.apache.hadoop.ozone.web.client.TestKeys$PutHelper.access$100(TestKeys.java:168)
	at org.apache.hadoop.ozone.web.client.TestKeys.runTestPutAndGetKeyWithDnRestart(TestKeys.java:297)
	at org.apache.hadoop.ozone.web.client.TestKeys.testPutAndGetKeyWithDnRestart(TestKeys.java:287)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
Caused by: java.net.SocketTimeoutException: Read timed out
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
	at java.net.SocketInputStream.read(SocketInputStream.java:171)
	at java.net.SocketInputStream.read(SocketInputStream.java:141)
	at org.apache.http.impl.io.SessionInputBufferImpl.streamRead(SessionInputBufferImpl.java:139)
	at org.apache.http.impl.io.SessionInputBufferImpl.fillBuffer(SessionInputBufferImpl.java:155)
	at org.apache.http.impl.io.SessionInputBufferImpl.readLine(SessionInputBufferImpl.java:284)
	at org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:140)
	at org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:57)
	at org.apache.http.impl.io.AbstractMessageParser.parse(AbstractMessageParser.java:261)
	at org.apache.http.impl.DefaultBHttpClientConnection.receiveResponseHeader(DefaultBHttpClientConnection.java:165)
	at org.apache.http.impl.conn.CPoolProxy.receiveResponseHeader(CPoolProxy.java:167)
	at org.apache.http.protocol.HttpRequestExecutor.doReceiveResponse(HttpRequestExecutor.java:272)
	at org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:124)
	at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:271)
	at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:184)
	at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:88)
	at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110)
	at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:184)
	at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)
	at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:107)
	at org.apache.hadoop.ozone.web.client.OzoneBucket.executePutKey(OzoneBucket.java:276)
	at org.apache.hadoop.ozone.web.client.OzoneBucket.putKey(OzoneBucket.java:250)
	... 13 more
{code}"
Need Clarity on Replica Placement: The First Baby Steps in HDFS Architecture documentation,HDFS-12827,"The placement should be this: 
https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html

HDFS鈥檚 placement policy is to put one replica on one node in the local rack, another on a node in a different (remote) rack, and the last on a different node in the same remote rack.

Hadoop Definitive guide says the same and I have tested and saw the same behavior as above.

############
But the documentation in the versions after r2.5.2 it was mentioned as:
http://hadoop.apache.org/docs/r2.5.2/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html

HDFS鈥檚 placement policy is to put one replica on one node in the local rack, another on a different node in the local rack, and the last on a different node in a different rack. 
"
Ignore unknown StorageTypes,HDFS-12410,"A storage configured with an unknown type will cause runtime exceptions. Instead, these storages can be ignored/skipped."
Ozone: KSM: Unable to put keys with zero length,HDFS-11921,"As part of working on HDFS-11909, I was trying to put zero length keys. I found that put key refuses to do that. Here is the call trace, 

bq.	at ScmBlockLocationProtocolClientSideTranslatorPB.allocateBlock 

we check if the block size is greater than 0, which makes sense since we should not call into SCM to allocate a block of zero size.

However these 2 calls are invoked for creating the key, so that metadata for key can be created, we should probably take care of this behavior here.
bq. ksm.KeyManagerImpl.allocateKey
bq. ksm.KeySpaceManager.allocateKey(KeySpaceManager.java:428)

Another way to fix this might be to just allocate a block with at least 1 byte always, which might be easier than special casing code.

[~vagarychen] Would you like to fix this in the next patch you are working on ? 

"
NNThroughputBenchmark should use cumulativeTime to calculate ops per second.,HDFS-12277,"{{NNThroughputBenchmark$getOpsPerSecond}} is using {{elapsedTime}} to calculate ops per second. {{elapsedTime}} is the time used to measure the time from the start to the finish of the benchmark main thread, and it's also the total execution time of the single slowest benchmark worker thread. To measure ops per second, we should use the total execution time of all worker threads, which is {{cumulativeTime}}. Otherwise, we could get bogus result, e.g. given same load, the more client threads we have (controlled by {{-threads}}), the larger ops per second the number would be, with the same NN and its server side configurations."
Update the package names for hsftp / hftp in the documentation,HDFS-5465,HDFS-5436 move HftpFileSystem and HsftpFileSystem to a different package. The documentation should be updated as well.
Ozone: Add finer error codes for container operaions,HDFS-11763,Ii would be better to add more finer error codes for operation relevant operations as this suggested in the codes. It will benefit for us to find and describe the exception cause. Currently all the container operation error was using the same result code {{CONTAINER_INTERNAL_ERROR}}. This seems too coarse-grained.
Update description for dfs.datanode.outliers.report.interval,HDFS-11521,"dfs.datanode.outliers.report.interval is used to control how frequently DataNodes will report their peer latencies to the NameNode via heartbeats. From HDFS-11461 onwards, this setting is also used to set the interval between disk outlier detections. The description for this setting should be updated in hdfs-default.xml to capture this change. "
DFS admin rollEdits command should do audit log.,HDFS-7974,"DFS admin rollEdits command execution is not logging in audit log.

{code}
./hdfs dfsadmin -rollEdits
{code}"
Accelerate TestQuotaByStorageType using shared MiniDFSCluster,HDFS-11079,"-When tests are added to TestQuotaByStorageType by HDFS-11065, it's been noticed that it takes minutes to launch MiniDFSClusters for every tests. This tries to refactor the code to use a static shared MiniDFSCluster to accelerate tests.-"
Change the hard-code value to variable,HDFS-9883,"In some class of HDFS, there are many hard-code value places. Like this: 
{code}
  /** Constructor 
   * @param bandwidthPerSec bandwidth allowed in bytes per second. 
   */
  public DataTransferThrottler(long bandwidthPerSec) {
    this(500, bandwidthPerSec);  // by default throttling period is 500ms
  }
{code}
It will be better replace these value to variables so that it will not be easily ignored."
hadoop superusergroup supergroup issue,HDFS-10863,"I want to match my unix user to HDFS: hduser:hadoop.

For the user I use the VE.

$ echo $HADOOP_HDFS_USER
hduser

For the group I use the hdfs-site.xml :

<property>
    <name>dfs.permissions.superusergroup</name>
    <value>hadoop</value>
</property>

The namenode log file show the parameter user/group values.

INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = hduser (auth:SIMPLE)
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = hadoop
INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true

Everything seems to be OK, but when I copy file form FS to HDFS the group is not correct. It keeps the supergroup default value.

Thoses shell commands show the issue:

$ ll /srv/downloads/zk.tar
-rw-r--r-- 1 hduser hadoop 41984000 Aug 18 13:25 /srv/downloads/zk.tar
$ hdfs dfs -put /srv/downloads/zk.tar /tmp
$ hdfs dfs -ls /tmp/zk.tar
-rw-r--r--   2 hduser supergroup   41984000 2016-09-14 12:47 /tmp/zk.tar

I have:

-rw-r--r-- 2 hduser supergroup 41984000 2016-09-14 12:47 /tmp/zk.tar

I expect :

-rw-r--r-- 2 hduser hadoop 41984000 2016-09-14 12:47 /tmp/zk.tar

Why the HDFS group is not the value of the dfs.permissions.superusergroup property ?

@jbigdata.fr"
Make HDFS Federation docs up to date,HDFS-9912,"_HDFS Federation_ documentation has a few places that are out-dated:

* dfsclusterhealth.jsp is already removed
* It should mention how to configure Federation with High availability, because the configuration appears incompatible."
Quick Thread Local Storage set-up has a small flaw,HDFS-10257,"In   jni_helper.c   in the   getJNIEnv    function 

The     鈥淭HREAD_LOCAL_STORAGE_SET_QUICK(env);鈥?  Macro   is   in the  wrong location;   

It should precede   the  鈥渢hreadLocalStorageSet(env)鈥?  as follows ::  

    THREAD_LOCAL_STORAGE_SET_QUICK(env);

    if (threadLocalStorageSet(env)) {
      return NULL;
    }

AND IN   鈥渢hread_local_storage.h鈥?  the macro:   鈥淭HREAD_LOCAL_STORAGE_SET_QUICK鈥?should be as follows :: 

#ifdef HAVE_BETTER_TLS
  #define THREAD_LOCAL_STORAGE_GET_QUICK() \
    static __thread JNIEnv *quickTlsEnv = NULL; \
    { \
      if (quickTlsEnv) { \
        return quickTlsEnv; \
      } \
    }

  #define THREAD_LOCAL_STORAGE_SET_QUICK(env) \
    { \
      quickTlsEnv = (env); \
      return env;
    }
#else
  #define THREAD_LOCAL_STORAGE_GET_QUICK()
  #define THREAD_LOCAL_STORAGE_SET_QUICK(env)
#endif


"
Update HDFS calls to ProxyUsers#authorize.,HDFS-6251,HADOOP-10499 will remove an unnecessary overload of {{ProxyUsers#authorize}}.  This issue tracks updating call sites in the HDFS code.
provide means of escaping special characters to `hadoop fs` command,HDFS-3557,"When running an investigative job, I used a date parameter that selected multiple directories for the input (e.g. ""my_data/2012/06/{18,19,20}""). It used this same date parameter when creating the output directory.

But `hadoop fs` was unable to ls, getmerge, or rmr it until I used the regex operator ""?"" and mv to change the name (that is, `-mv output/2012/06/?18,19,20? foobar"").

Shells and filesystems for other systems provide a means of escaping ""special characters"" generically, but there seems to be no such means in HDFS/`hadoop fs`. Providing one would be a great way to make accessing HDFS more robust."
Handle overflow condition for txid going over Long.MAX_VALUE,HDFS-4936,"Hat tip to [~azuryy] for the question that lead to this (on mailing lists).

I hacked up my local NN's txids manually to go very large (close to max) and decided to try out if this causes any harm. I basically bumped up the freshly formatted files' starting txid to 9223372036854775805 (and ensured image references the same by hex-editing it):

{code}
鉃? current  ls
VERSION
fsimage_9223372036854775805.md5
fsimage_9223372036854775805
seen_txid
鉃? current  cat seen_txid
9223372036854775805
{code}

NameNode started up as expected.

{code}
13/06/25 18:30:08 INFO namenode.FSImage: Image file of size 129 loaded in 0 seconds.
13/06/25 18:30:08 INFO namenode.FSImage: Loaded image for txid 9223372036854775805 from /temp-space/tmp-default/dfs-cdh4/name/current/fsimage_9223372036854775805
13/06/25 18:30:08 INFO namenode.FSEditLog: Starting log segment at 9223372036854775806
{code}

I could create a bunch of files and do regular ops (counting to much after the long max increments). I created over 10 files, just to make it go well over the Long.MAX_VALUE.

Quitting NameNode and restarting fails though, with the following error:

{code}
13/06/25 18:31:08 INFO namenode.FileJournalManager: Recovering unfinalized segments in /Users/harshchouraria/Work/installs/temp-space/tmp-default/dfs-cdh4/name/current
13/06/25 18:31:08 INFO namenode.FileJournalManager: Finalizing edits file /Users/harshchouraria/Work/installs/temp-space/tmp-default/dfs-cdh4/name/current/edits_inprogress_9223372036854775806 -> /Users/harshchouraria/Work/installs/temp-space/tmp-default/dfs-cdh4/name/current/edits_9223372036854775806-9223372036854775807
13/06/25 18:31:08 FATAL namenode.NameNode: Exception in namenode join
java.io.IOException: Gap in transactions. Expected to be able to read up until at least txid 9223372036854775806 but unable to find any edit logs containing txid -9223372036854775808
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.checkForGaps(FSEditLog.java:1194)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.selectInputStreams(FSEditLog.java:1152)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:616)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:267)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:592)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:435)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:397)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:399)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:433)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:609)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:590)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1141)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1205)
{code}

Looks like we also lose some edits when we restart, as noted by the finalized edits filename:

{code}
VERSION
edits_9223372036854775806-9223372036854775807
fsimage_9223372036854775805
fsimage_9223372036854775805.md5
seen_txid
{code}

It seems like we won't be able to handle the case where txid overflows. Its a very very large number so that's not an immediate concern but seemed worthy of a report."
Windows HDFS daemon - datanode.DirectoryScanner: Error compiling report (...) XXX is not a prefix of YYY,HDFS-8761,"I'm periodically seeing errors like the one below output by the HDFS daemon (started with start-dfs.cmd). This is with the default settings for data location (=not specified in my hdfs-site.xml). I assume it may be fixable by specifying a path with the drive letter in the config file, however I haven't be able to do that (see http://stackoverflow.com/questions/31353226/setting-hadoop-tmp-dir-on-windows-gives-error-uri-has-an-authority-component).

15/07/11 17:29:57 ERROR datanode.DirectoryScanner: Error compiling report
java.util.concurrent.ExecutionException: java.lang.RuntimeException: \tmp\hadoop-odelalleau\dfs\data is not a prefix of D:\tmp\hadoop-odelalleau\dfs\data\current\BP-1474392971-10.128.22.110-1436634926842\current\finalized\subdir0\subdir0\blk_1073741825
        at java.util.concurrent.FutureTask.report(FutureTask.java:122)
        at java.util.concurrent.FutureTask.get(FutureTask.java:192)
        at org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.getDiskReport(DirectoryScanner.java:566)
        at org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.scan(DirectoryScanner.java:425)
        at org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.reconcile(DirectoryScanner.java:406)
        at org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.run(DirectoryScanner.java:362)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)"
[HDFS-Quota] Verification is not done while setting dir namequota and size,HDFS-8159,"Name Quota and space quota is not verifying when setting a new value to a directory which already has subdirectories or contents.
Below are the steps to re-produce the cases:

*+Case-1+*

Step-1) Create a New folder 
hdfs dfs -mkdir /test
Step-2) Create sub folders
hdfs dfs -mkdir /test/one
hdfs dfs -mkdir /test/two
hdfs dfs -mkdir /test/three
Step-3) Set Name Quota as two 
hdfs dfsadmin  -setQuota 2 /test
Step-3) Quota will be set with out the validating the dirs 

+Output:+ Eventhough name quota value is lower than the existing number of dirs, its not validating and allowing to set the new value.

+Suggestion:+ Validate the name quota against the number of contents before setting the new value.

*+Case-2+*

Step-1) Add any new folder or file , it will give error message
mkdir: The NameSpace quota (directories and files) of directory /test is exceeded: quota=2 file count=5
Step-2) Clear the Quota 
hdfs dfsadmin -clrQuota /test
Step-3) Now Set the Size less than the folder size 
hdfs dfsadmin -setSpaceQuota 10 /test

+Output:+ Eventhough space quota value is less than the size of the existing dir contents, its not validating and allowing to set the new value.

+Suggestion:+ Validate the quota against the used space before setting the new value.
"
TestDatanodeStartupFixesLegacyStorageIDs fails on Windows due to failure to unpack old image tarball that contains hard links,HDFS-9138,"{{TestDatanodeStartupFixesLegacyStorageIDs#testUpgradeFrom22via26FixesStorageIDs}} uses a checked-in DataNode data directory that contains hard links.  The hard links cannot be handled correctly by the commons-compress library used in the Windows implementation of {{FileUtil#unTar}}.  The result is that the unpacked block files have 0 length, the block files reported to the NameNode are invalid, and therefore the mini-cluster never gets enough good blocks reported to leave safe mode."
BlockPoolSliceScanner fails when Datanode has too many blocks,HDFS-7633,"issue:
When Total blocks of one of my DNs reaches 33554432, It refuses to accept more blocks, this is the ERROR.
2015-01-16 15:21:44,571 | ERROR | DataXceiver for client  at /172.1.1.8:50490 [Receiving block BP-1976278848-172.1.1.2-1419846518085:blk_1221043436_147936990] | datasight-198:25009:DataXceiver error processing WRITE_BLOCK operation  src: /172.1.1.8:50490 dst: /172.1.1.11:25009 | org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:250)
java.lang.IllegalArgumentException: n must be positive
        at java.util.Random.nextInt(Random.java:300)
        at org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner.getNewBlockScanTime(BlockPoolSliceScanner.java:263)
        at org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner.addBlock(BlockPoolSliceScanner.java:276)
        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.addBlock(DataBlockScanner.java:193)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.closeBlock(DataNode.java:1733)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:765)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:124)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:71)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:232)
        at java.lang.Thread.run(Thread.java:745)


analysis:
in function org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner.getNewBlockScanTime()
when blockMap.size() is too big,
Math.max(blockMap.size(),1)  * 600  is int type, and negtive
Math.max(blockMap.size(),1) * 600 * 1000L is long type, and negtive
(int)period  is Integer.MIN_VALUE
Math.abs((int)period) is Integer.MIN_VALUE , which is negtive
DFSUtil.getRandom().nextInt(periodInt)  will thows IllegalArgumentException

I use Java HotSpot (build 1.7.0_05-b05)
"
TestDFSUpgrade leaks file descriptors.,HDFS-9136,"HDFS-8480 introduced code in {{TestDFSUpgrade#testPreserveEditLogs}} that opens edit log files and reads from them, but these files are never closed."
Html view for nntop: top users of name node,HDFS-7465,"nntop (HDFS-6982) maintains the list of top users of the name node and export the list via jmx. An external monitoring tool can then draw graphs of the top users using jmx interface. For small-scale deployments that are not yet equipped with an external monitoring tool, however, it would be useful to show the list of recent top users using a html view. The html page lists the top users of each name node command in the past 1, 5, 25 minutes (configurable). The details can be found in the design doc of nntop: https://issues.apache.org/jira/secure/attachment/12665990/nntop-design-v1.pdf"
Balancer shutdown synchronisation could do with a review,HDFS-852,"Looking at the source of the Balancer, there's a lot {{catch(InterruptedException)}} clauses, which runs the risk of swallowing exceptions, making it harder to shut down a balancer.

for example, the {{AccessKeyUpdater swallows the InterruptedExceptions which get used to tell it to shut down, and while it does poll the shared field {{shouldRun}}, that field isn't volatile: the shutdown may }}not work. 

Elsewhere, the {{dispatchBlocks()}} method swallows interruptions without even looking for any shutdown flag. 

This is all minor as it is shutdown logic, but it is the stuff that it hard to test and leads to problems in the field, the problems that leave the ops team resorting to {{kill -9}}, and we don't want that."
"race condition crashes ""hadoop ls -R"" when directories are moved/removed",HDFS-5546,"This seems to be a rare race condition where we have a sequence of events like this:
1. org.apache.hadoop.shell.Ls calls DFS#getFileStatus on directory D.
2. someone deletes or moves directory D
3. org.apache.hadoop.shell.Ls calls PathData#getDirectoryContents(D), which calls DFS#listStatus(D). This throws FileNotFoundException.
4. ls command terminates with FNF"
Problem in accessing NN JSP page,HDFS-5687,"In NN UI page After clicking the browse File System page,from that page,if you click GO Back TO DFS HOME ICon it is not accessing the dfshealth.jsp page

NN http URL is http://nnaddr///nninfoaddr/dfshealth.jsp,it is coming like this,due to this i think it is not browsing that page
It should be http://nninfoaddr/dfshealth.jsp/ like this"
Truncate should not be success when Truncate Size and Current Size are equal.,HDFS-8505,"
Truncate should not be success when Truncate Size and Current Size are equal.

$ ./hdfs dfs -cat /file
abcdefgh

$ ./hdfs dfs -truncate -w 2 /file
Waiting for /file ...
Truncated /file to length: 2

$ ./hdfs dfs -cat /file
ab

{color:red}
$ ./hdfs dfs -truncate -w 2 /file
Truncated /file to length: 2
{color}

$ ./hdfs dfs -cat /file
ab

Expecting to throw Truncate Error:
-truncate: Cannot truncate to a larger file size. Current size: 2, truncate size: 2"
Improve refreshNodes in DFSAdmin to support hdfs federation,HDFS-8500,"In hdfs federated cluster, we find the dfsadmin command ""hdfs dfsadmin -refreshNodes""  can only refresh the nameservice (or namenode) which configured in fs.defaultFS. The other nameservices configured in hdfs-site.xml can't be refreshed unless changing the fs.defaultFS value and run this command again. I think we need additional parameters as following to give a convenient way: 
[-a] fresh all namenodes configured in hdfs-site.xml.
[-ns <nameserviceId>] specify a nameservice to refresh.
[host:ipc_port] specify a nn to refresh.

please give your opinions. Thanks!"
"In WebHDFS, duplicate directory creation is not throwing exception.",HDFS-8452,"*Case 1 (CLI):*
    a. In HDFS Create a new Directory 
          {code}./hdfs dfs -mkdir /new  , A New directory will be created{code}
   b. Now Execute the same Command again 
{code}       mkdir: `/new': File exists  , Error message will be shown  {code}

*Case 2 (RestAPI) :*
    a. In HDFS Create a new Directory
 {code}curl -i -X PUT -L ""http://host1:50070/webhdfs/v1/new1?op=MKDIRS&overwrite=false""{code}
      A New Directory will be created 
 b. Now Execute the same webhdfs  command again 
    No exception will be thrown back to the client.
   {code}
HTTP/1.1 200 OK
Cache-Control: no-cache
Expires: Thu, 21 May 2015 15:11:57 GMT
Date: Thu, 21 May 2015 15:11:57 GMT
Pragma: no-cache
Content-Type: application/json
Transfer-Encoding: chunked

   {code}

     "
adding block pool % for each namespace on federated namenode webUI,HDFS-5301,
Move IP to FQDN conversion from DatanodeJSPHelper to DatanodeID,HDFS-3233,"In a handful of places DatanodeJSPHelper looks up the IP for a DN and then determines a FQDN for the IP. We should move this code to a single place, a new DatanodeID to return the FQDN for a DatanodeID."
HDFS progress bar does not display usage correctly,HDFS-4958,"in dfsnodelist.jsp, when datanode usage is very low or very high, the percentage bar in `Used` column does not represents the percentage number correctly.

it's a minor css issue. haven't verified if it applies to other pages as well."
redirectToRandomDataNode tries to get a delegation token without verifying security is enabled,HDFS-4789,"Saw a warning message stating ""trying to get DT with no secret manager running"".

The isAllowedDelegationTokenOp() check call is questionably faulty, as per https://issues.apache.org/jira/browse/HADOOP-9461. Since HADOOP-9461 is potentially incompatible, I'm adding this minor check here to limit spurious / scary warnings

"
getmerge file system shell command needs error message for user error,HDFS-5740,"I naively tried a {{getmerge}} operation but it didn't seem to do anything and there was no error message:

{noformat}[jpfuntner@h58 tmp]$ hadoop fs -mkdir /user/jpfuntner/tmp
[jpfuntner@h58 tmp]$ num=0; while [ $num -lt 5 ]; do echo file$num | hadoop fs -put - /user/jpfuntner/tmp/file$num; let num=num+1; done
[jpfuntner@h58 tmp]$ ls -A
[jpfuntner@h58 tmp]$ hadoop fs -getmerge /user/jpfuntner/tmp/file* files.txt
[jpfuntner@h58 tmp]$ ls -A
[jpfuntner@h58 tmp]$ hadoop fs -ls /user/jpfuntner/tmp
Found 5 items
-rw-------   3 jpfuntner hdfs          6 2014-01-08 17:37 /user/jpfuntner/tmp/file0
-rw-------   3 jpfuntner hdfs          6 2014-01-08 17:37 /user/jpfuntner/tmp/file1
-rw-------   3 jpfuntner hdfs          6 2014-01-08 17:37 /user/jpfuntner/tmp/file2
-rw-------   3 jpfuntner hdfs          6 2014-01-08 17:37 /user/jpfuntner/tmp/file3
-rw-------   3 jpfuntner hdfs          6 2014-01-08 17:37 /user/jpfuntner/tmp/file4
[jpfuntner@h58 tmp]$ {noformat}

It was pointed out to me that I made a mistake and my source should have been a directory not a set of regular files.  It works if I use the directory:

{noformat}[jpfuntner@h58 tmp]$ hadoop fs -getmerge /user/jpfuntner/tmp/ files.txt
[jpfuntner@h58 tmp]$ ls -A
files.txt  .files.txt.crc
[jpfuntner@h58 tmp]$ cat files.txt
file0
file1
file2
file3
file4
[jpfuntner@h58 tmp]$ {noformat}

I think the {{getmerge}} command should issue an error message to let the user know they made a mistake."
Number format Exception is displayed in Namenode UI when the chunk size field is blank or string value.. ,HDFS-1949,"In the Namenode UI we have a text box to enter the chunk size.

The expected value for the chunk size is a valid Integer value.

If any invalid value, string or empty spaces are provided it throws number format exception.

The existing behaviour is like we need to consider the default value if no value is specified.

Soln
====
We can handle numberformat exception and assign default value if invalid value is specified."
removedDst should be checked against null in the finally block of FSDirRenameOp#unprotectedRenameTo(),HDFS-7538,"{code}
        if (removedDst != null) {
          undoRemoveDst = false;
...
      if (undoRemoveDst) {
        // Rename failed - restore dst
        if (dstParent.isDirectory() &&
            dstParent.asDirectory().isWithSnapshot()) {
          dstParent.asDirectory().undoRename4DstParent(removedDst,
{code}
If the first if check doesn't pass, removedDst would be null and undoRemoveDst may be true.
This combination would lead to NullPointerException in the finally block."
Under replicated blocks with ONE replica should get replication priority over blocks with more than one replica.,HDFS-6074,We had two nodes fail at the same time.   There were over 2000 blocks at higher risk of causing corrupt files since they were single replica.   There were over 40000 under replicated blocks with most having two replicas and the namenode priority to recreate missing replicas clearly placed no priority of single replica blocks.
A partial rollback cause the new changes done after upgrade to be visible after rollback,HDFS-2017,"This is the scenario :

Namenode has 3 name dirs configured ..
1) Namenode upgrade starts - Upgrade fails after 1st directory is upgraded (2nd and 3rd dir is left unchanged ..) { like , Namenode process down }
2) Namenode starts and new files written .. 
3) Namenode shutdown and rollbacked

Since Namenode is saving the latest image dir(the upgraded 1st dir since checkpointtime is incremented during upgrade for this dir) will be loaded and saved to all dirs during loadfsimage ..

But if a ROLLBACK is done , the 1st dir will be rolled back (the older copy becomes current and its checkpointtime is now LESS than other dirs ..) and others left behind since they dont contain previous .. Now during loadfsimage , the 2nd dir will be selected since it has the highest checkpoint time and saved to all dirs (including 1st ) .. Now due to this , the new changes b/w UPGRADE and ROLLBACK present in 2nd dir gets reflected even after ROLLBACK ..
 

This is not the case with a SUCCESSFUL Upgrade/Rollback (New changes lost after rollback)..
"
"In a secure cluster, in the HDFS WEBUI , clicking on a datanode in the node list , gives an error",HDFS-4108,"This issue happens in secure cluster.

To reproduce :

Go to the NameNode WEB UI. (dfshealth.jsp)
Click to bring up the list of LiveNodes  (dfsnodelist.jsp)
Click on a datanode to bring up the filesystem  web page ( browsedirectory.jsp)

The page containing the directory listing does not come up.

"
NPE in block_info_xml JSP if the block has been deleted,HDFS-5345,"If you ask for a block info report on a block that has been deleted, you see a stack trace and a 500 error.

Steps to replicae
# create a file
# browse to it
# get the block info
# delete the file
# reload the block info page

Maybe a 404 is the response to raise instead"
Datanode and Task Tracker not starting,HDFS-6725,"I am unable to start the datanode and tasktracker daemons on one of my slave nodes. I have got two slave nodes for my test env.
The error is familiar to Jira, however the solutions provided in JIRA is not working for me.

Below are the errors 
Datanode log file:

2014-07-22 07:17:54,559 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = Hadslave1/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 1.2.0
STARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.2 -r 1479473; compiled by 'hortonfo' on Mon May  6 
06:59:37 UTC 2013
STARTUP_MSG:   java = 1.6.0_31
************************************************************/
2014-07-22 07:17:55,691 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2014-07-22 07:17:55,703 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.
2014-07-22 07:17:55,732 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2014-07-22 07:17:55,732 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2014-07-22 07:17:56,265 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source ugi registered.
2014-07-22 07:17:56,275 WARN org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Source name ugi already exists!
2014-07-22 07:17:57,536 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.lang.IllegalArgumentException: Does not contain a valid
 host:port authority: file:///
	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:164)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:212)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:244)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.getServiceAddress(NameNode.java:236)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:357)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:319)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1698)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1637)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1655)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:1781)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:1798)

2014-07-22 07:17:57,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at Hadslave1/127.0.1.1
************************************************************/


Task tracker's log files:

2014-07-22 07:17:59,297 INFO org.apache.hadoop.mapred.TaskTracker: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting TaskTracker
STARTUP_MSG:   host = Hadslave1/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 1.2.0
STARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.2 -r 1479473; compiled by 'hortonfo' on Mon May  6 
06:59:37 UTC 2013
STARTUP_MSG:   java = 1.6.0_31
************************************************************/
2014-07-22 07:17:59,671 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2014-07-22 07:17:59,814 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.
2014-07-22 07:17:59,815 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2014-07-22 07:17:59,815 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: TaskTracker metrics system started
2014-07-22 07:18:00,028 INFO org.apache.hadoop.util.NativeCodeLoader: Loaded the native-hadoop library
2014-07-22 07:18:00,158 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source ugi registered.
2014-07-22 07:18:00,160 WARN org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Source name ugi already exists!
2014-07-22 07:18:00,265 ERROR org.apache.hadoop.mapred.TaskTracker: Can not start task tracker because java.lang.IllegalArgumentException: 
Does not contain a valid host:port authority: local
	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:164)
	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:130)
	at org.apache.hadoop.mapred.JobTracker.getAddress(JobTracker.java:2121)
	at org.apache.hadoop.mapred.TaskTracker.<init>(TaskTracker.java:1540)
	at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:3937)

2014-07-22 07:18:00,265 INFO org.apache.hadoop.mapred.TaskTracker: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down TaskTracker at Hadslave1/127.0.1.1
************************************************************/

Below are my configuration files:
user@Hadmast:/opt/hadoop-1.2.0/conf$ cat core-site.xml 
<?xml version=""1.0""?>
<?xml-stylesheet type=""text/xsl"" href=""configuration.xsl""?>

<!-- Put site-specific property overrides in this file. -->

<configuration>

<property>

<name>fs.default.name</name>

<value>hdfs://192.168.111.131:8020</value>

</property>

</configuration>
user@Hadmast:/opt/hadoop-1.2.0/conf$ cat hdfs-site.xml 
<?xml version=""1.0""?>
<?xml-stylesheet type=""text/xsl"" href=""configuration.xsl""?>

<!-- Put site-specific property overrides in this file. -->

<configuration>

<property>
<name>dfs.replication</name>
<value>2</value>
</property>

<property>
<name>dfs.permissions</name>
<value>false</value>
</property>

</configuration>
user@Hadmast:/opt/hadoop-1.2.0/conf$ cat mapred-site.xml 
<?xml version=""1.0""?>
<?xml-stylesheet type=""text/xsl"" href=""configuration.xsl""?>

<!-- Put site-specific property overrides in this file. -->

<configuration>

	<property>

		<name>mapred.job.tracker</name>

		<value>localhost:8021</value>

	</property>

</configuration>
user@Hadmast:/opt/hadoop-1.2.0/conf$ 

Both the daemons start on Hadmast node but doesn't start on the other node.

user@Hadmast:~$ jps
7947 Jps
6421 NameNode
7661 DataNode
6941 JobTracker
6866 SecondaryNameNode
7172 TaskTracker
user@Hadmast:~$ 

user@Hadslave1:~$ jps
4826 Jps
user@Hadslave1:~$

I have formated the namenode multiple times and have also rebuilt the Hadslave1 once but no change."
KMS throws AuthenticationException when enabling kerberos authentication ,HDFS-6676,"When I made a request http://server-1941.novalocal:16000/kms/v1/names in firefox. (before, i set configs in firefox according https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/5/html/Deployment_Guide/sso-config-firefox.html), following info was found in logs/kms.log.
2014-07-14 19:18:30,461 WARN  AuthenticationFilter - Authentication exception: GSSException: Failure unspecified at GSS-API level (Mechanism level: EncryptedData is encrypted using keytype DES CBC mode with CRC-32 but decryption key is of type NULL)
org.apache.hadoop.security.authentication.client.AuthenticationException: GSSException: Failure unspecified at GSS-API level (Mechanism levelis of type NULL)
	at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.authenticate(KerberosAuthenticationHandler.java:380)
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:357)
	at org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter.doFilter(KMSAuthenticationFilter.java:100)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:293)
	at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:861)
	at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:606)
	at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:489)
	at java.lang.Thread.run(Thread.java:745)
Caused by: GSSException: Failure unspecified at GSS-API level (Mechanism level: EncryptedData is encrypted using keytype DES CBC mode with CRC-32 but decryption key is of type NULL)
	at sun.security.jgss.krb5.Krb5Context.acceptSecContext(Krb5Context.java:788)
	at sun.security.jgss.GSSContextImpl.acceptSecContext(GSSContextImpl.java:342)
	at sun.security.jgss.GSSContextImpl.acceptSecContext(GSSContextImpl.java:285)
	at sun.security.jgss.spnego.SpNegoContext.GSS_acceptSecContext(SpNegoContext.java:875)
	at sun.security.jgss.spnego.SpNegoContext.acceptSecContext(SpNegoContext.java:548)
	at sun.security.jgss.GSSContextImpl.acceptSecContext(GSSContextImpl.java:342)
	at sun.security.jgss.GSSContextImpl.acceptSecContext(GSSContextImpl.java:285)
	at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler$2.run(KerberosAuthenticationHandler.java:347)
	at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler$2.run(KerberosAuthenticationHandler.java:329)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.authenticate(KerberosAuthenticationHandler.java:329)
	... 14 more
Caused by: KrbException: EncryptedData is encrypted using keytype DES CBC mode with CRC-32 but decryption key is of type NULL
	at sun.security.krb5.EncryptedData.decrypt(EncryptedData.java:169)
	at sun.security.krb5.KrbCred.<init>(KrbCred.java:131)
	at sun.security.jgss.krb5.InitialToken$OverloadedChecksum.<init>(InitialToken.java:282)
	at sun.security.jgss.krb5.InitSecContextToken.<init>(InitSecContextToken.java:130)
	at sun.security.jgss.krb5.Krb5Context.acceptSecContext(Krb5Context.java:771)
	... 25 more
	
Kerberos is enabled successful in my environment:
klist
Ticket cache: FILE:/tmp/krb5cc_0
Default principal: HTTP/server-1941.novalocal@NOVALOCAL

Valid starting     Expires            Service principal
07/14/14 19:18:10  07/15/14 19:18:09  krbtgt/NOVALOCAL@NOVALOCAL
	renew until 07/14/14 19:18:10
07/14/14 19:18:30  07/15/14 19:18:09  HTTP/server-1941.novalocal@NOVALOCAL
	renew until 07/14/14 19:18:10

Following are kdc configs:
cat /etc/krb5.conf
[logging]
 default = FILE:/var/log/krb5libs.log
 kdc = FILE:/var/log/krb5kdc.log
 admin_server = FILE:/var/log/kadmind.log

[libdefaults]
 default_realm = NOVALOCAL
 dns_lookup_realm = false
 dns_lookup_kdc = false
 ticket_lifetime = 24h
 renew_lifetime = 7d
 forwardable = true
 udp_preference_limit = 1000000
 default_tkt_enctypes = des-cbc-md5 des-cbc-crc des3-cbc-sha1
 default_tgs_enctypes = des-cbc-md5 des-cbc-crc des3-cbc-sha1
 permitted_enctypes = des-cbc-md5 des-cbc-crc des3-cbc-sha1
 allow_weak_crypto = true


[realms]
 NOVALOCAL = {
  kdc = server-355:88
  admin_server = server-355:749
  default_domain=novalocal
 }

[domain_realm]
 .novalocal = NOVALOCAL
 novalocal = NOVALOCAL


cat /var/kerberos/krb5kdc/kdc.conf
[kdcdefaults]
kdc_ports = 88
kdc_tcp_ports = 88
 
[realms]
NOVALOCAL = {
  acl_file = /var/kerberos/krb5kdc/kadm5.acl
  dict_file = /usr/share/dict/words
  admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab
  master_key_type = des3-hmac-sha1
  supported_enctypes = arcfour-hmac:normal des3-hmac-sha1:normal des-cbc-crc:normal des:normal des:v4 des:norealm des:onlyrealm des:afs3
}
 
I have updated my jdk to build 1.7.0_60-b19

"
TransferFsImage#receiveFile() should perform validation on fsImageName parameter,HDFS-6368,"Currently only null check is performed:
{code}
          if (fsImageName == null) {
            throw new IOException(""No filename header provided by server"");
          }
          newLocalPaths.add(new File(localPath, fsImageName));
{code}
Value of fsImageName, obtained from HttpURLConnection header, may be tainted.
This may allow an attacker to access, modify, or test the existence of critical or sensitive files."
CryptoCode.generateSecureRandom should be a static method,HDFS-6479,This method should be a static so that we don't have to go looking for the CryptoCodec instance anywhere we want to use this.
Clean up audit logging in FSNamesystem xattr methods,HDFS-6341,"We call logAuditEvent multiple times in each RPC handler, sometimes split across different functions. It'd be cleaner if we did this once, in a try/finally."
Remove assignments to method arguments,HDFS-6176,There are many places in the code where assignments are made to method arguments. Eclipse is quite happy to flag this if the appropriate warning is enabled.
Even if we configure the property fs.checkpoint.size in both core-site.xml and hdfs-site.xml  the values are not been considered,HDFS-2734,Even if we configure the property fs.checkpoint.size in both core-site.xml and hdfs-site.xml  the values are not been considered
Error encountered while attempting a dfsadmin -report,HDFS-2571,"Saw this while trying some things on HDFS-2569

{code}
鉃? hadoop-0.23.0  bin/hdfs dfsadmin -report
log4j:ERROR Could not find value for key log4j.appender.NullAppender
log4j:ERROR Could not instantiate appender named ""NullAppender"".
Configured Capacity: 4096 (4 KB)
Present Capacity: 4096 (4 KB)
DFS Remaining: 0 (0 KB)
DFS Used: 4096 (4 KB)
DFS Used%: 100%
Under replicated blocks: 0
Blocks with corrupt replicas: 0
Missing blocks: 0

-------------------------------------------------
Datanodes available: 1 (3 total, 2 dead)

Live datanodes:
Name: 127.0.0.1:50010 (localhost)
Decommission Status : Decommissioned
Configured Capacity: 499763888128 (465.44 GB)
DFS Used: 4096 (4 KB)
Non DFS Used: 134297399296 (125.07 GB)
DFS Remaining: 365466484736 (340.37 GB)
DFS Used%: 0%
DFS Remaining%: 73.13%
Last contact: Sun Nov 20 10:35:33 GMT+05:30 2011


Dead datanodes:
report: String index out of range: -1
{code}

The *report: String index out of range: -1* shouldn't appear so.

Exclude file content is:
{code}

鉃? hadoop-0.23.0  cat ~/Work/installs/hadoop-0.23.0/build/test/excludes
192.168.1.23:50010
127.0.0.1:50010
localhost:50010
localhost
127.0.0.1
192.168.1.23
{code}"
"In HDFS HA setup, FileSystem.getUri returns hdfs://<dfs.nameservices>",HDFS-6001,"When hdfs is set up with HA enable, FileSystem.getUri returns hdfs://<the-value-of-dfs.nameservices>
Here dfs.nameservices is defined when HA is enabled. In documentation:
{quote}
dfs.nameservices聽- the logical name for this new nameserviceChoose a logical name for this nameservice, for example ""mycluster"", and use this logical name for the value of this config option. The name you choose is arbitrary. It will be used both for configuration and as the authority component of absolute HDFS paths in the cluster.
Note:聽If you are also using HDFS Federation, this configuration setting should also include the list of other nameservices, HA or otherwise, as a comma-separated list.
<property>
  <name>dfs.nameservices</name>
  <value>mycluster</value>
</property>
{quote}
This is probably ok or even intended.  But a caller may further process the URI, for example, call URI.getHost(). This will return the 'mycluster', which is not a valid host anywhere."
eclipse-files target needs to depend on 'ivy-retrieve-test',HDFS-803,When {{ant eclipse-files}} is executed only common jars are guarantee to be pulled in. To pull test jars one needs to manually run {{ant ivy-retrieve-test}} first.
DataNode should reuse delBlockFromDisk,HDFS-372,FSDataSet should reuse delBlcokFromDisk where it should/can be used like in invalidateBlock.
limit concurrent connections(data serving thread) in one datanode,HDFS-285,"i'm here after HADOOP-2341 and HADOOP-2346, in my hbase env, many opening mapfiles cause datanode OOME(stack memory), because 2000+ data serving threads in datanode process.

although HADOOP-2346 has implements timeouts, it will be some situation many connection created  before the read timeout(default 6min) reach. like hbase does, it open all files on regionserver startup. 

limit concurrent connections(data serving thread) will make datanode more stable. and i think it could be done in SocketIOWithTimeout$SelectorPool#select:
1. in SelectorPool#select, record all waiting SelectorInfo instances in a List at the beginning, and remove it after 'Selector#select' done.
2. before real 'select',  do a limitation check, if reached, close the first selectorInfo. "
GnuWin32 coreutils df output causes DF to throw,HDFS-174,"The output from GnuWin32's coreutils's df looks like this:

C:\Program Files\GnuWin32\bin>df -k ""C:\hadoop-0.13.0""
Filesystem           1K-blocks      Used Available Use% Mounted on
df: `NTFS': No such file or directory
-                     96124924  86288848   9836076  90% C:\

This causes DF's parsing to fail with the following exception:

Exception in thread ""main"" java.io.IOException: df: `NTFS': No such file or directory
	at org.apache.hadoop.fs.DF.doDF(DF.java:65)
	at org.apache.hadoop.fs.DF.<init>(DF.java:54)
	at org.apache.hadoop.fs.DF.main(DF.java:168)

Fixing this would be useful since it might allow for Hadoop to be used without installing Cygwin."
SecondaryNameNode doCheckpoint() renames current directory before asking NameNode to rollEditLog(),HDFS-184,"In SecondaryNameNode doCheckPoint() function invokes _startCheckpoint()_ before calling _namenode.rollEditLog()_
_startCheckpoint()_ internally invokes _CheckpointStorage::startCheckpoint()_ which renames current to lastcheckpoint.tmp. if call to namenode failed, then we would redo the above step renaming empty current directory in next iteration? Should we remove after we know namenode has successfully rolled edits?"
FSImage.needsResaveBasedOnStaleCheckpoint multiplies dfs.namenode.checkpoint.period by 1000 as if it were in milliseconds.,HDFS-4117,This is a mistake.
initializeSharedEdits should warn if a shared edits dir is not configured,HDFS-3472,"Running initializeSharedEdits w/o a shared edits dir currently yields the following rather than a user friendly message about a shared edits dir not being configured.

{noformat}
12/05/29 13:03:08 INFO common.Storage: Storage directory /tmp/hadoop-hdfs/dfs/name does not exist. 12/05/29 13:03:08 ERROR namenode.NameNode: Could not initialize shared edits dir org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /tmp/hadoop-hdfs/dfs/name is in an inconsistent state: storage directory does not exist or is not accessible. at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:274) at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:180) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:498) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:390) at org.apache.hadoop.hdfs.server.namenode.NameNode.initializeSharedEdits(NameNode.java:818) at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1118) at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1193) 12/05/29 13:03:08 INFO namenode.NameNode: SHUTDOWN_MSG: /************************************************************ SHUTDOWN_MSG: Shutting down NameNode at centos60-17.ent.cloudera.com/172.29.111.244 ************************************************************/
{noformat}
"
Can not browse the Data Node blocks from NameNode UI if there is no Data Node hostname mappings in clients.,HDFS-1724,"In Our Environment, we don't have any host mappings related to Data Nodes. When we browse the Blocks information from Name Node UI, it is using hostname to connect, So not able to connect. Instead of using the hostname mapping, if we use IP address directly, things will work fine."
JVM crash under writes:   ExceptionMark destructor expects no pending exceptions,HDFS-1587,"Datanode went down due to JVM fault.
A decent number of reads/writes going on at the time.
To be honest, I'm not too worried about this, because it hasn't happened too frequently.
But I thought I'd just record it.

hs_error_pid*log was:

#
# A fatal error has been detected by the Java Runtime Environment:
#
#  Internal Error (exceptions.cpp:364), pid=1662, tid=140679121118992
#  Error: ExceptionMark destructor expects no pending exceptions
#
# JRE version: 6.0_20-b02
# Java VM: Java HotSpot(TM) 64-Bit Server VM (16.3-b01 mixed mode linux-amd64 )
# If you would like to submit a bug report, please visit:
#   http://java.sun.com/webapps/bugreport/crash.jsp
#

---------------  T H R E A D  ---------------

Current thread (0x00007ff264330000):  JavaThread ""IPC Server handler 0 on 50020"" daemon [_thread_in_vm, id=1755, stack(0x00007ff268faa000,0x00007ff2690ab000)]

Stack: [0x00007ff268faa000,0x00007ff2690ab000],  sp=0x00007ff2690a4680,  free space=3e90000000000000018k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
V  [libjvm.so+0x70f420]
V  [libjvm.so+0x2e43f6]
V  [libjvm.so+0x3333fd]
V  [libjvm.so+0x27ea18]
V  [libjvm.so+0x27e062]
V  [libjvm.so+0x27e0e6]
V  [libjvm.so+0x27faf6]
V  [libjvm.so+0x6a0c6f]
V  [libjvm.so+0x69edbb]
V  [libjvm.so+0x69dc51]
V  [libjvm.so+0x69dc80]
V  [libjvm.so+0x44251b]

Java frames: (J=compiled Java code, j=interpreted, Vv=VM code)
j  java.lang.ClassLoader.findBootstrapClass(Ljava/lang/String;)Ljava/lang/Class;+0
j  java.lang.ClassLoader.findBootstrapClassOrNull(Ljava/lang/String;)Ljava/lang/Class;+12
j  java.lang.ClassLoader.loadClass(Ljava/lang/String;Z)Ljava/lang/Class;+32
j  java.lang.ClassLoader.loadClass(Ljava/lang/String;Z)Ljava/lang/Class;+23
j  sun.misc.Launcher$AppClassLoader.loadClass(Ljava/lang/String;Z)Ljava/lang/Class;+41
j  java.lang.ClassLoader.loadClass(Ljava/lang/String;)Ljava/lang/Class;+3
j  java.util.ResourceBundle$RBClassLoader.loadClass(Ljava/lang/String;)Ljava/lang/Class;+10
j  java.util.ResourceBundle$Control.newBundle(Ljava/lang/String;Ljava/util/Locale;Ljava/lang/String;Ljava/lang/ClassLoader;Z)Ljava/util/ResourceBundle;+24
j  java.util.ResourceBundle.loadBundle(Ljava/util/ResourceBundle$CacheKey;Ljava/util/List;Ljava/util/ResourceBundle$Control;Z)Ljava/util/ResourceBundle;+54
j  java.util.ResourceBundle.findBundle(Ljava/util/ResourceBundle$CacheKey;Ljava/util/List;Ljava/util/List;ILjava/util/ResourceBundle$Control;Ljava/util/ResourceBundle;)Ljava/util/ResourceBundle;+213
j  java.util.ResourceBundle.findBundle(Ljava/util/ResourceBundle$CacheKey;Ljava/util/List;Ljava/util/List;ILjava/util/ResourceBundle$Control;Ljava/util/ResourceBundle;)Ljava/util/ResourceBundle;+37
j  java.util.ResourceBundle.getBundleImpl(Ljava/lang/String;Ljava/util/Locale;Ljava/lang/ClassLoader;Ljava/util/ResourceBundle$Control;)Ljava/util/ResourceBundle;+187
j  java.util.ResourceBundle.getBundle(Ljava/lang/String;)Ljava/util/ResourceBundle;+10
j  com.sun.security.auth.PolicyFile$1.run()Ljava/lang/Object;+2
v  ~StubRoutines::call_stub
j  java.security.AccessController.doPrivileged(Ljava/security/PrivilegedAction;)Ljava/lang/Object;+0
j  com.sun.security.auth.PolicyFile.<clinit>()V+7
v  ~StubRoutines::call_stub
j  java.lang.Class.forName0(Ljava/lang/String;ZLjava/lang/ClassLoader;)Ljava/lang/Class;+0
j  java.lang.Class.forName(Ljava/lang/String;ZLjava/lang/ClassLoader;)Ljava/lang/Class;+32
j  javax.security.auth.Policy$3.run()Ljava/lang/Object;+8
v  ~StubRoutines::call_stub
J  java.security.AccessController.doPrivileged(Ljava/security/PrivilegedExceptionAction;)Ljava/lang/Object;
j  javax.security.auth.Policy.getPolicyNoCheck()Ljavax/security/auth/Policy;+51
j  javax.security.auth.Policy.getPolicy()Ljavax/security/auth/Policy;+21
j  javax.security.auth.SubjectDomainCombiner$5.run()Ljava/lang/Object;+0
v  ~StubRoutines::call_stub
j  java.security.AccessController.doPrivileged(Ljava/security/PrivilegedAction;)Ljava/lang/Object;+0
j  javax.security.auth.SubjectDomainCombiner.compatPolicy()Z+7
j  javax.security.auth.SubjectDomainCombiner.<clinit>()V+10
v  ~StubRoutines::call_stub
j  javax.security.auth.Subject$2.run()Ljava/lang/Object;+28
v  ~StubRoutines::call_stub
j  java.security.AccessController.doPrivileged(Ljava/security/PrivilegedAction;)Ljava/lang/Object;+0
j  javax.security.auth.Subject.createContext(Ljavax/security/auth/Subject;Ljava/security/AccessControlContext;)Ljava/security/AccessControlContext;+9
j  javax.security.auth.Subject.doAs(Ljavax/security/auth/Subject;Ljava/security/PrivilegedExceptionAction;)Ljava/lang/Object;+39
j  org.apache.hadoop.ipc.Server$Handler.run()V+168
v  ~StubRoutines::call_stub

---------------  P R O C E S S  ---------------

Java Threads: ( => current thread )
  0x00007ff2642f1000 JavaThread ""process reaper"" daemon [_thread_in_native, id=7220, stack(0x00007ff263bfc000,0x00007ff263cfd000)]
  0x000000004069b000 JavaThread ""PacketResponder 1 for Block blk_-2874635287821588874_1601382"" daemon [_thread_in_native, id=5238, stack(0x00007ff260bcc000,0x00007ff260ccd000)]
  0x00007ff264534800 JavaThread ""org.apache.hadoop.hdfs.server.datanode.DataXceiver@47fe9d9f"" daemon [_thread_in_native, id=5237, stack(0x00007ff2630f1000,0x00007ff2631f2000)]
  0x00007ff264580000 JavaThread ""org.apache.hadoop.hdfs.server.datanode.DataXceiver@1af438fd"" daemon [_thread_in_native, id=15176, stack(0x00007ff2632f3000,0x00007ff2633f4000)]
  0x00007ff264394800 JavaThread ""org.apache.hadoop.hdfs.server.datanode.DataBlockScanner@524c71d2"" daemon [_thread_blocked, id=1779, stack(0x00007ff263cfd000,0x00007ff263dfe000)]
  0x00000000402b3000 JavaThread ""org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@45490eb5"" daemon [_thread_in_native, id=1772, stack(0x00007ff263dfe000,0x00007ff263eff000)]
  0x00007ff26443f800 JavaThread ""DataNode: [/home/stu/hadoop/tmp/dfs/data,/mnt/drive2/hadoop-storage]"" daemon [_thread_blocked, id=1771, stack(0x00007ff263eff000,0x00007ff264000000)]
  0x00007ff2646d7000 JavaThread ""IPC Client (47) connection to ubuntu-namenode/192.168.195.82:54310 from stu"" daemon [_thread_blocked, id=1770, stack(0x00007ff26809b000,0x00007ff26819c000)]
  0x00007ff2646d5000 JavaThread ""IPC Server handler 14 on 50020"" daemon [_thread_blocked, id=1769, stack(0x00007ff26819c000,0x00007ff26829d000)]
  0x00007ff2646d3000 JavaThread ""IPC Server handler 13 on 50020"" daemon [_thread_blocked, id=1768, stack(0x00007ff26829d000,0x00007ff26839e000)]
  0x00007ff2646d1000 JavaThread ""IPC Server handler 12 on 50020"" daemon [_thread_blocked, id=1767, stack(0x00007ff26839e000,0x00007ff26849f000)]
  0x00007ff2644c0800 JavaThread ""IPC Server handler 11 on 50020"" daemon [_thread_blocked, id=1766, stack(0x00007ff26849f000,0x00007ff2685a0000)]
  0x00007ff2644be800 JavaThread ""IPC Server handler 10 on 50020"" daemon [_thread_blocked, id=1765, stack(0x00007ff2685a0000,0x00007ff2686a1000)]
  0x00007ff2644bc800 JavaThread ""IPC Server handler 9 on 50020"" daemon [_thread_blocked, id=1764, stack(0x00007ff2686a1000,0x00007ff2687a2000)]
  0x00007ff2644ba800 JavaThread ""IPC Server handler 8 on 50020"" daemon [_thread_blocked, id=1763, stack(0x00007ff2687a2000,0x00007ff2688a3000)]
  0x00007ff2642e0800 JavaThread ""IPC Server handler 7 on 50020"" daemon [_thread_blocked, id=1762, stack(0x00007ff2688a3000,0x00007ff2689a4000)]
  0x00007ff2642de800 JavaThread ""IPC Server handler 6 on 50020"" daemon [_thread_blocked, id=1761, stack(0x00007ff2689a4000,0x00007ff268aa5000)]
  0x00007ff2642dc800 JavaThread ""IPC Server handler 5 on 50020"" daemon [_thread_blocked, id=1760, stack(0x00007ff268aa5000,0x00007ff268ba6000)]
  0x00007ff2647d3800 JavaThread ""IPC Server handler 4 on 50020"" daemon [_thread_blocked, id=1759, stack(0x00007ff268ba6000,0x00007ff268ca7000)]
  0x00007ff264396800 JavaThread ""IPC Server handler 3 on 50020"" daemon [_thread_blocked, id=1758, stack(0x00007ff268ca7000,0x00007ff268da8000)]
  0x00007ff264395800 JavaThread ""IPC Server handler 2 on 50020"" daemon [_thread_blocked, id=1757, stack(0x00007ff268da8000,0x00007ff268ea9000)]
  0x00007ff2643fe800 JavaThread ""IPC Server handler 1 on 50020"" daemon [_thread_blocked, id=1756, stack(0x00007ff268ea9000,0x00007ff268faa000)]
=>0x00007ff264330000 JavaThread ""IPC Server handler 0 on 50020"" daemon [_thread_in_vm, id=1755, stack(0x00007ff268faa000,0x00007ff2690ab000)]
  0x00007ff2645c3800 JavaThread ""IPC Server listener on 50020"" daemon [_thread_in_native, id=1754, stack(0x00007ff2690ab000,0x00007ff2691ac000)]
  0x00007ff264657800 JavaThread ""IPC Server Responder"" daemon [_thread_in_native, id=1753, stack(0x00007ff2691ac000,0x00007ff2692ad000)]
  0x00007ff264578800 JavaThread ""Timer-0"" daemon [_thread_blocked, id=1752, stack(0x00007ff269be2000,0x00007ff269ce3000)]
  0x0000000040662000 JavaThread ""29876954@qtp0-1 - Acceptor0 SelectChannelConnector@0.0.0.0:50075"" [_thread_in_native, id=1751, stack(0x00007ff269fee000,0x00007ff26a0ef000)]
  0x0000000040208000 JavaThread ""1908393212@qtp0-0"" [_thread_blocked, id=1750, stack(0x00007ff269eec000,0x00007ff269fed000)]
  0x00000000401e5000 JavaThread ""refreshUsed-/mnt/drive2/hadoop-storage"" daemon [_thread_blocked, id=1741, stack(0x00007ff269ae1000,0x00007ff269be2000)]
  0x0000000040752800 JavaThread ""refreshUsed-/home/stu/hadoop-storage/dfs/data"" daemon [_thread_blocked, id=1729, stack(0x00007ff269ce3000,0x00007ff269de4000)]
  0x00007ff264228000 JavaThread ""RMI TCP Accept-0"" daemon [_thread_in_native, id=1699, stack(0x00007ff26a1f6000,0x00007ff26a2f7000)]
  0x00007ff264070000 JavaThread ""Low Memory Detector"" daemon [_thread_blocked, id=1697, stack(0x00007ff26a93c000,0x00007ff26aa3d000)]
  0x00007ff26406d800 JavaThread ""CompilerThread1"" daemon [_thread_blocked, id=1696, stack(0x00007ff26aa3d000,0x00007ff26ab3e000)]
  0x00007ff26406a800 JavaThread ""CompilerThread0"" daemon [_thread_blocked, id=1695, stack(0x00007ff26ab3e000,0x00007ff26ac3f000)]
  0x00007ff264068800 JavaThread ""Signal Dispatcher"" daemon [_thread_blocked, id=1694, stack(0x00007ff26ac3f000,0x00007ff26ad40000)]
  0x00007ff264048800 JavaThread ""Finalizer"" daemon [_thread_blocked, id=1693, stack(0x00007ff26ad7f000,0x00007ff26ae80000)]
  0x00007ff264046800 JavaThread ""Reference Handler"" daemon [_thread_blocked, id=1692, stack(0x00007ff26ae80000,0x00007ff26af81000)]
  0x00000000401ad800 JavaThread ""main"" [_thread_blocked, id=1688, stack(0x00007ff2f20f6000,0x00007ff2f21f7000)]

Other Threads:
  0x00007ff264042000 VMThread [stack: 0x00007ff26af81000,0x00007ff26b082000] [id=1691]
  0x00007ff264237000 WatcherThread [stack: 0x00007ff26a0f5000,0x00007ff26a1f6000] [id=1700]

VM state:not at safepoint (normal execution)

VM Mutex/Monitor currently owned by a thread: None

Heap
 PSYoungGen      total 48128K, used 42440K [0x00007ff2c47b0000, 0x00007ff2eb800000, 0x00007ff2ee250000)
  eden space 47808K, 88% used [0x00007ff2c47b0000,0x00007ff2c70d2138,0x00007ff2c7660000)
  from space 320K, 100% used [0x00007ff2eb7b0000,0x00007ff2eb800000,0x00007ff2eb800000)
  to   space 512K, 0% used [0x00007ff2eb700000,0x00007ff2eb700000,0x00007ff2eb780000)
 PSOldGen        total 29056K, used 16888K [0x00007ff271250000, 0x00007ff272eb0000, 0x00007ff2c47b0000)
  object space 29056K, 58% used [0x00007ff271250000,0x00007ff2722ce2f8,0x00007ff272eb0000)
 PSPermGen       total 21504K, used 16618K [0x00007ff26be50000, 0x00007ff26d350000, 0x00007ff271250000)
  object space 21504K, 77% used [0x00007ff26be50000,0x00007ff26ce8aa88,0x00007ff26d350000)

Dynamic libraries:
40000000-40009000 r-xp 00000000 08:01 3933705                            /usr/lib/jvm/java-6-sun-1.6.0.20/jre/bin/java (deleted)
40108000-4010a000 rwxp 00008000 08:01 3933705                            /usr/lib/jvm/java-6-sun-1.6.0.20/jre/bin/java (deleted)
401a3000-40de1000 rwxp 00000000 00:00 0                                  [heap]
7ff260bcc000-7ff260bcf000 ---p 00000000 00:00 0 
7ff260bcf000-7ff260ccd000 rwxp 00000000 00:00 0 
7ff2612d3000-7ff2612d6000 ---p 00000000 00:00 0 
7ff2612d6000-7ff2613d4000 rwxp 00000000 00:00 0 
7ff2613d4000-7ff2613d7000 ---p 00000000 00:00 0 
7ff2613d7000-7ff2614d5000 rwxp 00000000 00:00 0 
7ff2614d5000-7ff2614d8000 ---p 00000000 00:00 0 
7ff2614d8000-7ff2615d6000 rwxp 00000000 00:00 0 
7ff2615d6000-7ff2615d9000 ---p 00000000 00:00 0 
7ff2615d9000-7ff2616d7000 rwxp 00000000 00:00 0 
7ff2616d7000-7ff2616da000 ---p 00000000 00:00 0 
7ff2616da000-7ff2617d8000 rwxp 00000000 00:00 0 
7ff2617d8000-7ff2617db000 ---p 00000000 00:00 0 
7ff2617db000-7ff2618d9000 rwxp 00000000 00:00 0 
7ff2618d9000-7ff2618dc000 ---p 00000000 00:00 0 
7ff2618dc000-7ff2619da000 rwxp 00000000 00:00 0 
7ff2619da000-7ff2619dd000 ---p 00000000 00:00 0 
7ff2619dd000-7ff261adb000 rwxp 00000000 00:00 0 
7ff261adb000-7ff261ade000 ---p 00000000 00:00 0 
7ff261ade000-7ff261bdc000 rwxp 00000000 00:00 0 
7ff261bdc000-7ff261bdf000 ---p 00000000 00:00 0 
7ff261bdf000-7ff261cdd000 rwxp 00000000 00:00 0 
7ff261cdd000-7ff261ce0000 ---p 00000000 00:00 0 
7ff261ce0000-7ff261dde000 rwxp 00000000 00:00 0 
7ff261dde000-7ff261de1000 ---p 00000000 00:00 0 
7ff261de1000-7ff261edf000 rwxp 00000000 00:00 0 
7ff261edf000-7ff261ee2000 ---p 00000000 00:00 0 
7ff261ee2000-7ff261fe0000 rwxp 00000000 00:00 0 
7ff261fe0000-7ff261fe3000 ---p 00000000 00:00 0 
7ff261fe3000-7ff2620e1000 rwxp 00000000 00:00 0 
7ff2620e1000-7ff2620e4000 ---p 00000000 00:00 0 
7ff2620e4000-7ff2621e2000 rwxp 00000000 00:00 0 
7ff2621e2000-7ff2621e5000 ---p 00000000 00:00 0 
7ff2621e5000-7ff2622e3000 rwxp 00000000 00:00 0 
7ff2622e3000-7ff2622e6000 ---p 00000000 00:00 0 
7ff2622e6000-7ff2623e4000 rwxp 00000000 00:00 0 
7ff2623e4000-7ff2623e7000 ---p 00000000 00:00 0 
7ff2623e7000-7ff2624e5000 rwxp 00000000 00:00 0 
7ff2624e5000-7ff2624e8000 ---p 00000000 00:00 0 
7ff2624e8000-7ff2625e6000 rwxp 00000000 00:00 0 
7ff2625e6000-7ff2625e9000 ---p 00000000 00:00 0 
7ff2625e9000-7ff2626e7000 rwxp 00000000 00:00 0 
7ff2626e7000-7ff2626ea000 ---p 00000000 00:00 0 
7ff2626ea000-7ff2627e8000 rwxp 00000000 00:00 0 
7ff2627e8000-7ff2627eb000 ---p 00000000 00:00 0 
7ff2627eb000-7ff2628e9000 rwxp 00000000 00:00 0 
7ff2628e9000-7ff2628ec000 ---p 00000000 00:00 0 
7ff2628ec000-7ff2629ea000 rwxp 00000000 00:00 0 
7ff2629ea000-7ff2629ed000 ---p 00000000 00:00 0 
7ff2629ed000-7ff262aeb000 rwxp 00000000 00:00 0 
7ff262aeb000-7ff262aee000 ---p 00000000 00:00 0 
7ff262aee000-7ff262bec000 rwxp 00000000 00:00 0 
7ff262bec000-7ff262bef000 ---p 00000000 00:00 0 
7ff262bef000-7ff262ced000 rwxp 00000000 00:00 0 
7ff262ced000-7ff262cf0000 ---p 00000000 00:00 0 
7ff262cf0000-7ff262dee000 rwxp 00000000 00:00 0 
7ff262dee000-7ff262df1000 ---p 00000000 00:00 0 
7ff262df1000-7ff262eef000 rwxp 00000000 00:00 0 
7ff262eef000-7ff262ef2000 ---p 00000000 00:00 0 
7ff262ef2000-7ff262ff0000 rwxp 00000000 00:00 0 
7ff262ff0000-7ff262ff3000 ---p 00000000 00:00 0 
7ff262ff3000-7ff2630f1000 rwxp 00000000 00:00 0 
7ff2630f1000-7ff2630f4000 ---p 00000000 00:00 0 
7ff2630f4000-7ff2631f2000 rwxp 00000000 00:00 0 
7ff2631f2000-7ff2631f5000 ---p 00000000 00:00 0 
7ff2631f5000-7ff2632f3000 rwxp 00000000 00:00 0 
7ff2632f3000-7ff2632f6000 ---p 00000000 00:00 0 
7ff2632f6000-7ff2633f4000 rwxp 00000000 00:00 0 
7ff2633f4000-7ff2633f7000 ---p 00000000 00:00 0 
7ff2633f7000-7ff2634f5000 rwxp 00000000 00:00 0 
7ff2634f5000-7ff2634f8000 ---p 00000000 00:00 0 
7ff2634f8000-7ff2635f6000 rwxp 00000000 00:00 0 
7ff2635f6000-7ff2635f9000 ---p 00000000 00:00 0 
7ff2635f9000-7ff2636f7000 rwxp 00000000 00:00 0 
7ff2637f8000-7ff2637fb000 ---p 00000000 00:00 0 
7ff2637fb000-7ff2638f9000 rwxp 00000000 00:00 0 
7ff2638f9000-7ff2638fc000 ---p 00000000 00:00 0 
7ff2638fc000-7ff2639fa000 rwxp 00000000 00:00 0 
7ff2639fa000-7ff2639fd000 ---p 00000000 00:00 0 
7ff2639fd000-7ff263afb000 rwxp 00000000 00:00 0 
7ff263afb000-7ff263afe000 ---p 00000000 00:00 0 
7ff263afe000-7ff263bfc000 rwxp 00000000 00:00 0 
7ff263bfc000-7ff263bff000 ---p 00000000 00:00 0 
7ff263bff000-7ff263cfd000 rwxp 00000000 00:00 0 
7ff263cfd000-7ff263d00000 ---p 00000000 00:00 0 
7ff263d00000-7ff263dfe000 rwxp 00000000 00:00 0 
7ff263dfe000-7ff263e01000 ---p 00000000 00:00 0 
7ff263e01000-7ff263eff000 rwxp 00000000 00:00 0 
7ff263eff000-7ff263f02000 ---p 00000000 00:00 0 
7ff263f02000-7ff264000000 rwxp 00000000 00:00 0 
7ff264000000-7ff265890000 rwxp 00000000 00:00 0 
7ff265890000-7ff268000000 ---p 00000000 00:00 0 
7ff26809b000-7ff26809e000 ---p 00000000 00:00 0 
7ff26809e000-7ff26819c000 rwxp 00000000 00:00 0 
7ff26819c000-7ff26819f000 ---p 00000000 00:00 0 
7ff26819f000-7ff26829d000 rwxp 00000000 00:00 0 
7ff26829d000-7ff2682a0000 ---p 00000000 00:00 0 
7ff2682a0000-7ff26839e000 rwxp 00000000 00:00 0 
7ff26839e000-7ff2683a1000 ---p 00000000 00:00 0 
7ff2683a1000-7ff26849f000 rwxp 00000000 00:00 0 
7ff26849f000-7ff2684a2000 ---p 00000000 00:00 0 
7ff2684a2000-7ff2685a0000 rwxp 00000000 00:00 0 
7ff2685a0000-7ff2685a3000 ---p 00000000 00:00 0 
7ff2685a3000-7ff2686a1000 rwxp 00000000 00:00 0 
7ff2686a1000-7ff2686a4000 ---p 00000000 00:00 0 
7ff2686a4000-7ff2687a2000 rwxp 00000000 00:00 0 
7ff2687a2000-7ff2687a5000 ---p 00000000 00:00 0 
7ff2687a5000-7ff2688a3000 rwxp 00000000 00:00 0 
7ff2688a3000-7ff2688a6000 ---p 00000000 00:00 0 
7ff2688a6000-7ff2689a4000 rwxp 00000000 00:00 0 
7ff2689a4000-7ff2689a7000 ---p 00000000 00:00 0 
7ff2689a7000-7ff268aa5000 rwxp 00000000 00:00 0 
7ff268aa5000-7ff268aa8000 ---p 00000000 00:00 0 
7ff268aa8000-7ff268ba6000 rwxp 00000000 00:00 0 
7ff268ba6000-7ff268ba9000 ---p 00000000 00:00 0 
7ff268ba9000-7ff268ca7000 rwxp 00000000 00:00 0 
7ff268ca7000-7ff268caa000 ---p 00000000 00:00 0 
7ff268caa000-7ff268da8000 rwxp 00000000 00:00 0 
7ff268da8000-7ff268dab000 ---p 00000000 00:00 0 
7ff268dab000-7ff268ea9000 rwxp 00000000 00:00 0 
7ff268ea9000-7ff268eac000 ---p 00000000 00:00 0 
7ff268eac000-7ff268faa000 rwxp 00000000 00:00 0 
7ff268faa000-7ff268fad000 ---p 00000000 00:00 0 
7ff268fad000-7ff2690ab000 rwxp 00000000 00:00 0 
7ff2690ab000-7ff2690ae000 ---p 00000000 00:00 0 
7ff2690ae000-7ff2691ac000 rwxp 00000000 00:00 0 
7ff2691ac000-7ff2691af000 ---p 00000000 00:00 0 
7ff2691af000-7ff2692ad000 rwxp 00000000 00:00 0 
7ff2692ad000-7ff2692af000 r-xp 00000000 08:01 3207                       /lib/libnss_mdns4.so.2
7ff2692af000-7ff2694ae000 ---p 00002000 08:01 3207                       /lib/libnss_mdns4.so.2
7ff2694ae000-7ff2694af000 r-xp 00001000 08:01 3207                       /lib/libnss_mdns4.so.2
7ff2694af000-7ff2694b0000 rwxp 00002000 08:01 3207                       /lib/libnss_mdns4.so.2
7ff2694b0000-7ff2694c6000 r-xp 00000000 08:01 6443                       /lib/libresolv-2.11.1.so (deleted)
7ff2694c6000-7ff2696c5000 ---p 00016000 08:01 6443                       /lib/libresolv-2.11.1.so (deleted)
7ff2696c5000-7ff2696c6000 r-xp 00015000 08:01 6443                       /lib/libresolv-2.11.1.so (deleted)
7ff2696c6000-7ff2696c7000 rwxp 00016000 08:01 6443                       /lib/libresolv-2.11.1.so (deleted)
7ff2696c7000-7ff2696c9000 rwxp 00000000 00:00 0 
7ff2696c9000-7ff2696ce000 r-xp 00000000 08:01 3453                       /lib/libnss_dns-2.11.1.so (deleted)
7ff2696ce000-7ff2698cd000 ---p 00005000 08:01 3453                       /lib/libnss_dns-2.11.1.so (deleted)
7ff2698cd000-7ff2698ce000 r-xp 00004000 08:01 3453                       /lib/libnss_dns-2.11.1.so (deleted)
7ff2698ce000-7ff2698cf000 rwxp 00005000 08:01 3453                       /lib/libnss_dns-2.11.1.so (deleted)
7ff2698cf000-7ff2698d1000 r-xp 00000000 08:01 3211                       /lib/libnss_mdns4_minimal.so.2
7ff2698d1000-7ff269ad0000 ---p 00002000 08:01 3211                       /lib/libnss_mdns4_minimal.so.2
7ff269ad0000-7ff269ad1000 r-xp 00001000 08:01 3211                       /lib/libnss_mdns4_minimal.so.2
7ff269ad1000-7ff269ad2000 rwxp 00002000 08:01 3211                       /lib/libnss_mdns4_minimal.so.2
7ff269ae1000-7ff269ae4000 ---p 00000000 00:00 0 
7ff269ae4000-7ff269be2000 rwxp 00000000 00:00 0 
7ff269be2000-7ff269be5000 ---p 00000000 00:00 0 
7ff269be5000-7ff269ce3000 rwxp 00000000 00:00 0 
7ff269ce3000-7ff269ce6000 ---p 00000000 00:00 0 
7ff269ce6000-7ff269de4000 rwxp 00000000 00:00 0 
7ff269de4000-7ff269deb000 r-xp 00000000 08:01 3933875                    /usr/lib/jvm/java-6-sun-1.6.0.20/jre/lib/amd64/libnio.so (deleted)
7ff269deb000-7ff269eea000 ---p 00007000 08:01 3933875                    /usr/lib/jvm/java-6-sun-1.6.0.20/jre/lib/amd64/libnio.so (deleted)
7ff269eea000-7ff269eec000 rwxp 00006000 08:01 3933875                    /usr/lib/jvm/java-6-sun-1.6.0.20/jre/lib/amd64/libnio.so (deleted)
7ff269eec000-7ff269eef000 ---p 00000000 00:00 0 
7ff269eef000-7ff269fed000 rwxp 00000000 00:00 0 
7ff269fed000-7ff269fee000 r-xp 00000000 00:00 0 
7ff269fee000-7ff269ff1000 ---p 00000000 00:00 0 
7ff269ff1000-7ff26a0ef000 rwxp 00000000 00:00 0 
7ff26a0ef000-7ff26a0f5000 r-xs 00092000 08:01 3938501                    /usr/lib/jvm/java-6-sun-1.6.0.20/jre/lib/jsse.jar (deleted)
7ff26a0f5000-7ff26a0f6000 ---p 00000000 00:00 0 
7ff26a0f6000-7ff26a1f6000 rwxp 00000000 00:00 0 
7ff26a1f6000-7ff26a1f9000 ---p 00000000 00:00 0 
7ff26a1f9000-7ff26a2f7000 rwxp 00000000 00:00 0 
7ff26a2f7000-7ff26a304000 r-xs 000d3000 08:01 9702423                    /home/stu/hbase-0.20.6/lib/zookeeper-3.2.2.jar
7ff26a304000-7ff26a320000 r-xs 001d4000 08:01 9702385                    /home/stu/hbase-0.20.6/hbase-0.20.6-test.jar
7ff26a320000-7ff26a338000 r-xs 00169000 08:01 9701059                    /home/stu/hbase-0.20.6/hbase-0.20.6.jar
7ff26a338000-7ff26a33b000 r-xs 0001e000 08:01 265397                     /home/stu/hadoop-0.20.2/lib/jsp-2.1/jsp-api-2.1.jar
7ff26a33b000-7ff26a34c000 r-xs 000ea000 08:01 265396                     /home/stu/hadoop-0.20.2/lib/jsp-2.1/jsp-2.1.jar
7ff26a34c000-7ff26a34d000 r-xs 00003000 08:01 265417                     /home/stu/hadoop-0.20.2/lib/xmlenc-0.52.jar
7ff26a34d000-7ff26a367000 r-xs 00127000 08:01 265402                     /home/stu/hadoop-0.20.2/lib/mockito-all-1.8.0.jar
7ff26a367000-7ff26a36a000 r-xs 0001b000 08:01 265398                     /home/stu/hadoop-0.20.2/lib/junit-3.8.1.jar
7ff26a36a000-7ff26a36d000 r-xs 00025000 08:01 265395                     /home/stu/hadoop-0.20.2/lib/jetty-util-6.1.14.jar
7ff26a36d000-7ff26a374000 r-xs 00078000 08:01 265394                     /home/stu/hadoop-0.20.2/lib/jetty-6.1.14.jar
7ff26a374000-7ff26a379000 r-xs 0004a000 08:01 265393                     /home/stu/hadoop-0.20.2/lib/jets3t-0.6.1.jar
7ff26a379000-7ff26a37b000 r-xs 00011000 08:01 265383                     /home/stu/hadoop-0.20.2/lib/jasper-runtime-5.5.12.jar
7ff26a37b000-7ff26a380000 r-xs 0005e000 08:01 265382                     /home/stu/hadoop-0.20.2/lib/jasper-compiler-5.5.12.jar
7ff26a380000-7ff26a389000 r-xs 000a4000 08:01 265381                     /home/stu/hadoop-0.20.2/lib/hsqldb-1.8.0.10.jar
7ff26a389000-7ff26a3ac000 r-xs 00344000 08:01 265379                     /home/stu/hadoop-0.20.2/lib/core-3.1.1.jar
7ff26a3ac000-7ff26a3d0000 r-xs 0026d000 08:01 265363                     /home/stu/hadoop-0.20.2/hadoop-0.20.2-core.jar
7ff26a3d0000-7ff26a42f000 r-xs 00ba5000 08:01 3938424                    /usr/lib/jvm/java-6-sun-1.6.0.20/lib/tools.jar (deleted)
7ff26a42f000-7ff26a435000 r-xp 00000000 08:01 3933918                    /usr/lib/jvm/java-6-sun-1.6.0.20/jre/lib/amd64/libmanagement.so (deleted)
7ff26a435000-7ff26a534000 ---p 00006000 08:01 3933918                    /usr/lib/jvm/java-6-sun-1.6.0.20/jre/lib/amd64/libmanagement.so (deleted)
7ff26a534000-7ff26a536000 rwxp 00005000 08:01 3933918                    /usr/lib/jvm/java-6-sun-1.6.0.20/jre/lib/amd64/libmanagement.so (deleted)
7ff26a536000-7ff26a825000 rwxp 00000000 00:00 0 
7ff26a825000-7ff26a838000 r-xp 00000000 08:01 3933871                    /usr/lib/jvm/java-6-sun-1.6.0.20/jre/lib/amd64/libnet.so (deleted)
7ff26a838000-7ff26a939000 ---p 00013000 08:01 3933871                    /usr/lib/jvm/java-6-sun-1.6.0.20/jre/lib/amd64/libnet.so (deleted)
7ff26a939000-7ff26a93c000 rwxp 00014000 08:01 3933871                    /usr/lib/jvm/java-6-sun-1.6.0.20/jre/lib/amd64/libnet.so (deleted)
7ff26a93c000-7ff26a93f000 ---p 00000000 00:00 0 
7ff26a93f000-7ff26aa3d000 rwxp 00000000 00:00 0 
7ff26aa3d000-7ff26aa40000 ---p 00000000 00:00 0 
7ff26aa40000-7ff26ab3e000 rwxp 00000000 00:00 0 
7ff26ab3e000-7ff26ab41000 ---p 00000000 00:00 0 
7ff26ab41000-7ff26ac3f000 rwxp 00000000 00:00 0 
7ff26ac3f000-7ff26ac42000 ---p 00000000 00:00 0 
7ff26ac42000-7ff26ad40000 rwxp 00000000 00:00 0 
7ff26ad40000-7ff26ad7f000 r-xp 00000000 08:01 9699819                    /usr/lib/locale/en_US.utf8/LC_CTYPE
7ff26ad7f000-7ff26ad82000 ---p 00000000 00:00 0 
7ff26ad82000-7ff26ae80000 rwxp 00000000 00:00 0 
7ff26ae80000-7ff26ae83000 ---p 00000000 00:00 0 
7ff26ae83000-7ff26af81000 rwxp 00000000 00:00 0 
7ff26af81000-7ff26af82000 ---p 00000000 00:00 0 
7ff26af82000-7ff26b3b8000 rwxp 00000000 00:00 0 
7ff26b3b8000-7ff26b54e000 r-xs 02fd0000 08:01 3933864                    /usr/lib/jvm/java-6-sun-1.6.0.20/jre/lib/rt.jar (deleted)
7ff26b54e000-7ff26b576000 rwxp 00000000 00:00 0 
7ff26b576000-7ff26b577000 ---p 00000000 00:00 0 
7ff26b577000-7ff26b677000 rwxp 00000000 00:00 0 
7ff26b677000-7ff26b678000 ---p 00000000 00:00 0 
7ff26b678000-7ff26b783000 rwxp 00000000 00:00 0 
7ff26b783000-7ff26b789000 ---p 00000000 00:00 0 
7ff26b789000-7ff26b7a2000 rwxp 00000000 00:00 0 
7ff26b7a2000-7ff26b7b1000 rwxp 00000000 00:00 0 
7ff26b7b1000-7ff26b7b7000 ---p 00000000 00:00 0 
7ff26b7b7000-7ff26ba3d000 rwxp 00000000 00:00 0 
7ff26ba3d000-7ff26ba48000 rwxp 00000000 00:00 0 
7ff26ba48000-7ff26ba4e000 ---p 00000000 00:00 0 
7ff26ba4e000-7ff26ba67000 rwxp 00000000 00:00 0 
7ff26ba67000-7ff26ba76000 rwxp 00000000 00:00 0 
7ff26ba76000-7ff26ba7c000 ---p 00000000 00:00 0 
7ff26ba7c000-7ff26bd01000 rwxp 00000000 00:00 0 
7ff26bd01000-7ff26be3a000 rwxp 00000000 00:00 0 
7ff26be3a000-7ff26be4f000 ---p 00000000 00:00 0 
7ff26be4f000-7ff26d350000 rwxp 00000000 00:00 0 
7ff26d350000-7ff26df70000 ---p 00000000 00:00 0 
7ff26df70000-7ff271250000 rwxp 00000000 00:00 0 
7ff271250000-7ff272eb0000 rwxp 00000000 00:00 0 
7ff272eb0000-7ff273ba0000 ---p 00000000 00:00 0 
7ff273ba0000-7ff2c47b0000 rwxp 00000000 00:00 0 
7ff2c47b0000-7ff2eb800000 rwxp 00000000 00:00 0 
7ff2eb800000-7ff2ee250000 ---p 00000000 00:00 0 
7ff2ee250000-7ff2ee252000 r-xs 00001000 08:01 265416                     /home/stu/hadoop-0.20.2/lib/slf4j-log4j12-1.4.3.jar
7ff2ee252000-7ff2ee253000 r-xs 00003000 08:01 265415                     /home/stu/hadoop-0.20.2/lib/slf4j-api-1.4.3.jar
7ff2ee253000-7ff2ee256000 r-xs 0001e000 08:01 265414                     /home/stu/hadoop-0.20.2/lib/servlet-api-2.5-6.1.14.jar
7ff2ee256000-7ff2ee4e6000 rwxp 00000000 00:00 0 
7ff2ee4e6000-7ff2f1256000 rwxp 00000000 00:00 0 
7ff2f1256000-7ff2f1264000 r-xp 00000000 08:01 3933885                    /usr/lib/jvm/java-6-sun-1.6.0.20/jre/lib/amd64/libzip.so (deleted)
7ff2f1264000-7ff2f1366000 ---p 0000e000 08:01 3933885                    /usr/lib/jvm/java-6-sun-1.6.0.20/jre/lib/amd64/libzip.so (deleted)
7ff2f1366000-7ff2f1369000 rwxp 00010000 08:01 3933885                    /usr/lib/jvm/java-6-sun-1.6.0.20/jre/lib/amd64/libzip.so (deleted)
7ff2f1369000-7ff2f136a000 rwxp 00000000 00:00 0 
7ff2f136a000-7ff2f1376000 r-xp 00000000 08:01 4687                       /lib/libnss_files-2.11.1.so (deleted)
7ff2f1376000-7ff2f1575000 ---p 0000c000 08:01 4687                       /lib/libnss_files-2.11.1.so (deleted)
7ff2f1575000-7ff2f1576000 r-xp 0000b000 08:01 4687                       /lib/libnss_files-2.11.1.so (deleted)
7ff2f1576000-7ff2f1577000 rwxp 0000c000 08:01 4687                       /lib/libnss_files-2.11.1.so (deleted)
7ff2f1577000-7ff2f1581000 r-xp 00000000 08:01 5844                       /lib/libnss_nis-2.11.1.so (deleted)
7ff2f1581000-7ff2f1780000 ---p 0000a000 08:01 5844                       /lib/libnss_nis-2.11.1.so (deleted)
7ff2f1780000-7ff2f1781000 r-xp 00009000 08:01 5844                       /lib/libnss_nis-2.11.1.so (deleted)
7ff2f1781000-7ff2f1782000 rwxp 0000a000 08:01 5844                       /lib/libnss_nis-2.11.1.so (deleted)
7ff2f1782000-7ff2f178a000 r-xp 00000000 08:01 3452                       /lib/libnss_compat-2.11.1.so.dpkg-new (deleted)
7ff2f178a000-7ff2f1989000 ---p 00008000 08:01 3452                       /lib/libnss_compat-2.11.1.so.dpkg-new (deleted)
7ff2f1989000-7ff2f198a000 r-xp 00007000 08:01 3452                       /lib/libnss_compat-2.11.1.so.dpkg-new (deleted)
7ff2f198a000-7ff2f198b000 rwxp 00008000 08:01 3452                       /lib/libnss_compat-2.11.1.so.dpkg-new (deleted)
7ff2f198b000-7ff2f1992000 r-xp 00000000 08:01 3933886                    /usr/lib/jvm/java-6-sun-1.6.0.20/jre/lib/amd64/native_threads/libhpi.so (deleted)
7ff2f1992000-7ff2f1a93000 ---p 00007000 08:01 3933886                    /usr/lib/jvm/java-6-sun-1.6.0.20/jre/lib/amd64/native_threads/libhpi.so (deleted)
7ff2f1a93000-7ff2f1a95000 rwxp 00008000 08:01 3933886                    /usr/lib/jvm/java-6-sun-1.6.0.20/jre/lib/amd64/native_threads/libhpi.so (deleted)
7ff2f1a95000-7ff2f1a96000 rwxp 00000000 00:00 0 
7ff2f1a96000-7ff2f1aad000 r-xp 00000000 08:01 3451                       /lib/libnsl-2.11.1.so (deleted)
7ff2f1aad000-7ff2f1cac000 ---p 00017000 08:01 3451                       /lib/libnsl-2.11.1.so (deleted)
7ff2f1cac000-7ff2f1cad000 r-xp 00016000 08:01 3451                       /lib/libnsl-2.11.1.so (deleted)
7ff2f1cad000-7ff2f1cae000 rwxp 00017000 08:01 3451                       /lib/libnsl-2.11.1.so (deleted)
7ff2f1cae000-7ff2f1cb0000 rwxp 00000000 00:00 0 
7ff2f1cb0000-7ff2f1cd9000 r-xp 00000000 08:01 3933896                    /usr/lib/jvm/java-6-sun-1.6.0.20/jre/lib/amd64/libjava.so (deleted)
7ff2f1cd9000-7ff2f1dd8000 ---p 00029000 08:01 3933896                    /usr/lib/jvm/java-6-sun-1.6.0.20/jre/lib/amd64/libjava.so (deleted)
7ff2f1dd8000-7ff2f1ddf000 rwxp 00028000 08:01 3933896                    /usr/lib/jvm/java-6-sun-1.6.0.20/jre/lib/amd64/libjava.so (deleted)
7ff2f1ddf000-7ff2f1dec000 r-xp 00000000 08:01 3933880                    /usr/lib/jvm/java-6-sun-1.6.0.20/jre/lib/amd64/libverify.so (deleted)
7ff2f1dec000-7ff2f1eeb000 ---p 0000d000 08:01 3933880                    /usr/lib/jvm/java-6-sun-1.6.0.20/jre/lib/amd64/libverify.so (deleted)
7ff2f1eeb000-7ff2f1eee000 rwxp 0000c000 08:01 3933880                    /usr/lib/jvm/java-6-sun-1.6.0.20/jre/lib/amd64/libverify.so (deleted)
7ff2f1eee000-7ff2f1ef5000 r-xp 00000000 08:01 6444                       /lib/librt-2.11.1.so (deleted)
7ff2f1ef5000-7ff2f20f4000 ---p 00007000 08:01 6444                       /lib/librt-2.11.1.so (deleted)
7ff2f20f4000-7ff2f20f5000 r-xp 00006000 08:01 6444                       /lib/librt-2.11.1.so (deleted)
7ff2f20f5000-7ff2f20f6000 rwxp 00007000 08:01 6444                       /lib/librt-2.11.1.so (deleted)
7ff2f20f6000-7ff2f20f9000 ---p 00000000 00:00 0 
7ff2f20f9000-7ff2f21f7000 rwxp 00000000 00:00 0 
7ff2f21f7000-7ff2f2279000 r-xp 00000000 08:01 3449                       /lib/libm-2.11.1.so (deleted)
7ff2f2279000-7ff2f2478000 ---p 00082000 08:01 3449                       /lib/libm-2.11.1.so (deleted)
7ff2f2478000-7ff2f2479000 r-xp 00081000 08:01 3449                       /lib/libm-2.11.1.so (deleted)
7ff2f2479000-7ff2f247a000 rwxp 00082000 08:01 3449                       /lib/libm-2.11.1.so (deleted)
7ff2f247a000-7ff2f2c2f000 r-xp 00000000 08:01 3933904                    /usr/lib/jvm/java-6-sun-1.6.0.20/jre/lib/amd64/server/libjvm.so (deleted)
7ff2f2c2f000-7ff2f2d2e000 ---p 007b5000 08:01 3933904                    /usr/lib/jvm/java-6-sun-1.6.0.20/jre/lib/amd64/server/libjvm.so (deleted)
7ff2f2d2e000-7ff2f2eb9000 rwxp 007b4000 08:01 3933904                    /usr/lib/jvm/java-6-sun-1.6.0.20/jre/lib/amd64/server/libjvm.so (deleted)
7ff2f2eb9000-7ff2f2ef2000 rwxp 00000000 00:00 0 
7ff2f2ef2000-7ff2f306c000 r-xp 00000000 08:01 1141                       /lib/libc-2.11.1.so (deleted)
7ff2f306c000-7ff2f326b000 ---p 0017a000 08:01 1141                       /lib/libc-2.11.1.so (deleted)
7ff2f326b000-7ff2f326f000 r-xp 00179000 08:01 1141                       /lib/libc-2.11.1.so (deleted)
7ff2f326f000-7ff2f3270000 rwxp 0017d000 08:01 1141                       /lib/libc-2.11.1.so (deleted)
7ff2f3270000-7ff2f3275000 rwxp 00000000 00:00 0 
7ff2f3275000-7ff2f3277000 r-xp 00000000 08:01 3448                       /lib/libdl-2.11.1.so (deleted)
7ff2f3277000-7ff2f3477000 ---p 00002000 08:01 3448                       /lib/libdl-2.11.1.so (deleted)
7ff2f3477000-7ff2f3478000 r-xp 00002000 08:01 3448                       /lib/libdl-2.11.1.so (deleted)
7ff2f3478000-7ff2f3479000 rwxp 00003000 08:01 3448                       /lib/libdl-2.11.1.so (deleted)
7ff2f3479000-7ff2f3491000 r-xp 00000000 08:01 6362                       /lib/libpthread-2.11.1.so (deleted)
7ff2f3491000-7ff2f3690000"
BlockSender and BlockReceiver should be refactored into separate classes,HDFS-298,"Data-node subclasses BlockSender and BlockReceiver can and should be factored out int stand alone classes.
BlockSender does not have any dependencies on DataNode, BlockReceiver will require some work.
"
"IOException at task startup ""No valid local directories in property: dfs.client.buffer.dir""",HDFS-72,"When tasks started up on a brand new mapred cluster, I saw couples of them fail immediately  by 

java.io.IOException: No valid local directories in property: dfs.client.buffer.dir
	at org.apache.hadoop.conf.Configuration.getFile(Configuration.java:401)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.newBackupFile(DFSClient.java:1037)
	at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.(DFSClient.java:1004)
	at org.apache.hadoop.dfs.DFSClient.create(DFSClient.java:276)
	at org.apache.hadoop.dfs.DistributedFileSystem$RawDistributedFileSystem.create(DistributedFileSystem.java:143)
	at org.apache.hadoop.fs.ChecksumFileSystem$FSOutputSummer.(ChecksumFileSystem.java:367)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:442)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:253)
	at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.copy(CopyFiles.java:215)
	at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.map(CopyFiles.java:410)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:175)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1445)

(Run on version 0.12.3 + many patches, so the line number could be a little off)"
UnderReplicationBlocks should use generic types,HDFS-111,"1. HADOOP-659 introduced new class UnderReplicationBlocks, and
two new warnings about use of generic types
        TreeSet<Block>[] priorityQueues = new TreeSet[LEVEL];
        Iterator<Block>[] iterator = new Iterator[LEVEL];
should resp. read
        TreeSet<Block>[] priorityQueues = new TreeSet<Block>[LEVEL];
        Iterator<Block>[] iterator = new Iterator<Block>[LEVEL];

2. I'd rename class UnderReplicationBlocks to UnderReplicatedBlocks while it is still internal."
NPE in FSNamesystem.checkDecommissionStateInternal,HDFS-129,"When bringing back a decommissioned node, we forgot to take out the hostname from dfs.hosts.exclude and call dfsadmin -refreshNodes.  
Somehow, instead of getting 'reject' message, datanode shutdown with NPE.  After dfsadmin -refreshNodes, datanode was able to join back. 

Stack trace, 
{noformat} 
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = ____.____.com/99.9.99.9
STARTUP_MSG:   args = []
************************************************************/
2008-02-26 20:30:56,523 INFO org.apache.hadoop.metrics.jvm.JvmMetrics: Initializing JVM Metrics with
processName=DataNode, sessionId=null
2008-02-26 20:30:57,818 INFO org.apache.hadoop.dfs.DataNode: Opened server at -----
2008-02-26 20:30:57,938 INFO org.mortbay.util.Credential: Checking Resource aliases
2008-02-26 20:30:57,982 INFO org.mortbay.http.HttpServer: Version Jetty/5.1.4
2008-02-26 20:30:58,000 INFO org.mortbay.util.Container: Started HttpContext[/static,/static]
2008-02-26 20:30:58,001 INFO org.mortbay.util.Container: Started HttpContext[/logs,/logs]
2008-02-26 20:30:58,360 INFO org.mortbay.util.Container: Started org.mortbay.jetty.servlet.WebApplicationHandler@20fa83
2008-02-26 20:30:58,462 INFO org.mortbay.util.Container: Started WebApplicationContext[/,/]
2008-02-26 20:30:58,464 INFO org.mortbay.http.SocketListener: Started SocketListener on 0.0.0.0:-----
2008-02-26 20:30:58,464 INFO org.mortbay.util.Container: Started org.mortbay.jetty.Server@16dc861
2008-02-26 20:30:58,464 INFO org.apache.hadoop.dfs.DataNode: Starting to run script to get datanode network location
2008-02-26 20:30:58,591 INFO org.mortbay.util.ThreadedServer: Stopping Acceptor
ServerSocket[addr=0.0.0.0/0.0.0.0,port=0,localport=-----]
2008-02-26 20:30:58,593 INFO org.mortbay.http.SocketListener: Stopped SocketListener on 0.0.0.0:-----
2008-02-26 20:30:58,642 INFO org.mortbay.util.Container: Stopped HttpContext[/static,/static]
2008-02-26 20:30:58,680 INFO org.mortbay.util.Container: Stopped HttpContext[/logs,/logs]
2008-02-26 20:30:58,681 INFO org.mortbay.util.Container: Stopped org.mortbay.jetty.servlet.WebApplicationHandler@20fa83
2008-02-26 20:30:58,718 INFO org.mortbay.util.Container: Stopped WebApplicationContext[/,/]
2008-02-26 20:30:58,719 INFO org.mortbay.util.Container: Stopped org.mortbay.jetty.Server@16dc861
2008-02-26 20:30:58,719 ERROR org.apache.hadoop.dfs.DataNode: org.apache.hadoop.ipc.RemoteException:
java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.dfs.FSNamesystem.checkDecommissionStateInternal(FSNamesystem.java:2918)
        at org.apache.hadoop.dfs.FSNamesystem.verifyNodeRegistration(FSNamesystem.java:3134)
        at org.apache.hadoop.dfs.FSNamesystem.registerDatanode(FSNamesystem.java:1679)
        at org.apache.hadoop.dfs.NameNode.register(NameNode.java:538)
        at sun.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:379)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:596)

        at org.apache.hadoop.ipc.Client.call(Client.java:482)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:184)
        at org.apache.hadoop.dfs.$Proxy0.register(Unknown Source)
        at org.apache.hadoop.dfs.DataNode.register(DataNode.java:391)
        at org.apache.hadoop.dfs.DataNode.startDataNode(DataNode.java:287)
        at org.apache.hadoop.dfs.DataNode.<init>(DataNode.java:206)
        at org.apache.hadoop.dfs.DataNode.makeInstance(DataNode.java:1575)
        at org.apache.hadoop.dfs.DataNode.run(DataNode.java:1519)
        at org.apache.hadoop.dfs.DataNode.createDataNode(DataNode.java:1540)
        at org.apache.hadoop.dfs.DataNode.main(DataNode.java:1711)

2008-02-26 20:30:58,720 INFO org.apache.hadoop.dfs.DataNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ___.____.com/99.9.99.9
************************************************************/


{noformat} "
NameNode to blat total number of files and blocks,HDFS-300,"Right now, the namenode reports lots of rates (block read per sec, removed per sec, etc etc) but it doesn't actually report how many files and blocks total exist in the system. It'd be great if we could have this, so that our reporting systems can show the growth trends over time.
"
Avoid ConcurrentModificationException when FSImage initialization,HDFS-2176,"Below code may cause ConcurrentModificationException when some of fsimage directory equals editlog directory :
Method: FSImage.setStorageDirectories(Collection<URI> fsNameDirs, Collection<URI> fsEditsDirs)
Code:
 for (URI dirName : fsNameDirs) {
      ...     
      boolean isAlsoEdits = false;
      for (URI editsDirName : fsEditsDirs) {
        if (editsDirName.compareTo(dirName) == 0) {
          isAlsoEdits = true;
          fsEditsDirs.remove(editsDirName);
          break;
        }
      }
   }"
"downloading a file from dfs using the WI, using firefox, creates local files that start with a '-'",HDFS-128,"'/' characters are converted to '-' when downloading a file from dfs using the WI. That's a good thing.
But using firefox, where file names can not be modified when saving to disk, creates local files that start with a '-', which is inconvenient on some OS's.

the first '/' character should be dropped rather than converted to a '-'"
DataNode log message includes toString of an array,HDFS-106,"DataNode.java line 596:
        LOG.info(""Starting thread to transfer block "" + blocks[i] + "" to "" + xferTargets[i]);

xferTargets is a two dimensional array, so this line calls toString on the array referenced by xferTargets[i]."
Ozone: Update the allocatedBlock size in SCM when delete blocks happen.,HDFS-13067,"We rely on Container Reports to understand the actually allocated size of a container. We also maintain another counter that keeps track of the logical allocations. That is the number of blocks allocated in the container,聽while this number is used only to queue containers for closing it might be a good idea to make sure that this number is updated when a delete block operation is performed, Simply because聽we have the data."
Update NOTICE for AssertJ dependency,HDFS-12944,HDFS-12665 added a dependency on the ALv2 [AssertJ|https://github.com/joel-costigliola/assertj-core] library. We should update the notice.
"DataXceiver and DFSOutputStream call Thread.setName(), which is not thread safe",HDFS-5896,"org.apache.hadoop.hdfs.server.datanode.DataXceiver and org.apache.hadoop.hdfs.DFSOutputStream renames running threads, but Thread.setName() is not thread safe. Thread.setName() is currently intended to call before running the thread."
Correct javadoc for BackupNode#startActiveServices,HDFS-12493,"Following javadoc warning needs to be fixed for {{BackupNode#startActiveServices}}
Javadoc links are not linked correctly.

{code}
    /**
     * Start services for BackupNode.
     * <p>
     * The following services should be muted
     * (not run or not pass any control commands to DataNodes)
     * on BackupNode:
     * {@link LeaseManager.Monitor} protected by SafeMode.
     * {@link BlockManager.RedundancyMonitor} protected by SafeMode.
     * {@link HeartbeatManager.Monitor} protected by SafeMode.
     * {@link DatanodeAdminManager.Monitor} need to prohibit refreshNodes().
     * {@link PendingReconstructionBlocks.PendingReconstructionMonitor}
     * harmless, because RedundancyMonitor is muted.
     */
    @Override
    public void startActiveServices() throws IOException {
      try {
        namesystem.startActiveServices();
      } catch (Throwable t) {
        doImmediateShutdown(t);
      }
    }
{code}"
Fix broken NameNode metrics documentation,HDFS-12160,HDFS-11261 introduced documentation in {{Metrics.md}} for the metrics added in HDFS-10872. The metrics have a pipe ({{|}}) in them which breaks the markdown table. 
balance include Parameter Usage Error,HDFS-11871,"[hadoop@bigdata-hdp-apache505 hadoop-2.7.2]$ bin/hdfs balancer -h
Usage: hdfs balancer
        [-policy <policy>]      the balancing policy: datanode or blockpool
        [-threshold <threshold>]        Percentage of disk capacity
        [-exclude [-f <hosts-file> | <comma-separated list of hosts>]]  Excludes the specified datanodes.
        [-include [-f <hosts-file> | <comma-separated list of hosts>]]  Includes only the specified datanodes.
        [-idleiterations <idleiterations>]      Number of consecutive idle iterations (-1 for Infinite) before exit.


Parameter Description:
-f <hosts-file> | <comma-separated list of hosts> 
The parse separator in the code is:
String[] nodes = line.split(""[ \t\n\f\r]+"");"
Incomplete annotation in FSNamesystem#checkFileProgress,HDFS-9673,"In method {{FSNamesystem#checkFileProgress}}, the annotation of this method is not complete. When check penultimate block and the penultimate block not exist, it will also return true. But the annotation of this special case is not  referred. And that will be misunderstanding for users. The current annotation of method {{checkFileProgress}}:
{code}
  /**
   * Check that the indicated file's blocks are present and
   * replicated.  If not, return false. If checkall is true, then check
   * all blocks, otherwise check only penultimate block.
   */
  boolean checkFileProgress(String src, INodeFile v, boolean checkall) {
    assert hasReadLock();
    if (checkall) {
      return blockManager.checkBlocksProperlyReplicated(src, v
          .getBlocks());
    } else {
      // check the penultimate block of this file
      BlockInfo b = v.getPenultimateBlock();
      return b == null ||
          blockManager.checkBlocksProperlyReplicated(
              src, new BlockInfo[] { b });
    }
  }
{code}"
Various documents still refer to 'hadoop fs' ,HDFS-6628,"Various HDFS documents refer to 'hadoop fs' even though this throws a deprecated warning when you actually use it.

"
Refactor a private internal class DataTransferEncryptor.SaslParticipant,HDFS-5894,It is appropriate to use polymorphism for SaslParticipant instead of scattering if-else statements.
Namenode web ui references missing hadoop.css,HDFS-2608,"The namenode web-ui tries to load /static/hadoop.css, which doesn't exist. I see an error like: {noformat}GET http://localhost:50070/static/hadoop.css 404 (Not Found){noformat} in the chrome console."
DataNode always READ_BLOCK or WRITE_BLOCK,HDFS-5720,"I find many following errors on my datanode's log. What is the reason for this and how can I resolved it?
{code}
2014-01-05 00:14:40,589 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: date51:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.246.57:35455 dest: /192.168.246.75:50010
java.io.IOException: Premature EOF from inputStream
        at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:194)
        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:435)
        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:693)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:569)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:115)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:68)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:221)
        at java.lang.Thread.run(Thread.java:744)

2014-01-05 02:50:52,552 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: date51:50010:DataXceiver error processing READ_BLOCK operation  src: /192.168.246.39:46693 dest: /192.168.246.75:50010
org.apache.hadoop.hdfs.server.datanode.ReplicaNotFoundException: Replica not found for BP-1398136447-192.168.246.60-1386067202761:blk_1089253308_1099605945003
        at org.apache.hadoop.hdfs.server.datanode.BlockSender.getReplica(BlockSender.java:418)
        at org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>(BlockSender.java:228)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:327)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:101)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:65)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:221)
        at java.lang.Thread.run(Thread.java:744)


2014-01-05 07:39:30,939 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DataNode{data=FSDataset{dirpath='[/data1/hadoop/dfs/current, /data2/hadoop/dfs/current, /data3/hadoop/dfs/current, /data4/hadoop/dfs/current, /data5/hadoop/dfs/current, /data6/hadoop/dfs/current, /data7/hadoop/dfs/current, /data8/hadoop/dfs/current, /data9/hadoop/dfs/current, /data10/hadoop/dfs/current, /data11/hadoop/dfs/current, /data12/hadoop/dfs/current]'}, localName='date51:50010', storageID='DS-1312530819-192.168.246.75-50010-1388062043867', xmitsInProgress=0}:Exception transfering block BP-1398136447-192.168.246.60-1386067202761:blk_1089401766_1099606093471 to mirror 192.168.242.51:50010: java.io.IOException: Connection reset by peer
2014-01-05 07:39:30,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-1398136447-192.168.246.60-1386067202761:blk_1089401766_1099606093471 received exception java.io.IOException: Connection reset by peer
2014-01-05 07:39:30,939 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: date51:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.246.75:44779 dest: /192.168.246.75:50010
java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
        at sun.nio.ch.IOUtil.read(IOUtil.java:197)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
        at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:118)
        at java.io.FilterInputStream.read(FilterInputStream.java:83)
        at java.io.FilterInputStream.read(FilterInputStream.java:83)
        at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:1490)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:511)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:115)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:68)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:221)
        at java.lang.Thread.run(Thread.java:744)


2014-01-05 09:47:58,332 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: date51:50010:DataXceiver error processing READ_BLOCK operation  src: /192.168.246.40:48386 dest: /192.168.246.75:50010
java.net.SocketTimeoutException: 480000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/192.168.246.75:50010 remote=/192.168.246.40:48386]
        at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:246)
        at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:172)
        at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:220)
        at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:546)
        at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:710)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:340)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:101)
        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:65)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:221)
        at java.lang.Thread.run(Thread.java:744)
{code}"
Fix Eclipse template,HDFS-533,The entry for the AspectJ runtime does not match the version downloaded by Ivy
Chance of Resource Leak in org.apache.hadoop.hdfs.server.namenode.FsImage.,HDFS-1752,"In loadFSEdits method,
1) EditLogFileInputStream edits = new EditLogFileInputStream(NNStorage.getStorageFile(sd, NameNodeFile.EDITS));
numEdits = loader.loadFSEdits(edits);
edits.close(); 
2) edits = new EditLogFileInputStream(editsNew);
numEdits += loader.loadFSEdits(edits);
edits.close();

Here if loadFSEdits throws exception then close will not be executed.


 

"
HTTPFS proxy server needs pluggable-auth support,HDFS-7983,"Now that WebHDFS has been fixed to support pluggable auth, the httpfs proxy server also needs support."
libhdfs should use doxygen plugin to generate mvn site output,HDFS-9031,"Rather than point people to the hdfs.h file, we should take advantage of the doxyfile and actually generate for mvn site so it shows up on the website."
Documentation needs to be exposed,HDFS-9464,"From the few builds I've done, there doesn't appear to be any user-facing documentation that is actually exposed when mvn site is built.  HDFS-8745 allegedly added doxygen support, but even those docs aren't tied into the docs and/or site build. "
No header files in mvn package,HDFS-9465,The current build appears to only include the shared library and no header files to actually use the library in the final maven binary build.
Update NN/DN min software version to 3.0.0-beta1,HDFS-10398,"Before we release the first 3.0.0 beta, we need to update the min software version to exclude the alpha releases since we will not support alpha -> beta compatibility. beta->GA compatibility will work though."
"libwebhdfs lacks headers, documentation; not part of mvn package",HDFS-9030,This library is useless without header files to include and documentation on how to use it.  Both appear to be missing from the mvn package and site documentation.
Test : Hadoop-HdfsTrunk test cases after HDFS-4937,HDFS-9352,"{noformat}
Stack Trace:
org.apache.hadoop.ipc.RemoteException: File /TestAbandonBlock_test_quota1 could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and 1 node(s) are excluded in this operation.
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1736)
        at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:299)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2457)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:796)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:500)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:637)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:976)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2305)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2301)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1669)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2301)
{noformat}

 https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/558/"
Replica recovery doesn't distinguish between flushed-but-corrupted last chunk and unflushed last chunk,HDFS-1103,"When the DN creates a replica under recovery, it calls validateIntegrity, which truncates the last checksum chunk off of a replica if it is found to be invalid. Then when the block recovery process happens, this shortened block wins over a longer replica from another node where there was no corruption. Thus, if just one of the DNs has an invalid last checksum chunk, data that has been sync()ed to other datanodes can be lost."
Secondary NameNode failed to rollback from 2.4.1 to 2.2.0,HDFS-7114,"Can upgrade from 2.2.0 to 2.4.1, but failed to rollback the secondary namenode with following issue.

2014-09-22 10:41:28,358 FATAL org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Failed to start secondary namenode
org.apache.hadoop.hdfs.server.common.IncorrectVersionException: Unexpected version of storage directory /var/hadoop/tmp/hdfs/dfs/namesecondary. Reported: -56. Expecting = -47.
        at org.apache.hadoop.hdfs.server.common.Storage.setLayoutVersion(Storage.java:1082)
        at org.apache.hadoop.hdfs.server.common.Storage.setFieldsFromProperties(Storage.java:890)
        at org.apache.hadoop.hdfs.server.namenode.NNStorage.setFieldsFromProperties(NNStorage.java:585)
        at org.apache.hadoop.hdfs.server.common.Storage.readProperties(Storage.java:921)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.recoverCreate(SecondaryNameNode.java:913)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.initialize(SecondaryNameNode.java:249)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.<init>(SecondaryNameNode.java:199)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.main(SecondaryNameNode.java:652)
2014-09-22 10:41:28,360 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2014-09-22 10:41:28,363 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG:"
Failed to rollback hdfs version from 2.4.1 to 2.2.0,HDFS-7053,"I can successfully upgrade from 2.2.0 to 2.4.1 with QJM HA enabled and with downtime, but failed to rollback from 2.4.1 to 2.2.0. The error message:
 2014-09-10 16:50:29,599 FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Exception in namenode join
 org.apache.hadoop.HadoopIllegalArgumentException: Invalid startup option. Cannot perform DFS upgrade with HA enabled.
              at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1207)
               at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1320)
 2014-09-10 16:50:29,601 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
"
Unable to reset password,HDFS-6241,"I tried emailing root@apache.org - as indicated in INFRA-5241 - about these difficulties but it seems to be bouncing. Here is the email that I sent:

Greetings -

I am trying to reset my password and have encountered the following problems:

1. it seems that the public key associated with my account is erroneously that of the lead of our project (Knox). His must have set up my account in the beginning and provided his key maybe? Anyway, this means that he has to decrypt my email.

2. Once he does decrypt it and I follow the link to reset it - I get a No Such Token error message and am unable to reset my password.

The email below indicated that I should email root with problems.

Please let me know if I should file and Infra jira. I did find a similar one there that told them to email root. So, that is where I am starting.

We are in the process of trying to get a release out - so I would greatly appreciate the help here.

thanks!

--larry


Hi Larry McCay,

96.235.186.40 has asked Apache ID <https://id.apache.org>

to initiate a password reset for your apache.org account 'lmccay'.

If you requested this password reset, please use the following link to

reset your Apache LDAP password:

<deleted-url>

If you did not request this password reset, please email root@apache.org --- but

delete the above URL from the text of the reply email before sending it.

This link will expire at 2014-04-14 14:31:25 +0000, and can only be used from 96.235.186.40.

--

Best Regards,

Apache Infrastructure"
CLONE - Unable to reset password,HDFS-6242,"I tried emailing root@apache.org - as indicated in INFRA-5241 - about these difficulties but it seems to be bouncing. Here is the email that I sent:

Greetings -

I am trying to reset my password and have encountered the following problems:

1. it seems that the public key associated with my account is erroneously that of the lead of our project (Knox). His must have set up my account in the beginning and provided his key maybe? Anyway, this means that he has to decrypt my email.

2. Once he does decrypt it and I follow the link to reset it - I get a No Such Token error message and am unable to reset my password.

The email below indicated that I should email root with problems.

Please let me know if I should file and Infra jira. I did find a similar one there that told them to email root. So, that is where I am starting.

We are in the process of trying to get a release out - so I would greatly appreciate the help here.

thanks!

--larry


Hi Larry McCay,

96.235.186.40 has asked Apache ID <https://id.apache.org>

to initiate a password reset for your apache.org account 'lmccay'.

If you requested this password reset, please use the following link to

reset your Apache LDAP password:

<deleted-url>

If you did not request this password reset, please email root@apache.org --- but

delete the above URL from the text of the reply email before sending it.

This link will expire at 2014-04-14 14:31:25 +0000, and can only be used from 96.235.186.40.

--

Best Regards,

Apache Infrastructure"
namenode OOMs under Bigtop's TestCLI,HDFS-4940,"Bigtop's TestCLI when executed against Hadoop 2.1.0 seems to make it OOM quite reliably regardless of the heap size settings. I'm attaching a heap dump URL. Alliteratively anybody can just take Bigtop's tests, compiled them against Hadoop 2.1.0 bits and try to reproduce it.  "
Hftp should support both SPNEGO and KSSL,HDFS-3983,"Hftp currently doesn't work against a secure cluster unless you configure {{dfs.https.port}} to be the http port, otherwise the client can't fetch tokens:

{noformat}
$ hadoop fs -ls hftp://c1225.hal.cloudera.com:50070/
12/09/26 18:02:00 INFO fs.FileSystem: Couldn't get a delegation token from http://c1225.hal.cloudera.com:50470 using http.
ls: Security enabled but user not authenticated by filter
{noformat}

This is due to Hftp still using the https port. Post HDFS-2617 it should use the regular http port. Hsftp should still use the secure port, however now that we have HADOOP-8581 it's worth considering removing Hsftp entirely. I'll start a separate thread about that.  

"
Enable retries for create and append operations.,HDFS-4849,"create, append and delete operations can be made retriable. This will reduce chances for a job or other app failures when NN fails over."
want to integrate oracle nosql to hadoop 1.0.3,HDFS-4907,"I had been asked by my customer to integrate oracle nosql with hadoop 1.0.3.
How to go about"
HA: Transition to active can cause NN deadlock,HDFS-2823,"On transition to active, we have to take the FSNS write lock. In {{EditLogTailer#stop}}, we interrupt the edit log tailer thread and then join on that thread. When tailing edits, the edit log tailer thread acquires the FSNS write lock interruptibly, precisely so that we avoid deadlocks on transition to active. However, the edit log tailer thread now also triggers edit log rolls. Several places in {{ipc.Client}} catch and ignore {{InterruptedException}}, and in so doing may cause the {{Thread#interrupt}} call to be missed by the edit log tailer thread."
Datanode is going OOM due to small files in hdfs,HDFS-4630,"Hi, 

We have very small files(size ranging 10KB-1MB) in our hdfs and no of files are in tens of millions. Due to this namenode and datanode both going out of memory very frequently. When we analyse the head dump of datanode most of the memory was used by ReplicaMap. 

Can we use EhCache or other to not to store all the data in memory? 

Thanks
Ankush"
TestBlockTokenWithDFS fails on trunk,HDFS-1469,"TestBlockTokenWithDFS is failing on trunk:

Testcase: testAppend took 31.569 sec
  FAILED
null
junit.framework.AssertionFailedError: null
  at org.apache.hadoop.hdfs.server.namenode.TestBlockTokenWithDFS.testAppend(TestBlockTokenWithDFS.java:223)"
Hadoop mapreduce job to process S3 logs gets hung at INFO mapred.JobClient:  map 0% reduce 0%.,HDFS-2583,"I am trying to run a mapreduce job to process the Amazon S3 logs. However, the code hangs at INFO mapred.JobClient:  map 0% reduce 0% and does not even attempt to launch the tasks. The sample code for the job setup is given below:
public int run(CommandLine cl) throws Exception 
{
       Configuration conf = getConf();
       String inputPath = """";
       String outputPath = """";
       try
       {
           Job job = new Job(conf, ""Dummy"");
           job.setNumReduceTasks(0);
           job.setMapperClass(Mapper.class);
           inputPath = cl.getOptionValue(""input""); //input is an s3n path
           outputPath = cl.getOptionValue(""output"");
           FileInputFormat.setInputPaths(job, inputPath);
           FileOutputFormat.setOutputPath(job, new Path(outputPath));
           _log.info(""Input path set as "" + inputPath);
           _log.info(""Output path set as "" + outputPath);
           job.waitForCompletion(true);
           return 0;
       } catch (Exception ex)
       {
           _log.error(ex);
           return 1;
       }
}
The above code works on the staging machine. However, it fails on the production machine which is same as the staging machine with more capacity.

Does anyone know what could be the possible reason for the error? 

Thanks in advance!

Nitika"
bin/hdfs conflicts with common user shortcut,HDFS-1253,The 'hdfs' command introduced in 0.21 (unreleased at this time) conflicts with a common user alias and wrapper script.  This change should either be reverted or moved from $HADOOP_HOME/bin to somewhere else in $HADOOP_HOME (perhaps sbin?) so that users do not accidentally hit it.
JMX values for RPC Activity is always zero,HDFS-2244,jconsole is showing that the RPC metrics gathered for the datanode is always zero.  Other metrics for the DN appears fine.
TestAuthorizationFilter is failing,HDFS-1666,two test cases were failing for a number of builds (see attached logs)
HDFS build is broken: hadoop-core snapshot can't be resolved.,HDFS-749,"{noformat}
ivy-resolve-common:
[ivy:resolve] :: resolving dependencies :: org.apache.hadoop#Hadoop-Hdfs;0.22.0-SNAPSHOT
[ivy:resolve]   confs: [common]
[ivy:resolve]   found commons-logging#commons-logging;1.0.4 in maven2
[ivy:resolve]   found log4j#log4j;1.2.15 in maven2
[ivy:resolve]   found org.aspectj#aspectjrt;1.6.4 in maven2
[ivy:resolve]   found org.aspectj#aspectjtools;1.6.4 in maven2
[ivy:resolve] :: resolution report :: resolve 3088ms :: artifacts dl 30ms
        ---------------------------------------------------------------------
        |                  |            modules            ||   artifacts   |
        |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
        ---------------------------------------------------------------------
        |      common      |   5   |   0   |   0   |   0   ||   4   |   0   |
        ---------------------------------------------------------------------
[ivy:resolve] 
[ivy:resolve] :: problems summary ::
[ivy:resolve] :::: WARNINGS
[ivy:resolve]           module not found: org.apache.hadoop#hadoop-core;0.22.0-SNAPSHOT
[ivy:resolve]   ==== apache-snapshot: tried
[ivy:resolve]     https://repository.apache.org/content/repositories/snapshots/org/apache/hadoop/hadoop-core/0.22.0-SNAPSHOT/hadoop-core-0.22.0-SNAPSHOT.pom
[ivy:resolve]     -- artifact org.apache.hadoop#hadoop-core;0.22.0-SNAPSHOT!hadoop-core.jar:
[ivy:resolve]     https://repository.apache.org/content/repositories/snapshots/org/apache/hadoop/hadoop-core/0.22.0-SNAPSHOT/hadoop-core-0.22.0-SNAPSHOT.jar
[ivy:resolve]   ==== maven2: tried
[ivy:resolve]     http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-core/0.22.0-SNAPSHOT/hadoop-core-0.22.0-SNAPSHOT.pom
[ivy:resolve]     -- artifact org.apache.hadoop#hadoop-core;0.22.0-SNAPSHOT!hadoop-core.jar:
[ivy:resolve]     http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-core/0.22.0-SNAPSHOT/hadoop-core-0.22.0-SNAPSHOT.jar
[ivy:resolve]           ::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve]           ::          UNRESOLVED DEPENDENCIES         ::
[ivy:resolve]           ::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve]           :: org.apache.hadoop#hadoop-core;0.22.0-SNAPSHOT: not found
[ivy:resolve]           ::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve] 
[ivy:resolve] :: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS

BUILD FAILED
/homes/cos/work/Hdfs.trunk/build.xml:1337: impossible to resolve dependencies:
{noformat}"
TestFileCreation times out,HDFS-1376,"I see TestFileCreation periodically time out on a host when ant test runs but w/o anything thing else running. 

{noformat}
Testsuite: org.apache.hadoop.hdfs.TestFileCreation
Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec

Testcase: testFileCreationNonRecursive took 0.002 sec
        Caused an ERROR
Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
junit.framework.AssertionFailedError: Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
{noformat}"
TestDiskError.testShutdown fails with port out of range: -1 error,HDFS-834,"The current build is broken on the TestDiskError.testShutdown unit test with the following error:
port out of range:-1

http://hudson.zones.apache.org/hudson/view/Hadoop/job/Hadoop-Hdfs-trunk/171/"
TestDiskError.testReplicationError fails with locked storage error ,HDFS-833,"The current build is failing with on TestDiskError.testReplication with the following error:
Cannot lock storage /grid/0/hudson/hudson-slave/workspace/Hadoop-Hdfs-trunk/trunk/build/test/data/dfs/name1. The directory is already locked.

 http://hudson.zones.apache.org/hudson/view/Hadoop/job/Hadoop-Hdfs-trunk/171/ "
file size is fluctuating although file is closed,HDFS-743,"I am seeing that the length of a file sometimes becomes zero after a namenode restart. These files have only one block. All the three replicas of that block on the datanode(s) has non-zero size. Increasing the replication factor of the file causes the file to show its correct non-zero length.

I am marking this as a blocker because it is still to be investigated which releases it affects. I am seeing this on 0.17.x very frequently. I might have seen this on 0.20.x but do not have a reproducible case yet."
start-all.sh / stop-all.sh does not seem to work with HDFS,HDFS-1288,"The start-all.sh / stop-all.sh script shipping with the ""combined"" hadoop-0.21.0-rc1 does not start/stop the DFS daemons unless $HADOOP_HDFS_HOME is explicitly set."
"Secondary Name Node crash, NPE in edit log replay",HDFS-1002,"An NPE in SNN, the core of the message looks like yay so:

2010-02-25 11:54:05,834 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addChild(FSDirectory.java:1152)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addChild(FSDirectory.java:1164)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addNode(FSDirectory.java:1067)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedAddFile(FSDirectory.java:213)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.loadEditRecords(FSEditLog.java:511)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.loadFSEdits(FSEditLog.java:401)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.loadFSEdits(FSEditLog.java:368)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSEdits(FSImage.java:1172)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.doMerge(SecondaryNameNode.java:594)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.access$000(SecondaryNameNode.java:476)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doMerge(SecondaryNameNode.java:353)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:317)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:219)
        at java.lang.Thread.run(Thread.java:619)

This happens even if I restart SNN over and over again."
TestFsck times out on branch 0.20.1,HDFS-784,"Two tests are currently failing on Hudson:

chttp://hudson.zones.apache.org/hudson/view/Hadoop/job/Hadoop-20-Build/lastBuild/testReport

>>> org.apache.hadoop.hdfs.TestDatanodeBlockScanner.testBlockCorruptionPolicy 	0.0020	9
>>> org.apache.hadoop.hdfs.server.namenode.TestFsck.testCorruptBlock 

HDFS-734 has already been filed for TestDatanodeBlockScanner.
"
rollingupgrade needs some guard rails,HDFS-7231,See first comment.
Hadoop compilation fails on gcc 7.3,HDFS-13737,"As per document we have specified  *GCC 4.8.1 or later*

Compilation fails on 7.3

{code}
root@bibinpc:~# /usr/bin/gcc-7 --version
gcc-7 (Ubuntu 7.3.0-16ubuntu3) 7.3.0
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

{code}

{code}
   [exec] [ 28%] Building CXX object main/native/libhdfspp/lib/common/CMakeFiles/common_obj.dir/ioservice_impl.cc.o
     [exec] In file included from /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/lib/common/ioservice_impl.h:22:0,
     [exec]                  from /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/lib/common/ioservice_impl.cc:19:
     [exec] /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/include/hdfspp/ioservice.h:109:30: error: 鈥榮td::function鈥?has not been declared
     [exec]    virtual void PostTask(std::function<void(void)> asyncTask) = 0;
     [exec]                               ^~~~~~~~
     [exec] /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/include/hdfspp/ioservice.h:109:38: error: expected 鈥?鈥?or 鈥?..鈥?before 鈥?鈥?token
     [exec]    virtual void PostTask(std::function<void(void)> asyncTask) = 0;
     [exec]                                       ^
     [exec] /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/include/hdfspp/ioservice.h: In member function 鈥榲oid hdfs::IoService::PostLambda(LambdaInstance&&)鈥?
     [exec] /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/include/hdfspp/ioservice.h:117:10: error: 鈥榝unction鈥?is not a member of 鈥榮td鈥?
     [exec]      std::function<void(void)> typeEraser = func;
     [exec]           ^~~~~~~~
     [exec] /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/include/hdfspp/ioservice.h:117:10: note: suggested alternative: 鈥榠s_function鈥?
     [exec]      std::function<void(void)> typeEraser = func;
     [exec]           ^~~~~~~~
     [exec]           is_function
     [exec] /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/include/hdfspp/ioservice.h:117:19: error: expected primary-expression before 鈥榲oid鈥?
     [exec]      std::function<void(void)> typeEraser = func;
     [exec]                    ^~~~
     [exec] In file included from /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/lib/common/ioservice_impl.cc:19:0:
     [exec] /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/lib/common/ioservice_impl.h: At global scope:
     [exec] /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/lib/common/ioservice_impl.h:45:22: error: 鈥榮td::function鈥?has not been declared
     [exec]    void PostTask(std::function<void(void)> asyncTask) override;
     [exec]                       ^~~~~~~~
     [exec] /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/lib/common/ioservice_impl.h:45:30: error: expected 鈥?鈥?or 鈥?..鈥?before 鈥?鈥?token
     [exec]    void PostTask(std::function<void(void)> asyncTask) override;
     [exec]                               ^
     [exec] /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/lib/common/ioservice_impl.cc:104:35: error: variable or field 鈥楶ostTask鈥?declared void
     [exec]  void IoServiceImpl::PostTask(std::function<void(void)> asyncTask) {
     [exec]                                    ^~~~~~~~
     [exec] /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/lib/common/ioservice_impl.cc:104:35: error: 鈥榝unction鈥?is not a member of 鈥榮td鈥?
     [exec] /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/lib/common/ioservice_impl.cc:104:35: note: suggested alternative: 鈥榠s_function鈥?
     [exec]  void IoServiceImpl::PostTask(std::function<void(void)> asyncTask) {
     [exec]                                    ^~~~~~~~
     [exec]                                    is_function
     [exec] /opt/apacheprojects/hadoop/YARN3409/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/lib/common/ioservice_impl.cc:104:44: error: expected primary-expression before 鈥榲oid鈥?
     [exec]  void IoServiceImpl::PostTask(std::function<void(void)> asyncTask) {
     [exec]                                             ^~~~
     [exec] main/native/libhdfspp/lib/common/CMakeFiles/common_obj.dir/build.make:88: recipe for target 'main/native/libhdfspp/lib/common/CMakeFiles/common_obj.dir/ioservice_impl.cc.o' failed
     [exec] CMakeFiles/Makefile2:1966: recipe for target 'main/native/libhdfspp/lib/common/CMakeFiles/common_obj.dir/all' failed
     [exec] Makefile:140: recipe for target 'all' failed
     [exec] make[2]: *** [main/native/libhdfspp/lib/common/CMakeFiles/common_obj.dir/ioservice_impl.cc.o] Error 1
     [exec] make[1]: *** [main/native/libhdfspp/lib/common/CMakeFiles/common_obj.dir/all] Error 2
     [exec] make: *** [all] Error 2

{code}
"
HADOOP_HDFS_LOG_DIR should be HDFS_LOG_DIR in deprecations,HDFS-7913,"The wrong variable is deprecated in hdfs-config.sh.  It should be HDFS_LOG_DIR, not HADOOP_HDFS_LOG_DIR.  This is breaking backward compatibility.

It might be worthwhile to doublecheck the other dep's to make sure they are correct as well.

Also, release notes for the deprecation jira should be updated to reflect this change."
HdfsFileStatus#getPath returning null.,HDFS-12970,"After HDFS-12681, HdfsFileStatus#getPath() returns null.
I don't think this is expected.

Both the implementation of {{HdfsFileStatus}} sets the path to null.
{code:title=HdfsNamedFileStatus.java|borderStyle=solid}
  HdfsNamedFileStatus(long length, boolean isdir, int replication,
                      long blocksize, long mtime, long atime,
                      FsPermission permission, Set<Flags> flags,
                      String owner, String group,
                      byte[] symlink, byte[] path, long fileId,
                      int childrenNum, FileEncryptionInfo feInfo,
                      byte storagePolicy, ErasureCodingPolicy ecPolicy) {
    super(length, isdir, replication, blocksize, mtime, atime,
        HdfsFileStatus.convert(isdir, symlink != null, permission, flags),
        owner, group, null, null,           ------ The last null is for path.
        HdfsFileStatus.convert(flags));
{code}


{code:title=HdfsLocatedFileStatus.java|borderStyle=solid}
  HdfsLocatedFileStatus(long length, boolean isdir, int replication,
                        long blocksize, long mtime, long atime,
                        FsPermission permission, EnumSet<Flags> flags,
                        String owner, String group,
                        byte[] symlink, byte[] path, long fileId,
                        int childrenNum, FileEncryptionInfo feInfo,
                        byte storagePolicy, ErasureCodingPolicy ecPolicy,
                        LocatedBlocks hdfsloc) {
    super(length, isdir, replication, blocksize, mtime, atime,
        HdfsFileStatus.convert(isdir, symlink != null, permission, flags),
        owner, group, null, null, HdfsFileStatus.convert(flags),  -- The last null on this line is for path.
        null);
{code}
"
DFS.concat should throw exception if files have different EC policies. ,HDFS-12923,"{{DFS#concat}} appends blocks from different files to a single file. However, if these files have different EC policies, or mixed with replicated and EC files, the resulted file would be problematic to read, because the EC codec is defined in INode instead of in a block. 

"
NameNode fails to start after upgrade - Missing state in ECPolicy Proto ,HDFS-12918,"According to documentation and code comments, the default setting for erasure coding policy is disabled:

/** Policy is disabled. It's policy default state. */
 DISABLED(1),

However, HDFS-12258 appears to have incorrectly set the policy state in the protobuf to enabled:

{code:java}
 message ErasureCodingPolicyProto {
    ooptional string name = 1;
    optional ECSchemaProto schema = 2;
    optional uint32 cellSize = 3;
    required uint32 id = 4; // Actually a byte - only 8 bits used
 + optional ErasureCodingPolicyState state = 5 [default = ENABLED];
  }
{code}

This means the parameter can't actually be optional, it must always be included, and existing serialized data without this optional field will be incorrectly interpreted as having erasure coding enabled.

This unnecessarily breaks compatibility and will require existing HDFS installations that store metadata in protobufs to require reformatting.

It looks like a simple mistake that was overlooked in code review."
"I didn't find the ""-find"" command on hadoop2.6",HDFS-12715,"When I looked for files on HDFS, I found no ""find"" command. I didn't find the ""find"" command by using ""Hadoop FS -help"".
"
WebHdfsFileSystem#getFileBlockLocations will always return BlockLocation#corrupt as false,HDFS-12442,"Was going through {{JsonUtilClient#toBlockLocation}} code.
Below is the relevant code snippet.
{code:title=JsonUtilClient.java|borderStyle=solid}
 /** Convert a Json map to BlockLocation. **/
  static BlockLocation toBlockLocation(Map<?, ?> m)
      throws IOException{
    ...
    ...  
    boolean corrupt = Boolean.
        getBoolean(m.get(""corrupt"").toString());
    ...
    ...
  }
{code}
According to java docs for {{Boolean#getBoolean}}
{noformat}
Returns true if and only if the system property named by the argument exists and is equal to the string ""true"". 
{noformat}
I assume, the map value for key {{corrupt}} will be populated with either {{true}} or {{false}}.
On the client side, {{Boolean#getBoolean}} will look for system property for true or false.
So it will always return false unless the system property is set for true or false."
namenode crash in fsimage download/transfer,HDFS-9126,"In our product Hadoop cluster,when active namenode begin download/transfer 
fsimage from standby namenode.some times zkfc monitor health of NameNode socket timeout,zkfs judge active namenode status SERVICE_NOT_RESPONDING ,happen hadoop namenode ha failover,fence old active namenode.

zkfc logs:
2015-09-24 11:44:44,739 WARN org.apache.hadoop.ha.HealthMonitor: Transport-level exception trying to monitor health of NameNode at hostname1/192.168.10.11:8020: Call From hostname1/192.168.10.11 to hostname1:8020 failed on socket timeout exception: java.net.SocketTimeoutException: 45000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/192.168.10.11:22614 remote=hostname1/192.168.10.11:8020]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
2015-09-24 11:44:44,740 INFO org.apache.hadoop.ha.HealthMonitor: Entering state SERVICE_NOT_RESPONDING
2015-09-24 11:44:44,740 INFO org.apache.hadoop.ha.ZKFailoverController: Local service NameNode at hostname1/192.168.10.11:8020 entered state: SERVICE_NOT_RESPONDING
2015-09-24 11:44:44,740 INFO org.apache.hadoop.ha.ZKFailoverController: Quitting master election for NameNode at hostname1/192.168.10.11:8020 and marking that fencing is necessary
2015-09-24 11:44:44,740 INFO org.apache.hadoop.ha.ActiveStandbyElector: Yielding from election
2015-09-24 11:44:44,761 INFO org.apache.zookeeper.ZooKeeper: Session: 0x54d81348fe503e3 closed
2015-09-24 11:44:44,761 WARN org.apache.hadoop.ha.ActiveStandbyElector: Ignoring stale result from old client with sessionId 0x54d81348fe503e3
2015-09-24 11:44:44,764 INFO org.apache.zookeeper.ClientCnxn: EventThread shut down

namenode logs:
2015-09-24 11:43:34,074 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 192.168.10.12
2015-09-24 11:43:34,074 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-09-24 11:43:34,075 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 2317430129
2015-09-24 11:43:34,253 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 272988 Total time for transactions(ms): 5502 Number of transactions batched in Syncs: 146274 Number of syncs: 32375 SyncTimes(ms): 274465 319599
2015-09-24 11:43:46,005 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-09-24 11:44:21,054 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: PendingReplicationMonitor timed out blk_1185804191_112164210
2015-09-24 11:44:36,076 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /software/data/hadoop-data/hdfs/namenode/current/edits_inprogress_0000000002317430129 -> /software/data/hadoop-data/hdfs/namenode/current/edits_0000000002317430129-0000000002317703116
2015-09-24 11:44:36,077 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 2317703117
2015-09-24 11:45:38,008 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 1 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 0 SyncTimes(ms): 0 61585
2015-09-24 11:45:38,009 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 222.88s at 63510.29 KB/s
2015-09-24 11:45:38,009 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000002317430128 size 14495092105 bytes.
2015-09-24 11:45:38,416 WARN org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Remote journal 192.168.10.13:8485 failed to write txns 2317703117-2317703117. Will try to write to this JN again after the next log roll.
org.apache.hadoop.ipc.RemoteException(java.io.IOException): IPC's epoch 44 is less than the last promised epoch 45
        at org.apache.hadoop.hdfs.qjournal.server.Journal.checkRequest(Journal.java:414)
        at org.apache.hadoop.hdfs.qjournal.server.Journal.checkWriteRequest(Journal.java:442)
        at org.apache.hadoop.hdfs.qjournal.server.Journal.journal(Journal.java:342)
        at org.apache.hadoop.hdfs.qjournal.server.JournalNodeRpcServer.journal(JournalNodeRpcServer.java:148)
        at org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolServerSideTranslatorPB.journal(QJournalProtocolServerSideTranslatorPB.java:158)
        at org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(QJournalProtocolProtos.java:25421)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)
        at org.apache.hadoop.ipc.Client.call(Client.java:1468)
        at org.apache.hadoop.ipc.Client.call(Client.java:1399)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
        at com.sun.proxy.$Proxy9.journal(Unknown Source)
        at org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolTranslatorPB.journal(QJournalProtocolTranslatorPB.java:167)
        at org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7.call(IPCLoggerChannel.java:385)
        at org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7.call(IPCLoggerChannel.java:378)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
		
#Similar log like above 

2015-09-24 11:45:38,418 WARN org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Aborting QuorumOutputStream starting at txid 2317703117
2015-09-24 11:45:38,505 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-09-24 11:45:38,549 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at hostname1/192.168.10.11
"
Ozone: Datanode is unable to register with scm if scm starts later,HDFS-12098,"Reproducing steps
1. Start namenode

{{./bin/hdfs --daemon start namenode}}

2. Start datanode

{{./bin/hdfs datanode}}

will see following connection issues

{noformat}
17/07/13 21:16:48 INFO ipc.Client: Retrying connect to server: ozone1.fyre.ibm.com/172.16.165.133:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
17/07/13 21:16:49 INFO ipc.Client: Retrying connect to server: ozone1.fyre.ibm.com/172.16.165.133:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
17/07/13 21:16:50 INFO ipc.Client: Retrying connect to server: ozone1.fyre.ibm.com/172.16.165.133:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
17/07/13 21:16:51 INFO ipc.Client: Retrying connect to server: ozone1.fyre.ibm.com/172.16.165.133:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
{noformat}

this is expected because scm is not started yet

3. Start scm

{{./bin/hdfs scm}}

expecting datanode can register to this scm, expecting the log in scm

{noformat}
17/07/13 21:22:30 INFO node.SCMNodeManager: Data node with ID: af22862d-aafa-4941-9073-53224ae43e2c Registered.
{noformat}

but did *NOT* see this log. (_I debugged into the code and found the datanode state was transited SHUTDOWN unexpectedly because the thread leaks, each of those threads counted to set to next state and they all set to SHUTDOWN state_)

4. Create a container from scm CLI

{{./bin/hdfs scm -container -create -c 20170714c0}}

this fails with following exception

{noformat}
Creating container : 20170714c0.
Error executing command:org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ozone.scm.exceptions.SCMException): Unable to create container while in chill mode
	at org.apache.hadoop.ozone.scm.container.ContainerMapping.allocateContainer(ContainerMapping.java:241)
	at org.apache.hadoop.ozone.scm.StorageContainerManager.allocateContainer(StorageContainerManager.java:392)
	at org.apache.hadoop.ozone.protocolPB.StorageContainerLocationProtocolServerSideTranslatorPB.allocateContainer(StorageContainerLocationProtocolServerSideTranslatorPB.java:73)
{noformat}

datanode was not registered to scm, thus it's still in chill mode.

*Note*, if we start scm first, there is no such issue, I can create container from CLI without any problem.

"
chooseRemoteRack() semantics broken in trunk,HDFS-12272,"The {{chooseRemoteRack()}} method in the default block placement policy was designed to pick from maximum number of racks. E.g. If asked to pick 2 and there are 2 or more racks available, the two will be on different racks. It wasn't implicit or accidental semantics. There was a specific logic in {{chooseRandom()}} that makes it happen.

This behavior is broken after HDFS-11530 as this logic was removed from {{chooseRandom()}}. Now the result is unpredictable. Sometimes the replicas end up in the same rack."
DatanodeHttpServer is not setting Endpoint based on configured policy and not loading ssl configuration.,HDFS-9045,"Always DN is starting in http mode.
{code}
    HttpServer2.Builder builder = new HttpServer2.Builder()
        .setName(""datanode"")
        .setConf(confForInfoServer)
        .setACL(new AccessControlList(conf.get(DFS_ADMIN, "" "")))
        .hostName(getHostnameForSpnegoPrincipal(confForInfoServer))
        .addEndpoint(URI.create(""http://localhost:0""))
        .setFindPort(true);

{code}
Should be based on configured policy"
"For HA, a logical name is visible in URIs - add an explicit logical name",HDFS-3153,Please see this [comment|https://issues.apache.org/jira/browse/HDFS-2839?focusedCommentId=13227729&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13227729] for a discussion of logical names.
Backport HDFS-3553 (hftp proxy tokens) to branch-1,HDFS-4368,"Proxy tokens are broken for hftp.  The impact is systems using proxy tokens, such as oozie jobs, cannot use hftp."
data node sudden killed ,HDFS-10340,"I tried to setup a new data node using ubuntu 16 
and get it join to an existed Hadoop Hdfs cluster ( there are 10 nodes in this cluster and they all run on centos Os 6 ) 
But when i try to boostrap this node , after about 10 or 20 minutes i get this strange errors : 

2016-04-26 20:12:09,394 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /10.3.24.65:55323, dest: /10.3.24.197:50010, bytes: 79902, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1379996362_1, offset: 0, srvID: 225f5b43-1dd3-4ac6-88d2-1e8d27dba55b, blockid: BP-352432948-10.3.24.65-1433821675295:blk_1074038505_789832, duration: 15331628
2016-04-26 20:12:09,394 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-352432948-10.3.24.65-1433821675295:blk_1074038505_789832, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2016-04-26 20:12:25,410 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-352432948-10.3.24.65-1433821675295:blk_1074038502_789829
2016-04-26 20:12:25,411 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-352432948-10.3.24.65-1433821675295:blk_1074038505_789832
2016-04-26 20:13:18,546 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1074038502_789829 file /data/hadoop_data/backup/data/current/BP-352432948-10.3.24.65-1433821675295/current/finalized/subdir4/subdir134/blk_1074038502 for deletion
2016-04-26 20:13:18,562 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-352432948-10.3.24.65-1433821675295 blk_1074038502_789829 file /data/hadoop_data/backup/data/current/BP-352432948-10.3.24.65-1433821675295/current/finalized/subdir4/subdir134/blk_1074038502
2016-04-26 20:15:46,481 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2016-04-26 20:15:46,504 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at bigdata-dw-24-197/10.3.24.197
************************************************************/
"
Block reports could be silently dropped by NN,HDFS-10563,"Reading through the block reporting code I think I've spotted a case when block reports can silently be dropped and leave thread waiting indefinitely on a FutureTask that never will be executed.

The BlockReportProcessingThread.enqueue method doesn't return any status on if the enqueuing of the task was successful and does not handle the case when the queue is full and offer return false.

Going back through the call stack to BlockManager.runBlockOp, which indirectly calls enqueue with a FutureTask and then proceeds to do get() om the task.

So if the internal queue in the BlockReportingProcessingThread is full, the BR would never be handled and the thread queuing the task would wait indefinitely on the FutureTask that will never be executed."
Secret manager saves expired items,HDFS-4539,Expired tokens and secrets should not be written out when the secret manager is serialized into the fs image.
DFSClient should ignore dfs.client.retry.policy.enabled for HA proxies,HDFS-8708,"DFSClient should ignore dfs.client.retry.policy.enabled for HA proxies to ensure fast failover. Otherwise, dfsclient retries the NN which is no longer active and delays the failover."
Replicas awaiting recovery should return a full visible length,HDFS-2288,"Currently, if the client calls getReplicaVisibleLength for a RWR, it returns a visible length of 0. This causes one of HBase's tests to fail, and I believe it's incorrect behavior."
TestFileAppend4 fails intermittently,HDFS-2433,"A Jenkins build we have running failed twice in a row with issues form TestFileAppend4.testAppendSyncReplication1 in an attempt to reproduce the error I ran TestFileAppend4 in a loop over night saving the results away.  (No clean was done in between test runs)

When TestFileAppend4 is run in a loop the testAppendSyncReplication[012] tests fail about 10% of the time (14 times out of 130 tries)  They all fail with something like the following.  Often it is only one of the tests that fail, but I have seen as many as two fail in one run.

{noformat}
Testcase: testAppendSyncReplication2 took 32.198 sec
        FAILED
Should have 2 replicas for that block, not 1
junit.framework.AssertionFailedError: Should have 2 replicas for that block, not 1
        at org.apache.hadoop.hdfs.TestFileAppend4.replicationTest(TestFileAppend4.java:477)
        at org.apache.hadoop.hdfs.TestFileAppend4.testAppendSyncReplication2(TestFileAppend4.java:425)
{noformat}

I also saw several other tests that are a part of TestFileApped4 fail during this experiment.  They may all be related to one another so I am filing them in the same JIRA.  If it turns out that they are not related then they can be split up later.

testAppendSyncBlockPlusBbw failed 6 out of the 130 times or about 5% of the time

{noformat}
Testcase: testAppendSyncBlockPlusBbw took 1.633 sec
        FAILED
unexpected file size! received=0 , expected=1024
junit.framework.AssertionFailedError: unexpected file size! received=0 , expected=1024
        at org.apache.hadoop.hdfs.TestFileAppend4.assertFileSize(TestFileAppend4.java:136)
        at org.apache.hadoop.hdfs.TestFileAppend4.testAppendSyncBlockPlusBbw(TestFileAppend4.java:401)
{noformat}

testAppendSyncChecksum[012] failed 2 out of the 130 times or about 1.5% of the time

{noformat}
Testcase: testAppendSyncChecksum1 took 32.385 sec
        FAILED
Should have 1 replica for that block, not 2
junit.framework.AssertionFailedError: Should have 1 replica for that block, not 2
        at org.apache.hadoop.hdfs.TestFileAppend4.checksumTest(TestFileAppend4.java:556)
        at org.apache.hadoop.hdfs.TestFileAppend4.testAppendSyncChecksum1(TestFileAppend4.java:500)
{noformat}

I will attach logs for all of the failures.  Be aware that I did change some of the logging messages in this test so I could better see when testAppendSyncReplication started and ended.  Other then that the code is stock 0.20.205 RC2"
Impala compilation breaks with libhdfs in 2.7 as getJNIEnv is not visible,HDFS-8474,"Impala in CDH 5.2.0 is not compiling with libhdfs.so in 2.7.0 on RedHat 6.4.
This is because getJNIEnv is not visible in the so file.

Compilation fails with below error message :
../../build/release/exec/libExec.a(hbase-table-scanner.cc.o): In function `impala::HBaseTableScanner::Init()':
/usr1/code/Impala/code/current/impala/be/src/exec/hbase-table-scanner.cc:113: undefined reference to `getJNIEnv'
../../build/release/exprs/libExprs.a(hive-udf-call.cc.o):/usr1/code/Impala/code/current/impala/be/src/exprs/hive-udf-call.cc:227: more undefined references to `getJNIEnv' follow
collect2: ld returned 1 exit status
make[3]: *** [be/build/release/service/impalad] Error 1
make[2]: *** [be/src/service/CMakeFiles/impalad.dir/all] Error 2
make[1]: *** [be/src/service/CMakeFiles/impalad.dir/rule] Error 2
make: *** [impalad] Error 2
Compiler Impala Failed, exit


libhdfs.so.0.0.0 returns nothing when following command is run.
""nm -D libhdfs.so.0.0.0  | grep getJNIEnv""

The change in HDFS-7879 breaks the backward compatibility of libhdfs although it can be argued that Impala shouldn't be using above API."
Datanode does not log reads,HDFS-8315,"HDFS-6836 made datanode read request logging DEBUG.  There is a good reason why it was at INFO for so many years.  This is very useful in debugging load issues. This jira will revert HDFS-6836.

We haven't seen it being a bottleneck on busy hbase clusters, but if someone thinks it is a serious overhead, please make it configurable in a separate jira."
Failed pipeline creation during append leaves lease hanging on NN,HDFS-1262,"Ryan Rawson came upon this nasty bug in HBase cluster testing. What happened was the following:
1) File's original writer died
2) Recovery client tried to open file for append - looped for a minute or so until soft lease expired, then append call initiated recovery
3) Recovery completed successfully
4) Recovery client calls append again, which succeeds on the NN
5) For some reason, the block recovery that happens at the start of append pipeline creation failed on all datanodes 6 times, causing the append() call to throw an exception back to HBase master. HBase assumed the file wasn't open and put it back on a queue to try later
6) Some time later, it tried append again, but the lease was still assigned to the same DFS client, so it wasn't able to recover.

The recovery failure in step 5 is a separate issue, but the problem for this JIRA is that the NN can think it failed to open a file for append when the NN thinks the writer holds a lease. Since the writer keeps renewing its lease, recovery never happens, and no one can open or recover the file until the DFS client shuts down."
loss of VERSION file on datanode when trying to startup with full disk,HDFS-60,"datanode working ok previously. subsequent bringup of datanode fails:

/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = hadoop003.sf2p.facebook.com/10.16.159.103
STARTUP_MSG:   args = []
************************************************************/
2008-01-08 08:23:38,400 INFO org.apache.hadoop.metrics.jvm.JvmMetrics: Initializing JVM Metrics with processN
ame=DataNode, sessionId=null
2008-01-08 08:23:48,491 INFO org.apache.hadoop.ipc.RPC: Problem connecting to server: hadoop001.sf2p.facebook
.com/10.16.159.101:9000
2008-01-08 08:23:59,495 INFO org.apache.hadoop.ipc.RPC: Problem connecting to server: hadoop001.sf2p.facebook
.com/10.16.159.101:9000
2008-01-08 08:24:01,597 ERROR org.apache.hadoop.dfs.DataNode: java.io.IOException: No space left on device
        at java.io.FileOutputStream.writeBytes(Native Method)
        at java.io.FileOutputStream.write(FileOutputStream.java:260)
        at sun.nio.cs.StreamEncoder$CharsetSE.writeBytes(StreamEncoder.java:336)
        at sun.nio.cs.StreamEncoder$CharsetSE.implFlushBuffer(StreamEncoder.java:404)
        at sun.nio.cs.StreamEncoder$CharsetSE.implFlush(StreamEncoder.java:408)
        at sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:152)
        at java.io.OutputStreamWriter.flush(OutputStreamWriter.java:213)
        at java.io.BufferedWriter.flush(BufferedWriter.java:236)
        at java.util.Properties.store(Properties.java:666)
        at org.apache.hadoop.dfs.Storage$StorageDirectory.write(Storage.java:176)
        at org.apache.hadoop.dfs.Storage$StorageDirectory.write(Storage.java:164)
        at org.apache.hadoop.dfs.Storage.writeAll(Storage.java:510)
        at org.apache.hadoop.dfs.DataStorage.recoverTransitionRead(DataStorage.java:146)
        at org.apache.hadoop.dfs.DataNode.startDataNode(DataNode.java:243)
        at org.apache.hadoop.dfs.DataNode.<init>(DataNode.java:206)
        at org.apache.hadoop.dfs.DataNode.makeInstance(DataNode.java:1391)
        at org.apache.hadoop.dfs.DataNode.run(DataNode.java:1335)
        at org.apache.hadoop.dfs.DataNode.createDataNode(DataNode.java:1356)
        at org.apache.hadoop.dfs.DataNode.main(DataNode.java:1525)

2008-01-08 08:24:01,597 INFO org.apache.hadoop.dfs.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at hadoop003.sf2p.facebook.com/10.16.159.103
"
WebHdfsFilesystem does not work within a proxyuser doAs call in secure mode,HDFS-3509,"It does not find kerberos credentials in the context (the UGI is logged in from a keytab) and it fails with the following trace:

{code}
java.lang.IllegalStateException: unknown char '<'(60) in org.mortbay.util.ajax.JSON$ReaderSource@23245e75
	at org.mortbay.util.ajax.JSON.handleUnknown(JSON.java:788)
	at org.mortbay.util.ajax.JSON.parse(JSON.java:777)
	at org.mortbay.util.ajax.JSON.parse(JSON.java:603)
	at org.mortbay.util.ajax.JSON.parse(JSON.java:183)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.jsonParse(WebHdfsFileSystem.java:259)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:268)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.run(WebHdfsFileSystem.java:427)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.getDelegationToken(WebHdfsFileSystem.java:722)
{code}"
DNS Issues during TrashEmptier initialization can silently leave it non-functional,HDFS-5850,"[~knoguchi] recently noticed that the trash directories of a restarted cluster were not cleaned up. It turned out that it was caused by a transient DNS problem during initialization.

TrashEmptier thread in namenode is actually a FileSystem client running in a loop, which makes RPC calls to itself in order  to list, rename and delete trash files.  In a secure setup, the client needs to create the right service principal name for the namenode for making a RPC connection. If there is a DNS issue at that moment, the SPN ends up with the IP address, not the fqdn.

Since KDC does not recognize this SPN, TrashEmptier does not work from that point on. I verified that the SPN with the IP address was what the TrashEmptier thread asked KDC for a service ticket for."
S-live: Rate operation count for delete is worse than 0.20.204 by 28.8%,HDFS-2984,Rate operation count for delete is worse than 0.20.204.xx by 28.8%
"Namenode can't restart due to corrupt edit logs, timing issue with shutdown and edit log rolling",HDFS-3771,"Our 0.23.3 nightly HDFS regression suite encountered a particularly nasty issue recently, which resulted in the cluster's default Namenode being unable to restart, this was on a 20 node Federated cluster with security. The cause appears to be that the NN was just starting to roll its edit log when a shutdown occurred, the shutdown was intentional to restart the cluster as part of an automated test.

The tests that were running do not appear to be the issue in themselves, the cluster was just wrapping up an adminReport subset and this failure case has not reproduce so far, nor was it failing previously. It looks like a chance occurrence of sending the shutdown just as the edit log roll was begun.

From the NN log, the following sequence is noted:

1. an InvalidateBlocks operation had completed
2. FSNamesystem: Roll Edit Log from [Secondary Namenode IPaddr]
3. FSEditLog: Ending log segment 23963
4. FSEditLog: Starting log segment at 23967
4. NameNode: SHUTDOWN_MSG
=> the NN shuts down and then is restarted...
5. FSImageTransactionalStorageInspector: Logs beginning at txid 23967 were are all in-progress
6. FSImageTransactionalStorageInspector: Marking log at /grid/[PATH]/edits_inprogress_0000000000000023967 as corrupt since it has no transactions in it.
7. NameNode: Exception in namenode join [main]java.lang.IllegalStateException: No non-corrupt logs for txid 23967
=> NN start attempts continue to cycle trying to restart but can't, failing on the same exception due to lack of non-corrupt edit logs

If observations are correct and issue is from shutdown happening as edit logs are rolling, does the NN have an equivalent to the conventional fs 'sync' blocking action that should be called, or perhaps has a timing hole?"
WebHDFS obtains/sets delegation token service hostname using wrong config leading to issues when NN is configured with 0.0.0.0 RPC IP,HDFS-4457,"If the NameNode RPC address is configured with an wildcard IP 0.0.0.0, then delegationotkens are configured with 0.0.0.0 as service and this breaks clients trying to use those tokens.

Looking at NamenodeWebHdfsMethods#generateDelegationToken() the problem is SecurityUtil.setTokenService(t, namenode.getHttpAddress());, tracing back what is being used to resolve getHttpAddress() the NameNodeHttpServer is resolving the httpAddress doing a httpAddress = new InetSocketAddress(bindAddress.getAddress(), httpServer.getPort());
, and if using ""0.0.0.0"" in the configuration, you get 0.0.0.0 from bindAddress.getAddress().

Normally (non webhdfs) this is not an issue because it is the responsibility of the client, but in the case of WebHDFS, WebHDFS does it before returning the string version of the token (it must be this way because the client may not be a java client at all and cannot manipulate the DelegationToken as such).

The solution (thanks to Eric Sammer for helping figure this out) is for WebHDFS to use the exacty hostname that came in the HTTP request as the service to set in the delegation tokens."
NameNode low on available disk space,HDFS-4425,"Hi,

Namenode switches into safemode when it has low disk space on the root fs / i have to manually run a command to leave it. Below are log messages for low space on root / fs. Is there any parameter so that i can reduce reserved amount.


2013-01-21 01:22:52,217 WARN org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker: Space available on volume '/dev/mapper/vg_lv_root' is 10653696, which is below the configured reserved amount 104857600
2013-01-21 01:22:52,218 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: NameNode low on available disk space. Entering safe mode.
2013-01-21 01:22:52,218 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is ON.


"
hdfs script does not work out of the box.,HDFS-2674,"As the title says, hadoop-config.sh doesn't add the hadoop-common jars, which makes the hdfs script fail.

To repro, follow the instructions from http://wiki.apache.org/hadoop/HowToSetupYourDevelopmentEnvironment
{code}
ivank@spokegrown-lm ~/src/hadoop-common Tue Dec 13 19:14:24 [0 jobs] [hist 1889] 
$ export HADOOP_COMMON_HOME=$(pwd)/$(ls -d hadoop-common-project/hadoop-common/target/hadoop-common-*-SNAPSHOT)
ivank@spokegrown-lm ~/src/hadoop-common Tue Dec 13 19:14:29 [0 jobs] [hist 1890] 
$ export HADOOP_HDFS_HOME=$(pwd)/$(ls -d hadoop-hdfs-project/hadoop-hdfs/target/hadoop-hdfs-*-SNAPSHOT)
ivank@spokegrown-lm ~/src/hadoop-common Tue Dec 13 19:14:36 [0 jobs] [hist 1891] 
$ export PATH=$HADOOP_COMMON_HOME/bin:$HADOOP_HDFS_HOME/bin:$PATH
ivank@spokegrown-lm ~/src/hadoop-common Tue Dec 13 19:14:42 [0 jobs] [hist 1892] 
$ cat > $HADOOP_COMMON_HOME/etc/hadoop/core-site.xml  << EOF
> <?xml version=""1.0""?><!-- core-site.xml -->
> <configuration>
>   <property>
>     <name>fs.default.name</name>
>     <value>hdfs://localhost/</value>
>   </property>
> </configuration>
> EOF
ivank@spokegrown-lm ~/src/hadoop-common Tue Dec 13 19:14:51 [0 jobs] [hist 1893] 
$ hdfs namenode -format
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/HadoopIllegalArgumentException
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.HadoopIllegalArgumentException
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)

{code}"
dfs.du.reserved not honored in 0.15/16 (regression from 0.14+patch for 2549),HDFS-74,"changes for https://issues.apache.org/jira/browse/HADOOP-1463

have caused a regression. earlier:

- we could set dfs.du.reserve to 1G and be *sure* that 1G would not be used.

now this is no longer true. I am quoting Pete Wyckoff's example:

<example>
Let's look at an example. 100 GB disk and /usr using 45 GB and dfs using 50 GBs now

Df -kh shows:

Capacity = 100 GB
Available = 1 GB (remember ~4 GB chopped out for metadata and stuff)
Used = 95 GBs   

remaining = 100 GB - 50 GB - 1GB = 49 GB 

Min(remaining, available) = 1 GB

98% of which is usable for DFS apparently - 

So, we're at the limit, but are free to use 98% of the remaining 1GB.
</example>

this is broke. based on the discussion on 1463 - it seems like the notion of 'capacity' as being the first field of 'df' is problematic. For example - here's what our df output looks like:

Filesystem            Size  Used Avail Use% Mounted on
/dev/sda3             130G  123G   49M 100% /


as u can see - 'Size' is a misnomer - that much space is not available. Rather the actual usable space is 123G+49M ~ 123G. (not entirely sure what the discrepancy is due to - but have heard this may be due to space reserved for file system metadata). Because of this discrepancy - we end up in a situation where file system is out of space.
"
Datanode stops cleaning disk space,HDFS-63,"Here is the situation - DFS cluster running Hadoop version 0.19.0. The cluster is running on multiple servers with practically identical hardware. 
Everything works perfectly well, except for one thing - from time to time one of the data nodes (every time it's a different node) starts to consume more and more disk space. The node keeps going and if we don't do anything - it runs out of space completely (ignoring 20GB reserved space settings). 
Once restarted - it cleans disk rapidly and goes back to approximately the same utilization as the rest of data nodes in the cluster.
"
MapReduce Streaming job hang when all replications of the input file has corrupted!,HDFS-183,"On some special cases, all replications of a given file has truncated to zero  but the namenode still hold the original size (we don't know why),  the mapreduce streaming job will hang if we don't specified mapred.task.timeout when the input files contain this corrupted file, even the dfs shell ""cat"" will hang when fetch data from this corrupted file.

We found that job hang at DFSInputStream.blockSeekTo() when chosing a datanode.  The following test will show:
1)	Copy a small file to hdfs. 
2)	Get the file blocks and login to these datanodes, and truncate these blocks to zero.
3)	Cat this file through dfs shell ""cat""
4)	Cat command will enter dead loop.
"
java.net.SocketTimeoutException: timed out waiting for rpc response ,HDFS-191,"Following exception happens when users run Pig queries over large data. Brought up this with Hadoop team and this is follow-up JIRA. 

java.net.SocketTimeoutException: timed out waiting for rpc response
        at org.apache.hadoop.ipc.Client.call(Client.java:484)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:184)
        at $Proxy1.getJobStatus(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
        at $Proxy1.getJobStatus(Unknown Source)
        at org.apache.hadoop.mapred.JobClient$NetworkedJob.ensureFreshStatus(JobClient.java:182)
        at org.apache.hadoop.mapred.JobClient$NetworkedJob.isComplete(JobClient.java:237)
        at org.apache.pig.impl.mapreduceExec.MapReduceLauncher.launchPig(MapReduceLauncher.java:189)
        at org.apache.pig.impl.physicalLayer.POMapreduce.open(POMapreduce.java:136)
        at org.apache.pig.impl.physicalLayer.PhysicalPlan.exec(PhysicalPlan.java:39)
        at org.apache.pig.impl.physicalLayer.IntermedResult.exec(IntermedResult.java:122)
        at org.apache.pig.PigServer.store(PigServer.java:445)
        at org.apache.pig.PigServer.store(PigServer.java:413)
        at org.apache.pig.tools.grunt.GruntParser.processStore(GruntParser.java:135)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:327)
        at org.apache.pig.tools.grunt.GruntParser.parseContOnError(GruntParser.java:64)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:48)
        at org.apache.pig.Main.main(Main.java:246)
timed out waiting for rpc response

Is this a bug or we have  a time-out setting directly related to this in Hadoop?"
separate space reservation for hdfs blocks and intermediate storage,HDFS-329,"both dfs client buffering (and i imagine map-reduce intermediate data) and datanode try to honor the same space reservation (dfs.du.reserved). But this is problematic because once hdfs/data-node fill up a node - there's no space left for map-reduce computations.

ideally - hdfs should be allowed to consume upto some watermark (say 60%) and then dfs buffering/intermediate storage should be allowed to consume space upto some higher watermark (say 90%). this way the node will always remain usable.

we are hitting this problem in a cluster where a few nodes have lower amount of space. while the cluster overall has space left, these nodes are hitting their space limits. but now tasks scheduled on these nodes fail because dfs client does not find space to buffer to. there's no workaround really i can think of.

another option would be to globally allocate hdfs blocks based on space availability (keep all nodes at the same space utilization % approx.)."
"DataNode fails to deliver blocks, holds thousands of open socket connections",HDFS-162,"9/27 update: uploaded the logs, with hopefully all the bits that should be examined. If other things are needed, just let me know. Note that all the paths refer to 0.18.1. This is still an 18.0 installation using the 18.0 core jar, just installed to a non-standard location.

9/26 update: we have successfully reproduced this using Hadoop 0.18 as well. The problem happens on both our own network infrastructure as well as on an Amazon EC2 cluster running CentOS5 images. I'll be attaching the logs Raghu asked for shortly.

A job that used to run correctly on our grid (in 0.15.0) now fails. The failure occurs after the map phase is complete, and about 2/3rds of the way through the reduce phase.   This job is processing a modest amount of input data (approximately 220G)

When the error occurs the nodes hosting DataNodes have literally thousands of open socket connections on them.  The DataNode instances are holding large amounts of memory.  Sometimes the DataNodes crash or exit, other times they continue to run.

The error which gets kicked out from the application perspective is:

08/05/27 11:30:08 INFO mapred.JobClient: map 100% reduce 89%
08/05/27 11:30:41 INFO mapred.JobClient: map 100% reduce 90%
08/05/27 11:32:45 INFO mapred.JobClient: map 100% reduce 86%
08/05/27 11:32:45 INFO mapred.JobClient: Task Id :
 task_200805271056_0001_r_000007_0, Status : FAILED
java.io.IOException: Could not get block locations. Aborting...
at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.processDatanode
 Error(DFSClient.java:1832)
at
 org.apache.hadoop.dfs.DFSClient$DFSOutputStream.access$1100(DFSClient.java:1487)
at
 org.apache.hadoop.dfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:1579)

I then discovered that 1 or more DataNode instances on the slave nodes
 are down (we run 1 DataNode instance per machine). The cause for at
 least some of the DataNode failures is a JVM internal error that gets
 raised due to a complete out-of-memory scenario (on a 4G, 4-way machine). 

Watching the DataNodes run, I can see them consuming more and more
 memory. For those failures for which there is a JVM traceback, I see (in
 part...NOTE 0.16.4 TRACEBACK):
#
# java.lang.OutOfMemoryError: requested 16 bytes for CHeapObj-new. Out
 of swap space?
#
# Internal Error (414C4C4F434154494F4E0E494E4C494E450E4850500017),
 pid=4246, tid=2283883408
#
# Java VM: Java HotSpot(TM) Server VM (1.6.0_02-b05 mixed mode)
# If you would like to submit a bug report, please visit:
# http://java.sun.com/webapps/bugreport/crash.jsp
#
--------------- T H R E A D ---------------
Current thread (0x8a942000): JavaThread
 ""org.apache.hadoop.dfs.DataNode$DataXceiver@3f4f44"" daemon [_thread_in_Java, id=15064]
Stack: [0x881c4000,0x88215000), sp=0x882139e0, free space=318k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code,
 C=native code)
V [libjvm.so+0x53b707]
V [libjvm.so+0x225fe1]
V [libjvm.so+0x16fdc5]
V [libjvm.so+0x22aef3]
Java frames: (J=compiled Java code, j=interpreted, Vv=VM code)
v blob 0xf4f235a7
J java.io.DataInputStream.readInt()I
j
 org.apache.hadoop.dfs.DataNode$BlockReceiver.receiveBlock(Ljava/io/DataOutputStream;Ljava/io/DataInputStream;Ljava/io/DataOutputStream;Ljava/lang/String;Lorg/a
pache/hadoop/dfs/DataNode$Throttler;I)V+126
j
 org.apache.hadoop.dfs.DataNode$DataXceiver.writeBlock(Ljava/io/DataInputStream;)V+746
j org.apache.hadoop.dfs.DataNode$DataXceiver.run()V+174
j java.lang.Thread.run()V+11
v ~StubRoutines::call_stub
--------------- P R O C E S S ---------------
Java Threads: ( => current thread )
0x0ae3f400 JavaThread ""process reaper"" daemon [_thread_blocked,
 id=26870]
0x852e6000 JavaThread
 ""org.apache.hadoop.dfs.DataNode$DataXceiver@e5dce1"" daemon [_thread_in_vm, id=26869]
0x08a1cc00 JavaThread ""PacketResponder 0 for Block
 blk_-6186975972786687394"" daemon [_thread_blocked, id=26769]
0x852e5000 JavaThread
 ""org.apache.hadoop.dfs.DataNode$DataXceiver@c40bf8"" daemon [_thread_in_native, id=26768]
0x0956e000 JavaThread ""PacketResponder 0 for Block
 blk_-2322514873363546651"" daemon [_thread_blocked, id=26767]
0x852e4400 JavaThread
 ""org.apache.hadoop.dfs.DataNode$DataXceiver@1ca61f9"" daemon [_thread_in_native, id=26766]
0x09d3a400 JavaThread ""PacketResponder 0 for Block
 blk_8926941945313450801"" daemon [_thread_blocked, id=26764]
0x852e3c00 JavaThread
 ""org.apache.hadoop.dfs.DataNode$DataXceiver@1e186d9"" daemon [_thread_in_native, id=26763]
0x0953d000 JavaThread ""PacketResponder 0 for Block
 blk_4785883052769066976"" daemon [_thread_blocked, id=26762]
0xb13a5c00 JavaThread
 ""org.apache.hadoop.dfs.DataNode$DataXceiver@13d62aa"" daemon [_thread_in_native, id=26761]

The interesting part here is that if I count the number of JavaThreads
 running org.apache.hadoop.dfs.DataNode I see 4,538 (!) in the
 traceback. The number of threads was surprising.

Other DataNodes just exit without panicking the JVM. In either failure
 mode, the last few lines of the DataNode log file is apparently
 innocuous:

2008-05-27 11:31:47,663 INFO org.apache.hadoop.dfs.DataNode: Datanode 2
 got response for connect ack from downstream datanode with
 firstbadlink as
2008-05-27 11:31:47,663 INFO org.apache.hadoop.dfs.DataNode: Datanode 2
 forwarding connect ack to upstream firstbadlink is
2008-05-27 11:31:48,268 INFO org.apache.hadoop.dfs.DataNode: Receiving
 block blk_-2241766430103062484 src: /10.2.14.10:33626 dest:
 /10.2.14.10:50010
2008-05-27 11:31:48,740 INFO org.apache.hadoop.dfs.DataNode: Receiving
 block blk_313239508245918539 src: /10.2.14.24:37836 dest:
 /10.2.14.24:50010
2008-05-27 11:31:48,740 INFO org.apache.hadoop.dfs.DataNode: Datanode 0
 forwarding connect ack to upstream firstbadlink is
2008-05-27 11:31:49,044 INFO org.apache.hadoop.dfs.DataNode: Receiving
 block blk_1684581399908730353 src: /10.2.14.16:51605 dest:
 /10.2.14.16:50010
2008-05-27 11:31:49,044 INFO org.apache.hadoop.dfs.DataNode: Datanode 0
 forwarding connect ack to upstream firstbadlink is
2008-05-27 11:31:49,509 INFO org.apache.hadoop.dfs.DataNode: Receiving
 block blk_2493969670086107736 src: /10.2.14.18:47557 dest:
 /10.2.14.18:50010
2008-05-27 11:31:49,513 INFO org.apache.hadoop.dfs.DataNode: Datanode 1
 got response for connect ack from downstream datanode with
 firstbadlink as
2008-05-27 11:31:49,513 INFO org.apache.hadoop.dfs.DataNode: Datanode 1
 forwarding connect ack to upstream firstbadlink is

Finally, the task-level output (in userlogs) doesn't reveal much
 either:

2008-05-27 11:38:30,724 INFO org.apache.hadoop.mapred.ReduceTask:
 task_200805271056_0001_r_000007_1 Need 34 map output(s)
2008-05-27 11:38:30,753 INFO org.apache.hadoop.mapred.ReduceTask:
 task_200805271056_0001_r_000007_1 done copying
 task_200805271056_0001_m_001976_0 output from worker9.
2008-05-27 11:38:31,727 INFO org.apache.hadoop.mapred.ReduceTask:
 task_200805271056_0001_r_000007_1: Got 0 new map-outputs & 0 obsolete
 map-outputs from tasktracker and 0 map-outputs from previous failures
2008-05-27 11:38:31,727 INFO org.apache.hadoop.mapred.ReduceTask:
 task_200805271056_0001_r_000007_1 Got 33 known map output location(s);
 scheduling...
2008-05-27 11:38:31,727 INFO org.apache.hadoop.mapred.ReduceTask:
 task_200805271056_0001_r_000007_1 Scheduled 1 of 33 known outputs (0 slow
 hosts and 32 dup hosts)
2008-05-27 11:38:31,727 INFO org.apache.hadoop.mapred.ReduceTask:
 task_200805271056_0001_r_000007_1 Copying task_200805271056_0001_m_001248_0
 output from worker8.
2008-05-27 11:38:31,727 INFO org.apache.hadoop.mapred.ReduceTask:
 task_200805271056_0001_r_000007_1 Need 33 map output(s)
2008-05-27 11:38:31,752 INFO org.apache.hadoop.mapred.ReduceTask:
 task_200805271056_0001_r_000007_1 done copying
 task_200805271056_0001_m_001248_0 output from worker8.
"
regarding HDFS-RAID,HDFS-2600,"I've tried to do HDFS-RAID over hadoop-0.21.0 ...
 
But, It's failed to raid when I've tried to start raidnode (start-raidnode.sh)
 
What do I have to correct it?
 
This is the information. Would you tell me how to correct it?
 
Namenode : xxx.23.129.40:9006
Datanode: xxx.23.129.39, 38
Raidnode : xxx.23.129.40 ( I want to seperate raidnoe from namenode, how can I do?)
 
* hdfs-site.xml
  <name>hdfs.raid.locations</name>
    <value>hdfs://xxx.23.129.40:9006/raid</value>   
=>  (What's the meaning of this, which address do I have to write down?)
    <description>The location for parity files. If this is
      is not defined, then defaults to /raid.
    </description>
  </property>
 
* raid.xml
<configuration>
    <srcPath prefix=""hdfs://xxx.23.129.40:9006/TEST"">  
=>  (What's the meaning of this, which address do I have to write down?)
      <policy name = ""TEST"">          =>  (What's this? )
        <property>
          <name>srcReplication</name>
          <value>3</value>
          <description> pick files for RAID only if their replication factor is
                        greater than or equal to this value.
          </description>
        </property>
        <property>
          <name>targetReplication</name>
          <value>2</value>
          <description> after RAIDing, decrease the replication factor of a file to
                        this value.
          </description>
        </property>
        <property>
          <name>metaReplication</name>
          <value>2</value>
          <description> the replication factor of the RAID meta file
          </description>
        </property>
        <property>
          <name>modTimePeriod</name>
          <value>100</value>
          <description> time (milliseconds) after a file is modified to make it a
                        candidate for RAIDing
          </description>
        </property>
 
 
*log:  when start-raidnode.sh

/************************************************************
SHUTDOWN_MSG: Shutting down RaidNode at test40/xxx.23.129.40
************************************************************/
2011-11-25 23:59:13,417 INFO org.apache.hadoop.raid.RaidNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting RaidNode
STARTUP_MSG:   host = test40/xxx.23.129.40
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 0.21.0
************************************************************/
2011-11-25 23:59:13,518 ERROR org.apache.hadoop.raid.ConfigManager: Reloading config 

file /KTBHOME/hadoop/hadoop/conf/raid.xml
2011-11-25 23:59:13,611 INFO org.apache.hadoop.security.Groups: Group mapping 

impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping; cacheTimeout=300000
2011-11-25 23:59:13,647 WARN org.apache.hadoop.conf.Configuration: mapred.task.id is 

deprecated. Instead, use mapreduce.task.attempt.id
2011-11-25 23:59:13,714 INFO org.apache.hadoop.raid.ConfigManager: 

TEST.srcReplication = 3
2011-11-25 23:59:13,714 INFO org.apache.hadoop.raid.ConfigManager: 

TEST.targetReplication = 2
2011-11-25 23:59:13,715 INFO org.apache.hadoop.raid.ConfigManager: 

TEST.metaReplication = 2
2011-11-25 23:59:13,715 INFO org.apache.hadoop.raid.ConfigManager: 

TEST.modTimePeriod = 100
2011-11-25 23:59:13,716 INFO org.apache.hadoop.raid.ConfigManager: 

k_table1.targetReplication = 1
2011-11-25 23:59:13,716 INFO org.apache.hadoop.raid.ConfigManager: 

k_table1.metaReplication = 2
2011-11-25 23:59:13,716 INFO org.apache.hadoop.raid.ConfigManager: 

k_table1.modTimePeriod = 100
2011-11-25 23:59:13,728 INFO org.apache.hadoop.ipc.Server: Starting SocketReader
2011-11-25 23:59:13,751 INFO org.apache.hadoop.ipc.metrics.RpcMetrics: Initializing 

RPC Metrics with hostName=RaidNode, port=60000
2011-11-25 23:59:13,810 INFO org.apache.hadoop.ipc.metrics.RpcDetailedMetrics: 

Initializing RPC Metrics with hostName=RaidNode, port=60000
2011-11-25 23:59:13,812 INFO org.apache.hadoop.raid.RaidNode: RaidNode up at: 

/127.0.0.1:60000
2011-11-25 23:59:13,813 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: 

starting
2011-11-25 23:59:13,813 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 

60000: starting
2011-11-25 23:59:13,814 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 

60000: starting
2011-11-25 23:59:13,814 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 

60000: starting
2011-11-25 23:59:13,815 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 

60000: starting
2011-11-25 23:59:13,815 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 

60000: starting
2011-11-25 23:59:13,815 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 

60000: starting
2011-11-25 23:59:13,815 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 

60000: starting
2011-11-25 23:59:13,815 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 

60000: starting
2011-11-25 23:59:13,816 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 

60000: starting
2011-11-25 23:59:13,816 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 

60000: starting
2011-11-25 23:59:13,816 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 

60000: starting
2011-11-25 23:59:13,818 INFO org.apache.hadoop.raid.RaidNode: Triggering Policy 

Filter TEST hdfs://xxx.23.129.40:9006/TEST
2011-11-25 23:59:13,819 INFO org.apache.hadoop.raid.RaidNode: Started archive scan
2011-11-25 23:59:13,828 INFO org.apache.hadoop.raid.RaidNode: No filtered paths for 

policy TEST
2011-11-25 23:59:13,828 INFO org.apache.hadoop.raid.RaidNode: Triggering Policy 
Filter k_table1 hdfs://xxx.23.129.40:9006
2011-11-25 23:59:13,829 INFO org.apache.hadoop.raid.RaidNode: No filtered paths for 
policy k_table1"
hftp token renewal uses wrong port,HDFS-2326,"MAPREDUCE-2764 introduced a new method for token renewal.  The change appears to have a problem renewing hftp tokens.  The {{setDelegationToken}} method will reset the token's service to contain the remote rpc port.  However, the renewer expects it to be the remote https port."
"when i put data in hadoop cluster,there is a severe problem ",HDFS-2262,"the datanode always give the message
Receiving one packet for block blk_******* of length 65536 seqno 204625 offsetInBlock 5917184 lastPacketInBlock false
PacketResponder 0 seqno=-2 for block blk_******** waiting for local datanode to finish write
Receiving one packet for block blk_******* of length 65536 seqno 204626 offsetInBlock 5982208 lastPacketInBlock false
PacketResponder 0 seqno=-2 for block blk_******** waiting for local datanode to finish write
...........
......
IOException in BlockReceiver.run()
java.io.IOException:No temprary file /mnt/data5/dfs/data/blocksBeingWritten/blk_****  for block_****_1461
    at org.apache.hadoop.hdfs.server.datanode.FSDateset.finalizeBlockInternal(FSdataset.java 1393)
    at org.apache.hadoop.hdfs.server.datanode.FSDateset.finalizeBlock(FSdataset.java 1370)
"
HDFS Contrib project ivy dependencies are not included in binary target,HDFS-768,"As in HADOOP-6370, only Hadoop's own library dependencies are promoted to ${build.dir}/lib; any libraries required by contribs are not redistributed."
TestFileConcurrentReader test case is still timing out / failing,HDFS-1401,"The unit test case, TestFileConcurrentReader after its most recent fix in HDFS-1310 still times out when using java 1.6.0_07.  When using java 1.6.0_07, the test case simply hangs.  On apache Hudson build ( which possibly is using a higher sub-version of java) this test case has presented an inconsistent test result that it sometimes passes, some times fails. For example, between the most recent build 423, 424 and build 425, there is no effective change, however, the test case failed on build 424 and passed on build 425

build 424 test failed
https://hudson.apache.org/hudson/job/Hadoop-Hdfs-trunk/424/testReport/org.apache.hadoop.hdfs/TestFileConcurrentReader/

build 425 test passed
https://hudson.apache.org/hudson/job/Hadoop-Hdfs-trunk/425/testReport/org.apache.hadoop.hdfs/TestFileConcurrentReader/"
Unit test org.apache.hadoop.dfs.TestBalancer fails on Solaris with a timeout,HDFS-40,"Unit test org.apache.hadoop.dfs.TestBalancer fails on Solaris with a timeout

The console output is located at:
http://hudson.zones.apache.org/hudson/job/Hadoop-trunk/lastBuild/console

Seems like the test was able to get the cluster balanced and then failed:

    [junit] The cluster is balanced. Exiting...
    [junit] 2008-02-05 17:18:22,215 INFO  dfs.StateChange (FSNamesystem.java:allocateBlock(1288)) - BLOCK* NameSystem.allocateBlock: /system/balancer.id. blk_3509566446573704077
    [junit] 2008-02-05 17:18:22,216 INFO  dfs.DataNode (DataNode.java:writeBlock(1048)) - Receiving block blk_3509566446573704077 src: /127.0.0.1:34105 dest: /127.0.0.1:34083
    [junit] 2008-02-05 17:18:22,217 INFO  dfs.DataNode (DataNode.java:writeBlock(1141)) - Datanode 0 forwarding connect ack to upstream firstbadlink is 
    [junit] 2008-02-05 17:18:22,218 INFO  dfs.DataNode (DataNode.java:receivePacket(2175)) - Receiving empty packet for block blk_3509566446573704077
    [junit] 2008-02-05 17:18:22,218 INFO  dfs.DataNode (DataNode.java:lastDataNodeRun(1775)) - Received block blk_3509566446573704077 of size 3 from /127.0.0.1
    [junit] 2008-02-05 17:18:22,218 INFO  dfs.DataNode (DataNode.java:lastDataNodeRun(1793)) - PacketResponder 0 for block blk_3509566446573704077 terminating
    [junit] 2008-02-05 17:18:22,219 INFO  dfs.StateChange (FSNamesystem.java:addStoredBlock(2485)) - BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:34083 is added to blk_3509566446573704077 size 3
    [junit] 2008-02-05 17:18:22,254 INFO  dfs.StateChange (FSNamesystem.java:deleteInternal(1473)) - BLOCK* NameSystem.delete: blk_5642678384995114695 is added to invalidSet of 127.0.0.1:34083
    [junit] 2008-02-05 17:18:22,254 INFO  dfs.StateChange (FSNamesystem.java:deleteInternal(1473)) - BLOCK* NameSystem.delete: blk_3982250783996683858 is added to invalidSet of 127.0.0.1:34083
    [junit] 2008-02-05 17:18:22,255 INFO  dfs.StateChange (FSNamesystem.java:deleteInternal(1473)) - BLOCK* NameSystem.delete: blk_3509566446573704077 is added to invalidSet of 127.0.0.1:34083
    [junit] Balancing took 1.773 seconds
    [junit] 2008-02-05 17:18:23,601 INFO  dfs.StateChange (FSNamesystem.java:blocksToInvalidate(3013)) - BLOCK* NameSystem.blockToInvalidate: ask 127.0.0.1:34066 to delete  blk_-7040479509416829015 blk_7804526723654242046 blk_-7913732279285380423 blk_-4517202454091217528 blk_-7200213733738623986 blk_3308335641891931015 blk_-7617910363506885261 blk_-6988117892488064923 blk_-8927068573243769130
    [junit] 2008-02-05 17:18:23,601 INFO  dfs.StateChange (FSNamesystem.java:blocksToInvalidate(3013)) - BLOCK* NameSystem.blockToInvalidate: ask 127.0.0.1:34083 to delete  blk_5642678384995114695 blk_3982250783996683858 blk_3509566446573704077
    [junit] Running org.apache.hadoop.dfs.TestBalancer
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] Test org.apache.hadoop.dfs.TestBalancer FAILED (timeout)
"
Backporting HDFS-12882 to branch-3.0: Support full open(PathHandle) contract in HDFS,HDFS-14159,"This task aims to backport聽HDFS-12882 and some connecting commits to branch-3.0 without introducing API incompatibilities.

In order to be able to cleanly backport, first HDFS-7878, then HDFS-12877 should be backported to that branch as well (both can be executed cleanly, and with build success).

Also, this patch would also introduce API backward incompatibilities in hadoop-hdfs-client, and we should聽modify it to a compat change (similar as in HDFS-13830 fought with this problem).

聽"
hdfsread crash when reading data reaches to 128M,HDFS-10369,"see code below, it would crash after   printf(""hdfsGetDefaultBlockSize2:%d, ret:%d\n"", hdfsGetDefaultBlockSize(fs), ret);
  
hdfsFile read_file = hdfsOpenFile(fs, ""/testpath"", O_RDONLY, 0, 0, 1); 
  int total = hdfsAvailable(fs, read_file);
  printf(""Total:%d\n"", total);
  char* buffer = (char*)malloc(sizeof(size+1) * sizeof(char));
  int ret = -1; 
  int len = 0;
  ret = hdfsSeek(fs, read_file, 134152192);
  printf(""hdfsGetDefaultBlockSize1:%d, ret:%d\n"", hdfsGetDefaultBlockSize(fs), ret);
  ret = hdfsRead(fs, read_file, (void*)buffer, size);
  printf(""hdfsGetDefaultBlockSize2:%d, ret:%d\n"", hdfsGetDefaultBlockSize(fs), ret);
  ret = hdfsRead(fs, read_file, (void*)buffer, size);
  printf(""hdfsGetDefaultBlockSize3:%d, ret:%d\n"", hdfsGetDefaultBlockSize(fs), ret);
  return 0;"
TestConfigurationFieldsBase.testCompareConfigurationClassAgainstXml fails due to missing dfs.image.string-tables.expanded from hdfs-defaults.xml,HDFS-14100,After聽HDFS-13882聽TestConfigurationFieldsBase.testCompareConfigurationClassAgainstXml聽requires hdfs-defaults.xml to have dfs.image.string-tables.expanded added and populated with a default value.
[SPS]: Fix the branch review comments,HDFS-13084,"Fix the review comments provided by [~daryn]

聽"
Add docs for NameNode initializeSharedEdits and bootstrapStandby commands,HDFS-3455,"We've made the HA setup easier by adding new flags to the namenode to automatically set up the standby. But, we didn't document them yet. We should amend the HDFSHighAvailability.apt.vm docs to include this."
TestHttpFSWithKerberos failed,HDFS-11255,"{noformat}
$ mvn test -P\!shelltest -Dtest=TestHttpFSWithKerberos
...
Running org.apache.hadoop.fs.http.server.TestHttpFSWithKerberos
Tests run: 6, Failures: 1, Errors: 5, Skipped: 0, Time elapsed: 7.356 sec <<< FAILURE! - in org.apache.hadoop.fs.http.server.TestHttpFSWithKerberos
testDelegationTokenWithWebhdfsFileSystem(org.apache.hadoop.fs.http.server.TestHttpFSWithKerberos)  Time elapsed: 4.73 sec  <<< ERROR!
org.apache.hadoop.security.KerberosAuthException: Login failure for user: client from keytab /Users/tucu/tucu.keytab javax.security.auth.login.LoginException: Unable to obtain password from user

        at com.sun.security.auth.module.Krb5LoginModule.promptForPass(Krb5LoginModule.java:897)
        at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:760)
        at com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:617)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at javax.security.auth.login.LoginContext.invoke(LoginContext.java:755)
        at javax.security.auth.login.LoginContext.access$000(LoginContext.java:195)
        at javax.security.auth.login.LoginContext$4.run(LoginContext.java:682)
        at javax.security.auth.login.LoginContext$4.run(LoginContext.java:680)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:680)
        at javax.security.auth.login.LoginContext.login(LoginContext.java:587)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:1092)
        at org.apache.hadoop.fs.http.server.TestHttpFSWithKerberos.testDelegationTokenWithinDoAs(TestHttpFSWithKerberos.java:239)
        at org.apache.hadoop.fs.http.server.TestHttpFSWithKerberos.testDelegationTokenWithWebhdfsFileSystem(TestHttpFSWithKerberos.java:270)

testInvalidadHttpFSAccess(org.apache.hadoop.fs.http.server.TestHttpFSWithKerberos)  Time elapsed: 1.581 sec  <<< FAILURE!
java.lang.AssertionError: expected:<503> but was:<401>
        at org.junit.Assert.fail(Assert.java:88)
        at org.junit.Assert.failNotEquals(Assert.java:743)
        at org.junit.Assert.assertEquals(Assert.java:118)
        at org.junit.Assert.assertEquals(Assert.java:555)
        at org.junit.Assert.assertEquals(Assert.java:542)
        at org.apache.hadoop.fs.http.server.TestHttpFSWithKerberos.testInvalidadHttpFSAccess(TestHttpFSWithKerberos.java:144)
...
Failed tests: 
  TestHttpFSWithKerberos.testInvalidadHttpFSAccess:144 expected:<503> but was:<401>

Tests in error: 
  TestHttpFSWithKerberos.testDelegationTokenWithWebhdfsFileSystem:270->testDelegationTokenWithinDoAs:239 禄 KerberosAuth
  TestHttpFSWithKerberos.testValidHttpFSAccess:120 禄 Login Unable to obtain pass...
  TestHttpFSWithKerberos.testDelegationTokenWithHttpFSFileSystem:262->testDelegationTokenWithinDoAs:239 禄 KerberosAuth
  TestHttpFSWithKerberos.testDelegationTokenWithHttpFSFileSystemProxyUser:279->testDelegationTokenWithinDoAs:239 禄 KerberosAuth
  TestHttpFSWithKerberos.testDelegationTokenHttpFSAccess:155 禄 Login Unable to o...
{noformat}"
HDFS FILE CREATE APPEND LEASE,HDFS-14031,"org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException): Failed to APPEND_FILE /mnt/data1/datadir/hive_cluster1/warehouse/singhand_ntcstore.db/singhand_ntc_ip_tmp1/singhand_ntc_ip_log2073 for DFSClient_NONMAPREDUCE_818722237_1 on 172.16.0.166 because DFSClient_NONMAPREDUCE_818722237_1 is already the current lease holder.
聽聽 聽at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:2883)
聽聽 聽at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFileInternal(FSNamesystem.java:2683)
聽聽 聽at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFileInt(FSNamesystem.java:2982)
聽聽 聽at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:2950)
聽聽 聽at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:654)
聽聽 聽at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.append(ClientNamenodeProtocolServerSideTranslatorPB.java:421)
聽聽 聽at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
聽聽 聽at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
聽聽 聽at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
聽聽 聽at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
聽聽 聽at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
聽聽 聽at java.security.AccessController.doPrivileged(Native Method)
聽聽 聽at javax.security.auth.Subject.doAs(Subject.java:422)
聽聽 聽at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
聽聽 聽at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2045)

聽聽 聽at org.apache.hadoop.ipc.Client.call(Client.java:1475)
聽聽 聽at org.apache.hadoop.ipc.Client.call(Client.java:1412)
聽聽 聽at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
聽聽 聽at com.sun.proxy.$Proxy8.append(Unknown Source)
聽聽 聽at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.append(ClientNamenodeProtocolTranslatorPB.java:328)
聽聽 聽at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
聽聽 聽at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
聽聽 聽at java.lang.reflect.Method.invoke(Method.java:498)
聽聽 聽at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
聽聽 聽at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
聽聽 聽at com.sun.proxy.$Proxy9.append(Unknown Source)
聽聽 聽at org.apache.hadoop.hdfs.DFSClient.callAppend(DFSClient.java:1808)
聽聽 聽at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1877)
聽聽 聽at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1847)
聽聽 聽at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:340)
聽聽 聽at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:336)
聽聽 聽at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
聽聽 聽at org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:348)
聽聽 聽at org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:318)
聽聽 聽at org.apache.hadoop.fs.FileSystem.append(FileSystem.java:1164)
聽聽 聽at com.singhand.hdfs.utils.HDFSWriter.appendWriter(HDFSWriter.java:90)
聽聽 聽at com.singhand.ntc.mock.ReadTopic2HDFS.loadData(ReadTopic2HDFS.java:149)
聽聽 聽at com.singhand.ntc.mock.ReadTopic2HDFS.kafka2HDFS(ReadTopic2HDFS.java:74)
聽聽 聽at com.singhand.ntc.mock.ReadTopic2HDFS.main(ReadTopic2HDFS.java:255)"
RBF: Adding trace support,HDFS-13365,We should support HTrace and add spans.
Need 'force close',HDFS-7307,"Until HDFS-4882 and HDFS-7306 get real fixes, operations teams need a way to force close files.  DNs are essentially held hostage by broken clients that never close.  This situation will get worse as longer/permanently running jobs start increasing."
Consolidate the HA NN documentation down to one,HDFS-7777,These are nearly the same document now.  Let's consolidate.
distribute-excludes and refresh-namenodes update to new shell framework,HDFS-7850,These need to get updated to use new shell framework.
NFS hard codes ShellBasedIdMapping,HDFS-7904,The current NFS doesn't allow one to configure an alternative to the shell-based id mapping provider.  
Move the synthetic load generator into its own package,HDFS-8251,"It doesn't really make sense for the HDFS load generator to be a part of the (extremely large) mapreduce jobclient package. It should be pulled out and put its own package, probably in hadoop-tools."
WebHDFS REST v2,HDFS-9055,There's starting to be enough changes to fix and add missing functionality to webhdfs that we should probably update to REST v2.  This also gives us an opportunity to deal with some incompatible issues.
add set/remove quota capability to webhdfs,HDFS-9056,It would be nice to be able to set and remove quotas via WebHDFS.
enable find via WebHDFS,HDFS-9058,It'd be useful to implement find over webhdfs rather than forcing the client to grab a lot of data.
hdfs groups should be exposed via WebHDFS,HDFS-9061,It would be extremely useful from a REST perspective to expose which groups the NN says the user belongs to.
Add liberasurecode support,HDFS-9778,It would be beneficial to use liberasurecode as either supplemental or in lieu of ISA-L in order to provide the widest possible hardware/OS platform and OOB support.  Major software platforms appear to be converging on this library and we should too.
httpfs generates docs in bin tarball ,HDFS-10509,"When building a release, httpfs generates a share/doc/hadoop/httpfs dir with content when it shouldn't."
clearCorruptLazyPersistFiles could crash NameNode,HDFS-13672,"I started a NameNode on a pretty large fsimage. Since the NameNode is started without any DataNodes, all blocks (100 million) are ""corrupt"".

Afterwards I observed FSNamesystem#clearCorruptLazyPersistFiles() held write lock for a long time:

{noformat}
18/06/12 12:37:03 INFO namenode.FSNamesystem: FSNamesystem write lock held for 46024 ms via
java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.util.StringUtils.getStackTrace(StringUtils.java:945)
org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.writeUnlock(FSNamesystemLock.java:198)
org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeUnlock(FSNamesystem.java:1689)
org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber.clearCorruptLazyPersistFiles(FSNamesystem.java:5532)
org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber.run(FSNamesystem.java:5543)
java.lang.Thread.run(Thread.java:748)
        Number of suppressed write-lock reports: 0
        Longest write-lock held interval: 46024
{noformat}

Here's the relevant code:

{code}
      writeLock();

      try {
        final Iterator<BlockInfo> it =
            blockManager.getCorruptReplicaBlockIterator();

        while (it.hasNext()) {
          Block b = it.next();
          BlockInfo blockInfo = blockManager.getStoredBlock(b);
          if (blockInfo.getBlockCollection().getStoragePolicyID() == lpPolicy.getId()) {
            filesToDelete.add(blockInfo.getBlockCollection());
          }
        }

        for (BlockCollection bc : filesToDelete) {
          LOG.warn(""Removing lazyPersist file "" + bc.getName() + "" with no replicas."");
          changed |= deleteInternal(bc.getName(), false, false, false);
        }
      } finally {
        writeUnlock();
      }
{code}
In essence, the iteration over corrupt replica list should be broken down into smaller iterations to avoid a single long wait.

Since this operation holds NameNode write lock for more than 45 seconds, the default ZKFC connection timeout, it implies an extreme case like this (100 million corrupt blocks) could lead to NameNode failover."
To detect fsimage corruption on the spot,HDFS-13031,"Since we fixed HDFS-9406, there are new cases reported from the field that similar fsimage corruption happens. We need good fsimage + editlogs to replay to reproduce the corruption. However, usually when the corruption is detected (at later NN restart), the good fsimage is already deleted.

We need to have a way to detect fsimage corruption on the spot. Currently what I think we could do is:
 # after SNN creates a new fsimage, it spawn a new modified NN process (NN with some new command line args) to just load the fsimage and do nothing else.聽
 # If the process failed, the currently running SNN will do either a) backup the fsimage + editlogs or b) no longer do checkpointing. And it need to somehow raise a flag to user that the fsimage is corrupt.

In step 2, if we do a, we need to introduce new NN->JN API to backup editlogs; if we do b, it changes SNN's behavior, and kind of not compatible.聽
"
"RBF: Remove FSCK from Router Web UI, because fsck is not supported currently",HDFS-13803,"When i click FSCK on Router Web UI Utilities, i got errors
{quote}
HTTP ERROR 404
Problem accessing /fsck. Reason:

    NOT_FOUND
Powered by Jetty://
{quote}
I deep into the source code and find that fsck is not supported currently, So i think we should remove FSCK from Router Web UI"
[DOC] update flag is not necessary to avoid verifying checksums,HDFS-13764,"We mentioned to use ""-update"" option to avoid checksum in the following doc:

[https://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/TransparentEncryption.html#Copying_between_encrypted_and_unencrypted_locations]
{code:java}
Copying between encrypted and unencrypted locations
By default, distcp compares checksums provided by the filesystem to verify that the data was successfully copied to the destination. When copying between an unencrypted and encrypted location, the filesystem checksums will not match since the underlying block data is different. In this case, specify the聽-skipcrccheck聽and聽-update聽distcp flags to avoid verifying checksums.
{code}
聽

But actually, ""-update"" option is not necessary, only ""-skipcrccheck"" is needed. Can we change it to:

聽
{code:java}
Copying between encrypted and unencrypted locations
By default, distcp compares checksums provided by the filesystem to verify that the data was successfully copied to the destination. When copying between an unencrypted and encrypted location, the filesystem checksums will not match since the underlying block data is different. In this case, specify the聽-skipcrccheck聽flags to avoid verifying checksums.
{code}
聽"
"""hdfs dfs -du -h /*"" shows local dir instead of HDFS dir",HDFS-13751,"Hi,

""hdfs dfs -du聽 -h / ""command shows below result.

0聽 聽 聽 聽 /app-logs

95.2 G 聽 /apps

0聽 聽 聽 聽 /ats

249.8 M聽 /hdp

0聽 聽 聽 聽 /mapred

0聽 聽 聽 聽 /mr-history

0聽 聽 聽 聽 /ngs

18.2 M 聽 /tmp

431.5 M聽 /user

聽

Where as adding a * as ""hdfs dfs -du聽 -h /* "" shows below result.

du: `/bin': No such file or directory

du: `/boot': No such file or directory

du: `/cgroup': No such file or directory

du: `/cgroups_test': No such file or directory

du: `/dev': No such file or directory

du: `/etc': No such file or directory

du: `/hadoop': No such file or directory

du: `/home': No such file or directory

du: `/lib': No such file or directory

du: `/lib64': No such file or directory

du: `/lost+found': No such file or directory

du: `/media': No such file or directory

du: `/mnt': No such file or directory

du: `/ngs1': No such file or directory

du: `/ngs2': No such file or directory

du: `/ngs3': No such file or directory

du: `/ngs4': No such file or directory

du: `/ngs5': No such file or directory

du: `/ngs6': No such file or directory

du: `/ngs7': No such file or directory

du: `/ngs8': No such file or directory

du: `/opt': No such file or directory

du: `/proc': No such file or directory

du: `/root': No such file or directory

du: `/sbin': No such file or directory

du: `/selinux': No such file or directory

du: `/srv': No such file or directory

du: `/sys': No such file or directory

聽

0聽 聽 聽 聽 /tmp/demo

0聽 聽 聽 聽 /tmp/demo1

0聽 聽 聽 聽 /tmp/entity-file-history

390.8 K聽 /tmp/examples

64.8 K 聽 /tmp/examples.tar.gz

3.3 K聽 聽 /tmp/ide20a8b61_date472418

3.3 K聽 聽 /tmp/ide20a9661_date432418

3.3 K聽 聽 /tmp/ide20a9961_date151318

3.3 K聽 聽 /tmp/ide20aa061_date082418

17.7 M 聽 /tmp/lib_20180406111044

聽

聽

Why does adding ""*"" re-directs to local dir instead of HDFS dir.

聽

Thanks"
Ozone: Replace Jersey container with Netty Container,HDFS-10357,"In the ozone branch, we have implemented Web Interface calls using JAX-RS. This was very useful when the REST interfaces where in flux. This JIRA proposes to replace Jersey based code with pure netty and remove any dependency that Ozone has on Jersey. This will create both faster and simpler code in Ozone web interface."
Ozone: SCM: Handle duplicate Datanode ID ,HDFS-11139,"The Datanode ID is used when a data node registers. It is assumed that datanodes are unique across the cluster. 
However due to operator error or other cases we might encounter duplicate datanode ID. SCM should be able to recognize this and handle in correctly. Here is a sub-set  of datanode scenarios it needs to handle.

1. Normal Datanode
2.  Copy of a Datanode metadata by operator to another node
3. A Datanode being renamed - hostname change
4. Container Reports -- 2 machines with same datanode ID. SCM thinks they are same node.
5. Decommission --  we decommission both nodes if IDs are same.
6. Commands will be send to both nodes.

So it is necessary that SCM identity when a datanode is reusing a datanode ID that is already used by another node.  

"
Edit tailing period configuration should accept time units,HDFS-13595,"The {{dfs.ha.tail-edits.period}} config should accept time units to be able to more easily specified across a wide range, and in particular for HDFS-13150 it is useful to have a period shorter than 1 second which is not currently possible."
Hftp should support namenode logical service names in URI,HDFS-5123,"For example if the dfs.nameservices is set to arpit

{code}
hdfs dfs -ls hftp://arpit:50070/tmp

or 

hdfs dfs -ls hftp://arpit/tmp
{code}
does not work

You have to provide the exact active namenode hostname. On an HA cluster using dfs client one should not need to provide the active nn hostname"
Fix TestEncryptionZonesWithKMS failure due to HADOOP-14445,HDFS-13430,"Unfortunately HADOOP-14445 had an HDFS test failure that's not caught in the hadoop-common precommit runs.

This is caught聽by our internal pre-commit using dist-test, and appears to be the only failure."
TestDistributedFileSystem.testAllWithNoXmlDefaults failed intermittently,HDFS-6589,"https://builds.apache.org/job/PreCommit-HDFS-Build/7207 is clean
https://builds.apache.org/job/PreCommit-HDFS-Build/7208 has the following failure. The code is essentially the same.

Running the same test locally doesn't reproduce. A flaky test there.

{code}
Stacktrace

java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertFalse(Assert.java:64)
	at org.junit.Assert.assertFalse(Assert.java:74)
	at org.apache.hadoop.hdfs.TestDistributedFileSystem.testDFSClient(TestDistributedFileSystem.java:263)
	at org.apache.hadoop.hdfs.TestDistributedFileSystem.testAllWithNoXmlDefaults(TestDistributedFileSystem.java:651)
{code}
"
Secure Datanode stop/start from cli does not throw a valid error if HDFS_DATANODE_SECURE_USER is not set,HDFS-13501,Secure Datanode start/stop from cli does not throw a valid error if HADOOP_SECURE_DN_USER/HDFS_DATANODE_SECURE_USER is not set. If HDFS_DATANODE_SECURE_USER and JSVC_HOME is not set start/stop is expected to fail (when privilege ports are used) but it should show some valid message.
#NAME?,HDFS-11515,"HDFS-10797 fixed a disk summary (-du) bug, but it introduced a new bug.

The bug can be reproduced running the following commands:
{noformat}
bash-4.1$ hdfs dfs -mkdir /tmp/d0
bash-4.1$ hdfs dfsadmin -allowSnapshot /tmp/d0
Allowing snaphot on /tmp/d0 succeeded
bash-4.1$ hdfs dfs -touchz /tmp/d0/f4
bash-4.1$ hdfs dfs -mkdir /tmp/d0/d1
bash-4.1$ hdfs dfs -createSnapshot /tmp/d0 s1
Created snapshot /tmp/d0/.snapshot/s1
bash-4.1$ hdfs dfs -mkdir /tmp/d0/d1/d2
bash-4.1$ hdfs dfs -mkdir /tmp/d0/d1/d3
bash-4.1$ hdfs dfs -mkdir /tmp/d0/d1/d2/d4
bash-4.1$ hdfs dfs -mkdir /tmp/d0/d1/d3/d5
bash-4.1$ hdfs dfs -createSnapshot /tmp/d0 s2
Created snapshot /tmp/d0/.snapshot/s2
bash-4.1$ hdfs dfs -rmdir /tmp/d0/d1/d2/d4
bash-4.1$ hdfs dfs -rmdir /tmp/d0/d1/d2
bash-4.1$ hdfs dfs -rmdir /tmp/d0/d1/d3/d5
bash-4.1$ hdfs dfs -rmdir /tmp/d0/d1/d3
bash-4.1$ hdfs dfs -du -h /tmp/d0
du: java.util.ConcurrentModificationException
0 0 /tmp/d0/f4
{noformat}

A ConcurrentModificationException forced du to terminate abruptly.

Correspondingly, NameNode log has the following error:
{noformat}
2017-03-08 14:32:17,673 WARN org.apache.hadoop.ipc.Server: IPC Server handler 4 on 8020, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getContentSumma
ry from 10.0.0.198:49957 Call#2 Retry#0
java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:922)
        at java.util.HashMap$KeyIterator.next(HashMap.java:956)
        at org.apache.hadoop.hdfs.server.namenode.ContentSummaryComputationContext.tallyDeletedSnapshottedINodes(ContentSummaryComputationContext.java:209)
        at org.apache.hadoop.hdfs.server.namenode.INode.computeAndConvertContentSummary(INode.java:507)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.getContentSummary(FSDirectory.java:2302)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getContentSummary(FSNamesystem.java:4535)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getContentSummary(NameNodeRpcServer.java:1087)
        at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.getContentSummary(AuthorizationProviderProxyClientProtocol.java:5
63)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getContentSummary(ClientNamenodeProtocolServerSideTranslatorPB.jav
a:873)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2216)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2212)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2210)
{noformat}

The bug is due to a improper use of HashSet, not concurrent operations. Basically, a HashSet can not be updated while an iterator is traversing it."
Add config for min percentage of data nodes to come out of chill mode in SCM,HDFS-13354,SCM will come out of ChillMode if one datanode reports in now. We need to support percentage of known datanodes before SCM comes out of Chill Mode.
1.x: Add a retention period for purged edit logs,HDFS-3653,"Occasionally we have a bug which causes something to go wrong with edits files. Even more occasionally the bug is such that the namenode mistakenly deletes an {{edits}} file without merging it into {{fsimage}} properly -- e.g if the bug mistakenly writes an OP_INVALID at the top of the log.

In trunk/2.0 we retain many edit log segments going back in time to be more robust to this kind of error. I'd like to implement something similar (but much simpler) in 1.x, which would be used only by HDFS developers in root-causing or repairing from these rare scenarios: the NN should never directly delete an edit log file. Instead, it should rename the file into some kind of ""trash"" directory inside the name dir, and associate it with a timestamp. Then, periodically a separate thread should scan the trash dirs and delete any logs older than a configurable time."
"If an edits file has more edits in it than expected by its name, should trigger an error",HDFS-3069,"In testing what happens in HA split brain scenarios, I ended up with an edits log that was named edits_47-47 but actually had two edits in it (#47 and #48). The edits loading process should detect this situation and barf. Otherwise, the problem shows up later during loading or even on the next restart, and is tough to fix."
QJM should validate startLogSegment() more strictly,HDFS-5058,"We've seen a small handful of times a case where one of the NNs in an HA cluster ends up with an fsimage checkpoint that falls in the middle of an edit segment. We're not sure yet how this happens, but one issue can happen as a result:
- Node has fsimage_500. Cluster has edits_1-1000, edits_1001_inprogress
- Node restarts, loads fsimage_500
- Node wants to become active. It calls selectInputStreams(500). Currently, this API logs a WARN that 500 falls in the middle of the 1-1000 segment, but continues and returns no results.
- Node calls startLogSegment(501).

Currently, the QJM will accept this (incorrectly). The node then crashes when it first tries to journal a real transaction, but it ends up leaving the edits_501_inprogress lying around, potentially causing more issues later."
Ozone: start-all script is missing ozone start,HDFS-12707,start-all script is missing ozone start
libhdfs++: Integrate logging with the C API,HDFS-10205,"Logging was added in HDFS-9118.  The C API has it's own API based on setting errno that is independent from the rest of the logs.  These should be tied together so C API events get logged as well when it is used.

It might also be useful to log the event handlers added in HDFS-9616 as part of this work."
"start-dfs.sh and hdfs --daemon start datanode say ""ERROR: Cannot set priority of datanode process XXXX""",HDFS-13397,"When executing
{code:java}
$HADOOP_HOME/bin/hdfs --daemon start datanode
{code}
as a regular user (e.g. ""hdfs"") you achieve fail saying
{code:java}
ERROR: Cannot set priority of datanode process XXXX
{code}
where XXXX is some PID.

It turned out that this is because at least on Gentoo Linux (and I think this is pretty well universal), by default a regular user process can't increase the priority of itself or any of the user's other processes. To fix this, I added these lines to /etc/security/limits.conf [NOTE: the users hdfs, yarn, and mapred are in the group called hadoop on this system]:
{code:java}
@hadoop聽聽 聽聽聽聽 hard聽聽聽 nice聽聽聽聽聽聽聽聽聽聽聽 -15
@hadoop聽聽聽聽聽聽聽 hard聽聽聽 priority聽聽聽聽聽聽聽 -15
{code}
This change will need to be made on all datanodes.

The need to enable [at minimum] the hdfs user to raise its processes' priority needs to be added to the documentation. This is not a problem I observed under 3.0.0."
TestFileCreation#testOverwriteOpenForWrite hangs,HDFS-7304,The test case times out. It has been observed in multiple pre-commit builds.
DistributedFileSystem#getStatus returns a misleading FsStatus,HDFS-10388,"The method {{DistributedFileSystem#getStatus}} returns the dfs's disk status.
{code}
   public FsStatus getStatus(Path p) throws IOException {
     statistics.incrementReadOps(1);
    return dfs.getDiskStatus();
   }
{code}
So the param path is no meaning here. And the object returned will mislead for users to use this method. I looked into the code, only when the file system has multiple partitions, the use and capacity of the partition pointed to by the specified path will be reflected. For example, in the subclass {{RawLocalFileSystem}}, it will be return correctly.

We should return a new meaningless FsStatus here (like new FsStatus(0, 0, 0)) and indicate that the invoked method isn't available in {{DistributedFileSystem}}."
TestDistributedFileSystem#testGetFileBlockStorageLocationsError is flaky,HDFS-6308,"Found this on pre-commit build of HDFS-6261
{code}
java.lang.AssertionError: Expected one valid and one invalid volume
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.apache.hadoop.hdfs.TestDistributedFileSystem.testGetFileBlockStorageLocationsError(TestDistributedFileSystem.java:837)
{code}"
StreamCapabilities.StreamCapability should be public.,HDFS-12260,"Client should use {{StreamCapability}} enum instead of raw string to query the capability of an OutputStream, for better type safety / IDE supports and etc."
Shaded jar for hadoop-hdfs-client,HDFS-12928,"This jira is to have a shaded client for hadoop-hdfs-client.

Similar to hadoop-client, which has hadoop-client-api and hadoop-client-runtime.

Similarly for hadoop-hdfs-client, we can have hadoop-hdfs-clienta-api and hadoop-hdfs-client-runtime.

This will help, when I am working on hdfs client programs, and want to use only hdfs client and does not require mapreduce-client and yarn-client, I can use this new jar, instead of hadoop-client-api jar which includes everything.

And also when I am breaking Hadoop in to modules like hdfs-client, yarn-client and mapred-client, and upgrade only one component this helps.

"
TestBlockToken fails on JDK 7,HDFS-6132,"Currently, UserGroupInformation.setConfiguration(conf) does not reset loginUser. This is causing this test case to fail.

For now, the work around solution is to use UserGroupInformation.reset() before UserGroupInformation.setConfiguration(conf). However, in general, we should do this reset whenever we call UserGroupInformation.setConfiguration(conf)"
Ambari UI deploy fails during startup of Ambari Metrics,HDFS-13169,"{noformat}
HDP version:    HDP-3.0.0.0-702
Ambari version: 2.99.99.0-77
{noformat}

/var/lib/ambari-agent/data/errors-52.txt:
{noformat}
Traceback (most recent call last):
  File ""/var/lib/ambari-agent/cache/common-services/AMBARI_METRICS/0.1.0/package/scripts/metrics_collector.py"", line 90, in <module>
    AmsCollector().execute()
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 371, in execute
    self.execute_prefix_function(self.command_name, 'post', env)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 392, in execute_prefix_function
    method(env)
  File ""/usr/lib/python2.6/site-packages/resource_management/libraries/script/script.py"", line 434, in post_start
    raise Fail(""Pid file {0} doesn't exist after starting of the component."".format(pid_file))
resource_management.core.exceptions.Fail: Pid file /var/run/ambari-metrics-collector//hbase-ams-master.pid doesn't exist after starting of the component.
{noformat}

/var/lib/ambari-agent/data/output-52.txt:
{noformat}
2018-01-11 13:03:40,753 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=3.0.0.0-702 -> 3.0.0.0-702
2018-01-11 13:03:40,755 - Using hadoop conf dir: /usr/hdp/3.0.0.0-702/hadoop/conf
2018-01-11 13:03:40,884 - Stack Feature Version Info: Cluster Stack=3.0, Command Stack=None, Command Version=3.0.0.0-702 -> 3.0.0.0-702
2018-01-11 13:03:40,885 - Using hadoop conf dir: /usr/hdp/3.0.0.0-702/hadoop/conf
2018-01-11 13:03:40,886 - Group['hdfs'] {}
2018-01-11 13:03:40,887 - Group['hadoop'] {}
2018-01-11 13:03:40,887 - Group['users'] {}
2018-01-11 13:03:40,887 - User['hive'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
2018-01-11 13:03:40,890 - User['infra-solr'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
2018-01-11 13:03:40,891 - User['zookeeper'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
2018-01-11 13:03:40,892 - User['atlas'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
2018-01-11 13:03:40,893 - User['ams'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
2018-01-11 13:03:40,893 - User['ambari-qa'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'users'], 'uid': None}
2018-01-11 13:03:40,894 - User['kafka'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
2018-01-11 13:03:40,894 - User['tez'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop', 'users'], 'uid': None}
2018-01-11 13:03:40,895 - User['hdfs'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hdfs', 'hadoop'], 'uid': None}
2018-01-11 13:03:40,895 - User['yarn'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
2018-01-11 13:03:40,896 - User['mapred'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
2018-01-11 13:03:40,897 - User['hbase'] {'gid': 'hadoop', 'fetch_nonlocal_groups': True, 'groups': ['hadoop'], 'uid': None}
2018-01-11 13:03:40,897 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
2018-01-11 13:03:40,898 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 0'] {'not_if': '(test $(id -u ambari-qa) -gt 1000) || (false)'}
2018-01-11 13:03:40,903 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh ambari-qa /tmp/hadoop-ambari-qa,/tmp/hsperfdata_ambari-qa,/home/ambari-qa,/tmp/ambari-qa,/tmp/sqoop-ambari-qa 0'] due to not_if
2018-01-11 13:03:40,903 - Directory['/tmp/hbase-hbase'] {'owner': 'hbase', 'create_parents': True, 'mode': 0775, 'cd_access': 'a'}
2018-01-11 13:03:40,904 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
2018-01-11 13:03:40,905 - File['/var/lib/ambari-agent/tmp/changeUid.sh'] {'content': StaticFile('changeToSecureUid.sh'), 'mode': 0555}
2018-01-11 13:03:40,906 - call['/var/lib/ambari-agent/tmp/changeUid.sh hbase'] {}
2018-01-11 13:03:40,913 - call returned (0, '1002')
2018-01-11 13:03:40,914 - Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase 1002'] {'not_if': '(test $(id -u hbase) -gt 1000) || (false)'}
2018-01-11 13:03:40,917 - Skipping Execute['/var/lib/ambari-agent/tmp/changeUid.sh hbase /home/hbase,/tmp/hbase,/usr/bin/hbase,/var/log/hbase,/tmp/hbase-hbase 1002'] due to not_if
2018-01-11 13:03:40,918 - Group['hdfs'] {}
2018-01-11 13:03:40,918 - User['hdfs'] {'fetch_nonlocal_groups': True, 'groups': ['hdfs', 'hadoop', 'hdfs']}
2018-01-11 13:03:40,919 - FS Type: 
2018-01-11 13:03:40,919 - Directory['/etc/hadoop'] {'mode': 0755}
2018-01-11 13:03:40,932 - File['/usr/hdp/3.0.0.0-702/hadoop/conf/hadoop-env.sh'] {'content': InlineTemplate(...), 'owner': 'hdfs', 'group': 'hadoop'}
2018-01-11 13:03:40,933 - Directory['/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir'] {'owner': 'hdfs', 'group': 'hadoop', 'mode': 01777}
2018-01-11 13:03:40,947 - Execute[('setenforce', '0')] {'not_if': '(! which getenforce ) || (which getenforce && getenforce | grep -q Disabled)', 'sudo': True, 'only_if': 'test -f /selinux/enforce'}
2018-01-11 13:03:40,962 - Directory['/var/log/hadoop'] {'owner': 'root', 'create_parents': True, 'group': 'hadoop', 'mode': 0775, 'cd_access': 'a'}
2018-01-11 13:03:40,964 - Directory['/var/run/hadoop'] {'owner': 'root', 'create_parents': True, 'group': 'root', 'cd_access': 'a'}
2018-01-11 13:03:40,964 - Directory['/tmp/hadoop-hdfs'] {'owner': 'hdfs', 'create_parents': True, 'cd_access': 'a'}
2018-01-11 13:03:40,967 - File['/usr/hdp/3.0.0.0-702/hadoop/conf/commons-logging.properties'] {'content': Template('commons-logging.properties.j2'), 'owner': 'hdfs'}
2018-01-11 13:03:40,969 - File['/usr/hdp/3.0.0.0-702/hadoop/conf/health_check'] {'content': Template('health_check.j2'), 'owner': 'hdfs'}
2018-01-11 13:03:40,973 - File['/usr/hdp/3.0.0.0-702/hadoop/conf/log4j.properties'] {'content': InlineTemplate(...), 'owner': 'hdfs', 'group': 'hadoop', 'mode': 0644}
2018-01-11 13:03:40,982 - File['/usr/hdp/3.0.0.0-702/hadoop/conf/hadoop-metrics2.properties'] {'content': InlineTemplate(...), 'owner': 'hdfs', 'group': 'hadoop'}
2018-01-11 13:03:40,983 - File['/usr/hdp/3.0.0.0-702/hadoop/conf/task-log4j.properties'] {'content': StaticFile('task-log4j.properties'), 'mode': 0755}
2018-01-11 13:03:40,983 - File['/usr/hdp/3.0.0.0-702/hadoop/conf/configuration.xsl'] {'owner': 'hdfs', 'group': 'hadoop'}
2018-01-11 13:03:40,987 - File['/etc/hadoop/conf/topology_mappings.data'] {'owner': 'hdfs', 'content': Template('topology_mappings.data.j2'), 'only_if': 'test -d /etc/hadoop/conf', 'group': 'hadoop', 'mode': 0644}
2018-01-11 13:03:40,991 - File['/etc/hadoop/conf/topology_script.py'] {'content': StaticFile('topology_script.py'), 'only_if': 'test -d /etc/hadoop/conf', 'mode': 0755}
2018-01-11 13:03:40,996 - Skipping stack-select on AMBARI_METRICS because it does not exist in the stack-select package structure.
2018-01-11 13:03:41,142 - Using hadoop conf dir: /usr/hdp/3.0.0.0-702/hadoop/conf
2018-01-11 13:03:41,144 - checked_call['hostid'] {}
2018-01-11 13:03:41,147 - checked_call returned (0, '007f0100')
2018-01-11 13:03:41,150 - Directory['/etc/ams-hbase/conf'] {'owner': 'ams', 'group': 'hadoop', 'create_parents': True, 'recursive_ownership': True}
2018-01-11 13:03:41,151 - Changing owner for /etc/ams-hbase/conf from 0 to ams
2018-01-11 13:03:41,151 - Changing group for /etc/ams-hbase/conf from 0 to hadoop
2018-01-11 13:03:41,151 - Directory['/var/lib/ambari-metrics-collector/hbase-tmp'] {'owner': 'ams', 'create_parents': True, 'recursive_ownership': True, 'cd_access': 'a'}
2018-01-11 13:03:41,159 - Creating directory Directory['/var/lib/ambari-metrics-collector/hbase-tmp'] since it doesn't exist.
2018-01-11 13:03:41,159 - Changing owner for /var/lib/ambari-metrics-collector/hbase-tmp from 0 to ams
2018-01-11 13:03:41,160 - Directory['/var/lib/ambari-metrics-collector/hbase-tmp/local/jars'] {'owner': 'ams', 'group': 'hadoop', 'create_parents': True, 'mode': 0775, 'cd_access': 'a'}
2018-01-11 13:03:41,160 - Creating directory Directory['/var/lib/ambari-metrics-collector/hbase-tmp/local/jars'] since it doesn't exist.
2018-01-11 13:03:41,160 - Changing owner for /var/lib/ambari-metrics-collector/hbase-tmp/local/jars from 0 to ams
2018-01-11 13:03:41,160 - Changing group for /var/lib/ambari-metrics-collector/hbase-tmp/local/jars from 0 to hadoop
2018-01-11 13:03:41,160 - Changing permission for /var/lib/ambari-metrics-collector/hbase-tmp/local/jars from 755 to 775
2018-01-11 13:03:41,161 - File['/etc/ams-hbase/conf/core-site.xml'] {'owner': 'ams', 'action': ['delete']}
2018-01-11 13:03:41,161 - File['/etc/ams-hbase/conf/hdfs-site.xml'] {'owner': 'ams', 'action': ['delete']}
2018-01-11 13:03:41,161 - XmlConfig['hbase-site.xml'] {'owner': 'ams', 'group': 'hadoop', 'conf_dir': '/etc/ams-hbase/conf', 'configuration_attributes': {'final': {'hbase.zookeeper.quorum': 'true'}}, 'configurations': ...}
2018-01-11 13:03:41,170 - Generating config: /etc/ams-hbase/conf/hbase-site.xml
2018-01-11 13:03:41,170 - File['/etc/ams-hbase/conf/hbase-site.xml'] {'owner': 'ams', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
2018-01-11 13:03:41,198 - Writing File['/etc/ams-hbase/conf/hbase-site.xml'] because contents don't match
2018-01-11 13:03:41,198 - Directory['/var/lib/ambari-metrics-collector/hbase-tmp/phoenix-spool'] {'owner': 'ams', 'group': 'hadoop', 'create_parents': True, 'mode': 0755, 'cd_access': 'a'}
2018-01-11 13:03:41,198 - Creating directory Directory['/var/lib/ambari-metrics-collector/hbase-tmp/phoenix-spool'] since it doesn't exist.
2018-01-11 13:03:41,199 - Changing owner for /var/lib/ambari-metrics-collector/hbase-tmp/phoenix-spool from 0 to ams
2018-01-11 13:03:41,199 - Changing group for /var/lib/ambari-metrics-collector/hbase-tmp/phoenix-spool from 0 to hadoop
2018-01-11 13:03:41,199 - XmlConfig['hbase-policy.xml'] {'owner': 'ams', 'group': 'hadoop', 'conf_dir': '/etc/ams-hbase/conf', 'configuration_attributes': {}, 'configurations': {'security.masterregion.protocol.acl': '*', 'security.admin.protocol.acl': '*', 'security.client.protocol.acl': '*'}}
2018-01-11 13:03:41,205 - Generating config: /etc/ams-hbase/conf/hbase-policy.xml
2018-01-11 13:03:41,205 - File['/etc/ams-hbase/conf/hbase-policy.xml'] {'owner': 'ams', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
2018-01-11 13:03:41,207 - Writing File['/etc/ams-hbase/conf/hbase-policy.xml'] because contents don't match
2018-01-11 13:03:41,213 - File['/etc/ams-hbase/conf/hbase-env.sh'] {'content': InlineTemplate(...), 'owner': 'ams'}
2018-01-11 13:03:41,215 - Writing File['/etc/ams-hbase/conf/hbase-env.sh'] because contents don't match
2018-01-11 13:03:41,219 - File['/etc/ams-hbase/conf/hadoop-metrics2-hbase.properties'] {'content': Template('hadoop-metrics2-hbase.properties.j2'), 'owner': 'ams', 'group': 'hadoop'}
2018-01-11 13:03:41,220 - Writing File['/etc/ams-hbase/conf/hadoop-metrics2-hbase.properties'] because contents don't match
2018-01-11 13:03:41,220 - TemplateConfig['/etc/ams-hbase/conf/regionservers'] {'owner': 'ams', 'template_tag': None}
2018-01-11 13:03:41,222 - File['/etc/ams-hbase/conf/regionservers'] {'content': Template('regionservers.j2'), 'owner': 'ams', 'group': None, 'mode': None}
2018-01-11 13:03:41,223 - Writing File['/etc/ams-hbase/conf/regionservers'] because it doesn't exist
2018-01-11 13:03:41,223 - Changing owner for /etc/ams-hbase/conf/regionservers from 0 to ams
2018-01-11 13:03:41,223 - Directory['/var/run/ambari-metrics-collector/'] {'owner': 'ams', 'create_parents': True, 'mode': 0755, 'cd_access': 'a'}
2018-01-11 13:03:41,224 - Creating directory Directory['/var/run/ambari-metrics-collector/'] since it doesn't exist.
2018-01-11 13:03:41,224 - Changing owner for /var/run/ambari-metrics-collector/ from 0 to ams
2018-01-11 13:03:41,225 - Directory['/var/log/ambari-metrics-collector'] {'owner': 'ams', 'create_parents': True, 'mode': 0755, 'cd_access': 'a'}
2018-01-11 13:03:41,225 - Creating directory Directory['/var/log/ambari-metrics-collector'] since it doesn't exist.
2018-01-11 13:03:41,225 - Changing owner for /var/log/ambari-metrics-collector from 0 to ams
2018-01-11 13:03:41,225 - Directory['/var/lib/ambari-metrics-collector/hbase'] {'owner': 'ams', 'create_parents': True, 'recursive_ownership': True, 'cd_access': 'a'}
2018-01-11 13:03:41,225 - Creating directory Directory['/var/lib/ambari-metrics-collector/hbase'] since it doesn't exist.
2018-01-11 13:03:41,226 - Changing owner for /var/lib/ambari-metrics-collector/hbase from 0 to ams
2018-01-11 13:03:41,226 - File['/var/run/ambari-metrics-collector//distributed_mode'] {'owner': 'ams', 'action': ['delete']}
2018-01-11 13:03:41,228 - File['/etc/ams-hbase/conf/log4j.properties'] {'content': InlineTemplate(...), 'owner': 'ams', 'group': 'hadoop', 'mode': 0644}
2018-01-11 13:03:41,229 - Writing File['/etc/ams-hbase/conf/log4j.properties'] because contents don't match
2018-01-11 13:03:41,230 - Directory['/etc/ams-hbase/conf'] {'owner': 'ams', 'group': 'hadoop', 'create_parents': True, 'recursive_ownership': True}
2018-01-11 13:03:41,230 - Directory['/var/lib/ambari-metrics-collector/hbase-tmp'] {'owner': 'ams', 'create_parents': True, 'recursive_ownership': True, 'cd_access': 'a'}
2018-01-11 13:03:41,230 - Directory['/var/lib/ambari-metrics-collector/hbase-tmp/local/jars'] {'owner': 'ams', 'group': 'hadoop', 'create_parents': True, 'mode': 0775, 'cd_access': 'a'}
2018-01-11 13:03:41,231 - File['/etc/ams-hbase/conf/core-site.xml'] {'owner': 'ams', 'action': ['delete']}
2018-01-11 13:03:41,231 - File['/etc/ams-hbase/conf/hdfs-site.xml'] {'owner': 'ams', 'action': ['delete']}
2018-01-11 13:03:41,231 - XmlConfig['hbase-site.xml'] {'owner': 'ams', 'group': 'hadoop', 'conf_dir': '/etc/ams-hbase/conf', 'configuration_attributes': {'final': {'hbase.zookeeper.quorum': 'true'}}, 'configurations': ...}
2018-01-11 13:03:41,237 - Generating config: /etc/ams-hbase/conf/hbase-site.xml
2018-01-11 13:03:41,237 - File['/etc/ams-hbase/conf/hbase-site.xml'] {'owner': 'ams', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
2018-01-11 13:03:41,264 - XmlConfig['hbase-policy.xml'] {'owner': 'ams', 'group': 'hadoop', 'conf_dir': '/etc/ams-hbase/conf', 'configuration_attributes': {}, 'configurations': {'security.masterregion.protocol.acl': '*', 'security.admin.protocol.acl': '*', 'security.client.protocol.acl': '*'}}
2018-01-11 13:03:41,270 - Generating config: /etc/ams-hbase/conf/hbase-policy.xml
2018-01-11 13:03:41,270 - File['/etc/ams-hbase/conf/hbase-policy.xml'] {'owner': 'ams', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
2018-01-11 13:03:41,280 - File['/etc/ams-hbase/conf/hbase-env.sh'] {'content': InlineTemplate(...), 'owner': 'ams'}
2018-01-11 13:03:41,283 - File['/etc/ams-hbase/conf/hadoop-metrics2-hbase.properties'] {'content': Template('hadoop-metrics2-hbase.properties.j2'), 'owner': 'ams', 'group': 'hadoop'}
2018-01-11 13:03:41,283 - TemplateConfig['/etc/ams-hbase/conf/regionservers'] {'owner': 'ams', 'template_tag': None}
2018-01-11 13:03:41,284 - File['/etc/ams-hbase/conf/regionservers'] {'content': Template('regionservers.j2'), 'owner': 'ams', 'group': None, 'mode': None}
2018-01-11 13:03:41,285 - Directory['/var/run/ambari-metrics-collector/'] {'owner': 'ams', 'create_parents': True, 'mode': 0755, 'cd_access': 'a'}
2018-01-11 13:03:41,285 - Directory['/var/log/ambari-metrics-collector'] {'owner': 'ams', 'create_parents': True, 'mode': 0755, 'cd_access': 'a'}
2018-01-11 13:03:41,287 - File['/etc/ams-hbase/conf/log4j.properties'] {'content': InlineTemplate(...), 'owner': 'ams', 'group': 'hadoop', 'mode': 0644}
2018-01-11 13:03:41,288 - Directory['/etc/ambari-metrics-collector/conf'] {'owner': 'ams', 'group': 'hadoop', 'create_parents': True, 'recursive_ownership': True}
2018-01-11 13:03:41,288 - Changing owner for /etc/ambari-metrics-collector/conf from 0 to ams
2018-01-11 13:03:41,288 - Changing group for /etc/ambari-metrics-collector/conf from 0 to hadoop
2018-01-11 13:03:41,289 - Directory['/var/lib/ambari-metrics-collector/checkpoint'] {'owner': 'ams', 'group': 'hadoop', 'create_parents': True, 'recursive_ownership': True, 'cd_access': 'a'}
2018-01-11 13:03:41,289 - Creating directory Directory['/var/lib/ambari-metrics-collector/checkpoint'] since it doesn't exist.
2018-01-11 13:03:41,289 - Changing owner for /var/lib/ambari-metrics-collector/checkpoint from 0 to ams
2018-01-11 13:03:41,289 - Changing group for /var/lib/ambari-metrics-collector/checkpoint from 0 to hadoop
2018-01-11 13:03:41,289 - XmlConfig['ams-site.xml'] {'owner': 'ams', 'group': 'hadoop', 'conf_dir': '/etc/ambari-metrics-collector/conf', 'configuration_attributes': {}, 'configurations': ...}
2018-01-11 13:03:41,296 - Generating config: /etc/ambari-metrics-collector/conf/ams-site.xml
2018-01-11 13:03:41,296 - File['/etc/ambari-metrics-collector/conf/ams-site.xml'] {'owner': 'ams', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
2018-01-11 13:03:41,339 - Writing File['/etc/ambari-metrics-collector/conf/ams-site.xml'] because contents don't match
2018-01-11 13:03:41,339 - XmlConfig['ssl-server.xml'] {'owner': 'ams', 'group': 'hadoop', 'conf_dir': '/etc/ambari-metrics-collector/conf', 'configuration_attributes': {}, 'configurations': ...}
2018-01-11 13:03:41,346 - Generating config: /etc/ambari-metrics-collector/conf/ssl-server.xml
2018-01-11 13:03:41,346 - File['/etc/ambari-metrics-collector/conf/ssl-server.xml'] {'owner': 'ams', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
2018-01-11 13:03:41,351 - Writing File['/etc/ambari-metrics-collector/conf/ssl-server.xml'] because it doesn't exist
2018-01-11 13:03:41,351 - Changing owner for /etc/ambari-metrics-collector/conf/ssl-server.xml from 0 to ams
2018-01-11 13:03:41,351 - Changing group for /etc/ambari-metrics-collector/conf/ssl-server.xml from 0 to hadoop
2018-01-11 13:03:41,351 - XmlConfig['hbase-site.xml'] {'owner': 'ams', 'group': 'hadoop', 'conf_dir': '/etc/ambari-metrics-collector/conf', 'configuration_attributes': {'final': {'hbase.zookeeper.quorum': 'true'}}, 'configurations': ...}
2018-01-11 13:03:41,357 - Generating config: /etc/ambari-metrics-collector/conf/hbase-site.xml
2018-01-11 13:03:41,357 - File['/etc/ambari-metrics-collector/conf/hbase-site.xml'] {'owner': 'ams', 'content': InlineTemplate(...), 'group': 'hadoop', 'mode': None, 'encoding': 'UTF-8'}
2018-01-11 13:03:41,384 - Writing File['/etc/ambari-metrics-collector/conf/hbase-site.xml'] because contents don't match
2018-01-11 13:03:41,385 - File['/etc/ambari-metrics-collector/conf/log4j.properties'] {'content': InlineTemplate(...), 'owner': 'ams', 'group': 'hadoop', 'mode': 0644}
2018-01-11 13:03:41,387 - Writing File['/etc/ambari-metrics-collector/conf/log4j.properties'] because contents don't match
2018-01-11 13:03:41,390 - File['/etc/ambari-metrics-collector/conf/ams-env.sh'] {'content': InlineTemplate(...), 'owner': 'ams'}
2018-01-11 13:03:41,391 - Writing File['/etc/ambari-metrics-collector/conf/ams-env.sh'] because contents don't match
2018-01-11 13:03:41,392 - Directory['/var/log/ambari-metrics-collector'] {'owner': 'ams', 'group': 'hadoop', 'create_parents': True, 'mode': 0755, 'cd_access': 'a'}
2018-01-11 13:03:41,392 - Changing group for /var/log/ambari-metrics-collector from 0 to hadoop
2018-01-11 13:03:41,392 - Directory['/var/run/ambari-metrics-collector'] {'owner': 'ams', 'group': 'hadoop', 'create_parents': True, 'mode': 0755, 'cd_access': 'a'}
2018-01-11 13:03:41,392 - Changing group for /var/run/ambari-metrics-collector from 0 to hadoop
2018-01-11 13:03:41,393 - File['/usr/lib/ams-hbase/bin/hadoop'] {'owner': 'ams', 'mode': 0755}
2018-01-11 13:03:41,393 - Writing File['/usr/lib/ams-hbase/bin/hadoop'] because it doesn't exist
2018-01-11 13:03:41,394 - Changing owner for /usr/lib/ams-hbase/bin/hadoop from 0 to ams
2018-01-11 13:03:41,394 - Changing permission for /usr/lib/ams-hbase/bin/hadoop from 644 to 755
2018-01-11 13:03:41,394 - Directory['/etc/security/limits.d'] {'owner': 'root', 'create_parents': True, 'group': 'root'}
2018-01-11 13:03:41,397 - File['/etc/security/limits.d/ams.conf'] {'content': Template('ams.conf.j2'), 'owner': 'root', 'group': 'root', 'mode': 0644}
2018-01-11 13:03:41,397 - Writing File['/etc/security/limits.d/ams.conf'] because it doesn't exist
2018-01-11 13:03:41,398 - Execute['/usr/lib/ams-hbase/bin/hbase-daemon.sh --config /etc/ams-hbase/conf stop regionserver'] {'on_timeout': 'ls /var/run/ambari-metrics-collector//hbase-ams-regionserver.pid >/dev/null 2>&1 && ps `cat /var/run/ambari-metrics-collector//hbase-ams-regionserver.pid` >/dev/null 2>&1 && ambari-sudo.sh -H -E kill -9 `ambari-sudo.sh cat /var/run/ambari-metrics-collector//hbase-ams-regionserver.pid`', 'timeout': 30, 'user': 'ams'}
2018-01-11 13:03:41,448 - File['/var/run/ambari-metrics-collector//hbase-ams-regionserver.pid'] {'action': ['delete']}
2018-01-11 13:03:41,449 - Execute['/usr/sbin/ambari-metrics-collector --config /etc/ambari-metrics-collector/conf stop'] {'user': 'ams'}
2018-01-11 13:03:41,505 - Execute['ambari-sudo.sh rm -rf /var/lib/ambari-metrics-collector/hbase-tmp/*.tmp'] {}
2018-01-11 13:03:41,511 - File['/etc/ambari-metrics-collector/conf/core-site.xml'] {'owner': 'ams', 'action': ['delete']}
2018-01-11 13:03:41,511 - File['/etc/ambari-metrics-collector/conf/hdfs-site.xml'] {'owner': 'ams', 'action': ['delete']}
2018-01-11 13:03:41,512 - Execute['/usr/sbin/ambari-metrics-collector --config /etc/ambari-metrics-collector/conf start'] {'user': 'ams'}
2018-01-11 13:08:55,668 - Skipping stack-select on AMBARI_METRICS because it does not exist in the stack-select package structure.

Command failed after 1 tries
{noformat}"
NameNode should use loginUser(hdfs) to serve iNotify requests,HDFS-10799,"When a NameNode serves iNotify requests from a client, it verifies the client has superuser permission and then uses the client's Kerberos principal to read edits from journal nodes.

However, if the client does not renew its tgt tickets, the connection from NameNode to journal nodes may fail. In which case, the NameNode thinks the edits are corrupt, and prints a scary error message:
""During automatic edit log failover, we noticed that all of the remaining edit log streams are shorter than the current one!  The best remaining edit log ends at transaction 11577603, but we thought we could read up to transaction 11577606.  If you continue, metadata will be lost forever!""

However, the edits are actually good. NameNode _should not freak out when an iNotify client's tgt ticket expires_.

I think that an easy solution to this bug, is that after NameNode verifies client has superuser permission, call {{SecurityUtil.doAsLoginUser}} and then read edits. This will make sure the operation does not fail due to an expired client ticket.

Excerpt of related logs:
{noformat}
2016-08-18 19:05:13,979 WARN org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:hdfs@EXAMPLE.COM (auth:KERBEROS) cause:java.io.IOException: We encountered an error reading http://jn1.example.com:8480/getJournal?jid=nameservice1&segmentTxId=11577487&storageInfo=yyy, http://jn1.example.com:8480/getJournal?jid=nameservice1&segmentTxId=11577487&storageInfo=yyy.  During automatic edit log failover, we noticed that all of the remaining edit log streams are shorter than the current one!  The best remaining edit log ends at transaction 11577603, but we thought we could read up to transaction 11577606.  If you continue, metadata will be lost forever!
2016-08-18 19:05:13,979 INFO org.apache.hadoop.ipc.Server: IPC Server handler 112 on 8020, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getEditsFromTxid from [client IP:port] Call#73 Retry#0
java.io.IOException: We encountered an error reading http://jn1.example.com:8480/getJournal?jid=nameservice1&segmentTxId=11577487&storageInfo=yyy, http://jn1.example.com:8480/getJournal?jid=nameservice1&segmentTxId=11577487&storageInfo=yyy.  During automatic edit log failover, we noticed that all of the remaining edit log streams are shorter than the current one!  The best remaining edit log ends at transaction 11577603, but we thought we could read up to transaction 11577606.  If you continue, metadata will be lost forever!
        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:213)
        at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.readOp(NameNodeRpcServer.java:1674)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getEditsFromTxid(NameNodeRpcServer.java:1736)
        at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.getEditsFromTxid(AuthorizationProviderProxyClientProtocol.java:1010)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getEditsFromTxid(ClientNamenodeProtocolServerSideTranslatorPB.java:1475)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)
{noformat}"
DFS.setReplication should throw exception on EC files,HDFS-12921,"This was checked from {{o.a.h.fs.shell.SetReplication#processPath}}, however, {{DistributedFileSystem#setReplication()}} API is also a public API, we should move the check to {{DistributedFileSystem}} to prevent directly call this API on EC file."
Asserts are disabled in unit tests,HDFS-4411,"Unlike 23, asserts are disabled for tests."
Backport HTTPFS to Branch 1,HDFS-4262,"There are interests to backport HTTPFS back to Hadoop 1 branch.  After the initial investigation, there're quite some changes in HDFS-2178, and several related patches, including:

HDFS-2284 Write Http access to HDFS
HDFS-2646 Hadoop HttpFS introduced 4 findbug warnings
HDFS-2649 eclipse:eclipse build fails for hadoop-hdfs-httpfs
HDFS-2657 TestHttpFSServer and TestServerWebApp are failing on trunk
HDFS-2658 HttpFS introduced 70 javadoc warnings

The most challenge of backporting is all these patches, including HDFS-2178 are for 2.X, which  code base has been refactored a lot and quite different from 1.X, so it seems we have to backport the changes manually."
fix test TestSecureNameNode and improve test TestSecureNameNodeWithExternalKdc,HDFS-4312,"TestSecureNameNode does not work on Java6 without ""dfs.web.authentication.kerberos.principal"" config property set.

Also the following improved:
1) keytab files are checked for existence and readability to provide fast-fail on config error.
2) added comment to TestSecureNameNode describing the required sys props.
3) string literals replaced with config constants."
replica.getGenerationStamp() may be >= recoveryId,HDFS-5012,"The following was first observed by [~jdcryans] in TestReplicationQueueFailover running against 2.0.5-alpha:
{code}
2013-07-16 17:14:33,340 ERROR [IPC Server handler 7 on 35081] security.UserGroupInformation(1481): PriviledgedActionException as:ec2-user (auth:SIMPLE) cause:java.io.IOException: THIS IS NOT SUPPOSED TO HAPPEN: replica.getGenerationStamp() >= recoveryId = 1041, block=blk_4297992342878601848_1041, replica=FinalizedReplica, blk_4297992342878601848_1041, FINALIZED
  getNumBytes()     = 794
  getBytesOnDisk()  = 794
  getVisibleLength()= 794
  getVolume()       = /home/ec2-user/jenkins/workspace/HBase-0.95-Hadoop-2/hbase-server/target/test-data/f2763e32-fe49-4988-ac94-eeca82431821/dfscluster_643a635e-4e39-4aa5-974c-25e01db16ff7/dfs/data/data3/current
  getBlockFile()    = /home/ec2-user/jenkins/workspace/HBase-0.95-Hadoop-2/hbase-server/target/test-data/f2763e32-fe49-4988-ac94-eeca82431821/dfscluster_643a635e-4e39-4aa5-974c-25e01db16ff7/dfs/data/data3/current/BP-1477359609-10.197.55.49-1373994849464/current/finalized/blk_4297992342878601848
  unlinked          =false
2013-07-16 17:14:33,341 WARN  [org.apache.hadoop.hdfs.server.datanode.DataNode$2@64a1fcba] datanode.DataNode(1894): Failed to obtain replica info for block (=BP-1477359609-10.197.55.49-1373994849464:blk_4297992342878601848_1041) from datanode (=127.0.0.1:47006)
java.io.IOException: THIS IS NOT SUPPOSED TO HAPPEN: replica.getGenerationStamp() >= recoveryId = 1041, block=blk_4297992342878601848_1041, replica=FinalizedReplica, blk_4297992342878601848_1041, FINALIZED
  getNumBytes()     = 794
  getBytesOnDisk()  = 794
  getVisibleLength()= 794
  getVolume()       = /home/ec2-user/jenkins/workspace/HBase-0.95-Hadoop-2/hbase-server/target/test-data/f2763e32-fe49-4988-ac94-eeca82431821/dfscluster_643a635e-4e39-4aa5-974c-25e01db16ff7/dfs/data/data3/current
  getBlockFile()    = /home/ec2-user/jenkins/workspace/HBase-0.95-Hadoop-2/hbase-server/target/test-data/f2763e32-fe49-4988-ac94-eeca82431821/dfscluster_643a635e-4e39-4aa5-974c-25e01db16ff7/dfs/data/data3/current/BP-1477359609-10.197.55.49-1373994849464/current/finalized/blk_4297992342878601848
  unlinked          =false
{code}"
Backport HDFS-3626 to branch-1 (Creating file with invalid path can corrupt edit log),HDFS-3821,Per [Todd's comment|https://issues.apache.org/jira/browse/HDFS-3626?focusedCommentId=13413509&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13413509] this issue affects v1 as well though the problem isn't as obvious because the shell doesn't use the Path(URI) constructor. To test the server side Todd modified the touchz command to use new Path(new URI(src)) and was able to reproduce the issue.
Combine READ_ONLY_SHARED DatanodeStorages with the same ID,HDFS-9808,"In HDFS-5318, each datanode that can reach a (read only) block reports itself as a valid location for the block. While accurate, this increases (redundant) block report traffic and- without partitioning on the backend- may return an overwhelming number of replica locations for each block.

Instead, a DN could report only that the shared storage is reachable. The contents of the storage could be reported separately/synthetically to the block manager, which can collapse all instances into a single storage. A subset of locations- closest to the client, etc.- can be returned, rather than all possible locations."
[SPS]: Use smaller batches of BlockMovingInfo into the block storage movement command,HDFS-11125,"This is a follow-up task of HDFS-11068, where it sends all the blocks under a trackID over single heartbeat response(DNA_BLOCK_STORAGE_MOVEMENT command). If blocks are many under a given trackID(For example: a file contains many blocks) then those requests go across a network and come with a lot of overhead. In this jira, we will discuss and implement a mechanism to limit the list of items into smaller batches with in trackID."
Port HDFS-4721 'Speed up lease/block recovery when DN fails and a block goes into recovery' to branch 1,HDFS-4796,"This was observed while doing HBase WAL recovery. HBase uses append to write to its write ahead log. So initially the pipeline is setup as

DN1 --> DN2 --> DN3

This WAL needs to be read when DN1 fails since it houses the HBase regionserver for the WAL.

HBase first recovers the lease on the WAL file. During recovery, we choose DN1 as the primary DN to do the recovery even though DN1 has failed and is not heartbeating any more.

To speedup lease/block recovery, we always choose the datanode with the most recent heartbeat."
TestAuditLogs#testAuditAllowedStat sometimes fails in trunk,HDFS-5831,"Running TestAuditLogs on Linux, I got:
{code}
testAuditAllowedStat[1](org.apache.hadoop.hdfs.server.namenode.TestAuditLogs)  Time elapsed: 6.677 sec  <<< FAILURE!
java.lang.AssertionError: null
        at org.junit.Assert.fail(Assert.java:92)
        at org.junit.Assert.assertTrue(Assert.java:43)
        at org.junit.Assert.assertNotNull(Assert.java:526)
        at org.junit.Assert.assertNotNull(Assert.java:537)
        at org.apache.hadoop.hdfs.server.namenode.TestAuditLogs.verifyAuditLogsRepeat(TestAuditLogs.java:312)
        at org.apache.hadoop.hdfs.server.namenode.TestAuditLogs.verifyAuditLogs(TestAuditLogs.java:295)
        at org.apache.hadoop.hdfs.server.namenode.TestAuditLogs.testAuditAllowedStat(TestAuditLogs.java:163)
{code}"
Ozone: More detailed documentation about the ozone components,HDFS-12464,"I started to write a more detailed introduction about the Ozone components. The goal is to explain the basic responsibility of the components and the basic network topology (which components sends messages and to where?). 

 "
FSPermissionChecker.checkTraverse doesn't pass FsAction access properly,HDFS-11924,"In 2.7.1, during file access check, the AccessControlEnforcer is called with the access parameter filled with FsAction values.

A thread dump in this case:
{code}
	FSPermissionChecker.checkPermission(INodesInPath, boolean, FsAction, FsAction, FsAction, FsAction, boolean) line: 189	
	FSDirectory.checkPermission(FSPermissionChecker, INodesInPath, boolean, FsAction, FsAction, FsAction, FsAction, boolean) line: 1698	
	FSDirectory.checkPermission(FSPermissionChecker, INodesInPath, boolean, FsAction, FsAction, FsAction, FsAction) line: 1682	
	FSDirectory.checkPathAccess(FSPermissionChecker, INodesInPath, FsAction) line: 1656	
	FSNamesystem.appendFileInternal(FSPermissionChecker, INodesInPath, String, String, boolean, boolean) line: 2668	
	FSNamesystem.appendFileInt(String, String, String, boolean, boolean) line: 2985	
	FSNamesystem.appendFile(String, String, String, EnumSet<CreateFlag>, boolean) line: 2952	
	NameNodeRpcServer.append(String, String, EnumSetWritable<CreateFlag>) line: 653	
	ClientNamenodeProtocolServerSideTranslatorPB.append(RpcController, ClientNamenodeProtocolProtos$AppendRequestProto) line: 421	
	ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(Descriptors$MethodDescriptor, RpcController, Message) line: not available	
	ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(RPC$Server, String, Writable, long) line: 616	
	ProtobufRpcEngine$Server(RPC$Server).call(RPC$RpcKind, String, Writable, long) line: 969	
	Server$Handler$1.run() line: 2049	
	Server$Handler$1.run() line: 2045	
	AccessController.doPrivileged(PrivilegedExceptionAction<T>, AccessControlContext) line: not available [native method]	
	Subject.doAs(Subject, PrivilegedExceptionAction<T>) line: 422	
	UserGroupInformation.doAs(PrivilegedExceptionAction<T>) line: 1657	

{code}

However, in 2.8.0 this value is changed to null, because in FSPermissionChecker.checkTraverse(FSPermissionChecker pc, INodesInPath iip, boolean resolveLink) couldn't pass the required information, so it's simply use 'null'.

This is a regression between 2.7.1 and 2.8.0, because external AccessControlEnforcer couldn't work properly"
Let distcp to bypass external attribute provider when calling getFileStatus etc at source cluster,HDFS-12294,"This is an alternative solution for HDFS-12202, which proposed introducing a new set of API, with an additional boolean parameter bypassExtAttrProvider, so to let NN bypass external attribute provider when getFileStatus. The goal is to avoid distcp from copying attributes from one cluster's external attribute provider and save to another cluster's fsimage.

The solution here is, instead of having an additional parameter, encode this parameter to the path itself, when calling getFileStatus (and some other calls), NN will parse the path, and figure out that whether external attribute provider need to be bypassed. The suggested encoding is to have a prefix to the path before calling getFileStatus, e.g. /ab/c becomes /.reserved/bypassExtAttr/a/b/c. NN will parse the path at the very beginning.

Thanks much to [~andrew.wang] for this suggestion. The scope of change is smaller and we don't have to change the FileSystem APIs.






 "
NameNode to support file path prefix /.reserved/bypassExtAttr,HDFS-12295,"Let NameNode to support prefix /.reserved/bypassExtAttr, so client can add thisprefix to a path before calling getFileStatus, e.g. /ab/c becomes /.reserved/bypassExtAttr/a/b/c. NN will parse the path at the very beginning, and bypass external attribute provider if the prefix is there.

"
Add MODIFY and REMOVE ECSchema editlog operations,HDFS-8295,"If MODIFY and REMOVE ECSchema operations are supported, then add these editlog operations to persist them. "
Move ErasureCodingPolicyManager to FSDirectory,HDFS-9604,"ErasureCodingPolicy is a part of directory metedata, it's better to put it in FSDirectory."
Erasure coding: updateBlockForPipeline sometimes returns non-striped block for striped file,HDFS-9386,"I've seen this bug a few times. The returned {{LocatedBlock}} from {{updateBlockForPipeline}} is sometimes not {{LocatedStripedBlock}}. However, {{FSNamesystem#bumpBlockGenerationStamp}} did return a {{LocatedStripedBlock}}. Maybe a bug in PB. I'm still debugging."
Provide new set of FileSystem API to bypass external attribute provider,HDFS-12202,"HDFS client uses 

{code}
  /**
   * Return a file status object that represents the path.
   * @param f The path we want information from
   * @return a FileStatus object
   * @throws FileNotFoundException when the path does not exist
   * @throws IOException see specific implementation
   */
  public abstract FileStatus getFileStatus(Path f) throws IOException;

  /**
   * List the statuses of the files/directories in the given path if the path is
   * a directory.
   * <p>
   * Does not guarantee to return the List of files/directories status in a
   * sorted order.
   * <p>
   * Will not return null. Expect IOException upon access error.
   * @param f given path
   * @return the statuses of the files/directories in the given patch
   * @throws FileNotFoundException when the path does not exist
   * @throws IOException see specific implementation
   */
  public abstract FileStatus[] listStatus(Path f) throws FileNotFoundException,
                                                         IOException;

{code}
to get FileStatus of files.

When external attribute provider (INodeAttributeProvider) is enabled for a cluster, the  external attribute provider is consulted to get back some relevant info (including ACL, group etc) and returned back in FileStatus, 

There is a problem here, when we use distcp to copy files from srcCluster to tgtCluster, if srcCluster has external attribute provider enabled, the data we copied would contain data from attribute provider, which we may not want.

Create this jira to add a new set of interface for distcp to use, so that distcp can copy HDFS data only and bypass external attribute provider data.

The new set API would look like
{code}
 /**
   * Return a file status object that represents the path.
   * @param f The path we want information from
   * @param bypassExtAttrProvider if true, bypass external attr provider
   *        when it's in use.
   * @return a FileStatus object
   * @throws FileNotFoundException when the path does not exist
   * @throws IOException see specific implementation
   */
  public FileStatus getFileStatus(Path f,
      final boolean bypassExtAttrProvider) throws IOException;

  /**
   * List the statuses of the files/directories in the given path if the path is
   * a directory.
   * <p>
   * Does not guarantee to return the List of files/directories status in a
   * sorted order.
   * <p>
   * Will not return null. Expect IOException upon access error.
   * @param f
   * @param bypassExtAttrProvider if true, bypass external attr provider
   *        when it's in use.
   * @return
   * @throws FileNotFoundException
   * @throws IOException
   */
  public FileStatus[] listStatus(Path f,
      final boolean bypassExtAttrProvider) throws FileNotFoundException,
                                                  IOException;
{code}

So when bypassExtAttrProvider is true, external attribute provider will be bypassed.

Thanks.
"
testSetrepDecreasing UT fails due to timeout error,HDFS-11459,"{code}
Error Message

test timed out after 120000 milliseconds
Stacktrace

java.lang.Exception: test timed out after 120000 milliseconds
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.fs.shell.SetReplication.waitForReplication(SetReplication.java:127)
	at org.apache.hadoop.fs.shell.SetReplication.processArguments(SetReplication.java:77)
	at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:119)
	at org.apache.hadoop.fs.shell.Command.run(Command.java:165)
	at org.apache.hadoop.fs.FsShell.run(FsShell.java:297)
	at org.apache.hadoop.hdfs.TestSetrepIncreasing.setrep(TestSetrepIncreasing.java:58)
	at org.apache.hadoop.hdfs.TestSetrepDecreasing.testSetrepDecreasing(TestSetrepDecreasing.java:27){code}"
NPE in Storage$StorageDirectory#unlock(),HDFS-9590,"The code looks to be possible to have race conditions in multiple-threaded runs.
{code}
    public void unlock() throws IOException {
      if (this.lock == null)
        return;
      this.lock.release();
      lock.channel().close();
      lock = null;
    }
{code}
This is called in a handful of places, and I don't see any protection. Shall we add some synchronization mechanism? Not sure if I missed any design assumptions here.
"
TestAclsEndToEnd#testCreateEncryptionZone failing very frequently.,HDFS-11944,"TestAclsEndToEnd#testCreateEncryptionZone is failing v frequently.
The way test is written makes very hard to debug.
Ideally each test case should test only one behavior.
But in this test case, it reset the dfs state many times in same test case.
It fails with the following stack trace.
{noformat}
Tests run: 9, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 35.17 sec <<< FAILURE! - in org.apache.hadoop.hdfs.TestAclsEndToEnd
testCreateEncryptionZone(org.apache.hadoop.hdfs.TestAclsEndToEnd)  Time elapsed: 3.844 sec  <<< FAILURE!
java.lang.AssertionError: Allowed zone creation of zone with blacklisted GENERATE_EEK
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertFalse(Assert.java:64)
	at org.apache.hadoop.hdfs.TestAclsEndToEnd.testCreateEncryptionZone(TestAclsEndToEnd.java:753)


Results :

Failed tests: 
  TestAclsEndToEnd.testCreateEncryptionZone:753 Allowed zone creation of zone with blacklisted GENERATE_EEK
{noformat}

It failed in the following pre-commits.
[HDFS-11885 precommit|https://issues.apache.org/jira/browse/HDFS-11885?focusedCommentId=16040117&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16040117]
[HDFS-11804 precommit|https://issues.apache.org/jira/browse/HDFS-11804?focusedCommentId=16039872&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16039872]"
fix spelling mistake in TestFsVolumeList.java ,HDFS-11812,We found a  spelling mistake in  TestFsVolumeList.java: // Mock reservedForReplcas should be // Mock reservedForReplicas銆?
Create a servlet for HDFS UI,HDFS-7239,"Currently the HDFS UI gathers most of its information from JMX. There are a couple disadvantages:

* JMX is also used by management tools, thus Hadoop needs to maintain compatibility across minor releases.
* JMX organizes information as <key, value> pairs. The organization does not fit well with emerging use cases like startup progress report and nntop.

This jira proposes to introduce a new servlet in the NN for the purpose of serving information to the UI.

It should be viewed as a part of the UI. There is *no* compatibility guarantees for the output of the servlet."
"Balancer.run() prints redundant included, excluded, source nodes.",HDFS-11731,"Included, excluded, and source nodes are printed twice by the Balancer. First as part of {{BalancerParameters.toString()}} in
{code}
    LOG.info(""parameters = "" + p);
{code}
And then separately
{code}
    LOG.info(""included nodes = "" + p.getIncludedNodes());
    LOG.info(""excluded nodes = "" + p.getExcludedNodes());
    LOG.info(""source nodes = "" + p.getSourceNodes());
{code}
The latter can be removed."
Ozone : implement StorageContainerManager#getStorageContainerLocations,HDFS-11872,"We should implement {{StorageContainerManager#getStorageContainerLocations}} . 

Although the comment says it will be moved to KSM, the functionality of container lookup by name it should actually be part of SCM functionality."
"HDFS client with hedged read, handle exceptions from callable  when the hedged read thread pool is exhausted",HDFS-11819,"When the hedged read thread pool is exhausted, the current behavior is that callable will be executed in the current thread context. The callable can throw out IOExceptions which is not handled and it will not start a 'hedged' read. 

https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java#L1131

Please see the following exception:
{code}
2017-05-11 22:42:35,883 WARN org.apache.hadoop.hdfs.BlockReaderFactory: I/O error constructing remote block reader.
org.apache.hadoop.net.ConnectTimeoutException: 3000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/*.*.*.*:50010]
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:533)
        at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3527)
        at org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:840)
        at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:755)
        at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:376)
        at org.apache.hadoop.hdfs.DFSInputStream.actualGetFromOneDataNode(DFSInputStream.java:1179)
        at org.apache.hadoop.hdfs.DFSInputStream.access$300(DFSInputStream.java:91)
        at org.apache.hadoop.hdfs.DFSInputStream$2.call(DFSInputStream.java:1141)
        at org.apache.hadoop.hdfs.DFSInputStream$2.call(DFSInputStream.java:1133)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor$CallerRunsPolicy.rejectedExecution(ThreadPoolExecutor.java:2022)
        at org.apache.hadoop.hdfs.DFSClient$2.rejectedExecution(DFSClient.java:3571)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369)
        at java.util.concurrent.ExecutorCompletionService.submit(ExecutorCompletionService.java:181)
        at org.apache.hadoop.hdfs.DFSInputStream.hedgedFetchBlockByteRange(DFSInputStream.java:1280)
        at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1477)
        at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1439)
        at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:92)
        at org.apache.hadoop.hbase.io.FileLink$FileLinkInputStream.read(FileLink.java:167)
        at org.apache.hadoop.fs.FSDataInputStream.read(FSDataInputStream.java:92)
        at org.apache.hadoop.hbase.io.hfile.HFileBlock.positionalReadWithExtra(HFileBlock.java:757)
        at org.apache.hadoop.hbase.io.hfile.HFileBlock$AbstractFSReader.readAtOffset(HFileBlock.java:1457)
        at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderImpl.readBlockDataInternal(HFileBlock.java:1682)
        at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderImpl.readBlockData(HFileBlock.java:1542)
        at org.apache.hadoop.hbase.io.hfile.HFileReaderV2.readBlock(HFileReaderV2.java:445)
        at org.apache.hadoop.hbase.util.CompoundBloomFilter.contains(CompoundBloomFilter.java:100)
        at org.apache.hadoop.hbase.regionserver.StoreFile$Reader.passesGeneralBloomFilter(StoreFile.java:1383)
        at org.apache.hadoop.hbase.regionserver.StoreFile$Reader.passesBloomFilter(StoreFile.java:1247)
        at org.apache.hadoop.hbase.regionserver.StoreFileScanner.shouldUseScanner(StoreFileScanner.java:469)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.selectScannersFrom(StoreScanner.java:393)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.getScannersNoCompaction(StoreScanner.java:312)
        at org.apache.hadoop.hbase.regionserver.StoreScanner.<init>(StoreScanner.java:192)
        at org.apache.hadoop.hbase.regionserver.HStore.createScanner(HStore.java:2106)
        at org.apache.hadoop.hbase.regionserver.HStore.getScanner(HStore.java:2096)
        at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.<init>(HRegion.java:5544)
        at org.apache.hadoop.hbase.regionserver.HRegion.instantiateRegionScanner(HRegion.java:2569)
        at org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:2555)
        at org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:2536)
        at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:6791)
        at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:6770)
        at org.apache.hadoop.hbase.regionserver.RSRpcServices.get(RSRpcServices.java:2025)
        at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:33644)
        at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2170)
        at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:109)
        at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:185)
        at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:165)

{code}"
Thread safety in logEdit?,HDFS-11820,"Hi there,

I am new to Hadoop and trying to understand how things work under the hood by browsing through some of the codes.

I noticed a potential thread safety issue in in FSEditLog.java in version 2.7.1 where the following patterns is used (the current trunk also use the same pattern):
1. Instance of FSEditLogOp is retrieved from cache for reuse 
2. Set the attributes (e.g. path, timestamp, etc)
3. Invoke logEdit(*op*). This method has synchronized block in it, but also has *wait* if auto-sync is scheduled

Now, if I have two almost simultaneous rename operations, right after each is about to write edit log:
Thread #1 acquired instance of RenameOp, set the attributes, and invoked logEdit, then it waits because auto-sync is scheduled.
Thread #2 catches up, and acquires same instance of RenameOp, sets *different* attributes, and invokes logEdit.. It blocks because of synchronized block inside logEdit(...), but it manages to modify the attributes of RenameOp.

The second renameOp could end up being logged twice because both renameOps are actually the same instance. 
The fix is to have synchronized(*op*) prior to calling logEdit(*op*) or clone the op before using it.

I could be wrong. Am I missing something?

Thanks,

Alexander Koentjara"
Convert FSImage.removedStorageDirs into a map.,HDFS-361,{{FSImage.removedStorageDirs}} is declared as an {{ArrayList}}. In order to avoid adding the same directory twice into {{removedStorageDirs}} we should convert it into a map.
getTurnOffTip computes needed block incorrectly for threshold < 1 in b2.7,HDFS-10459,GetTurnOffTip overstates the number of blocks necessary to come out of safe mode by 1 due to an arbitrary '+1' in the code. 
[SPS]: fix issue of moving blocks with satisfier while changing replication factor ,HDFS-11284," When the real replication number of block doesn't match the replication factor. For example, the real replication is 2 while the replication factor is 3, the satisfier may encounter issue."
Add support for byte-ranges to hsftp,HDFS-594,HsftpFileSystem should be modified to support byte-ranges so it has the same semantics as HftpFileSystem after committing HDFS-235. 
Is there a way of loading cvs files to create hive tables with desired lengths for columns?,HDFS-11501,"We just got on Hadoop environment. Our data sources are cvs files. The hive tables created from the sources are seen all character /string columns have same length of 255 bytes, even for gender which has value with one byte. Is there a way of loading cvs files to create hive tables with desired lengths for string columns instead of 255 across all tables? Thank you for your help!"
Caused by: javax.security.auth.login.LoginException: java.lang.NullPointerException: invalid null input: name,HDFS-5779,"xxxxxx@testhost:/home/xxxxxx/Lab/hdfs/namenodep$ hadoop namenode -format
Warning: $HADOOP_HOME is deprecated.

14/01/15 04:51:38 INFO namenode.NameNode: STARTUP_MSG:
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = testhost.testhost1.net/xx.xxx.xxx.xxx
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 1.0.3
STARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.0 -r 1335192; compiled by 'hortonfo' on Tue May  8 20:31:25 UTC 2012
************************************************************/
Re-format filesystem in /home/xxxxxx/Lab/hdfs/namenodep ? (Y or N) Y
14/01/15 04:51:40 INFO util.GSet: VM type       = 32-bit
14/01/15 04:51:40 INFO util.GSet: 2% max memory = 19.33375 MB
14/01/15 04:51:40 INFO util.GSet: capacity      = 2^22 = 4194304 entries
14/01/15 04:51:40 INFO util.GSet: recommended=4194304, actual=4194304
14/01/15 04:51:40 ERROR namenode.NameNode: java.io.IOException: failure to login
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:490)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:452)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setConfigurationParameters(FSNamesystem.java:475)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:464)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1162)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1271)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1288)
Caused by: javax.security.auth.login.LoginException: java.lang.NullPointerException: invalid null input: name
        at com.sun.security.auth.UnixPrincipal.<init>(Unknown Source)
        at com.sun.security.auth.module.UnixLoginModule.login(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at javax.security.auth.login.LoginContext.invoke(Unknown Source)
        at javax.security.auth.login.LoginContext.access$000(Unknown Source)
        at javax.security.auth.login.LoginContext$5.run(Unknown Source)
        at javax.security.auth.login.LoginContext$5.run(Unknown Source)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.login.LoginContext.invokeCreatorPriv(Unknown Source)
        at javax.security.auth.login.LoginContext.login(Unknown Source)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:471)
        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:452)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setConfigurationParameters(FSNamesystem.java:475)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:464)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1162)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1271)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1288)

        at javax.security.auth.login.LoginContext.invoke(Unknown Source)
        at javax.security.auth.login.LoginContext.access$000(Unknown Source)
        at javax.security.auth.login.LoginContext$5.run(Unknown Source)
        at javax.security.auth.login.LoginContext$5.run(Unknown Source)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.login.LoginContext.invokeCreatorPriv(Unknown Source)
        at javax.security.auth.login.LoginContext.login(Unknown Source)
        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:471)
        ... 6 more

14/01/15 04:51:40 INFO namenode.NameNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at testhost.testhost1.net/10.129.254.129
************************************************************/
"
"AlreadyBeingCreatedException ""current leaseholder is trying to recreate file"" when trying to append to file",HDFS-11367,"We have code which creates a file in HDFS and continuously appends lines to the file, then closes the file at the end. This is done by a single dedicated thread.

We specifically instrumented the code to make sure only one 'client'/thread ever writes to the file because we were seeing ""current leaseholder is trying to recreate file"" errors.

For some background see this for example: https://community.cloudera.com/t5/Storage-Random-Access-HDFS/How-to-append-files-to-HDFS-with-Java-quot-current-leaseholder/m-p/41369

This issue is very critical to us as any error terminates a mission critical application in production.

Intermittently, we see the below exception, regardless of what our code is doing which is create the file, keep appending, then close:

org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException): failed to create file /data/records_20170125_1.txt for DFSClient_NONMAPREDUCE_-167421175_1 for client 1XX.2XX.1XX.XXX because current leaseholder is trying to recreate file.
聽聽聽聽聽聽聽 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:3075)
聽聽聽聽聽聽聽 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFileInternal(FSNamesystem.java:2905)
聽聽聽聽聽聽聽 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFileInt(FSNamesystem.java:3189)
聽聽聽聽聽聽聽 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:3153)
聽聽聽聽聽聽聽 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:612)
聽聽聽聽聽聽聽 at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.append(AuthorizationProviderProxyClientProtocol.java:125)
聽聽聽聽聽聽聽 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.append(ClientNamenodeProtocolServerSideTranslatorPB.java:414)
聽聽聽聽聽聽聽 at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
聽聽聽聽聽聽聽 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
聽聽聽聽聽聽聽 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
聽聽聽聽聽聽聽 at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)
聽聽聽聽聽聽聽 at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)
聽聽聽聽聽聽聽 at java.security.AccessController.doPrivileged(Native Method)
聽聽聽聽聽聽聽 at javax.security.auth.Subject.doAs(Subject.java:415)
聽聽聽聽聽聽聽 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1767)
聽聽聽聽聽聽聽 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)
聽
聽聽聽聽聽聽聽 at org.apache.hadoop.ipc.Client.call(Client.java:1411)
聽聽聽聽聽聽聽 at org.apache.hadoop.ipc.Client.call(Client.java:1364)
聽聽聽聽聽聽聽 at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
聽聽聽聽聽聽聽 at com.sun.proxy.$Proxy24.append(Unknown Source)
聽聽聽聽聽聽聽 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
聽聽聽聽聽聽聽 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
聽聽聽聽聽聽聽 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
聽聽聽聽聽聽聽 at java.lang.reflect.Method.invoke(Method.java:483)
聽聽聽聽聽聽聽 at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
聽聽聽聽聽聽聽 at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
聽聽聽聽聽聽聽 at com.sun.proxy.$Proxy24.append(Unknown Source)
聽聽聽聽聽聽聽 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.append(ClientNamenodeProtocolTranslatorPB.java:282)
聽聽聽聽聽聽聽 at org.apache.hadoop.hdfs.DFSClient.callAppend(DFSClient.java:1586)
聽聽聽聽聽聽聽 at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1626)
聽聽聽聽聽聽聽 at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1614)
聽聽聽聽聽聽聽 at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:313)
聽聽聽聽聽聽聽 at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:309)
聽聽聽聽聽聽聽 at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
聽聽聽聽聽聽聽 at org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:309)
聽聽聽聽聽聽聽 at org.apache.hadoop.fs.FileSystem.append(FileSystem.java:1161)
聽聽聽聽聽聽聽 at com.myco.MyAppender.getOutputStream(MyAppender.java:147)"
Erasure coding: merge HDFS-8499 to EC branch and refactor BlockInfoStriped,HDFS-8796,"Separating this change from the HDFS-8728 discussion. Per suggestion from [~szetszwo], clarifying the description of the change."
hadoop-7285-power,HDFS-11297,hadoop-7285-power
Patch for PPC64,HDFS-8519,"The attached patch enables Hadoop to work on PPC64.
That deals with SystemPageSize and BloclSize , which are not 4096 on PPC64.

There are changes in 3 files:
- hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/nativeio/NativeIO.java
- hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestFsDatasetCache.java
- hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java

where 4096 is replaced by getOperatingSystemPageSize() or by using PAGE_SIZE

The patch has been built on branch-2.7 ."
Patch for PPC64,HDFS-8518,"The attached patch enables Hadoop to work on PPC64.
That deals with SystemPageSize and BloclSize , which are not 4096 on PPC64.

There are changes in 3 files:
- hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/nativeio/NativeIO.java
- hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestFsDatasetCache.java
- hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java

where 4096 is replaced by getOperatingSystemPageSize() or by using PAGE_SIZE

The patch has been built on branch-2.7 ."
HDFS ignores HADOOP_CONF_DIR,HDFS-11245,"It seems that HDFS on trunk is ignoring {{HADOOP_CONF_DIR}}. On {{branch-2}} I could export {{HADOOP_CONF_DIR}} and use that to store my {{hdfs-site.xml}} and {{log4j.properties}}. But on trunk it appears to ignore the environment variable.

Also, even if hdfs can find the {{log4j.properties}}, it doesn't seem interested in opening and loading it.

On Ubuntu 16.10:

{code}
$ source env.sh
$ cat env.sh 
#!/bin/bash
export JAVA_HOME=/usr/lib/jvm/java-8-oracle
export HADOOP_HOME=""$HOME""/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT
export HADOOP_LOG_DIR=""$(pwd)/log""
PATH=""$HADOOP_HOME""/bin:$PATH
export HADOOP_CLASSPATH=$(hadoop classpath):""$HADOOP_HOME""/share/hadoop/tools/lib/*
export HADOOP_USER_CLASSPATH_FIRST=true
{code}


Then I set the HADOOP_CONF_DIR:
{code}
$ export HADOOP_CONF_DIR=""$(pwd)/conf/nn""
$ ls $HADOOP_CONF_DIR
hadoop-env.sh  hdfs-site.xml  log4j.properties
{code}

Now, we try to run a namenode:
{code}
$ hdfs namenode
2016-12-14 14:04:51,193 ERROR [main] namenode.NameNode: Failed to start namenode.
java.lang.IllegalArgumentException: Invalid URI for NameNode address (check fs.defaultFS): file:/// has no authority.
        at org.apache.hadoop.hdfs.DFSUtilClient.getNNAddress(DFSUtilClient.java:648)
        at org.apache.hadoop.hdfs.DFSUtilClient.getNNAddressCheckLogical(DFSUtilClient.java:677)
        at org.apache.hadoop.hdfs.DFSUtilClient.getNNAddress(DFSUtilClient.java:639)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.getRpcServerAddress(NameNode.java:556)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.loginAsNameNodeUser(NameNode.java:687)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:707)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:937)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:916)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1633)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1701)
{code}

This is weird. We have the {{fs.defaultFS}} set:

{code}
$ grep -n2 fs.defaultFS $HADOOP_CONF_DIR/hdfs-site.xml
3-<configuration>
4-    <property>
5:        <name>fs.defaultFS</name>
6-        <value>hdfs://localhost:60010</value>
7-    </property>
{code}

So if isn't finding this config. Where is is looking and finding {{file:///}}?
{code}
$ strace -f -eopen,stat hdfs namenode 2>&1 | grep hdfs-site.xml
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/jdiff/hdfs-site.xml"", 0x7f05eb6d21d0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/lib/hdfs-site.xml"", 0x7f05eb6d21d0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/sources/hdfs-site.xml"", 0x7f05eb6d21d0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/templates/hdfs-site.xml"", 0x7f05eb6d21d0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/webapps/hdfs-site.xml"", 0x7f05eb6d21d0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/jdiff/hdfs-site.xml"", 0x7f05eb6d21d0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/lib/hdfs-site.xml"", 0x7f05eb6d21d0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/sources/hdfs-site.xml"", 0x7f05eb6d21d0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml"", {st_mode=S_IFREG|0664, st_size=775, ...}) = 0
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml"", {st_mode=S_IFREG|0664, st_size=775, ...}) = 0
[pid 16271] open(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml"", O_RDONLY) = 218
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/jdiff/hdfs-site.xml"", 0x7f05eb6d17e0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/lib/hdfs-site.xml"", 0x7f05eb6d17e0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/sources/hdfs-site.xml"", 0x7f05eb6d17e0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/templates/hdfs-site.xml"", 0x7f05eb6d17e0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/webapps/hdfs-site.xml"", 0x7f05eb6d17e0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/jdiff/hdfs-site.xml"", 0x7f05eb6d17e0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/lib/hdfs-site.xml"", 0x7f05eb6d17e0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/sources/hdfs-site.xml"", 0x7f05eb6d17e0) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml"", {st_mode=S_IFREG|0664, st_size=775, ...}) = 0
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml"", {st_mode=S_IFREG|0664, st_size=775, ...}) = 0
[pid 16271] open(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml"", O_RDONLY) = 218
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/jdiff/hdfs-site.xml"", 0x7f05eb6d2070) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/lib/hdfs-site.xml"", 0x7f05eb6d2070) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/sources/hdfs-site.xml"", 0x7f05eb6d2070) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/templates/hdfs-site.xml"", 0x7f05eb6d2070) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/webapps/hdfs-site.xml"", 0x7f05eb6d2070) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/jdiff/hdfs-site.xml"", 0x7f05eb6d2070) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/lib/hdfs-site.xml"", 0x7f05eb6d2070) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/sources/hdfs-site.xml"", 0x7f05eb6d2070) = -1 ENOENT (No such file or directory)
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml"", {st_mode=S_IFREG|0664, st_size=775, ...}) = 0
[pid 16271] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml"", {st_mode=S_IFREG|0664, st_size=775, ...}) = 0
[pid 16271] open(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml"", O_RDONLY) = 218
{code}

So it's ignoring {{HADOOP_CONF_DIR}}. We can work around it using {{-conf $(pwd)/conf/nn/hdfs-site.xml}}:
{code}
$ strace -f -eopen,stat hdfs namenode -conf $(pwd)/conf/nn/hdfs-site.xml 2>&1 | grep hdfs-site.xml
[pid 16493] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/jdiff/hdfs-site.xml"", 0x7f9f60afb1d0) = -1 ENOENT (No such file or directory)
[pid 16493] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/lib/hdfs-site.xml"", 0x7f9f60afb1d0) = -1 ENOENT (No such file or directory)
[pid 16493] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/sources/hdfs-site.xml"", 0x7f9f60afb1d0) = -1 ENOENT (No such file or directory)
[pid 16493] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/templates/hdfs-site.xml"", 0x7f9f60afb1d0) = -1 ENOENT (No such file or directory)
[pid 16493] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/common/webapps/hdfs-site.xml"", 0x7f9f60afb1d0) = -1 ENOENT (No such file or directory)
[pid 16493] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/jdiff/hdfs-site.xml"", 0x7f9f60afb1d0) = -1 ENOENT (No such file or directory)
[pid 16493] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/lib/hdfs-site.xml"", 0x7f9f60afb1d0) = -1 ENOENT (No such file or directory)
[pid 16493] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/sources/hdfs-site.xml"", 0x7f9f60afb1d0) = -1 ENOENT (No such file or directory)
[pid 16493] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml"", {st_mode=S_IFREG|0664, st_size=775, ...}) = 0
[pid 16493] stat(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml"", {st_mode=S_IFREG|0664, st_size=775, ...}) = 0
[pid 16493] open(""/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/share/hadoop/hdfs/templates/hdfs-site.xml"", O_RDONLY) = 218
[pid 16493] stat(""/home/ehigg90120/src/hadoop-run/tutorial/conf/nn/hdfs-site.xml"", {st_mode=S_IFREG|0644, st_size=2107, ...}) = 0
[pid 16493] open(""/home/ehigg90120/src/hadoop-run/tutorial/conf/nn/hdfs-site.xml"", O_RDONLY) = 218

------8<------
{code}

Great! However, it's not finding my  log4j.properties for some reason. This is annoying because hdfs isn't printing anything or logging anywhere. Where is it looking?
{code}
$ strace -f hdfs namenode -conf $(pwd)/conf/nn/hdfs-site.xml 2>&1 | grep log4j.properties
stat(""/home/ehigg90120/src/hadoop-run/tutorial/conf/nn/log4j.properties"", {st_mode=S_IFREG|0644, st_size=13641, ...}) = 0
{code}

It found it, but it only statted it. It never opened it! So it seems there's at least one bug here where {{log4j.properties}} is being ignored. But shouldn't {{HADOOP_OPTS}} be set and configuring it to print to the console and to my log dir?

{code}
$ hdfs --debug namenode -conf $(pwd)/conf/nn/hdfs-site.xml 2>&1 | grep HADOOP_OPTS
DEBUG: HADOOP_OPTS accepted -Dhdfs.audit.logger=INFO,NullAppender
DEBUG: Appending HDFS_NAMENODE_OPTS onto HADOOP_OPTS
DEBUG: HADOOP_OPTS accepted -Dyarn.log.dir=/home/ehigg90120/src/hadoop-run/tutorial/log
DEBUG: HADOOP_OPTS accepted -Dyarn.log.file=hadoop.log
DEBUG: HADOOP_OPTS accepted -Dyarn.home.dir=/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT
DEBUG: HADOOP_OPTS accepted -Dyarn.root.logger=INFO,console
DEBUG: HADOOP_OPTS accepted -Djava.library.path=/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/lib/native
DEBUG: HADOOP_OPTS accepted -Dhadoop.log.dir=/home/ehigg90120/src/hadoop-run/tutorial/log
DEBUG: HADOOP_OPTS accepted -Dhadoop.log.file=hadoop.log
DEBUG: HADOOP_OPTS accepted -Dhadoop.home.dir=/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT
DEBUG: HADOOP_OPTS accepted -Dhadoop.id.str=ehigg90120
DEBUG: HADOOP_OPTS accepted -Dhadoop.root.logger=INFO,console
DEBUG: HADOOP_OPTS accepted -Dhadoop.policy.file=hadoop-policy.xml
DEBUG: HADOOP_OPTS declined -Dhadoop.security.logger=INFO,NullAppender
DEBUG: Final HADOOP_OPTS: -Djava.net.preferIPv4Stack=true -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dyarn.log.dir=/home/ehigg90120/src/hadoop-run/tutorial/log -Dyarn.log.file=hadoop.log -Dyarn.home.dir=/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT -Dyarn.root.logger=INFO,console -Djava.library.path=/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT/lib/native -Dhadoop.log.dir=/home/ehigg90120/src/hadoop-run/tutorial/log -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/home/ehigg90120/src/hadoop/hadoop-dist/target/hadoop-3.0.0-alpha2-SNAPSHOT -Dhadoop.id.str=ehigg90120 -Dhadoop.root.logger=INFO,console -Dhadoop.policy.file=hadoop-policy.xml
{code}

So it seems it is being configured and passed to the namenode. It's just not obeying it as far as I can see. 

So maybe there are two possibly related bugs:

1. {{HADOOP_CONF_DIR}} is ignored
2. The logger is not using {{log4j.properties}} or the command line. I would expect it to use the {{log4j.properties}} in the {{HADOOP_CONF_DIR}}.

I feel like I must be misunderstanding something since this seems like a pretty big issue but I didn't find any open tickets about it or any tickets describing a new way of configuring clusters."
dfshealth_nonsecure.html should not be accessble in secure cluster.,HDFS-7958,"In secure environment (kerberous + https)
the following url should not be accessible.
{code}
https://nn1:25003/dfshealth_nonsecure.html#tab-overview
{code}"
LightWeightHashSet can't remove blocks correctly which have a large number blockId,HDFS-11179,"Our test cluster has faced a problem that {{postponedMisreplicatedBlocksCount}} has been going below zero. The version of the cluster is a recent 3.0. We haven't created any EC files yet. This is the NN's log:

{noformat}
Rescan of postponedMisreplicatedBlocks completed in 13 msecs. 448 blocks are left. 176 blocks are removed.
Rescan of postponedMisreplicatedBlocks completed in 13 msecs. 272 blocks are left. 176 blocks are removed.
Rescan of postponedMisreplicatedBlocks completed in 14 msecs. 96 blocks are left. 176 blocks are removed.
Rescan of postponedMisreplicatedBlocks completed in 327 msecs. -77 blocks are left. 177 blocks are removed.
Rescan of postponedMisreplicatedBlocks completed in 15 msecs. -253 blocks are left. 179 blocks are removed.
Rescan of postponedMisreplicatedBlocks completed in 14 msecs. -432 blocks are left. 179 blocks are removed.
{noformat}

I looked into this issue and found that it is caused by {{LightWeightHashSet}} which is used for {{postponedMisreplicatedBlocks}} recently. When {{LightWeightHashSet}} remove blocks which have a large number blockId, overflows happen and the blocks can't be removed correctly(, let alone ec blocks whose blockId starts with the minimum of long)."
HDFS doesn't raise FileNotFoundException if the source of a rename() is missing,HDFS-6262,"HDFS's {{rename(src, dest)}} returns false if src does not exist -all the other filesystems raise {{FileNotFoundException}}

This behaviour is defined in {{FSDirectory.unprotectedRenameTo()}} -the attempt is logged, but the operation then just returns false.

I propose changing the behaviour of {{DistributedFileSystem}} to be the same as that of the others -and of {{FileContext}}, which does reject renames with nonexistent sources"
DN decommissioning quirks,HDFS-2569,"Decommissioning a node is working slightly odd in 0.23+:

The steps I did:

- Start HDFS via {{hdfs namenode}} and {{hdfs datanode}}. 1-node cluster.
- Zero files/blocks, so I go ahead and exclude-add my DN and do {{hdfs dfsadmin -refreshNodes}}
- I see the following log in NN tails, which is fine:
{code}
11/11/20 09:28:10 INFO util.HostsFileReader: Setting the includes file to 
11/11/20 09:28:10 INFO util.HostsFileReader: Setting the excludes file to build/test/excludes
11/11/20 09:28:10 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list
11/11/20 09:28:10 INFO util.HostsFileReader: Adding 192.168.1.23 to the list of hosts from build/test/excludes
{code}
- However, DN log tail gets no new messages. DN still runs.
- The dfshealth.jsp page shows this table, which makes no sense -- why is there 1 live and 1 dead?:

|Live Nodes|1 (Decommissioned: 1)|
|Dead Nodes|1 (Decommissioned: 0)|
|Decommissioning Nodes|0|

- The live nodes page shows this, meaning DN is still up and heartbeating but is decommissioned:

|Node|Last Contact|Admin State|
|192.168.1.23|0|Decommissioned|

- The dead nodes page shows this, and the link to the DN is broken cause the port is linked as -1. Also, showing 'false' for decommissioned makes no sense when live node page shows that it is already decommissioned:

|Node|Decommissioned|
|192.168.1.23|false|

Investigating if this is a quirk only observed when the DN had 0 blocks on it in sum total."
Reschedule CompletedActionXCommand if the job is not completed,HDFS-11173,"We've encountered cases when the LauncherMapper stuck around after sending out the notifications to Oozie. If the callback is processed before the external job's status is updated to FINISHED, Oozie won't update the action's status for 10 minutes.

We could add a delayed check to [CompletedAcitonXCommand|https://github.com/apache/oozie/blob/master/core/src/main/java/org/apache/oozie/command/wf/CompletedActionXCommand.java#120 ] to avoid this.
"
viewfs shows resolved path in FileNotFoundException,HDFS-5692,"With the following config, if I call fs.listStatus(""/nn1/a/b"") when {{/nn1/a/b}} does not exist then ...

{noformat}
<configuration>
  <property>
    <name>fs.default.name</name>
    <value>viewfs:///</value>
  </property>
  <property>
    <name>fs.viewfs.mounttable.default.link./nn1</name>
    <value>hdfs://host1:9000</value>
  </property>
  <property>
    <name>fs.viewfs.mounttable.default.link./nn2</name>
    <value>hdfs://host2:9000</value>
  </property>
</configuration>
{noformat}

I will see an error message like the following.  

{noformat}
java.io.FileNotFoundException: File /a/b does not exist.
        at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:644)
        at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:92)
        at org.apache.hadoop.hdfs.DistributedFileSystem$14.doCall(DistributedFileSystem.java:702)
        at org.apache.hadoop.hdfs.DistributedFileSystem$14.doCall(DistributedFileSystem.java:698)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:698)
        at org.apache.hadoop.fs.FilterFileSystem.listStatus(FilterFileSystem.java:222)
        at org.apache.hadoop.fs.viewfs.ChRootedFileSystem.listStatus(ChRootedFileSystem.java:228)
        at org.apache.hadoop.fs.viewfs.ViewFileSystem.listStatus(ViewFileSystem.java:366)
{noformat}

I think it would be useful for ViewFS to wrap the FileNotFoundException from the inner filesystem, giving an error message like the following.  The following error message has the resolved and unresolved paths which is very useful for debugging.

{noformat}
java.io.FileNotFoundException: File /nn1/a/b does not exist.
        at org.apache.hadoop.fs.viewfs.ViewFileSystem.listStatus(ViewFileSystem.java:366)
Caused by: java.io.FileNotFoundException: File /a/b does not exist.
        at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:644)
        at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:92)
        at org.apache.hadoop.hdfs.DistributedFileSystem$14.doCall(DistributedFileSystem.java:702)
        at org.apache.hadoop.hdfs.DistributedFileSystem$14.doCall(DistributedFileSystem.java:698)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:698)
        at org.apache.hadoop.fs.FilterFileSystem.listStatus(FilterFileSystem.java:222)
        at org.apache.hadoop.fs.viewfs.ChRootedFileSystem.listStatus(ChRootedFileSystem.java:228)
        at org.apache.hadoop.fs.viewfs.ViewFileSystem.listStatus(ViewFileSystem.java:366)
{noformat}"
Refactor NN WebUI to no longer pass IP addresses in the URL,HDFS-4483,"Right now, the namenode passes its RPC address in WebUI URLs when it redirects to datanodes for things like browsing the filesystem. This is brittle and fails in different ways when wildcard addresses are configured (see HDFS-3932 and HDFS-4471).

A better solution would be to instead pass the NN's nameservice ID in the URL, and make DNs look up the appropriate RPC address for the nameservice from their conf. This fixes the wildcard issues and has the additional benefit of making browsing work after a NN failover."
Add unit tests to verify ACLs in safemode,HDFS-10950,"This proposes adding unit tests to validate that getting Acls works when namende is in safemode, while setting Acls fails. Specifically, the following needs being covered in newly added tests.
test_getfacl_recursive
test_resetacl
test_setfacl_default"
dfsadmin set/clrSpaceQuota fail to recognize StorageType option,HDFS-11017,"dfsadmin setSpaceQuota or clrSpaceQuota don't recognize valid StorageType options, such as DISK or SSD, however, It's been supported by DFS."
multiple BlockFixer should be supported in order to improve scalability and reduce too much work on single BlockFixer,HDFS-4360,"current implementation can only run single BlockFixer since the fsck (in RaidDFSUtil.getCorruptFiles) only check the whole DFS file system. multiple BlockFixer will do the same thing and try to fix same file if multiple BlockFixer launched. 

the change/fix will be mainly in BlockFixer.java and RaidDFSUtil.getCorruptFile(), to enable fsck to check the different paths defined in separated Raid.xml for single RaidNode/BlockFixer"
TestRaidNode is failing,HDFS-3554,"After MAPREDUCE-3868 re-enabled raid, TestRaidNode has been failing in Jenkins builds."
Consider remaining space during block blockplacement if dfs space is highly utilized,HDFS-8041,"This feature is helpful in avoiding smaller nodes (i.e. heterogeneous environment) getting constantly being full when the overall space utilization is over a certain threshold.  When the utilization is low, balancer can keep up, but once the average per-node byte goes over the capacity of the smaller nodes, they get full so quickly even after perfect balance.

This jira proposes an improvement that can be optionally enabled in order to slow down the rate of space usage growth of smaller nodes if the overall storage utilization is over a configured threshold.  It will not replace balancer, rather will help balancer keep up. Also, the primary replica placement will not be affected. Only the replicas typically placed in a remote rack will be subject to this check.

The appropriate threshold is cluster configuration specific. There is no generally good value to set, thus it is disabled by default. We have seen cases where the threshold of 85% - 90% would help. Figuring when {{totalSpaceUsed / numNodes}} becomes close to the capacity of a smaller node is helpful in determining the threshold."
Ability to use SimpleRegeratingCode to fix missing blocks,HDFS-3544,"ReedSolomon encoding (n, k) has n storage nodes and can tolerate n-k failures. Regenerating a block needs to access k blocks. This is a problem when n and k are large. Instead, we can use simple regenerating codes (n, k, f) that does first does ReedSolomon (n,k) and then does XOR with f stripe size. Then, a single disk failure needs to access only f nodes and f can be very small."
Support for pluggable erasure coding policy for HDFS,HDFS-600,"HDFS-503 introduces erasure coding for HDFS files. It currently uses ""xor"" algoritm as the Erasure coding algorithm. It would be nice if that Erasure Coding framework supports a pluggable API to allow plugging in other Erasure Coding policies.  A few of these policies are mentioned by Hong at https://issues.apache.org/jira/browse/HDFS-503?focusedCommentId=12735011&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12735011"
Handle disconnect and session timeout events at BKJM,HDFS-3562,"# Retry zookeeper operations for some amount of time in case of CONNECTIONLOSS/OPERATIONTIMEOUT exceptions.
# In case of Session expiry trigger shutdown"
"In HA mode, when there is a ledger in BK missing, which is generated after the last checkpoint, NN can not restore it.",HDFS-3908,"If not HA, when the num of edits.dir is larger than 1. Missing of one editlog file in a dir will not relust problem cause of the replica in the other dir. 
However, when in HA mode(using BK as ShareStorage), if an ledger missing, the missing ledger will not restored at the phase of NN starting even if the related editlog file existing in local dir.
The missing maintains when NN is still in standby state. However, when the NN enters active state, it will read the editlog file(related to the missing ledger) in local. But, unfortunately, the ledger after the missing one in BK can't be readed at such a phase(cause of gap).
Therefore in the following situation, editlogs will not be restored even there is an editlog file either in BK or in local dir: 

In such a stituation, editlog can't be restored:
1銆乫siamge file: fsimage_0000000000000005946.md5
2銆乴egder in zk:
	\[zk: localhost:2181(CONNECTED) 0\] ls /hdfsEdit/ledgers/edits_00000000000000594
	edits_000000000000005941_000000000000005942
	edits_000000000000005943_000000000000005944
	edits_000000000000005945_000000000000005946
	edits_000000000000005949_000000000000005949   
锛坢issing edits_000000000000005947_000000000000005948锛?3銆乪ditlog in local editlog dir锛?	\-rw-r--r-- 1 root root      30 Sep  8 03:24 edits_0000000000000005947-0000000000000005948
	\-rw-r--r-- 1 root root 1048576 Sep  8 03:35 edits_0000000000000005950-0000000000000005950
	\-rw-r--r-- 1 root root 1048576 Sep  8 04:42 edits_0000000000000005951-0000000000000005951
	锛坢iss edits_0000000000000005949-0000000000000005919锛?4銆乤nd the seen_txid
	vm2:/tmp/hadoop-root/dfs/name/current # cat seen_txid
	5949

Here, we want to restored editlog from txid 5946(image) to txid 5949(seen_txid). The 5947-5948 is missing in BK, 5949-5949 is missing in local dir.
When start the NN, the following exception is thrown:

2012-09-08 06:26:10,031 FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Error encountered requiring NN shutdown. Shutting down immediately.
java.io.IOException: There appears to be a gap in the edit log.  We expected txid 5949, but got txid 5950.
        at org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext.editLogLoaderPrompt(MetaRecoveryContext.java:94)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:163)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:93)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:692)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:223)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.catchupDuringFailover(EditLogTailer.java:182)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:599)
        at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1325)
        at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)
        at org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:63)
        at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1233)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:990)
        at org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive(HAServiceProtocolServerSideTranslatorPB.java:107)
        at org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceProtocolService$2.callBlockingMethod(HAServiceProtocolProtos.java:3633)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:427)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:924)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1692)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1688)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1232)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1686)
2012-09-08 06:26:10,036 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at vm2/160.161.0.155
************************************************************/"
WebHdfsFileSystem#toUrl does not perform character escaping for rename ,HDFS-6277,"Found this issue while testing HDFS-6141. WebHdfsFileSystem#toUrl  does not perform character escaping for rename and causes the operation to fail. 
This bug does not exist on 2.x

For e.g: 
$ hadoop dfs -rmr 'webhdfs://<namenode>:<port>/tmp/test dirname with spaces'
Problem with Trash.Unexpected HTTP response: code=400 != 200, op=RENAME, message=Bad Request. Consider using -skipTrash option
rmr: Failed to move to trash: webhdfs://<namenode>:<port>/tmp/test dirname with spaces"
DataNode should report&remove volume failures if DU cannot access files,HDFS-10777,"HADOOP-12973 refactored DU and makes it pluggable. The refactory has a side-effect that if DU encounters an exception, the exception is caught, logged and ignored, essentially fixes HDFS-9908 (in which case runaway exceptions prevent DataNodes from handshaking with NameNodes).

However, this ""fix"" is not good, in the sense that if the disk is bad, there is no immediate action made by the DataNode other than logging the exception. Existing {{FsDatasetSpi#checkDataDir}} has been reduced to only check a few number of directories blindly. If a disk goes bad, it is often possible that only a few files are bad initially and that by checking only a small number of directories it is easy to overlook the degraded disk.

I propose: in addition to logging the exception, DataNode should proactively verify the files are not accessible, remove the volume, and make the failure visible by showing it in JMX, so that administrators can spot the failure via monitoring systems.

A different fix, based on HDFS-9908, is needed before Hadoop 2.8.0"
Fix TestWebHdfsProxySelector in branch-2.8,HDFS-10853,"Similar to HDFS-8948, we need to use GenericTestUtils to set log levels to avoid multiple binding errors."
libhdfs++:  Expose an InputStream interface for the apache ORC project,HDFS-10708,It seems fitting to connect a pure c++ implementation of the HDFS client to a pure c++ implementation of a parser for the ORC file format.  Implementing the orc::InputStream API is pretty straightforward.
webhdfs fails with filenames including semicolons,HDFS-10574,"Via webhdfs or native HDFS, we can create files with semicolons in their names:

{code}
bhansen@::1 /tmp$ hdfs dfs -copyFromLocal /tmp/data ""webhdfs://localhost:50070/foo;bar""
bhansen@::1 /tmp$ hadoop fs -ls /
Found 1 items
-rw-r--r--   2 bhansen supergroup          9 2016-06-24 12:20 /foo;bar
{code}

Attempting to fetch the file via webhdfs fails:
{code}
bhansen@::1 /tmp$ curl -L ""http://localhost:50070/webhdfs/v1/foo%3Bbar?user.name=bhansen&op=OPEN""
{""RemoteException"":{""exception"":""FileNotFoundException"",""javaClassName"":""java.io.FileNotFoundException"",""message"":""File does not exist: /foo\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:66)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:56)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsUpdateTimes(FSNamesystem.java:1891)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1832)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1812)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1784)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:542)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:362)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)\n""}}
{code}

It appears (from the attached TCP dump in curl_request.txt) that the namenode's redirect unescapes the semicolon, and the DataNode's HTTP server is splitting the request at the semicolon, and failing to find the file ""foo"".



Interesting side notes:
* In the attached dfs_copyfrom_local_traffic.txt, you can see the copyFromLocal command writing the data to ""foo;bar_COPYING_"", which is then redirected and just writes to ""foo"".  The subsequent rename attempts to rename ""foo;bar_COPYING_"" to ""foo;bar"", but has the same parsing bug so effectively renames ""foo"" to ""foo;bar"".

Here is the full range of special characters that we initially started with that led to the minimal reproducer above:
{code}
hdfs dfs -copyFromLocal /tmp/data webhdfs://localhost:50070/'~`!@#$%^& ()-_=+|<.>]}"",\\\[\{\*\?\;'\''data'
curl -L ""http://localhost:50070/webhdfs/v1/%7E%60%21%40%23%24%25%5E%26+%28%29-_%3D%2B%7C%3C.%3E%5D%7D%22%2C%5C%5B%7B*%3F%3B%27data?user.name=bhansen&op=OPEN&offset=0""
{code}

Thanks to [~anatoli.shein] for making a concise reproducer.
"
libhdfs++: Add additional type-safe getters to the Configuration class,HDFS-9632,"Notably, URIs and byte sizes are missing"
Datanode should tolerate disk scan failure during NN handshake,HDFS-9908,"DN may treat a disk scan failure exception as an NN handshake exception, and this can prevent a DN to join a cluster even if most of its disks are healthy.

During NN handshake, DN initializes block pools. It will create a lock files per disk, and then scan the volumes. However, if the scanning throws exceptions due to disk failure, DN will think it's an exception because NN is inconsistent with the local storage (see {{DataNode#initBlockPool}}. As a result, it will attempt to reconnect to NN again.

However, at this point, DN has not deleted its lock files on the disks. If it reconnects to NN again, it will think the same disks are already being used, and then it will fail handshake again because all disks can not be used (due to locking), and repeatedly. This will happen even if the DN has multiple disks, and only one of them fails. The DN will not be able to connect to NN despite just one failing disk. Note that it is possible to successfully create a lock file on a disk, and then has error scanning the disk.

We saw this on a CDH 5.3.3 cluster (which is based on Apache Hadoop 2.5.0, and we still see the same bug in 3.0.0 trunk branch). The root cause is that DN treats an internal error (single disk failure) as an external one (NN handshake failure) and we should fix it.

{code:title=DataNode.java}
/**
   * One of the Block Pools has successfully connected to its NN.
   * This initializes the local storage for that block pool,
   * checks consistency of the NN's cluster ID, etc.
   * 
   * If this is the first block pool to register, this also initializes
   * the datanode-scoped storage.
   * 
   * @param bpos Block pool offer service
   * @throws IOException if the NN is inconsistent with the local storage.
   */
  void initBlockPool(BPOfferService bpos) throws IOException {
    NamespaceInfo nsInfo = bpos.getNamespaceInfo();
    if (nsInfo == null) {
      throw new IOException(""NamespaceInfo not found: Block pool "" + bpos
          + "" should have retrieved namespace info before initBlockPool."");
    }
    
    setClusterId(nsInfo.clusterID, nsInfo.getBlockPoolID());

    // Register the new block pool with the BP manager.
    blockPoolManager.addBlockPool(bpos);
    
    // In the case that this is the first block pool to connect, initialize
    // the dataset, block scanners, etc.
    initStorage(nsInfo);

    // Exclude failed disks before initializing the block pools to avoid startup
    // failures.
    checkDiskError();

    data.addBlockPool(nsInfo.getBlockPoolID(), conf);  <----- this line throws disk error exception
    blockScanner.enableBlockPoolId(bpos.getBlockPoolId());
    initDirectoryScanner(conf);
  }
{code}

{{FsVolumeList#addBlockPool}} is the source of exception.
{code:title=FsVolumeList.java}
  void addBlockPool(final String bpid, final Configuration conf) throws IOException {
    long totalStartTime = Time.monotonicNow();
    
    final List<IOException> exceptions = Collections.synchronizedList(
        new ArrayList<IOException>());
    List<Thread> blockPoolAddingThreads = new ArrayList<Thread>();
    for (final FsVolumeImpl v : volumes) {
      Thread t = new Thread() {
        public void run() {
          try (FsVolumeReference ref = v.obtainReference()) {
            FsDatasetImpl.LOG.info(""Scanning block pool "" + bpid +
                "" on volume "" + v + ""..."");
            long startTime = Time.monotonicNow();
            v.addBlockPool(bpid, conf);
            long timeTaken = Time.monotonicNow() - startTime;
            FsDatasetImpl.LOG.info(""Time taken to scan block pool "" + bpid +
                "" on "" + v + "": "" + timeTaken + ""ms"");
          } catch (ClosedChannelException e) {
            // ignore.
          } catch (IOException ioe) {
            FsDatasetImpl.LOG.info(""Caught exception while scanning "" + v +
                "". Will throw later."", ioe);
            exceptions.add(ioe);
          }
        }
      };
      blockPoolAddingThreads.add(t);
      t.start();
    }
    for (Thread t : blockPoolAddingThreads) {
      try {
        t.join();
      } catch (InterruptedException ie) {
        throw new IOException(ie);
      }
    }
    if (!exceptions.isEmpty()) {
      throw exceptions.get(0); <----- here's the original of exception
    }
    
    long totalTimeTaken = Time.monotonicNow() - totalStartTime;
    FsDatasetImpl.LOG.info(""Total time to scan all replicas for block pool "" +
        bpid + "": "" + totalTimeTaken + ""ms"");
  }
{code}"
downgrade from 2.7.2 to 2.5.0,HDFS-10767,"I have already upgrade my cluster鈥檚 namenodes(with one stand by for HA) and several datanodes from 2.5.0 folloing https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HdfsRollingUpgrade.html#Downgrade_and_Rollback;
 
I take following steps:
1. hdfs dfsadmin -rollingUpgrade prepare;
2. hdfs dfsadmin -rollingUpgrade query;
3. hdfs dfsadmin -shutdownDatanode <host:port> upgrade
4. restart and upgrade datanode;

However, I terminated the upgrade by mistake with command ""hfs dfsadmin -rollingUpgrade finalize""

Currently, I have two 2.7.2 nematodes, and three 2.7.2 datanodes and 63 2.5.0 datanodes; Now I want to downgrade the nematodes and datanodes from 2.7.2 back to 2.5.0;

But when I try to downgrade nematode and restart with 鈥?rollingUpgrade downgrade鈥? namenode cannot get started, I get rolling exception:
2016-08-16 20:37:08,642 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Encountered exception loading fsimage
org.apache.hadoop.hdfs.server.common.IncorrectVersionException: Unexpected version of storage directory /home/maintain/hadoop/data/hdfs-namenode. Reported: -63. Expecting = -57.
        at org.apache.hadoop.hdfs.server.common.StorageInfo.setLayoutVersion(StorageInfo.java:178)
        at org.apache.hadoop.hdfs.server.common.StorageInfo.setFieldsFromProperties(StorageInfo.java:131)
        at org.apache.hadoop.hdfs.server.namenode.NNStorage.setFieldsFromProperties(NNStorage.java:608)
        at org.apache.hadoop.hdfs.server.common.StorageInfo.readProperties(StorageInfo.java:228)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:323)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:202)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:955)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:700)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:529)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:585)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:751)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:735)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1407)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1473)
2016-08-16 20:37:08,645 INFO org.mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@dx-pipe-sata61-pm:50070
2016-08-16 20:37:08,745 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping NameNode metrics system...
2016-08-16 20:37:08,746 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system stopped.
2016-08-16 20:37:08,746 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system shutdown complete.
2016-08-16 20:37:08,746 FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Exception in namenode join
org.apache.hadoop.hdfs.server.common.IncorrectVersionException: Unexpected version of storage directory /home/maintain/hadoop/data/hdfs-namenode. Reported: -63. Expecting = -57.
        at org.apache.hadoop.hdfs.server.common.StorageInfo.setLayoutVersion(StorageInfo.java:178)
        at org.apache.hadoop.hdfs.server.common.StorageInfo.setFieldsFromProperties(StorageInfo.java:131)
        at org.apache.hadoop.hdfs.server.namenode.NNStorage.setFieldsFromProperties(NNStorage.java:608)
        at org.apache.hadoop.hdfs.server.common.StorageInfo.readProperties(StorageInfo.java:228)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:323)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:202)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:955)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:700)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:529)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:585)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:751)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:735)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1407)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1473)"
TestBalancer#testExitZeroOnSuccess fails intermittently,HDFS-6983,"TestBalancer#testExitZeroOnSuccess fails intermittently on branch-2. And probably fails on trunk too.

The test fails 1 in 20 times when I ran it in a loop. Here is the how it fails.

{noformat}
org.apache.hadoop.hdfs.server.balancer.TestBalancer
testExitZeroOnSuccess(org.apache.hadoop.hdfs.server.balancer.TestBalancer)  Time elapsed: 53.965 sec  <<< ERROR!
java.util.concurrent.TimeoutException: Rebalancing expected avg utilization to become 0.2, but on datanode 127.0.0.1:35502 it remains at 0.08 after more than 40000 msec.
	at org.apache.hadoop.hdfs.server.balancer.TestBalancer.waitForBalancer(TestBalancer.java:321)
	at org.apache.hadoop.hdfs.server.balancer.TestBalancer.runBalancerCli(TestBalancer.java:632)
	at org.apache.hadoop.hdfs.server.balancer.TestBalancer.doTest(TestBalancer.java:549)
	at org.apache.hadoop.hdfs.server.balancer.TestBalancer.doTest(TestBalancer.java:437)
	at org.apache.hadoop.hdfs.server.balancer.TestBalancer.oneNodeTest(TestBalancer.java:645)
	at org.apache.hadoop.hdfs.server.balancer.TestBalancer.testExitZeroOnSuccess(TestBalancer.java:845)


Results :

Tests in error: 
  TestBalancer.testExitZeroOnSuccess:845->oneNodeTest:645->doTest:437->doTest:549->runBalancerCli:632->waitForBalancer:321 Timeout
{noformat}"
error while creating collection in solr,HDFS-10669,"Hello Team,
   I have configured Solr in cloud mode on my apache hadoop 4 node cluster. I ""have created a collection with name tweets"". able to use the collection without any issues.

 When I try to create  new collection . I am getting below error but but directory gets created under solr in hdfs.  please help

user@Hadoop3:/usr/local/solr_download/solr-5.5.2$ sudo ./bin/solr create -c tweets1  -d data_driven_schema_configs

Connecting to ZooKeeper at localhost:9983 ...
Re-using existing configuration directory tweets1

Creating new collection 'tweets1' using command:
http://172.16.16.129:8983/solr/admin/collections?action=CREATE&name=tweets1&numShards=1&replicationFactor=1&maxShardsPerNode=1&collection.configName=tweets1


ERROR: Failed to create collection 'tweets1' due to: {172.16.16.129:8983_solr=org.apache.solr.client.solrj.impl.HttpSolrClient$RemoteSolrException:Error from server at http://172.16.16.129:8983/solr: Error CREATEing SolrCore 'tweets1_shard1_replica1': Unable to create core [tweets1_shard1_replica1] Caused by: Illegal pattern component: T}

"
Namenode doesn't pass config to UGI in format,HDFS-9770,"The {{NameNode.format()}} method should call {{UserGroupInformation.setConfiguration(conf)}} before using the UGI.  Otherwise, the config that the UGI is using is not the same as what the NN is using."
repair test org.apache.hadoop.fs.http.server.TestHttpFSWithKerberos,HDFS-4311,"Some of the test cases in this test class are failing because they are affected by static state changed by the previous test cases. Namely this is the static field org.apache.hadoop.security.UserGroupInformation.loginUser .
The suggested patch solves this problem.
Besides, the following improvements are done:
1) parametrized the user principal and keytab values via system properties;
2) shutdown of the Jetty server and the minicluster between the test cases is added to make the test methods independent on each other."
Create a generic function to synchronize async functions and methods. ,HDFS-9326,"The majority of the functionality in libhdfs++ is asynchronous, but some applications need synchronous operations.  At the time of filing this only happens in 3 places in the C API, however that number is going to grow a lot once the C and high level C++ APIs expose all of the namenode functions.

This synchronization is typically implemented like this:
auto promise = std::make_shared<std::promise<T>>()
std::future<T> = future(promise->get_future());

auto async_callback = [promise] () {promise->set_value(val);};

SomeClass::AsyncMethod(async_callback); 

auto result = future.get()

Ideally this could all be pushed into a templated function so that the promise and future don't need to be defined at the call site.  This would probably take the form of doing a std::bind to get all the arguments in place at the call site and then passing that to the synchronize function.

This appears to require some template magic that isn't always well supported; see https://gcc.gnu.org/bugzilla/show_bug.cgi?id=51979.
"
Erasure Coding: Rename CorruptReplicasMap to CorruptRedundancyMap in BlockManager to more generic,HDFS-10407,"The idea of this jira is to rename the following entity in BlockManager,

- {{CorruptReplicasMap}} to {{CorruptRedundancyMap}}"
DFSClient filesBeingWritten memory leak when client gets RemoteException - could only be replicated to 0 nodes instead of minReplication (=1),HDFS-10504,"I'm trying to migrate data from nfs to hdfs. I have about 2million files with small sizes. That takes about 4 hours in my env, but I randomly get an exception during migration. Got 12 of those during the test (stack below). 

Now when I'm getting the exception, I'm doing a sleep for one second, after I check if the file is there (api says yes, but it's reported size is zero bytes). So I'm removing the file, then start writing it again and at that point it succeeds. 

Here is the stack:
org.apache.hadoop.ipc.RemoteException(java.io.IOException): File xxx/xxx/xxx could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1592)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3158)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3082)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:822)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:500)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)

	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:418)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1459)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1255)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:449)



When I write I'm using the try with resource which should call close method on the FSDataOutputStream. This triggers the 
dfsClient.endFileLease(fileId) to be called which should remove the ref from:
DFSClient:
synchronized(filesBeingWritten) {
      filesBeingWritten.remove(inodeId);
      if (filesBeingWritten.isEmpty()) {
        lastLeaseRenewal = 0;
      }
    }


But when the process finishes, I get:

2016-06-07 22:26:54,734 - ERROR [Thread-3] (DFSClient.closeAllFilesBeingWritten:940) - Failed to close inode 1675022
org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /xxx/xxx/xxx could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1592)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3158)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3082)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:822)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:500)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)


Now, when there is no space on the datanode, I get this error a lot which causes my migration java client to die with OutOfMemory. The cause is DFSClient.filesBeingWritten taking almost 1GB."
DiskBalancer: Refactor Execute Command,HDFS-9564,This is used to track refactoring execute command.
DiskBalancer: Refactor Plan Command,HDFS-9563,This is used to track refactoring plan command.
Enabled memory locking and now HDFS won't start up,HDFS-10502,"My goal is to speed up reads.  I have about 500k small files (2k to 15k) and I'm trying to use HDFS as a cache for serialized instances of java objects.

I've written the code to construct and serialize all the objects out to HDFS, and am now hoping to improve read performance, because accessing the objects from disk-based storage is proving to be too slow for my application's SLA's.

So my first question is, is using memory locking and hdfs cacheadmin pools and directives the right way to go, to cache my objects into memory, or should I create RAM disks, and do memory-based storage instead?

If hdfs cacheadmin is the way to go (it's the path I'm going down so far), then I need to figure out if what's happening is a bug or if I've configured something wrong, because when I start up HDFS with a gig of memory locked (both in limits.d for ulimit -l and also in hdfs-site.xml) and the server starts up, and presumably tries to cache things into memory, I get hours and hours of timeouts in the logs like this:

2016-06-08 07:42:50,856 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.SocketTimeoutException: Call From stgb-fe1.litle.com/10.1.9.66 to localhost:8020 failed on socket timeout exception: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/127.0.0.1:51647 remote=localhost/127.0.0.1:8020]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:751)
	at org.apache.hadoop.ipc.Client.call(Client.java:1479)
	at org.apache.hadoop.ipc.Client.call(Client.java:1412)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy13.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:153)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:554)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:653)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:824)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/127.0.0.1:51647 remote=localhost/127.0.0.1:8020]
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:520)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1084)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:979)
"
HDFS ZKFC HealthMonitor Throw a Exception Cause AutoFailOver,HDFS-10373,"HDFS ZKFC HealthMonitor Throw a Exception 
2016-05-05 02:00:59,475 WARN org.apache.hadoop.ha.HealthMonitor: Transport-level exception trying to monitor health of NameNode at XXX-XXX-XXX-hadoop.jd.local/172.22.17
1.XX:8021: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: ""XXX-XXX-XXX-hadoop.jd.local/172.22.171.XX""; destinat
ion host is: XXX-XXX-XXX-hadoop.jd.local"":8021;

Cause HA AutoFailOver"
"Can not read file from java.io.IOException: Need XXX bytes, but only YYY  bytes available",HDFS-10484,"We are running CDH 4.1.2 distro and trying to read file from HDFS. It ends up with exception @datanode saying


2016-06-02 10:43:26,354 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(X.X.X.X, storageID=DS-404876644-X.X.X.X-50010-1462535537579, infoPort=50075, ipcPort=50020, storageInfo=lv=-40;cid=cluster18;nsid=2115086255;c=0):Got exception while serving BP-2091182050-X.X.X.X-1358362115729:blk_5037101550399368941_420502314 to /X.X.X.X:58614
java.io.IOException: Need 10172416 bytes, but only 10072576 bytes available
at org.apache.hadoop.hdfs.server.datanode.BlockSender.waitForMinLength(BlockSender.java:387)
at org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>(BlockSender.java:189)
at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:268)
at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:88)
at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:63)
at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:219)
at java.lang.Thread.run(Thread.java:662)
2016-06-02 10:43:26,354 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: app112.rutarget.ru:50010:DataXceiver error processing READ_BLOCK operation src: /X.X.X.X:58614 dest: /X.X.X.X:50010
java.io.IOException: Need 10172416 bytes, but only 10072576 bytes available
at org.apache.hadoop.hdfs.server.datanode.BlockSender.waitForMinLength(BlockSender.java:387)
at org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>(BlockSender.java:189)
at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:268)
at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:88)
at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:63)
at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:219)
at java.lang.Thread.run(Thread.java:662)



FSCK shows file as being open for write, however hdfs client that handles writes to this file closed it long time ago -- so file stucked in RBW for a few last days. How can we get actual data  block in this case? I found only binary .meta file on datanode but not actual block with data.



-- "
Move block replication logic from BlockManager to a new class ReplicationManager,HDFS-9442,"Currently the {{BlockManager}} is managing all replication logic for over- , under- and mis-replicated blocks. This jira proposes to move that code to a new class named {{ReplicationManager}} for cleaner code logic, shorter source files, and easier lock separating work in future.

The {{ReplicationManager}} is a package local class, providing {{BlockManager}} with methods that accesses its internal data structures of replication queue. Meanwhile, the class maintains the lifecycle of {{replicationThread}} and {{replicationQueuesInitializer}} daemon."
 balancer-Test failed because of time out,HDFS-3747,When run a given test case for Banlancer Test. In the test the banlancer thread try to move some block cross the rack but it can't find any available blocks in the source rack. Then the thread won't interrupt until the tag isTimeUp reaches 20min. But maven judges the test failed because the thread have runned for 15min.
Invalid counter tag in HDFS balancer which lead to infinite loop,HDFS-3746,Everytime banlancer try to move a block cross the rack. For every NameNodeConnector it will instance a new banlancer. The tag notChangedIterations is reseted to 0. Then it won't reach 5 and exit the thread for not moved in 5 consecutive iterations. This lead to a infinite loop
"read(long position, byte[] buffer, int offset, int length) is not  behaving as expected",HDFS-3361,"Start NN and DN
write a file with size 1024
now try to read file using following api
fsin.read(writeBuff, 1024, fsin.available())..This is retuning correctly as expected.
fsin.read(10, writeBuff, 10, fsin.available())(this is retunring zero.)

Here it's returning zero..But actual file length is 1024
 
  *Java Docs provided* 
{code}
/**
   * Read bytes from the given position in the stream to the given buffer.
   *
   * @param position  position in the input stream to seek
   * @param buffer    buffer into which data is read
   * @param offset    offset into the buffer in which data is written
   * @param length    maximum number of bytes to read
   * @return total number of bytes read into the buffer, or <code>-1</code>
   *         if there is no more data because the end of the stream has been
   *         reached
   */
  public int read(long position, byte[] buffer, int offset, int length)
    throws IOException {
    return ((PositionedReadable)in).read(position, buffer, offset, length);
  }
{code}

But If I try with only ->fsin.read(10, writeBuff, 10, fsin.available())(client prog contains only one read method)

I am getting actual length...I am not sure actual cause..

Please correct me If I am wrong.........
"
Rolling upgrade won't finish if SBN is configured without StandbyCheckpointer,HDFS-6637,"In HA setup cluster, for rolling upgrade, the image file ""fsimage_rollback"" is generated by StandbyCheckpointer thread of SBN. While if configuration ""dfs.ha.standby.checkpoints"" is set false, there will be no StandbyCheckpointer thread in SBN. This will lead to the rolling upgrade never finish. "
Browsing filesystem from specific datanode in live nodes page also should include delegation token in the url,HDFS-4223,"Browsing file system from the 'Browse the filesystem' link includes 'tokenString' as a parameter in the URL.

Same way browsing using specific datanode from live nodes page also should include 'tokenString' as a parameter to avoid following exception

{noformat}javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]{noformat}"
ViewFS should check the existence of the mapped namespace directories in the mount table,HDFS-5712,"ViewFS doesn't validate the mount table mapping. Even the mapped directory on NameNode doesn't exist, list directories or ""dfs -ls""  command can still show the mapped directory.

This confuses users and applications when they try to create files under the mapped directories. They will get file-not-exist error but viewfs shows the directory exists.

It would be less misleading if ViewFS can validate the  mount table and report found errors."
BKJM: Two namenodes usng bkjm can race to create the version znode,HDFS-4154,"nd one will get the following error.

2012-11-06 10:04:00,200 INFO hidden.bkjournal.org.apache.zookeeper.ClientCnxn: Session establishment complete on server 109-231-69-172.flexiscale.com/109.231.69.172:2181, sessionid = 0x13ad528fcfe0005, negotiated timeout = 4000
2012-11-06 10:04:00,710 FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Exception in namenode join
java.lang.IllegalArgumentException: Unable to construct journal, bookkeeper://109.231.69.172:2181;109.231.69.173:2181;109.231.69.174:2181/hdfsjournal
        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.createJournal(FSEditLog.java:1251)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.initJournals(FSEditLog.java:226)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.initSharedJournalsForRead(FSEditLog.java:206)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.initEditLog(FSImage.java:657)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:590)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:259)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:544)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:423)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:385)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:401)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:435)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:611)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:592)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1135)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1201)
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.createJournal(FSEditLog.java:1249)
        ... 14 more
Caused by: java.io.IOException: Error initializing zk
        at org.apache.hadoop.contrib.bkjournal.BookKeeperJournalManager.<init>(BookKeeperJournalManager.java:233)
        ... 19 more
Caused by: hidden.bkjournal.org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /hdfsjournal/version
        at hidden.bkjournal.org.apache.zookeeper.KeeperException.create(KeeperException.java:119)
        at hidden.bkjournal.org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at hidden.bkjournal.org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:778)
        at org.apache.hadoop.contrib.bkjournal.BookKeeperJournalManager.<init>(BookKeeperJournalManager.java:222)
        ... 19 more"
"WebHdfsFileSystem execute get, renew and cancel delegationtoken operation should use spnego to authenticate",HDFS-6436,"while in kerberos secure mode, when using WebHdfsFileSystem to access HDFS, it allways get an *org.apache.hadoop.security.authentication.client.AuthenticationException: Unauthorized*, for example, when call WebHdfsFileSystem.listStatus it will execute a LISTSTATUS Op, and this Op should authenticate via *delegation token*, so it will execute a GETDELEGATIONTOKEN Op to get a delegation token(actually GETDELEGATIONTOKEN authenticates via *SPNEGO*), but it still use delegation token to authenticate, so it allways get an Unauthorized Exception.
Exception is like this:
{code:java}
19:05:11.758 [main] DEBUG o.a.h.hdfs.web.URLConnectionFactory - open URL connection
java.io.IOException: org.apache.hadoop.security.authentication.client.AuthenticationException: Unauthorized
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:287)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$200(WebHdfsFileSystem.java:82)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.runWithRetry(WebHdfsFileSystem.java:538)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.access$100(WebHdfsFileSystem.java:406)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner$1.run(WebHdfsFileSystem.java:434)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.run(WebHdfsFileSystem.java:430)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.getDelegationToken(WebHdfsFileSystem.java:1058)
19:05:11.766 [main] DEBUG o.a.h.security.UserGroupInformation - PrivilegedActionException as:bangtao@CYHADOOP.COM (auth:KERBEROS) cause:java.io.IOException: org.apache.hadoop.security.authentication.client.AuthenticationException: Unauthorized
	at org.apache.hadoop.hdfs.web.TokenAspect.ensureTokenInitialized(TokenAspect.java:134)
19:05:11.767 [main] DEBUG o.a.h.security.UserGroupInformation - PrivilegedActionException as:bangtao@CYHADOOP.COM (auth:KERBEROS) cause:java.io.IOException: org.apache.hadoop.security.authentication.client.AuthenticationException: Unauthorized
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.getDelegationToken(WebHdfsFileSystem.java:213)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.getAuthParameters(WebHdfsFileSystem.java:371)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toUrl(WebHdfsFileSystem.java:392)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractFsPathRunner.getUrl(WebHdfsFileSystem.java:602)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.runWithRetry(WebHdfsFileSystem.java:533)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.access$100(WebHdfsFileSystem.java:406)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner$1.run(WebHdfsFileSystem.java:434)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.run(WebHdfsFileSystem.java:430)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.listStatus(WebHdfsFileSystem.java:1037)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1483)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1523)
	at org.apache.hadoop.fs.FileSystem$4.<init>(FileSystem.java:1679)
	at org.apache.hadoop.fs.FileSystem.listLocatedStatus(FileSystem.java:1678)
	at org.apache.hadoop.fs.FileSystem.listLocatedStatus(FileSystem.java:1661)
	at org.apache.hadoop.fs.FileSystem$5.<init>(FileSystem.java:1723)
	at org.apache.hadoop.fs.FileSystem.listFiles(FileSystem.java:1720)
	at com.cyou.marketing.hop.filesystem.App$1.run(App.java:34)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:356)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1594)
	at com.cyou.marketing.hop.filesystem.App.main(App.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:120)
Caused by: org.apache.hadoop.security.authentication.client.AuthenticationException: Unauthorized
	... 40 more
{code}"
JspHelper#bestNode() doesn't handle bad datanodes correctly,HDFS-5146,"JspHelper#bestNode() doesn't handle correctly if the chosen datanode is down.

{code}    while (s == null) {
      if (chosenNode == null) {
        do {
          if (doRandom) {
            index = DFSUtil.getRandom().nextInt(nodes.length);
          } else {
            index++;
          }
          chosenNode = nodes[index];
        } while (deadNodes.contains(chosenNode));
      }
      chosenNode = nodes[index];
{code}

In this part of the code, choosing the datanode will be done only once.
If the chosen datanode is down, then definitely exception will be thrown instead of re-chosing the available node."
OutOfMemory by BPServiceActor.offerService() takes down DataNode,HDFS-4475,"In DataNode, there are catchs around BPServiceActor.offerService() call but no catch for OutOfMemory as there is for the DataXeiver as introduced in 0.22.0.

The issue can be replicated like this:
1) Create a cluster of X DataNodes and 1 NameNode and low memory settings (-Xmx128M or something similar).
2) Flood HDFS with small file creations (any should work actually).
3) DataNodes will hit OoM, stop blockpool service, and shutdown.

The resolution is to catch the OoMException and handle it properly when calling BPServiceActor.offerService() in DataNode.java; like as done in 0.22.0 of Hadoop. DataNodes should not shutdown or crash but remain in a sort of frozen state until memory issues are resolved by GC.

LOG ERROR:
2013-02-04 11:46:01,854 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected exception in block pool Block pool BP-1105714849-10.10.10.110-1360005776467 (storage id DS-1952316202-10.10.10.112-50010-1360005820993) service to vmhost2-vm0/10.10.10.110:8020
java.lang.OutOfMemoryError: GC overhead limit exceeded
2013-02-04 11:46:01,854 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool BP-1105714849-10.10.10.110-1360005776467 (storage id DS-1952316202-10.10.10.112-50010-1360005820993) service to vmhost2-vm0/10.10.10.110:8020"
Erasure Coding: Improve exception handling in ErasureCodingWorker#ReconstructAndTransferBlock,HDFS-9832,"There are three places in {{ErasureCodingWorker#ReconstructAndTransferBlock}} that I think can be improved.
1.In run method, the step3 transfer data will be failed sometimes, and this will cause buffers not be cleared completely, is better to invoke clearBuffer again in finally handling?
{code}
        while (positionInBlock < maxTargetLength) {
          final int toReconstruct = (int) Math.min(
              bufferSize, maxTargetLength - positionInBlock);
          // step1: read from minimum source DNs required for reconstruction.
          // The returned success list is the source DNs we do real read from
          Map<ExtendedBlock, Set<DatanodeInfo>> corruptionMap = new HashMap<>();
          try {
            success = readMinimumStripedData4Reconstruction(success,
                toReconstruct, corruptionMap);
          } finally {
            // report corrupted blocks to NN
            reportCorruptedBlocks(corruptionMap);
          }

          // step2: decode to reconstruct targets
          reconstructTargets(success, targetsStatus, toReconstruct);

          // step3: transfer data
          if (transferData2Targets(targetsStatus) == 0) {
            String error = ""Transfer failed for all targets."";
            throw new IOException(error);
          }

          clearBuffers();
          positionInBlock += toReconstruct;
        }
{code}

2.Is better to set null to buffers objects, targetsOutput and socket objects in finally handling code?
{code}
      } finally {
        datanode.decrementXmitsInProgress();
        // close block readers
        for (StripedReader stripedReader : stripedReaders) {
          closeBlockReader(stripedReader.blockReader);
        }
        for (int i = 0; i < targets.length; i++) {
          IOUtils.closeStream(targetOutputStreams[i]);
          IOUtils.closeStream(targetInputStreams[i]);
          IOUtils.closeStream(targetSockets[i]);
        }
      }
{code}

3.buffers in {{ReconstructAndTransferBlock}} are not released. In {{clearBuffers}}, it will finally invoke buffer.clear(), but this only change the index position and not really release the space. So this seems not a small problem."
TestDataNodeLifeline always fails on trunk on mac.,HDFS-10362,"TestDataNodeLifeline fails always on my local box but haven't seen any failure in jenkins build.
{noformat}
Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 29.861 sec <<< FAILURE! - in org.apache.hadoop.hdfs.server.datanode.TestDataNodeLifeline
testSendLifelineIfHeartbeatBlocked(org.apache.hadoop.hdfs.server.datanode.TestDataNodeLifeline)  Time elapsed: 18.81 sec  <<< FAILURE!
java.lang.AssertionError: Expect DataNode to be kept alive by lifeline. expected:<1> but was:<0>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:555)
	at org.apache.hadoop.hdfs.server.datanode.TestDataNodeLifeline.testSendLifelineIfHeartbeatBlocked(TestDataNodeLifeline.java:185)


Results :

Failed tests: 
  TestDataNodeLifeline.testSendLifelineIfHeartbeatBlocked:185 Expect DataNode to be kept alive by lifeline. expected:<1> but was:<0>
{noformat}"
Inculde file id in these ClientProtocol RPCs which were originally only using path to identify a file,HDFS-4469,"Like HDFS-4340(add fileId to addBlock), this JIRA is to track the change to add fileId to other RPC calls, such as append, complete and etc."
hdfs dfs -count of a .snapshot directory fails claiming file does not exist,HDFS-4847,"I successfully allow snapshots for /tmp and create three snapshots. I verify that the three snapshots are in /tmp/.snapshot.

However, when I attempt _hdfs dfs -count /tmp/.snapshot_ I get a file does not exist exception.

Running -count on /tmp finds /tmp successfully.

{code}
schu-mbp:~ schu$ hadoop fs -ls /tmp/.snapshot
2013-05-24 10:27:10,070 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 3 items
drwxr-xr-x   - schu supergroup          0 2013-05-24 10:26 /tmp/.snapshot/s1
drwxr-xr-x   - schu supergroup          0 2013-05-24 10:27 /tmp/.snapshot/s2
drwxr-xr-x   - schu supergroup          0 2013-05-24 10:27 /tmp/.snapshot/s3
schu-mbp:~ schu$ hdfs dfs -count /tmp
2013-05-24 10:27:20,510 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
          12            0                  0 /tmp
schu-mbp:~ schu$ hdfs dfs -count /tmp/.snapshot
2013-05-24 10:27:30,397 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
count: File does not exist: /tmp/.snapshot
schu-mbp:~ schu$ hdfs dfs -count -q /tmp/.snapshot
2013-05-24 10:28:23,252 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
count: File does not exist: /tmp/.snapshot
schu-mbp:~ schu$
{code}

In the NN logs, I see:
{code}
2013-05-24 10:27:30,857 INFO  [IPC Server handler 6 on 8020] FSNamesystem.audit (FSNamesystem.java:logAuditEvent(6143)) - allowed=true	ugi=schu (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/tmp/.snapshot	dst=null	perm=null
2013-05-24 10:27:30,891 ERROR [IPC Server handler 7 on 8020] security.UserGroupInformation (UserGroupInformation.java:doAs(1492)) - PriviledgedActionException as:schu (auth:SIMPLE) cause:java.io.FileNotFoundException: File does not exist: /tmp/.snapshot
2013-05-24 10:27:30,891 INFO  [IPC Server handler 7 on 8020] ipc.Server (Server.java:run(1864)) - IPC Server handler 7 on 8020, call org.apache.hadoop.hdfs.protocol.ClientProtocol.getContentSummary from 127.0.0.1:49738: error: java.io.FileNotFoundException: File does not exist: /tmp/.snapshot
java.io.FileNotFoundException: File does not exist: /tmp/.snapshot
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.getContentSummary(FSDirectory.java:2267)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getContentSummary(FSNamesystem.java:3188)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getContentSummary(NameNodeRpcServer.java:829)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getContentSummary(ClientNamenodeProtocolServerSideTranslatorPB.java:726)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:48057)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:527)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1033)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1842)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1838)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1489)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1836)
{code}

Likewise, the _hdfs dfs du_ command fails with the same problem. 

Hadoop version:
{code}
schu-mbp:~ schu$ hadoop version
Hadoop 3.0.0-SNAPSHOT
Subversion git://github.com/apache/hadoop-common.git -r ccaf5ea09118eedbe17fd3f5b3f0c516221dd613
Compiled by schu on 2013-05-24T04:45Z
From source with checksum ee94d984bcf5cc38ca12a1efedb68fc
This command was run using /Users/schu/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/hadoop-common-3.0.0-SNAPSHOT.jar
{code}"
DNs may OOM under high webhdfs load,HDFS-6967,"Webhdfs uses jetty.  The size of the request thread pool is limited, but jetty will accept and queue infinite connections.  Every queued connection is ""heavy"" with buffers, etc.  Unlike data streamer connections, thousands of webhdfs connections will quickly OOM a DN.  The accepted requests must be bounded and excess clients rejected so they retry on a new DN."
Add the possibility to mark a node as 'low priority' for read in the DFSClient,HDFS-3705,"This has been partly discussed in HBASE-6435.

The DFSClient includes a 'bad nodes' management for reads and writes. Sometimes, the client application already know that some deads are dead or likely to be dead.
An example is the 'HBase Write-Ahead-Log': when HBase reads this file, it knows that the HBase regionserver died, and it's very likely that the box died so the datanode on the same box is dead as well. This is actually critical, because:
- it's the hbase recovery that reads these log files
- if we read them it means that we lost a box, so we have 1 dead replica out the the 3. 
- for all files read, we have 33% of chance to go to the dead datanode
- as the box just died, we're very likely to get a timeout exception so we're delaying the hbase recovery by 1 minute. For HBase, it means that the data is not available during this minute.
"
TestBlockReaderLocalLegacy flakes in MiniDFSCluster#shutdown,HDFS-4764,"I've seen this fail on two test-patch runs, and I'm pretty sure it's unrelated.

{noformat}
Error Message

Test resulted in an unexpected exit
Stacktrace

java.lang.AssertionError: Test resulted in an unexpected exit
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:1416)
	at org.apache.hadoop.hdfs.TestBlockReaderLocalLegacy.testBothOldAndNewShortCircuitConfigured(TestBlockReaderLocalLegacy.java:152)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.runners.BlockJUnit4ClassRunner.runNotIgnored(BlockJUnit4ClassRunner.java:79)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:71)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:49)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
{noformat}"
Only keep successfully loaded volumes in the configuration.,HDFS-7173,"Hot swapping data volumes might fail. The user should be able to fix the failed volumes and disks, then ask the {{DataNode}} to retry the previously failed volumes. 

To attempt to reload the failed volume again on the same directory, this failed directory must not be presented in the {{Configuration}} object that {{DataNode has}}. Therefore, it should only put successfully loaded volumes into the {{Configuration}} object."
skip checksums when reading a cached block via non-local reads,HDFS-5521,"The DataNode needs to skip checksumming when reading a cached block via non-local reads.  This is like HDFS-5182, but for non-short-circuit."
TestDatanodeManager#testNumVersionsReportedCorrect occasionally fails,HDFS-7471,"From https://builds.apache.org/job/Hadoop-Hdfs-trunk/1957/ :

{code}
FAILED:  org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testNumVersionsReportedCorrect

Error Message:
The map of version counts returned by DatanodeManager was not what it was expected to be on iteration 237 expected:<0> but was:<1>

Stack Trace:
java.lang.AssertionError: The map of version counts returned by DatanodeManager was not what it was expected to be on iteration 237 expected:<0> but was:<1>
        at org.junit.Assert.fail(Assert.java:88)
        at org.junit.Assert.failNotEquals(Assert.java:743)
        at org.junit.Assert.assertEquals(Assert.java:118)
        at org.junit.Assert.assertEquals(Assert.java:555)
        at org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testNumVersionsReportedCorrect(TestDatanodeManager.java:150)
{code}"
Track new transactions per thread so that unnecessary logSync() calls can be avoided,HDFS-4191,"With this fix, logSync() can be called anytime by a thread without affecting the batched syncs metrics. As discussed in HDFS-4186, we will put this feature only to trunk first and let it soak for some time. "
Target port chosen by Hftp/Hsftp for getting delegation token may be incorrect,HDFS-5275,"The port selection to get the delegation token is confusing. Also the code documentation and tests appear to conflict.

The comment in {{HftpFileSystem#getCanonicalServiceName}} seems to indicate that the configured secure port should be chosen, ignoring the port from the URI.
{code}
  public String getCanonicalServiceName() {
    // unlike other filesystems, hftp's service is the secure port, not the
    // actual port in the uri
    return SecurityUtil.buildTokenService(nnSecureUri).toString();
  }
{code}

However {{TestHftpFileSystem#testHsftpCustomUriPortWithCustomDefaultPorts}} tests that the returned port is the one from the URI.
{code}
@Test
public void testHsftpCustomUriPortWithCustomDefaultPorts() throws IOException {
  conf.setInt(DFSConfigKeys.DFS_NAMENODE_HTTPS_PORT_KEY, 456);

  URI uri = URI.create(""hsftp://localhost:789"");
  HsftpFileSystem fs = (HsftpFileSystem) FileSystem.get(uri, conf);

  assertEquals(456, fs.getDefaultPort());
  assertEquals(456, fs.getDefaultSecurePort());

  assertEquals(uri, fs.getUri());
  assertEquals(
      ""127.0.0.1:789"",
      fs.getCanonicalServiceName()
  );
}
{code}

The test still passes because {{HsftpFileSystem}} (incorrectly?) overrides {{getNamenodeSecureAddr}}.


Either the code needs to be fixed or we should document the correct behavior."
Implement asynchronous setOwner for DistributedFileSystem,HDFS-10350,This is proposed to implement an asynchronous setOwner.
Allow trigger block report from all datanodes,HDFS-10359,"Since we have HDFS-7278 allows trigger block report from one certain datanode. It would be helpful to add a option to this command to trigger block report from all datanodes.
Command maybe like this:
*hdfs dfsadmin -triggerBlockReport \[-incremental\] <datanode_host:ipc_port|all>*"
Support extensions to WebHdfsFileSystem,HDFS-9938,"This JIRA is to request opinion from the community, Can new file system use an extension of WebHDFS file system. 
Considering all the known limitation WebHDFS has over implementing a new client by extending FileSystem class.

Option we have is 
1. Use the namespace org.apache.hadoop.hdfs.web in new file system implementation to access protected functionality from WebHdfsFileSystem.
2. Change the WebHdfs to support extensions
3. Suggestion on different approach like new client by sub classing FileSystem.
"
Potential deadlock #HeartbeatManager,HDFS-8000,"Cluster loaded with 90000000+ Blocks
Restart DN
 access NN UI..Then NN will  got Hang

Will attach td.."
hdfs-default.xml shouldn't use hadoop.tmp.dir for dfs.data.dir (0.20 and lower) / dfs.datanode.dir (0.21 and up),HDFS-964,"This question/problem pops up all the time.  Can we *please* eliminate hadoop.tmp.dir's usage from the default in dfs.data.dir.  It is confusing to new people and results in all sorts of weird accidents.  If we want the same value, fine, but there are a lot of implied things by the variable re-use."
FSDataOutputStream.write() allocates new byte buffer on each operation,HDFS-10194,"This is the code:
{code}
 private DFSPacket createPacket(int packetSize, int chunksPerPkt, long offsetInBlock, long seqno, boolean lastPacketInBlock) throws InterruptedIOException {
     final byte[] buf;
     final int bufferSize = PacketHeader.PKT_MAX_HEADER_LEN +   packetSize;
 
     try {
       buf = byteArrayManager.newByteArray(bufferSize);
     } catch (InterruptedException ie) {
       final InterruptedIOException iioe = new InterruptedIOException(
           ""seqno="" + seqno);
       iioe.initCause(ie);
       throw iioe;
     }
 
     return new DFSPacket(buf, chunksPerPkt, offsetInBlock, seqno,
                          getChecksumSize(), lastPacketInBlock);
}
{code}

"
Jenkins pre-commit build does not pick up the correct attachment.,HDFS-2852,"When two files are attached to a jira, slaves build twice but only the latest attachement.

For example, the patch_tested.txt from PreCommit-Admin shows correct attachment numbers for In HDFS-2784.
From https://builds.apache.org/job/PreCommit-Admin/56284/artifact/patch_tested.txt
{noformat}
...
HBASE-5271,12511722
HDFS-2784,12511725
HDFS-2836,12511727
HDFS-2784,12511726
{noformat}

But the Jenkins build slaves had built #12511726 twice."
"HDFS build is broken, ivy-resolve-common does not find hadoop-common",HDFS-1519,"HADOOP_DIR/hdfs$ ant ivy-resolve-common
Buildfile: build.xml

ivy-download:
      [get] Getting: http://repo2.maven.org/maven2/org/apache/ivy/ivy/2.1.0/ivy-2.1.0.jar
      [get] To: /usr/products/hadoop/v0_21_0/ANY/hdfs/ivy/ivy-2.1.0.jar
      [get] Not modified - so not downloaded

ivy-init-dirs:

ivy-probe-antlib:

ivy-init-antlib:

ivy-init:
[ivy:configure] :: Ivy 2.1.0 - 20090925235825 :: http://ant.apache.org/ivy/ ::
[ivy:configure] :: loading settings :: file = /usr/products/hadoop/v0_21_0/ANY/hdfs/ivy/ivysettings.xml

ivy-resolve-common:
[ivy:resolve] 
[ivy:resolve] :: problems summary ::
[ivy:resolve] :::: WARNINGS
[ivy:resolve] 		module not found: org.apache.hadoop#hadoop-common;0.21.0
[ivy:resolve] 	==== apache-snapshot: tried
[ivy:resolve] 	  https://repository.apache.org/content/repositories/snapshots/org/apache/hadoop/hadoop-common/0.21.0/hadoop-common-0.21.0.pom
[ivy:resolve] 	  -- artifact org.apache.hadoop#hadoop-common;0.21.0!hadoop-common.jar:
[ivy:resolve] 	  https://repository.apache.org/content/repositories/snapshots/org/apache/hadoop/hadoop-common/0.21.0/hadoop-common-0.21.0.jar
[ivy:resolve] 	==== maven2: tried
[ivy:resolve] 	  http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/0.21.0/hadoop-common-0.21.0.pom
[ivy:resolve] 	  -- artifact org.apache.hadoop#hadoop-common;0.21.0!hadoop-common.jar:
[ivy:resolve] 	  http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/0.21.0/hadoop-common-0.21.0.jar
[ivy:resolve] 		::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve] 		::          UNRESOLVED DEPENDENCIES         ::
[ivy:resolve] 		::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve] 		:: org.apache.hadoop#hadoop-common;0.21.0: not found
[ivy:resolve] 		::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve] 
[ivy:resolve] :: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS

BUILD FAILED
/usr/products/hadoop/v0_21_0/ANY/hdfs/build.xml:1549: impossible to resolve dependencies:
	resolve failed - see output for details

Total time: 3 seconds
"
FsVolume should tolerate few times check-dir failed due to deletion by mistake,HDFS-9819,"FsVolume should tolerate few times check-dir failed because sometimes we will do a delete dir/file operation by mistake in datanode data-dirs. Then the {{DataNode#startCheckDiskErrorThread}} will invoking checkDir method periodicity and find dir not existed, throw exception. The checked volume will be added to failed volume list. The blocks on this volume will be replicated again. But actually, this is not needed to do. We should let volume can be tolerated few times check-dir failed like config {{dfs.datanode.failed.volumes.tolerated}}."
Get input/output error while copying 800 small files to NFS Gateway mount point ,HDFS-9152,"We have around *800 3-5K* files on local file system, we have nfs gateway mounted on */hdfs/*, when we tried to copy these files to HDFS by 

*cp ~/userdata/* /hdfs/user/cqdemo/demo3.data/*

most of files are failed because of 

cp: writing `/hdfs/user/cqdemo/demo3.data/TRAFF_201408011220.csv': Input/output error
cp: writing `/hdfs/user/cqdemo/demo3.data/TRAFF_201408011221.csv': Input/output error
cp: writing `/hdfs/user/cqdemo/demo3.data/TRAFF_201408011222.csv': Input/output error

for same set of files, I tried to use hadoop dfs -put command to do the copy, it works fine."
Change all read only operation audit log to debug level,HDFS-9828,"Audit log should only log important operations such as create new file/folder , , delete file/folder and so on, but the read only operation need to be kept in log, otherwise the log size will become very big quickly if heavy load."
DataNode doesn't log any shutdown info when the process of DataNode exiting,HDFS-9863,"One of my datanodes exited without any shutdown info. 
{code}
2016-02-25 14:46:00,283 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1942012336-XX.XX.2.191-1406726500544:blk_1730224536_658031130, type=HAS_DOWNSTREAM_IN_PIPELINE terminating
2016-02-25 15:03:55,639 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG:
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = XX.XX6032/XX.XX.6.32
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
{code}
I think maybe full gc causes this problem, so I looked the datanode gc log. There is a cms gc but the time of this gc is after than restart datanode time. 
{code}
2016-02-25T15:03:57.930+0800: 2.756: [GC2016-02-25T15:03:57.930+0800: 2.756: [ParNew: 1677824K->24417K(1887488K), 0.0249280 secs] 1677824K->24417K(8178944K), 0.0251010 secs] [Times: user=0.24 sys=0.07, real=0.02 secs]
2016-02-25T15:12:46.498+0800: 531.324: [GC [1 CMS-initial-mark: 0K(6291456K)] 780481K(8178944K), 0.0554170 secs] [Times: user=0.06 sys=0.00, real=0.07 secs]
2016-02-25T15:12:46.567+0800: 531.393: [CMS-concurrent-mark-start]
2016-02-25T15:12:46.574+0800: 531.400: [CMS-concurrent-mark: 0.006/0.007 secs] [Times: user=0.07 sys=0.02, real=0.01 secs]
2016-02-25T15:12:46.574+0800: 531.400: [CMS-concurrent-preclean-start]
2016-02-25T15:12:46.589+0800: 531.415: [CMS-concurrent-preclean: 0.015/0.015 secs] [Times: user=0.16 sys=0.06, real=0.01 secs]
{code}
It seems this is not the main reason. Gc of time before datanode exiting seems normal.
{code}
2016-02-25T14:45:39.743+0800: 5431411.796: [GC2016-02-25T14:45:39.743+0800: 5431411.796: [ParNew: 1686799K->22696K(1887488K), 0.0385700 secs] 2908579K->1244476K(8178944K) icms_dc=0 , 0.0388280 secs] [Times: user=0.23 sys=0.01, real=0.04 secs]
{code}
So it looks confusion. Attach the complete gc logs and datanode log."
inconsistent message while running rename command if target exists,HDFS-9824,"In the following case, the message <mv: `/tmp/src/1.log': Input/output error> is not friendly, it's better to show <mv: `/tmp/dest/1.log': File exists>.

Source dir:
{noformat}
-rw-r--r--   3 root hdfs       8526 2016-02-18 00:23 /tmp/src/1.log
{noformat}
Dest dir:
{noformat}
-rw-r--r--   3 root hdfs       8526 2016-02-17 22:00 /tmp/dest/1.log
-rw-r--r--   3 root hdfs       8526 2016-02-18 00:17 /tmp/dest/2.log
-rw-r--r--   3 root hdfs       8526 2016-02-18 00:18 /tmp/dest/3.log
-rw-r--r--   3 root hdfs       8526 2016-02-18 00:18 /tmp/dest/4.log
{noformat}
Running <hadoop fs -mv /tmp/src/1.log /tmp/dest> displays inconsistent message that complains input/output error, while <hadoop fs -mv /tmp/src/1.log /tmp/dest/1.log> will show <mv: `/tmp/dest/1.log': File exists>. The behavior of the two should be similar."
Make DelegationTokenFetcher a Tool,HDFS-9846,"Currently the {{org.apache.hadoop.hdfs.tools.DelegationTokenFetcher}} is not implementing the {{Tool}} interface, while it should.

This jira is to track the effort of refactoring the code to implement the {{Tool}} interface. The main benefits are unified generic option parsing, modifying the configurations, and conjunction with {{ToolRunner}}."
HDFS Non DFS Used (History server),HDFS-9813,"<Reproduce Steps>
1. su hdfs
2. cd /home/hdfs
3. hadoop jar tkfc-static-wordlist-10.jar 1 issue money 20150601 (extracting data elements from webpages)

<Actual Result>
1. usage non used DFS Continued to increase
2. HDFS service will shut down"
PeerCache evicts too frequently causing connection restablishments,HDFS-9520,"Env: 20 node setup
dfs.client.socketcache.capacity = 16

Issue:
======
Monitored PeerCache and it was evicting lots of connections during close. Set ""dfs.client.socketcache.capacity=20"" and tested again. Evictions still happened. Screenshot of profiler is attached in the JIRA.

Workaround:
===========
Temp fix was to set ""dfs.client.socketcache.capacity=1000"" to prevent eviction. 


Added more debug logs revealed that multimap.size() was 40 instead of 20. LinkedListMultimap returns the total values instead of key size causing lots of evictions.

{code}
   if (capacity == multimap.size()) {
      evictOldest();
    }
{code}

Should this be (capacity == multimap.keySet().size())  or is it expected that the ""dfs.client.socketcache.capacity"" be set to very high value?

\cc [~gopalv], [~sseth]
"
Read apis in ByteRangeInputStream does not read all the bytes specified when chunked transfer-encoding is used in the server,HDFS-8943,"With the default Webhdfs server implementation the read apis in ByteRangeInputStream work as expected reading the correct number of bytes for these apis :

{{public int read(byte b[], int off, int len)}}

{{public int read(long position, byte[] buffer, int offset, int length)}}

But when a custom Webhdfs server implementation is plugged in which uses chunked Transfer-encoding, these apis read only the first chunk. Simple fix would be to loop and read till bytes specified similar to {{readfully()}}"
o.a.h.hdfs.TestRecoverStripedFile fails intermittently in trunk,HDFS-9716,"See recent builds:
* https://builds.apache.org/job/PreCommit-HDFS-Build/14269/testReport/org.apache.hadoop.hdfs/TestRecoverStripedFile/testRecoverThreeDataBlocks1/
* https://builds.apache.org/job/PreCommit-HADOOP-Build/8477/testReport/org.apache.hadoop.hdfs/TestRecoverStripedFile/testRecoverThreeDataBlocks/"
Use a throttler for replica write in datanode,HDFS-7265,"BlockReceiver process packets in BlockReceiver.receivePacket() as follows
# read from socket
# enqueue the ack
# write to downstream
# write to disk

The above steps is repeated for each packet in a single thread.  When there are a lot of concurrent writes in a datanode, the write time in #4 becomes very long.  As a result, it leads to SocketTimeoutException since it cannot read from the socket for a long time."
Long running Balancer should renew TGT,HDFS-9698,"When the {{Balancer}} runs beyond the configured TGT lifetime, the current logic won't renew TGT."
HDFS file append failing in single node configuration,HDFS-6953,"The following issue happens in both fully distributed and single node setup. 
I have looked to the thread(https://issues.apache.org/jira/browse/HDFS-4600) about simiral issue in multinode cluster and made some changes of my configuration however it does not changed anything. The configuration files and application sources are attached.

Steps to reproduce:

$ ./test_hdfs

2014-08-27 14:23:08,472 WARN  [Thread-5] hdfs.DFSClient (DFSOutputStream.java:run(628)) - DataStreamer Exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[127.0.0.1:50010], original=[127.0.0.1:50010]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:969)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1035)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1184)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:532)
FSDataOutputStream#close error:
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[127.0.0.1:50010], original=[127.0.0.1:50010]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:969)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1035)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1184)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:532)

I have tried to run a simple example in java, that uses append function. It failed too.

I have tried to get hadoop environment settings from java application. It has shown the default ones. Not the settings that ones that are mentioned in core-site.xml and hdfs-site.xml files.
"
WebHDFS write commands fail when running multiple DataNodes on one machine,HDFS-8701,"For testing purposes, we are running multiple DataNodes per machine. {{hadoop fs}} commands work fine when using the {{hdfs://}} protocol, but when using {{webhdfs://}}, any command that writes to HDFS (e.g.: {{-put}} or {{-touchz}}) fails:
{code}
$ hadoop fs -put test.txt webhdfs://<host>:<port>/user/foo
put: <machine>-dn-4
{code}"
Erasure Coding: Move DFSStripedIO stream related classes to hadoop-hdfs-client,HDFS-9172,"The idea of this jira is to move the striped stream related classes to {{hadoop-hdfs-client}} project. This will help to be in sync with the HDFS-6200 proposal.

- DFSStripedInputStream
- DFSStripedOutputStream
- StripedDataStreamer"
FileContext tests fail on Windows,HDFS-815,"The following FileContext-related tests are failing on windows because of incorrect use ""test.build.data"" system property for setting hdfs paths, which end up containing ""C:"" as a path component, which hdfs does not support.
{code}
org.apache.hadoop.fs.TestFcHdfsCreateMkdir
org.apache.hadoop.fs.TestFcHdfsPermission
org.apache.hadoop.fs.TestHDFSFileContextMainOperations
org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics
{code}"
Wrong checking penultimate block replicated in FSNamesystem,HDFS-9662,"There is a bug in checking penultimate block replicated in FSNamesystem.
{code}
/**
   * Check that the indicated file's blocks are present and
   * replicated.  If not, return false. If checkall is true, then check
   * all blocks, otherwise check only penultimate block.
   */
  boolean checkFileProgress(String src, INodeFile v, boolean checkall) {
    assert hasReadLock();
    if (checkall) {
      return blockManager.checkBlocksProperlyReplicated(src, v
          .getBlocks());
    } else {
      // check the penultimate block of this file
      BlockInfo b = v.getPenultimateBlock();
      return b == null ||
          blockManager.checkBlocksProperlyReplicated(
              src, new BlockInfo[] { b });
    }
  }
{code}
When the param checkall is true, the checking operations is true.But if checkall is false, it will check the penultimate block of this file. And if the BlockInfo b is null, it will return true by this code, but actually it should be return false because the penultimate block is not replicated and has no blockInfo."
Erasure Coding: cover more test situations of datanode failure during client writing,HDFS-8889,"Currently 9 streamers are working together for the client writing. A small number of failed datanodes (<= 3) for a block group should not influence the writing. There鈥檙e a lot of datanode failure cases and we should cover as many as possible in unit test.
Suppose streamer 4 fails, the following situations for the next block group should be considered:
1)	all streamers succeed
2)	Streamer 4 still fails
3)	only streamer 1 fails
4)	only streamer 8 fails (test parity streamer)
5)	streamer 4 and 6 fail
6)	streamer 4 and 1,6 fail
7)	streamer 4 and 1,2,6 fail
8)	streamer 2, 6 fail
Suppose streamer 2 and 4 fail, the following situations for the next block group should be considered:
1)	only streamer 2 and 4 fail
2)	streamer 2, 4, 8 fail
3)	only streamer 2 fails
4)	streamer 3 , 8 fail
For a single streamer, we should consider the following situations of the time of datanode failure:
1)	before writing the first byte
2)	before finishing writing the first cell
3)	right after finishing writing the first cell
4)	before writing the last byte of the block
Other situations:
1)	more than 3 streamers fail at the first block group
2)	more than 3 streamers fail at the last block group
<more 鈥?"
"my java client use muti-thread to put a same file to a same hdfs uri, after no lease error锛宼hen client OutOfMemoryError",HDFS-9617,"org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /Tmp2/43.bmp.tmp (inode 2913263): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_2084151715_1, pendingcreates: 250]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3358)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3160)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3042)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:615)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:188)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:476)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1653)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy14.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:391)
	at sun.reflect.GeneratedMethodAccessor66.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy15.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1473)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1290)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:536)


my java client(JVM -Xmx=2G) :
jmap TOP15锛?num     #instances         #bytes  class name
----------------------------------------------
   1:         48072     2053976792  [B
   2:         45852        5987568  <constMethodKlass>
   3:         45852        5878944  <methodKlass>
   4:          3363        4193112  <constantPoolKlass>
   5:          3363        2548168  <instanceKlassKlass>
   6:          2733        2299008  <constantPoolCacheKlass>
   7:           533        2191696  [Ljava.nio.ByteBuffer;
   8:         24733        2026600  [C
   9:         31287        2002368  org.apache.hadoop.hdfs.DFSOutputStream$Packet
  10:         31972         767328  java.util.LinkedList$Node
  11:         22845         548280  java.lang.String
  12:         20372         488928  java.util.concurrent.atomic.AtomicLong
  13:          3700         452984  java.lang.Class
  14:           981         439576  <methodDataKlass>
  15:          5583         376344  [S"
Add valgrind suppression for statically initialized library objects,HDFS-9192,"When using --leak-check=full there's a lot of noise due to static initialization of constants and memory pools, most of them from protobuf.

Add a suppression file that helps cut down on this noise but is selective enough that real issues aren't going to be masked as well."
Fix NPE in MiniKMS.start(),HDFS-9508,"Sometimes, KMS resource file can not be loaded. When this happens, an InputStream variable will be a null pointer which will subsequently throw NPE.

This is a supportability JIRA that makes the error message more explicit, and explain why NPE is thrown. Ultimately, leads us to understand why the resource files can not be loaded."
NameNode and DataNode metric log file name should follow the other log file name format.,HDFS-9114,"Currently datanode and namenode metric log file name is {{datanode-metrics.log}} and {{namenode-metrics.log}}.
This file name should be like {{hadoop-hdfs-namenode-metric-host192.log}} same as namenode log file {{hadoop-hdfs-namenode-host192.log}}.
This will help when we will copy log for issue analysis from different node.
"
LlapServiceDriver can fail if only the packaged logger config is present,HDFS-9567,"I was incrementally updating my setup on some VM and didn't have the logger config file, so the packaged one was picked up apparently, which caused this:
{noformat}
java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: jar:file:/home/vagrant/llap/apache-hive-2.0.0-SNAPSHOT-bin/lib/hive-llap-server-2.0.0-SNAPSHOT.jar!/llap-daemon-log4j2.properties
	at org.apache.hadoop.fs.Path.initialize(Path.java:205)
	at org.apache.hadoop.fs.Path.<init>(Path.java:171)
	at org.apache.hadoop.hive.llap.cli.LlapServiceDriver.run(LlapServiceDriver.java:234)
	at org.apache.hadoop.hive.llap.cli.LlapServiceDriver.main(LlapServiceDriver.java:58)
Caused by: java.net.URISyntaxException: Relative path in absolute URI: jar:file:/home/vagrant/llap/apache-hive-2.0.0-SNAPSHOT-bin/lib/hive-llap-server-2.0.0-SNAPSHOT.jar!/llap-daemon-log4j2.properties
	at java.net.URI.checkPath(URI.java:1823)
	at java.net.URI.<init>(URI.java:745)
	at org.apache.hadoop.fs.Path.initialize(Path.java:202)
	... 3 more
{noformat}"
standby nn can't started,HDFS-8011,"We have seen crash when starting the standby namenode, with fatal errors. Any solutions, workarouds, or ideas would be helpful for us.
1. Here is the context: 
	At begining we have 2 namenodes, take A as active and B as standby. For some resons, namenode A was dead, so namenode B is working as active.
	When we try to restart A after a minute, it can't work. During this time a lot of files were put to HDFS, and a lot of files were renamed. 
	Nodenode A crashed when ""awaiting reported blocks in safemode"" each time.
 
2. We can see error log below:
	1)2015-03-30  ERROR org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: Encountered exception on operation CloseOp [length=0, inodeId=0, path=/xxx/_temporary/xxx/part-r-00074.bz2, replication=3, mtime=1427699913947, atime=1427699081161, blockSize=268435456, blocks=[blk_2103131025_1100889495739], permissions=dm:dm:rw-r--r--, clientName=, clientMachine=, opCode=OP_CLOSE, txid=7632753612]
java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoUnderConstruction.setGenerationStampAndVerifyReplicas(BlockInfoUnderConstruction.java:247)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoUnderConstruction.commitBlock(BlockInfoUnderConstruction.java:267)
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.forceCompleteBlock(BlockManager.java:639)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.updateBlocks(FSEditLogLoader.java:813)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:383)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:209)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:122)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:737)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:227)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:321)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$0(EditLogTailer.java:302)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:296)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:356)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1528)
        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:413)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:292)
        
   2)2015-03-30  FATAL org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unknown error encountered while tailing edits. Shutting down standby N
N.
java.io.IOException: Failed to apply edit log operation AddBlockOp [path=/xxx/_temporary/xxx/part-m-00121, penultimateBlock=blk_2102331803_1100888911441, lastBlock=blk_2102661068_1100889009168, RpcClientId=, RpcCallId=-2]: error
null
        at org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext.editLogLoaderPrompt(MetaRecoveryContext.java:94)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:215)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:122)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:737)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:227)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:321)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$0(EditLogTailer.java:302)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:296)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:356)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1528)
        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:413)
        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:292)
        
"
Confusing WebHDFS exception when host doesn't resolve,HDFS-4488,"{noformat}
$ hadoop fs -ls webhdfs://unresolvable-host/
ls: unresolvable-host
$ echo $?
1
{noformat}"
Re-replication for files with enough replicas in single rack,HDFS-9127,"Found while debugging testcases in HDFS-8647

 *Scenario:* 
=======

Start a cluster with Single rack with three DN's
write a file with RF=3
adde two Nodes with different racks

As per blockplacement policy ([Rack Awareness|http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/RackAwareness.html]) atleast one replica needs to replicate to newly added rack.But it is not happening..Because of following reason.

{color:blue}
when cluster was single rack,block will be removed from {{neededReplications}} after 3 replicas.

later, after adding new rack, only replications will happen which are present in {{neededReplications}}

So for the blocks which already have enough replicas, new rack replications will not take place..
{color}"
"Balancer exits if fs.defaultFS is set to a different, but semantically identical, URI from dfs.namenode.rpc-address",HDFS-3439,"The balancer determines the set of NN URIs to balance by looking at fs.defaultFS and all possible dfs.namenode.(service)rpc-address settings. If fs.defaultFS is, for example, set to ""hdfs://foo.example.com:8020/"" (note the trailing ""/"") and the rpc-address is set to ""hdfs://foo.example.com:8020"" (without a ""/""), then the balancer will conclude that there are two NNs and try to balance both. However, since both of these URIs refer to the same actual FS instance, the balancer will exit with ""java.io.IOException: Another balancer is running.  Exiting ..."""
balancer.Balancer: java.lang.NullPointerException while HADOOP_CONF_DIR is empty or wrong,HDFS-2220,"When HADOOP_CONF_DIR is empty or wrongly set and balancer is called without proper --config , in clientside STDOUT we get NPE.

$ echo $HADOOP_CONF_DIR
$ hadoop balancer
Balancing took 46.0 milliseconds
11/06/13 05:14:04 ERROR balancer.Balancer: java.lang.NullPointerException
        at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:136)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:176)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:206)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.getServiceAddress(NameNode.java:200)
        at org.apache.hadoop.hdfs.server.balancer.Balancer.createNamenode(Balancer.java:911)
        at org.apache.hadoop.hdfs.server.balancer.Balancer.init(Balancer.java:860)
        at org.apache.hadoop.hdfs.server.balancer.Balancer.run(Balancer.java:1475)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.hdfs.server.balancer.Balancer.main(Balancer.java:811)

I think it would be good to give more meaningful error messege instead of NPE"
NN old UI (block_info_xml) not available in 2.7.x,HDFS-9064,"In 2.6.x hadoop deploys, given a blockId it was very easy to find out the file name and the locations of replicas (also whether they are corrupt or not).
This was the REST call:
{noformat}
 http://<nnAddress>:<port#>/block_info_xml.jsp?blockId=xxx
{noformat}
But this was removed by HDFS-6252 in 2.7 builds.
Creating this jira to restore that functionality."
ACL permission check does not union groups to determine effective permissions,HDFS-8748,"In the ACL permission checking routine, the implemented named group section does not match the design document.

In the design document, its shown in the pseudo-code that if the requester is not the owner or a named user, then the applicable groups are unioned together to form effective permissions for the requester.


Instead, the current implementation will search for the first group that grants access and will use that. It will not union the permissions together.

Here is the design document's description of the desired behavior
{quote}
If the user is a member of the file's group or at least one group for which there is a
named group entry in the ACL, then effective permissions are calculated from groups.
This is the union of the file group permissions (if the user is a member of the file group)
and all named group entries matching the user's groups. For example, consider a user
that is a member of 2 groups: sales and execs. The user is not the file owner, and the
ACL contains no named user entries. The ACL contains named group entries for both
groups as follows: group:sales:r颅颅\-\-, group:execs:\-颅w\-颅. In this case, the user's effective
permissions are rw颅-.
{quote}

 ??https://issues.apache.org/jira/secure/attachment/12627729/HDFS-ACLs-Design-3.pdf page 10??

The design document's algorithm matches that description:

*Design Document Algorithm*
{code:title=DesignDocument}
if (user == fileOwner) {
    effectivePermissions = aclEntries.getOwnerPermissions()
} else if (user 鈭?aclEntries.getNamedUsers()) {
    effectivePermissions = aclEntries.getNamedUserPermissions(user)
} else if (userGroupsInAcl != 鈭? {
    effectivePermissions = 鈭?    if (fileGroup 鈭?userGroupsInAcl) {
        effectivePermissions = effectivePermissions 鈭?        aclEntries.getGroupPermissions()
    }
    for ({group | group 鈭?userGroupsInAcl}) {
        effectivePermissions = effectivePermissions 鈭?        aclEntries.getNamedGroupPermissions(group)
    }
} else {
    effectivePermissions = aclEntries.getOthersPermissions()
}
{code}
??https://issues.apache.org/jira/secure/attachment/12627729/HDFS-ACLs-Design-3.pdf page 9??

The current implementation does NOT match the description.
*Current Trunk*
{code:title=FSPermissionChecker.java}
    // Use owner entry from permission bits if user is owner.
    if (getUser().equals(inode.getUserName())) {
      if (mode.getUserAction().implies(access)) {
        return;
      }
      foundMatch = true;
    }

    // Check named user and group entries if user was not denied by owner entry.
    if (!foundMatch) {
      for (int pos = 0, entry; pos < aclFeature.getEntriesSize(); pos++) {
        entry = aclFeature.getEntryAt(pos);
        if (AclEntryStatusFormat.getScope(entry) == AclEntryScope.DEFAULT) {
          break;
        }
        AclEntryType type = AclEntryStatusFormat.getType(entry);
        String name = AclEntryStatusFormat.getName(entry);
        if (type == AclEntryType.USER) {
          // Use named user entry with mask from permission bits applied if user
          // matches name.
          if (getUser().equals(name)) {
            FsAction masked = AclEntryStatusFormat.getPermission(entry).and(
                mode.getGroupAction());
            if (masked.implies(access)) {
              return;
            }
            foundMatch = true;
            break;
          }
        } else if (type == AclEntryType.GROUP) {
          // Use group entry (unnamed or named) with mask from permission bits
          // applied if user is a member and entry grants access.  If user is a
          // member of multiple groups that have entries that grant access, then
          // it doesn't matter which is chosen, so exit early after first match.
          String group = name == null ? inode.getGroupName() : name;
          if (getGroups().contains(group)) {
            FsAction masked = AclEntryStatusFormat.getPermission(entry).and(
                mode.getGroupAction());
            if (masked.implies(access)) {
              return;
            }
            foundMatch = true;
          }
        }
      }
    }
{code}

As seen in the GROUP section, the permissions check will succeed if and only if a single group (either owning group or named group) has all of the requested permissions. The permissions check should instead succeed if the requested permissions can be obtained by unioning all of the groups permissions."
Move the quota commands out from dfsadmi.,HDFS-8638,"Currently for setQuota() API in FSNamesystem we don't have any superuser check.
So with the reference of [HDFS-7323 | https://issues.apache.org/jira/browse/HDFS-7323] we should move quota commands from dfsadmin or if we want it in dfsadmin then we should add check for superuser privileges. 
"
resolved,HDFS-8318,resolved
Get HDFS file name based on block pool id and block id,HDFS-8246,"This feature provides HDFS shell command and C/Java API to retrieve HDFS file name based on block pool id and block id.

1. The Java API in class DistributedFileSystem
public String getFileName(String poolId, long blockId) throws IOException
2. The C API in hdfs.c
char* hdfsGetFileName(hdfsFS fs, const char* poolId, int64_t blockId)
3. The HDFS shell command 
 hdfs dfs [generic options] -fn <poolId> <blockId>

This feature is useful if you have HDFS block file name in local file system and want to  find out the related HDFS file name in HDFS name space (http://stackoverflow.com/questions/10881449/how-to-find-file-from-blockname-in-hdfs-hadoop).  Each HDFS block file name in local file system contains both block pool id and block id, for sample HDFS block file name /hdfs/1/hadoop/hdfs/data/current/BP-97622798-10.3.11.84-1428081035160/current/finalized/subdir0/subdir0/blk_1073741825,  the block pool id is BP-97622798-10.3.11.84-1428081035160 and the block id is 1073741825. The block  pool id is uniquely related to a HDFS name node/name space,  and the block id is uniquely related to a HDFS file within a HDFS name node/name space, so the combination of block pool id and a block id is uniquely related a HDFS file name. 

The shell command and C/Java API do not map the block pool id to name node, so it鈥檚 user鈥檚 responsibility to talk to the correct name node in federation environment that has multiple name nodes. The block pool id is used by name node to check if the user is talking with the correct name node.

The implementation is straightforward. The client request to get HDFS file name reaches the new method String getFileName(String poolId, long blockId) in FSNamesystem in name node through RPC,  and the new method does the followings,
(1)	Validate the block pool id.
(2)	Create Block  based on the block id.
(3)	Get BlockInfoContiguous from Block.
(4)	Get BlockCollection from BlockInfoContiguous.
(5)	Get file name from BlockCollection.
"
Erasure Coding: client fails to write large file when one datanode fails,HDFS-8704,"I test current code on a 5-node cluster using RS(3,2).  When a datanode is corrupt, client succeeds to write a file smaller than a block group but fails to write a large one. {{TestDFSStripeOutputStreamWithFailure}} only tests files smaller than a block group, this jira will add more test situations.


A streamer may encounter some bad datanodes when writing blocks allocated to it. When it fails to connect datanode or send a packet, the streamer needs to prepare for the next block. First it removes the packets of current  block from its data queue. If the first packet of next block has already been in the data queue, the streamer will reset its state and start to wait for the next block allocated for it; otherwise it will just wait for the first packet of next block. The streamer will check periodically if it is asked to terminate during its waiting.
"
A bug in BlocksMap that  cause NameNode  memory leak.,HDFS-7592,"In our HDFS production environment, NameNode FGC frequently after running for 2 months, we have to restart NameNode manually.
We dumped NameNode's Heap for objects statistics.
Before restarting NameNode:
    num #instances #bytes class name
    ----------------------------------------------
聽聽聽     1: 59262275 3613989480 [Ljava.lang.Object;
聽聽聽聽聽聽聽     ...
聽聽      10: 8549361 615553992 org.apache.hadoop.hdfs.server.namenode.BlockInfoUnderConstruction
聽聽      11: 5941511 427788792 org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction
After restarting NameNode:
    num #instances #bytes class name
    ----------------------------------------------
聽聽聽      1: 44188391 2934099616 [Ljava.lang.Object;
聽聽聽聽聽聽聽聽聽     ...
聽聽      23: 721763 51966936 org.apache.hadoop.hdfs.server.namenode.BlockInfoUnderConstruction
聽聽      24: 620028 44642016 org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction
We find the number of BlockInfoUnderConstruction is abnormally large before restarting NameNode.
As we know, BlockInfoUnderConstruction keeps block state when the file is being written. But the write pressure of
our cluster is far less than million/sec. We think there is a memory leak in NameNode.
We fixed the bug as followsing patch.
diff --git a/hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlocksMap.java b/hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlocksMap.java
index 7a40522..857d340 100644
--- a/hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlocksMap.java
+++ b/hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlocksMap.java
@@ -205,6 +205,8 @@ class BlocksMap {
       DatanodeDescriptor dn = currentBlock.getDatanode(idx);
       dn.replaceBlock(currentBlock, newBlock);
     }
+    // change to fix bug about memory leak of NameNode
+    map.remove(newBlock);
     // replace block in the map itself
     map.put(newBlock, newBlock);
     return newBlock;"
Delegation token is not created generateNodeDataHeader method of NamenodeJspHelper$NodeListJsp,HDFS-5263,"When Kerberos authentication is enabled, we are unable to browse to the data nodes using ( Name node web page --> Live Nodes --> Select any of the data nodes). The reason behind this is the delegation token is not provided as part of the url in the method (generateNodeDataHeader method of NodeListJsp)

{code}
      String url = HttpConfig.getSchemePrefix() + d.getHostName() + "":""
          + d.getInfoPort()
          + ""/browseDirectory.jsp?namenodeInfoPort="" + nnHttpPort + ""&dir=""
          + URLEncoder.encode(""/"", ""UTF-8"")
          + JspHelper.getUrlParam(JspHelper.NAMENODE_ADDRESS, nnaddr);
{code}

But browsing the file system using name node web page --> Browse the file system -> <any directory> is working fine as the redirectToRandomDataNode method of NamenodeJspHelper creates the delegation token

{code}
    redirectLocation = HttpConfig.getSchemePrefix() + fqdn + "":"" + redirectPort
        + ""/browseDirectory.jsp?namenodeInfoPort=""
        + nn.getHttpAddress().getPort() + ""&dir=/""
        + (tokenString == null ? """" :
           JspHelper.getDelegationTokenUrlParam(tokenString))
        + JspHelper.getUrlParam(JspHelper.NAMENODE_ADDRESS, addr);
{code}

I will work on providing a patch for this issue."
FSImage.getFsImageName should check whether fsimage exists,HDFS-5396,"In https://issues.apache.org/jira/browse/HDFS-5367, fsimage may not write to all IMAGE dir, so we need to check whether fsimage exists before FSImage.getFsImageName returned."
Create EC zone should not need superuser privilege,HDFS-8333,"create EC zone should not need superuser privilege, for example, in multiple tenant scenario, common users only manage their own directory and subdirectory."
hadoop-hdfs-client dependency convergence error,HDFS-8128,"Found the following in https://builds.apache.org/job/PreCommit-HDFS-Build/10258/consoleFull
{noformat}
[WARNING] Rule 0: org.apache.maven.plugins.enforcer.DependencyConvergence failed with message:
Failed while enforcing releasability the error(s) are [
Dependency convergence error for org.apache.hadoop:hadoop-annotations:3.0.0-SNAPSHOT paths to dependency are:
+-org.apache.hadoop:hadoop-hdfs-client:3.0.0-SNAPSHOT
  +-org.apache.hadoop:hadoop-common:3.0.0-SNAPSHOT
    +-org.apache.hadoop:hadoop-annotations:3.0.0-SNAPSHOT
and
+-org.apache.hadoop:hadoop-hdfs-client:3.0.0-SNAPSHOT
  +-org.apache.hadoop:hadoop-annotations:3.0.0-20150410.234534-6484
]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
{noformat}"
remove replica and not add replica with wrong genStamp,HDFS-9298,"currently, in setGenerationStampAndVerifyReplicas, replica with wrong gen stamp is not really removed, only StorageLocation of that replica is removed. Moreover, we should check genStamp before addReplicaIfNotPresent"
"Erasure Coding: unifying common constructs like coding work, block reader and block writer across client and DataNode",HDFS-7679,"Based on the work done, we will have similar constructs like coding work, local/remote block reader/writer in both client and DataNode side, so it's possible to refactor the codes further and unify these constructs to eliminate possible duplicate codes."
Block Readers and Writers used in both client side and datanode side,HDFS-7653,"There're a lot of block read/write operations in HDFS-EC, for example, when client writes a file in striping layout, client has to write several blocks to several different datanodes; if a datanode wants to do an encoding/decoding task, it has to read several blocks from itself and other datanodes, and writes one or more blocks to itself or other datanodes.  "
Erasure Coding: Update last cellsize calculation according to whether the erasure codec has chunk boundary,HDFS-8376,"Current calculation for last cell size is as following. For parity cell, the last cell size is the same as the first data cell.  But some erasure codec has chunk boundary, then the last cellsize for parity block is the codec chunk size.
{code}
private static int lastCellSize(int size, int cellSize, int numDataBlocks,
      int i) {
    if (i < numDataBlocks) {
      // parity block size (i.e. i >= numDataBlocks) is the same as 
      // the first data block size (i.e. i = 0).
      size -= i*cellSize;
      if (size < 0) {
        size = 0;
      }
    }
    return size > cellSize? cellSize: size;
  }
{code}"
"Namenode shutdown for ""ReplicationMonitor thread received Runtime exception""",HDFS-8426,"1.Last git commit code : 041c936e3b677f9d61e8a2c5deb20e7b2dd8292a
2.NameNode log for this error: [^5-15 NameNode shutdow log segment]"
hdfs-test artifact doesn't include config file for cluster execution,HDFS-2442,With HDFS-1762 in place testConfCluster.xml needs to be packaged along with test classes so it can be used for testing on a real cluster.
Separate Platform specific funtions,HDFS-7768,"Current code has several platform-specific parts (e.g., get environment variables, get local addresses, print stack). We should separate these parts into platform folders.

This issue will do just that. Posix systems will be able to compile successfully. Windows will fail to compile due to unimplemented parts. The implementation for the Windows parts will be handle at HDFS-7188 "
Turn off TestDFSOverAvroRpc,HDFS-2660,"With HDFS-2647, protobuf based RPCs are enabled for some of the protocol. With this, Avro RPC based protocol is not working. Avro based RPC needs to be turned on similar to how protobuf based RPCs are done. Until such a time, I propose turning off the test."
HDFS ivy dependencies aren't on classpath of bin/hdfs script,HDFS-1771,"In working on another patch, I added guava as a dependency of HDFS. It ended up in ./build/ivy/lib/hadoop-hdfs/common/guava-r07.jar, but running bin/hdfs doesn't put this directory on the classpath. Instead it only puts the ivy dependencies of Common"
Remove TestDFSOverAvroRpc,HDFS-2664,"With HDFS-2647, HDFS has transitioned to protocol buffers. The server side implementation registers <Protocol>PB.class and a BlockingService as implementation. Client side uses <Protocol>PB.class as the interface. The RPC engine used is protobuf both for the RPC proxy and the server. With this TestDFSOverAvroRpc fails. I propose removing this test."
TestDfsOverAvroRpc is failing on trunk,HDFS-2298,"The relevant bit of the error:

{noformat}
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.hdfs.TestDfsOverAvroRpc
-------------------------------------------------------------------------------
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1.486 sec <<< FAILURE!
testWorkingDirectory(org.apache.hadoop.hdfs.TestDfsOverAvroRpc)  Time elapsed: 1.424 sec  <<< ERROR!
org.apache.avro.AvroTypeException: Two methods with same name: delete
{noformat}"
"""hdfs version"" should print out information similar to what ""hadoop version"" prints out",HDFS-2119,Implement version in hdfs CLI
TestNNWithQJM#testNewNamenodeTakesOverWriter occasionally fails in trunk,HDFS-5897,"From https://builds.apache.org/job/Hadoop-Hdfs-trunk/1665/testReport/junit/org.apache.hadoop.hdfs.qjournal/TestNNWithQJM/testNewNamenodeTakesOverWriter/ :
{code}
java.lang.Exception: test timed out after 30000 milliseconds
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.read(SocketInputStream.java:129)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:258)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:317)
	at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:687)
	at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:632)
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1195)
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:379)
	at org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream$URLLog$1.run(EditLogFileInputStream.java:412)
	at org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream$URLLog$1.run(EditLogFileInputStream.java:401)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
{code}
I saw:
{code}
2014-02-06 11:38:37,970 ERROR namenode.EditLogInputStream (RedundantEditLogInputStream.java:nextOp(221)) - Got error reading edit log input stream http://localhost:40509/getJournal?jid=myjournal&segmentTxId=3&storageInfo=-51%3A1571339494%3A0%3AtestClusterID; failing over to edit log http://localhost:56244/getJournal?jid=myjournal&segmentTxId=3&storageInfo=-51%3A1571339494%3A0%3AtestClusterID
org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream$PrematureEOFException: got premature end-of-file at txid 0; expected file to go up to 4
	at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:194)
	at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:83)
	at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.skipUntil(EditLogInputStream.java:140)
	at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:178)
	at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:83)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:167)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:120)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:708)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:606)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:263)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:874)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:634)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:446)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:502)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:643)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1291)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:939)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:824)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:678)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:359)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:340)
	at org.apache.hadoop.hdfs.qjournal.TestNNWithQJM.testNewNamenodeTakesOverWriter(TestNNWithQJM.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
{code}"
BlockInfo#numNodes should be numStorages,HDFS-8784,The method actually returns the number of storages holding a block.
move hasClusterEverBeenMultiRack to NetworkTopology,HDFS-8689,
Convert BlockInfoUnderConstruction as an interface,HDFS-8835,"Per discussion under HDFS-8499, this JIRA aims to convert {{BlockInfoUnderConstruction}} as an interface and {{BlockInfoContiguousUnderConstruction}} as its implementation. The HDFS-7285 branch will add {{BlockInfoStripedUnderConstruction}} as another implementation."
Implement ShrinkableHashMap extends java HashMap and use properly,HDFS-8912,"Currently {{LightWeightHashSet}} and {{LightWeightLinkedSet}} are used in hdfs, there are two advantages compared to java HashSet: one is the entry requires fewer memory, another is it's shrinkable.  In real cluster, hdfs is a long running service, and {{set}} may become large at some time and may become small after that, so shrinking the {{set}} when size hits the shrink threshold is necessary, it can improve the NN memory.

Same situation for {{map}}, some HashMap used in BlockManager (e.g., the hashmap in CorruptReplicasMap), it's better to be shrinkable. 
 I think it's worth to implement ShrinkableHashMap extends the java HashMap, for quick glance, seems few code is needed."
OzoneHandler : Enable stand-alone local testing mode,HDFS-8756,"Enable ""jetty run"" in pom.xml , this will allow Ozone server to run in a stand-alone mode not as part of WebHDFS. This will allow a development mode for ozone where ozone will listen on Port 8080 and commands can be run against Ozone. This is strictly used for development purposes."
"S3:Hadoop tools jars should be added in ""dfs"" class path.",HDFS-8685,"{code}
./hdfs dfs -ls s3a://xyz:xyz/
-ls: Fatal internal error
java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
        at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2224)
        at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2638)
{code}"
TestPersistBlocks#TestRestartDfsWithFlush appears to be flaky,HDFS-3811,"This test failed on a recent Jenkins build, but passes for me locally. Seems flaky.

See:

https://builds.apache.org/job/PreCommit-HDFS-Build/3021//testReport/org.apache.hadoop.hdfs/TestPersistBlocks/TestRestartDfsWithFlush/"
TestDatanodeBlockScanner.testBlockCorruptionRecoveryPolicy1 times out,HDFS-3532,I've seen this test time out on recent trunk jenkins test patch runs even though HDFS-3266 was put in a couple weeks ago.
Erasure Coding: DFSStripedOutputStream#close throws NullPointerException exception in some cases,HDFS-8313,"{code}
java.io.IOException: java.lang.NullPointerException
	at org.apache.hadoop.hdfs.DataStreamer$LastException.check(DataStreamer.java:193)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.closeImpl(DFSStripedOutputStream.java:422)
{code}

 DFSStripedOutputStream#close throws NullPointerException exception in some cases"
Track BlockInfo instead of Block in CorruptReplicasMap,HDFS-8652,"Currently {{CorruptReplicasMap}} uses {{Block}} as its key and records the list of DataNodes with corrupted replicas. For Erasure Coding since a striped block group contains multiple internal blocks with different block ID, we should use {{BlockInfo}} as the key.

HDFS-8619 is the jira to fix this for EC. To ease merging we will use jira to first make changes in trunk/branch-2.

"
ClassCastException in BlockManager.addStoredBlock() due to that blockReceived came after file was closed.,HDFS-4746,"In some cases the last block replica of a file can be reported after the file was closed. In this case file inode is of type INodeFile. BlockManager.addStoredBlock() though expects it to be INodeFileUnderConstruction, and therefore class cast to MutableBlockCollection fails."
"Always read DU value from the cached ""dfsUsed"" file on datanode startup",HDFS-8710,"Currently, DataNode will cache DU value in ""dfsUsed"" file termly. When DataNode starts or restarts, it will read in the cached DU value from ""dfsUsed"" file if the value is less than 600 seconds old, otherwise, it will run DU command, which is a very time-consuming operation(may up to dozens of minutes) when DataNode has huge number of blocks.

Since slight imprecision of dfsUsed is not critical, and the DU value will be updated every 600 seconds (the default DU interval) after DataNode started, we can always read DU value from the cached file (Regardless of whether this value is less than 600 seconds old or not) and skip DU operation on DataNode startup to significantly shorten the startup time.
"
"""no suitable constructor found"" while building Hadoop 2.6.0",HDFS-7724,"I'm getting the following error while building Hadoop 2.6.0. My objective is to compile Hadoop, and run Pig on it.

[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 01:25 min
[INFO] Finished at: 2015-02-02T14:38:51+05:30
[INFO] Final Memory: 49M/117M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hadoop-auth-examples: Compilation failure
[ERROR] D:\h\hadoop-2.6.0-src\hadoop-common-project\hadoop-auth-examples\src\main\java\org\apache\hadoop\security\authentication\examples\WhoClient.java:[36,31] error: no suitable constructor found for AuthenticatedURL(no arguments)
[ERROR] -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hadoop-auth-examples: Compilation failure
D:\h\hadoop-2.6.0-src\hadoop-common-project\hadoop-auth-examples\src\main\java\org\apache\hadoop\security\authentication\examples\WhoClient.java:[36,31] error: no suitable constructor found for AuthenticatedURL(no arguments)

	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:212)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:347)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:154)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:582)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:214)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:158)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: org.apache.maven.plugin.CompilationFailureException: Compilation failure
D:\h\hadoop-2.6.0-src\hadoop-common-project\hadoop-auth-examples\src\main\java\org\apache\hadoop\security\authentication\examples\WhoClient.java:[36,31] error: no suitable constructor found for AuthenticatedURL(no arguments)

	at org.apache.maven.plugin.AbstractCompilerMojo.execute(AbstractCompilerMojo.java:729)
	at org.apache.maven.plugin.CompilerMojo.execute(CompilerMojo.java:128)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
	... 19 more
[ERROR] 
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hadoop-auth-examples
"
Add and optimize for get LocatedFileStatus  in DFSClient,HDFS-8598,"If we want to get all files block locations in one directory, we have to call getFileBlockLocations for each file, it will take long time because of too many request. 
LocatedFileStatus has block location, but we can find it also call getFileBlockLocations  for each file in DFSClient. this jira is trying to optimize with only one RPC. "
2NN doesn't start with fs.defaultFS set to a viewfs URI unless service RPC address is also set,HDFS-3465,"Looks like the 2NN first tries servicerpc-address then falls back on fs.defaultFS, which won't work in the case of federation since fs.defaultFS doesn't refer to an RPC address. Instead, the 2NN should first check servicerpc-address, then rpc-address, then fall back on fs.defaultFS.

{noformat}
Exception in thread ""main"" java.lang.IllegalArgumentException: Invalid
URI for NameNode address (check fs.defaultFS): viewfs:/// has no
authority.
       at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:315)
       at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:303)
       at org.apache.hadoop.hdfs.server.namenode.NameNode.getServiceAddress(NameNode.java:296)
       at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.initialize(SecondaryNameNode.java:214)
       at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.<init>(SecondaryNameNode.java:178)
       at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.main(SecondaryNameNode.java:582)
{noformat}"
count with -h option displays namespace quota in human readable format,HDFS-8625,"When 'count' command is executed with '-h' option , namespace quota is displayed in human readable format --

Example :

hdfs dfsadmin -setQuota {color:red}1048576{color} /test

hdfs dfs -count -q -h -v /test
       {color:red}QUOTA       REM_QUOTA{color}     SPACE_QUOTA REM_SPACE_QUOTA    DIR_COUNT   FILE_COUNT       CONTENT_SIZE PATHNAME
         {color:red}1 M           1.0 M{color}            none             inf            1            0                  0 /test

QUOTA and REM_QUOTA shows 1 M (human readable format) which actually should give count value 1048576"
Implement GETDELEGATIONTOKEN and GETDELEGATIONTOKENS operation for WebImageViewer,HDFS-8621,"In Hadoop 2.7.0, WebImageViewer supports the following operations:
{code}
   .GETFILESTATUS
   .LISTSTATUS
   .GETACLSTATUS
{code}

I'm thinking it would be better for administrators if  {code} .GETDELEGATIONTOKEN  {code}  and  {code} .GETDELEGATIONTOKENS  {code}  are supported."
hadoop archive command looks at local files when using wildcard,HDFS-8514,"When using a wildcard in the {{hadoop archive}} command, it looks at the local filesystem.  For example:

{noformat}
>> [56] 18:13 : localdir :: hadoop fs -ls /tmp/dir
Found 2 items
-rw-r--r--   1 rkanter supergroup          0 2015-06-01 17:57 /tmp/dir/f1.txt
-rw-r--r--   1 rkanter supergroup          0 2015-06-01 17:57 /tmp/dir/f2.txt
>> [57] 18:13 : localdir :: ls -l
total 0
-rw-r--r--  1 rkanter  staff  0 Jun  1 18:11 local-file.log
-rw-r--r--  1 rkanter  staff  0 Jun  1 18:08 local-file.txt
>> [58] 18:14 : localdir :: hadoop archive -archiveName foo.har -p /tmp/dir/ * /tmp/
The resolved paths set is empty.  Please check whether the srcPaths exist, where srcPaths = [/tmp/dir/local-file.log, /tmp/dir/local-file.txt]
>> [59] 18:15 : localdir :: hadoop archive -archiveName foo.har -p /tmp/dir/ *.txt /tmp/
The resolved paths set is empty.  Please check whether the srcPaths exist, where srcPaths = [/tmp/dir/local-file.txt]
>> [60] 18:15 : localdir :: hadoop archive -archiveName foo.har -p hdfs://localhost:8020/tmp/dir/ *.txt /tmp/
The resolved paths set is empty.  Please check whether the srcPaths exist, where srcPaths = [hdfs://localhost:8020/tmp/dir/local-file.txt]
{noformat}"
Question: Why Namenode doesn't judge the status of replicas when convert block status from commited to complete? ,HDFS-8459,"  Why Namenode doesn't judge the status of replicas when convert block status from commited to complete?
  When client finished write block and call namenode::complete(), namenode do things as follow
  (in BlockManager::commitOrCompleteLastBlock):
       final boolean b = commitBlock((BlockInfoUnderConstruction)lastBlock, commitBlock);
      if(countNodes(lastBlock).liveReplicas() >= minReplication)
        completeBlock(bc, bc.numBlocks()-1, false);
      return b;
 
  But  the NameNode doesn't care how many replicas which status is finalized this block has! 
  It should be this: if there is no one replica which status is not finalized, the block should not convert to complete status!

  Because According to the appendDesign3.pdf (https://issues.apache.org/jira/secure/attachment/12445209/appendDesign3.pdf):
   Complete:鈥〢 鈥ヽomplete 鈥゜lock 鈥﹊s 鈥゛ 鈥゜lock 鈥﹚hose 鈥﹍ength鈥?and鈥?GS 鈥゛re 鈥ゝinalized 鈥゛nd鈥?NameNode鈥?has 鈥﹕een鈥?a鈥?GS/len 鈥﹎atched鈥?finalized 鈥﹔eplica 鈥﹐f 鈥﹖he鈥? block.鈥? 
"
Erasure Coding: test failed in TestDFSStripedInputStream.testStatefulRead() when use ByteBuffer,HDFS-8343,"It's failed since last commit

{code}
commit c61c9c855e7cd1d20f654c061ff16341ce2d9936
{code}"
Changing the replication factor for a directory should apply to new files under the directory too,HDFS-8436,"Changing the replication factor for a directory will only affect the existing files and the new files under the directory will get created with the default replication factor (dfs.replication from hdfs-site.xml) of the cluster. 

I would expect new files written under a directory to have the same replication factor set for the directory itself."
NFS gateway should throttle the data dumped on local storage,HDFS-7558,"During file uploading, NFS gateway could dump the reordered write on local storage when the accumulated data size exceeds a limit.

Currently there is no data throttle for the data dumping, which could easily saturate the local disk especially when the client is on the same host as the gateway. 
"
hadoop fs -ls globbing gives inconsistent exit code,HDFS-2685,"_hadoop fs -ls_ command gives exit code for globbed input path, which is the exit code for the last resolved absolute path. Whereas _ls_ command always give same exit code regardless of position of non-existent path in globbing.

{code}$ hadoop fs -mkdir input/20110{1,2,3}/{A,B,C,D}/{1,2} {code}

Since directory 'input/201104/' is not present, the following command gives 255 as exit code.
{code}$ hadoop fs -ls input/20110{1,2,3,4}/ ; echo $? {code}
{noformat}
Found 4 items
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201101/A
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201101/B
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201101/C
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201101/D
Found 4 items
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201102/A
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201102/B
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201102/C
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201102/D
Found 4 items
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201103/A
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201103/B
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201103/C
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201103/D
ls: Cannot access input/201104/: No such file or directory.
255
{noformat}


The directory 'input/201104/' is not present but given as second last parameter in globbing.
The following command gives 0 as exit code, because directory 'input/201103/' is present.
{code}$ hadoop fs -ls input/20110{1,2,4,3}/ ; echo $? {code}
{noformat}
Found 4 items
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201101/A
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201101/B
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201101/C
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201101/D
Found 4 items
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201102/A
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201102/B
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201102/C
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201102/D
ls: Cannot access input/201104/: No such file or directory.
Found 4 items
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201103/A
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201103/B
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201103/C
drwxr-xr-x   - mitesh supergroup          0 2011-12-15 11:51 /user/mitesh/input/201103/D
0
{noformat}



Whereas, on Linux, ls command gives non-zero(2) as exit code, irrespective of position of non-existent path in globbing.
{code}$ mkdir -p input/20110{1,2,3,4}/{A,B,C,D}/{1,2} {code}


{code}$ ls input/20110{1,2,4,3}/ ; echo $? {code}
{noformat}
/bin/ls: input/201104/: No such file or directory
input/201101/:
./  ../  A/  B/  C/  D/

input/201102/:
./  ../  A/  B/  C/  D/

input/201103/:
./  ../  A/  B/  C/  D/
2
{noformat}



{code}$ ls input/20110{1,2,3,4}/ ; echo $? {code}
{noformat}
/bin/ls: input/201104/: No such file or directory
input/201101/:
./  ../  A/  B/  C/  D/

input/201102/:
./  ../  A/  B/  C/  D/

input/201103/:
./  ../  A/  B/  C/  D/
2
{noformat}
"
Namenode in trunk has much slower performance than Namenode in MR-279 branch,HDFS-2142,"I am measureing the performance of the namenode by running the org.apache.hadoop.fs.loadGenerator.LoadGenerator application. This application shows there is a very large slowdown in the processing of opens, writes, closes, and operations per second in the trunk when compared to the MR-279 branch

There have been some race conditions and locking issues fixed in trunk, which is a very good thing because these race conditions were causing the namenode to crash under load conditions (see HDFS:1257). However, the slowdown to the namenode is considerable.

I am still trying to verify which changes caused the slowdown. It was originally suggested that the HDFS:988 may have caused the slowdown, but I don't think it was the culprit. I have checked out and built from SVN 3 revisions previous to HDFS988 and they all have about the same performance.

Here is my environment:
Host0: namenode daemon
Host1-9: simulate many datanodes using org.apache.hadoop.hdfs.DataNodeCluster
 
LoadGenerator output on MR-279 branch:
Average open execution time: 1.8496516782773909ms
Average deletion execution time: 2.956340167046317ms
Average create execution time: 3.725259427992913ms
Average write_close execution time: 11.151860288534548ms
Average operations per second: 1053.3666666666666ops/s

LoadGenerator output on trunk:
Average open execution time: 28.603515625ms
Average deletion execution time: 32.20792079207921ms
Average create execution time: 32.37326732673267ms
Average write_close execution time: 82.84752475247525ms
Average operations per second: 135.13333333333333ops/s
"
Unnecessary disk check triggered when socket operation has problem.,HDFS-5745,"When BlockReceiver transfer data fails, it can be found SocketOutputStream translates the exception as IOException with the message ""The stream is closed"":
2014-01-06 11:48:04,716 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in BlockReceiver.run():
java.io.IOException: The stream is closed
at org.apache.hadoop.net.SocketOutputStream.write
at java.io.BufferedOutputStream.flushBuffer
at java.io.BufferedOutputStream.flush
at java.io.DataOutputStream.flush
at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run
at java.lang.Thread.run

Which makes the checkDiskError method of DataNode called and triggers the disk scan.

Can we make the modifications like below in checkDiskError to avoiding this unneccessary disk scan operations?:
{code}
--- a/src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java
+++ b/src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java
@@ -938,7 +938,8 @@ public class DataNode extends Configured
          || e.getMessage().startsWith(""An established connection was aborted"")
          || e.getMessage().startsWith(""Broken pipe"")
          || e.getMessage().startsWith(""Connection reset"")
-         || e.getMessage().contains(""java.nio.channels.SocketChannel"")) {
+         || e.getMessage().contains(""java.nio.channels.SocketChannel"")
+         || e.getMessage().startsWith(""The stream is closed"")) {
       LOG.info(""Not checking disk as checkDiskError was called on a network"" +
         "" related exception""); 
       return;
{code}
"
libhdfs hdfs_read example uses hdfsRead wrongly,HDFS-923,"In the examples of libhdfs,  the hdfs_read.c uses hdfsRead wrongly. 

{noformat}
    // read from the file
    tSize curSize = bufferSize;
    for (; curSize == bufferSize;) {
        curSize = hdfsRead(fs, readFile, (void*)buffer, curSize);
    }
{noformat} 

the condition curSize == bufferSize has problem."
resolved,HDFS-8119,resolved
test-patch comment doesn't show names of failed FI tests,HDFS-2009,"Looks like test-patch.sh only looks at the build/test/*xml test results, but it should also look at build-fi/test/*xml I think"
libhdfs: add hdfsFile cache,HDFS-7693,Add an hdfsFile cache inside libhdfs.
Find a way to make encryption zone deletion work with HDFS trash.,HDFS-7271,"Currently when HDFS trash is enabled, deletion of encryption zone will have issue:
{quote}
rmr: Failed to move to trash: ... can't be moved from an encryption zone.
{quote}
A simple way is to add ignore trash flag for fs rm operation."
BlockReceiver did not close ReplicaOuputStreams ,HDFS-7569,"{{BlockReceiver#streams}} is a {{ReplicaOutputStreams}}, which holds two {{FileOutputStream}}s, i.e., {{ReplicaOutputStream#dataOut}} and {{checksumOut}}. The {{ReplicaOutputStreams#close}} is never be called in non-test code to close these two streams."
haadmin command usage prints incorrect command name,HDFS-7324,"Scenario:
=======
Try the help command for hadadmin like following..
Here usage is coming as ""DFSHAAdmin -ns"", Ideally this not availble which we can check following command.


[root@linux156 bin]#  *{color:red}./hdfs haadmin{color}* 
No GC_PROFILE is given. Defaults to medium.
 *{color:red}Usage: DFSHAAdmin [-ns <nameserviceId>]{color}* 
    [-transitionToActive <serviceId> [--forceactive]]
    [-transitionToStandby <serviceId>]
    [-failover [--forcefence] [--forceactive] <serviceId> <serviceId>]
    [-getServiceState <serviceId>]
    [-checkHealth <serviceId>]
    [-help <command>]

Generic options supported are
-conf <configuration file>     specify an application configuration file
-D <property=value>            use value for given property
-fs <local|namenode:port>      specify a namenode
-jt <local|jobtracker:port>    specify a job tracker
-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster
-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.
-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.

The general command line syntax is
bin/hadoop command [genericOptions] [commandOptions]

 *{color:blue}[root@linux156 bin]# ./hdfs DFSHAAdmin -ns 100{color}*  
Error: Could not find or load main class DFSHAAdmin"
Consider renaming StorageID,HDFS-5264,"We should consider renaming StorageID to something else since we have changed the meaning of the field. Previously it was used to identify the single logical storage attached to a datanode and hence it was a de-facto identifier for a Datanode. Now the StorageID identifies a single storage. To avoid confusion of meaning especially when merging with other feature branches it may be best to rename it to something else.

We can do so when merging phase 1 of the Heterogeneous Storage work into trunk.

A partial list of places to update:
# FsVolumeSpi#storageID
# DatanodeStorageInfo#storageID
# DatanodeStorage#storageID
# StorageReceivedDeletedBlocks#storageID
# StorageReport#storageID
# LocatedBlock#storageIDs
# processFirstBlockReport
# DatanodeStorage#getStorageInfo
# TestDatanodeDescriptor#testBlocksCounter
# TestBlockManager.java
# FsDatasetSpi#getBlockReports"
Warnings When Starting hadoop,HDFS-7975,"When i am starting hadoop getting some warnings

how to ramove them ?/

hduser@sajid:~$ start-all.sh 
This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh
15/03/23 20:06:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Starting namenodes on [localhost]
localhost: starting namenode, logging to /usr/local/hadoop/logs/hadoop-hduser-namenode-sajid.out
localhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-hduser-datanode-sajid.out
Starting secondary namenodes [0.0.0.0]
0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-hduser-secondarynamenode-sajid.out
15/03/23 20:07:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
starting yarn daemons
starting resourcemanager, logging to /usr/local/hadoop/logs/yarn-hduser-resourcemanager-sajid.out
Java HotSpot(TM) 64-Bit Server VM warning: You have loaded library /usr/local/hadoop/lib/native/libhadoop.so.1.0.0 which might have disabled stack guard. The VM will try to fix the stack guard now.
It's highly recommended that you fix the library with 'execstack -c <libfile>', or link it with '-z noexecstack'.
localhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-hduser-nodemanager-sajid.out
"
getStoragePolicy() regards HOT policy as EC policy,HDFS-7981,"Now, {{testStoragePoliciesCK()}} in {{TestFsck}} is failed in EC branch.

A part of the test result is below:
{noformat}
A part of the test result is odd:
Blocks NOT satisfying the specified storage policy:
Storage Policy                  Specified Storage Policy      # of blocks       % of blocks
DISK:3(EC)                          HOT                           1              33.3333%
{noformat}

I found that {{getStoragePolicy(StorageType[] storageTypes)}} in {{StoragePolicySummary}} regarding HOT policy as EC policy. We should fix this problem.
"
Report removed storages after removing them by DataNode#checkDirs(),HDFS-8006,"Similar to HDFS-7961,  after DN removes storages due to disk errors (HDFS-7722), DN should send a full block report to NN to remove storages (HDFS-7960)

"
HTTP request queue size limit can be made configurable,HDFS-7992,"The queue size for httpserver is hardcoded as 128 in 

{code}
  public static Connector createDefaultChannelConnector() {
    SelectChannelConnector ret = new SelectChannelConnector();
    ret.setLowResourceMaxIdleTime(10000);
    ret.setAcceptQueueSize(128);
{code}

It will be better if this can made configurable

In the connection can add a configurable limit of queue size for the http request (鈥渉adoop.http.max.queue.size鈥?."
WebHDFS cannot open file,HDFS-6496,WebHDFS cannot open the file on the name node web UI. I attched screen.
"Unifying HA support in HftpFileSystem, HsftpFileSystem and WebHdfsFileSystem",HDFS-5193,"Recent changes in HDFS-5122 implement the HA support for the WebHDFS client. 
Similar to WebHDFS client, both HftpFileSystem and HsftpFilesystem access HDFS via HTTP, but their current implementation hinders the implementation of HA support.

I propose to refactor HftpFileSystem, HsftpFileSystem, and WebHdfsFileSystem to provide unified abstractions to support HA cluster over HTTP."
Initial refactoring to allow ConsensusNode implementation,HDFS-6940,Minor refactoring of FSNamesystem to open private methods that are needed for CNode implementation.
Enable journal protocol based editlog streaming for standby namenode,HDFS-3092,"Currently standby namenode relies on reading shared editlogs to stay current with the active namenode, for namespace changes. BackupNode used streaming edits from active namenode for doing the same. This jira is to explore using journal protocol based editlog streams for the standby namenode. A daemon in standby will get the editlogs from the active and write it to local edits. To begin with, the existing standby mechanism of reading from a file, will continue to be used, instead of from shared edits, from the local edits."
HA: Support multiple shared edits dirs,HDFS-2782,Supporting multiple shared dirs will improve availability (eg see HDFS-2769). You may want to use multiple shared dirs on a single filer (eg for better fault isolation) or because you want to use multiple filers/mounts. Per HDFS-2752 (and HDFS-2735) we need to do things like use the JournalSet in EditLogTailer and add tests.
Namenode format should not create the storage directory if it doesn't exist ,HDFS-3095,"The storage directory can be a mount point. 
Automatically creating the mount point could be problematic. "
Balancer tests failing in trunk,HDFS-2721,"Looks Balancer tests started failing.....
https://builds.apache.org/job/Hadoop-Hdfs-trunk/lastCompletedBuild/testReport/
Also seen timing out of TestBalancer in precommit build https://builds.apache.org/job/PreCommit-HDFS-Build/1737//console"
move webapps/ into the JAR,HDFS-3011,"Currently the webapps dir is in the filesystem and added to the classpath.

As effectively it is picked up from the classpath, we could move into the component JAR itself.
"
hadoop distcp hftp://192.168.80.31:50070/user/wp hdfs://192.168.210.10:8020/,HDFS-7605,"Error: java.io.IOException: File copy failed: hftp://192.168.80.31:50070/user/wp/test.txt --> hdfs://192.168.210.10:8020/wp/test.txt
	at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:284)
	at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:252)
	at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:50)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.io.IOException: Couldn't run retriable-command: Copying hftp://192.168.80.31:50070/user/wp/test.txt to hdfs://192.168.210.10:8020/wp/test.txt
	at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:101)
	at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:280)
	... 10 more
Caused by: org.apache.hadoop.tools.mapred.RetriableFileCopyCommand$CopyReadException: java.net.SocketTimeoutException: connect timed out
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.getInputStream(RetriableFileCopyCommand.java:303)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.copyBytes(RetriableFileCopyCommand.java:248)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.copyToFile(RetriableFileCopyCommand.java:184)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doCopy(RetriableFileCopyCommand.java:124)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doExecute(RetriableFileCopyCommand.java:100)
	at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:87)
	... 11 more
Caused by: java.net.SocketTimeoutException: connect timed out
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:351)
	at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:213)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:200)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
	at java.net.Socket.connect(Socket.java:529)
	at sun.net.NetworkClient.doConnect(NetworkClient.java:158)
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:411)
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:525)
	at sun.net.www.http.HttpClient.<init>(HttpClient.java:208)
	at sun.net.www.http.HttpClient.New(HttpClient.java:291)
	at sun.net.www.http.HttpClient.New(HttpClient.java:310)
	at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:987)
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:923)
	at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:841)
	at sun.net.www.protocol.http.HttpURLConnection.followRedirect(HttpURLConnection.java:2156)
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1390)
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:379)
	at org.apache.hadoop.hdfs.web.HftpFileSystem$RangeHeaderUrlOpener.connect(HftpFileSystem.java:370)
	at org.apache.hadoop.hdfs.web.ByteRangeInputStream.openInputStream(ByteRangeInputStream.java:120)
	at org.apache.hadoop.hdfs.web.ByteRangeInputStream.getInputStream(ByteRangeInputStream.java:104)
	at org.apache.hadoop.hdfs.web.ByteRangeInputStream.<init>(ByteRangeInputStream.java:89)
	at org.apache.hadoop.hdfs.web.HftpFileSystem$RangeHeaderInputStream.<init>(HftpFileSystem.java:383)
	at org.apache.hadoop.hdfs.web.HftpFileSystem$RangeHeaderInputStream.<init>(HftpFileSystem.java:388)
	at org.apache.hadoop.hdfs.web.HftpFileSystem.open(HftpFileSystem.java:404)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.getInputStream(RetriableFileCopyCommand.java:299)
	... 16 more

15/01/12 18:04:06 INFO mapreduce.Job:  map 67% reduce 0%
15/01/12 18:04:06 INFO mapreduce.Job: Task Id : attempt_1420685403662_0029_m_000001_0, Status : FAILED
Error: java.io.IOException: File copy failed: hftp://192.168.80.31:50070/user/wp/t.txt --> hdfs://192.168.210.10:8020/wp/t.txt
	at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:284)
	at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:252)
	at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:50)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.io.IOException: Couldn't run retriable-command: Copying hftp://192.168.80.31:50070/user/wp/t.txt to hdfs://192.168.210.10:8020/wp/t.txt
	at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:101)
	at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:280)
	... 10 more
Caused by: org.apache.hadoop.tools.mapred.RetriableFileCopyCommand$CopyReadException: java.net.SocketTimeoutException: connect timed out
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.getInputStream(RetriableFileCopyCommand.java:303)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.copyBytes(RetriableFileCopyCommand.java:248)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.copyToFile(RetriableFileCopyCommand.java:184)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doCopy(RetriableFileCopyCommand.java:124)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doExecute(RetriableFileCopyCommand.java:100)
	at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:87)
	... 11 more
Caused by: java.net.SocketTimeoutException: connect timed out
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:351)
	at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:213)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:200)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
	at java.net.Socket.connect(Socket.java:529)
	at sun.net.NetworkClient.doConnect(NetworkClient.java:158)
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:411)
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:525)
	at sun.net.www.http.HttpClient.<init>(HttpClient.java:208)
	at sun.net.www.http.HttpClient.New(HttpClient.java:291)
	at sun.net.www.http.HttpClient.New(HttpClient.java:310)
	at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:987)
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:923)
	at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:841)
	at sun.net.www.protocol.http.HttpURLConnection.followRedirect(HttpURLConnection.java:2156)
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1390)
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:379)
	at org.apache.hadoop.hdfs.web.HftpFileSystem$RangeHeaderUrlOpener.connect(HftpFileSystem.java:370)
	at org.apache.hadoop.hdfs.web.ByteRangeInputStream.openInputStream(ByteRangeInputStream.java:120)
	at org.apache.hadoop.hdfs.web.ByteRangeInputStream.getInputStream(ByteRangeInputStream.java:104)
	at org.apache.hadoop.hdfs.web.ByteRangeInputStream.<init>(ByteRangeInputStream.java:89)
	at org.apache.hadoop.hdfs.web.HftpFileSystem$RangeHeaderInputStream.<init>(HftpFileSystem.java:383)
	at org.apache.hadoop.hdfs.web.HftpFileSystem$RangeHeaderInputStream.<init>(HftpFileSystem.java:388)
	at org.apache.hadoop.hdfs.web.HftpFileSystem.open(HftpFileSystem.java:404)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.getInputStream(RetriableFileCopyCommand.java:299)
	... 16 more"
Null pointer exception comes when Namenode recovery happens and there is no response from client to NN more than the hardlimit for NN recovery and the current block is more than the prev block size in NN ,HDFS-1951,"Null pointer exception comes when Namenode recovery happens and there is no response from client to NN more than the hardlimit for NN recovery and the current block is more than the prev block size in NN 
1. Write using a client to 2 datanodes
2. Kill one data node and allow pipeline recovery.
3. write somemore data to the same block
4. Parallely allow the namenode recovery to happen
Null pointer exception will come in addStoreBlock api.


 

"
tomcat tar in the apache archive is corrupted,HDFS-2723,"when running mvn package , getting the following error and hence not able to create tarball

{noformat}
    [mkdir] Created dir: /root/ravi/mapred/hadoop-hdfs-project/hadoop-hdfs-httpfs/target/tomcat.exp
     [exec] Current OS is Linux
     [exec] Executing 'sh' with arguments:
     [exec] './tomcat-untar.sh'
     [exec] 
     [exec] The ' characters around the executable and arguments are
     [exec] not part of the command.
Execute:Java13CommandLauncher: Executing 'sh' with arguments:
'./tomcat-untar.sh'

The ' characters around the executable and arguments are
not part of the command.
     [exec] 
     [exec] gzip: stdin: unexpected end of file
     [exec] tar: Unexpected EOF in archive
     [exec] tar: Unexpected EOF in archive
     [exec] tar: Error is not recoverable: exiting now
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Apache Hadoop Main ................................ SUCCESS [7.984s]
[INFO] Apache Hadoop Project POM ......................... SUCCESS [3.028s]
[INFO] Apache Hadoop Annotations ......................... SUCCESS [5.451s]
[INFO] Apache Hadoop Assemblies .......................... SUCCESS [2.987s]
[INFO] Apache Hadoop Project Dist POM .................... SUCCESS [13.675s]
[INFO] Apache Hadoop Auth ................................ SUCCESS [5.766s]
[INFO] Apache Hadoop Auth Examples ....................... SUCCESS [6.258s]
[INFO] Apache Hadoop Common .............................. SUCCESS [3:50.945s]
[INFO] Apache Hadoop Common Project ...................... SUCCESS [0.539s]
[INFO] Apache Hadoop HDFS ................................ SUCCESS [3:01.761s]
[INFO] Apache Hadoop HttpFS .............................. FAILURE [30.532s]

{noformat}

It is because the tomcat tarball available in ""http://archive.apache.org/dist/tomcat/tomcat-6/v6.0.32/bin/apache-tomcat-6.0.32.tar.gz"" is corrupted.
Getting ""Unexpected End of Archive"" when trying to untar this tarball.
"
"Use TestDFSIO to test HDFS, and Failed with the exception: All datanodes are bad. Aborting...",HDFS-2774,"use TestDFSIO to test the HDFS
use the commond:  hadoop jar ....TestDFSIO - write -nrFiles 10 -fileSize 500
when running ,errors occurs:
12/01/09 16:00:45 INFO mapred.JobClient: Task Id : attempt_201201091556_0001_m_000006_2, Status : FAILED
java.io.IOException: All datanodes 192.168.0.17:50010 are bad. Aborting...
 at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:2556)
 at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$1600(DFSClient.java:2102)
 at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2265)
attempt_201201091637_0002_m_000005_0: log4j:WARN No appenders could be found for logger (org.apache.hadoop.hdfs.DFSClient).
attempt_201201091637_0002_m_000005_0: log4j:WARN Please initialize the log4j system properly.

I don't know why?"
New architectural documentation created,HDFS-1961,"This material provides an overview of the HDFS architecture and is intended for contributors. The goal of this document is to provide a guide to the overall structure of the HDFS code so that contributors can more effectively understand how changes that they are considering can be made, and the consequences of those changes. The assumption is that the reader has a basic understanding of HDFS, its purpose, and how it fits into the Hadoop project suite. 

An HTML version of the architectural documentation can be found at:  http://kazman.shidler.hawaii.edu/ArchDoc.html

All comments and suggestions for improvements are appreciated."
Null pointer exception is thrown when NN restarts with a block lesser in size than the block that is present in DN1 but the generation stamp is greater in the NN ,HDFS-1982,"Conisder the following scenario. 
WE have a cluster with one NN and 2 DN.

We write some file.

One of the block is written in DN1 but not yet completed in DN2 local disk.

Now DN1 gets killed and so pipeline recovery happens for the block with the size as in DN2 but the generation stamp gets updated in the NN.

DN2 also gets killed.

Now restart NN and DN1
Now if NN restarts, the block that NN has greater time stamp but the size is lesser in the NN.

This leads to Null pointer exception in addstoredblock api


"
Improvements to HDFS-1204 test,HDFS-1247,The test from HDFS-1204 currently generates some warnings when compiling. Here's a small patch to clean up the test.
Hftp file read should retry a different datanode if the chosen best datanode fails to connect to NameNode,HDFS-1553,"Currently when reading a file through HftpFileSystem interface, namenode deterministically selects the ""best"" datanode from which the file is read. But this can cause the read to fail if the best datanode fails to connect to namenode because it never tries another datanode.

The proposed solution is to send a list of datanode candidates when namenode redirects the read request to the chosen best datanode. The the datanode could redirect the request to the next good datanode when it fails to connect to namenode."
hftp only supports remote hdfs servers,HDFS-2336,"The new token renewal implementation appears to introduce invalid assumptions regarding token kinds.

Any token acquired over http is assumed to be hftp, so the kind is unconditionally changed to hftp.  This precludes the acquisition of any other token types over http.  This new limitation was added to a generic method in a public class.  It should have been encapsulated in the hftp class, not the generic http token fetching methods.

Furthermore, hftp will unconditionally change a hftp token's kind to hdfs.  I believe this assumption means that hftp is now broken if the remote cluster's default filesystem is not hdfs."
Update HDFS dependency of Java for deb package,HDFS-2192,"Java dependency for Debian package is specified as open JDK, but it should depends on Sun version of Java. This dependency can be implicitly defined by hadoop-common dependency. Hence, there is no need to explicitly defined in hadoop-hdfs."
Avoid duplicate entries in Block Scan verification logs ,HDFS-6111,"Duplicate entries could be generated in datanode verification logs if the datanode gets shutdown during the rolling of scan verification logs.
If this happens multiple times in huge cluster, then size of the verification logs could be huge.

Avoid these duplicate entries in the next roll."
New blocks scanning will be delayed due to issue in BlockPoolSliceScanner#updateBytesToScan(..),HDFS-6147,"New blocks scanning will be delayed if old blocks deleted after datanode restart.

Steps:
1. Write some blocks and wait till all scans over
2. Restart the datanode
3. Delete some of the blocks
4. Write new blocks which are less in size compared to deleted blocks.

Problem:
{{BlockPoolSliceScanner#updateBytesToScan(..)}} updates {{bytesLeft}} based on following comparison
{code}   if (lastScanTime < currentPeriodStart) {
      bytesLeft += len;
    }{code}

But in {{BlockPoolSliceScanner#assignInitialVerificationTimes()}} {{bytesLeft}} decremented using below comparison
{code}if (now - entry.verificationTime < scanPeriod) {{code}

Hence when the old blocks are deleted {{bytesLeft}} going negative.
new blocks will not be scanned until it becomes positive again.

So in both places verificationtime should be compared against scanperiod."
TestIncrementalBlockReports#testReplaceReceivedBlock fails occasionally in trunk,HDFS-6037,"From https://builds.apache.org/job/Hadoop-Hdfs-trunk/1688/testReport/junit/org.apache.hadoop.hdfs.server.datanode/TestIncrementalBlockReports/testReplaceReceivedBlock/ :
{code}
datanodeProtocolClientSideTranslatorPB.blockReceivedAndDeleted(
    <any>,
    <any>,
    <any>
);
Wanted 1 time:
-> at org.apache.hadoop.hdfs.server.datanode.TestIncrementalBlockReports.testReplaceReceivedBlock(TestIncrementalBlockReports.java:198)
But was 2 times. Undesired invocation:
-> at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.reportReceivedDeletedBlocks(BPServiceActor.java:303)
{code}"
Wire-encription in QJM,HDFS-5688,"When HA is implemented with QJM and using kerberos, it's not possible to set wire-encrypted data.
If it's set property hadoop.rpc.protection to something different to authentication it doesn't work propertly, getting the error:

ERROR security.UserGroupInformation: PriviledgedActionException as:principal@REALM (auth:KERBEROS) cause:javax.security.sasl.SaslException: No common protection layer between client and ser

With NFS as shared storage everything works like a charm"
TestRetryCacheWithHA#testCreateSymlink occasionally fails in trunk,HDFS-6081,"From https://builds.apache.org/job/Hadoop-Hdfs-trunk/1696/testReport/junit/org.apache.hadoop.hdfs.server.namenode.ha/TestRetryCacheWithHA/testCreateSymlink/ :
{code}
2014-03-09 13:18:47,515 WARN  security.UserGroupInformation (UserGroupInformation.java:doAs(1600)) - PriviledgedActionException as:jenkins (auth:SIMPLE) cause:java.io.IOException: failed to create link /testlink either because the filename is invalid or the file exists
2014-03-09 13:18:47,515 INFO  ipc.Server (Server.java:run(2093)) - IPC Server handler 0 on 39303, call org.apache.hadoop.hdfs.protocol.ClientProtocol.createSymlink from 127.0.0.1:32909 Call#682 Retry#1: error: java.io.IOException: failed to create link /testlink either because the filename is invalid or the file exists
java.io.IOException: failed to create link /testlink either because the filename is invalid or the file exists
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.createSymlinkInt(FSNamesystem.java:2053)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.createSymlink(FSNamesystem.java:2023)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.createSymlink(NameNodeRpcServer.java:965)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.createSymlink(ClientNamenodeProtocolServerSideTranslatorPB.java:844)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:605)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:932)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2071)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2067)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1597)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2065)
2014-03-09 13:18:47,522 INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2475)) - Total number of blocks            = 1
2014-03-09 13:18:47,523 INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2476)) - Number of invalid blocks          = 0
2014-03-09 13:18:47,523 INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(2477)) - Number of under-replicated blocks = 0
2014-03-09 13:18:47,523 INFO  ha.TestRetryCacheWithHA (TestRetryCacheWithHA.java:run(1162)) - Got Exception while calling createSymlink
org.apache.hadoop.ipc.RemoteException(java.io.IOException): failed to create link /testlink either because the filename is invalid or the file exists
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.createSymlinkInt(FSNamesystem.java:2053)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.createSymlink(FSNamesystem.java:2023)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.createSymlink(NameNodeRpcServer.java:965)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.createSymlink(ClientNamenodeProtocolServerSideTranslatorPB.java:844)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:605)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:932)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2071)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2067)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1597)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2065)

	at org.apache.hadoop.ipc.Client.call(Client.java:1409)
	at org.apache.hadoop.ipc.Client.call(Client.java:1362)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at $Proxy17.createSymlink(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.createSymlink(ClientNamenodeProtocolTranslatorPB.java:794)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:189)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA$DummyRetryInvocationHandler.invokeMethod(TestRetryCacheWithHA.java:114)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at $Proxy18.createSymlink(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.createSymlink(DFSClient.java:1507)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA$CreateSymlinkOp.invoke(TestRetryCacheWithHA.java:658)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA$1.run(TestRetryCacheWithHA.java:1154)
{code}
In a successful run, I don't see the above error."
FSImage layout version should be only once file is complete,HDFS-957,"Right now, the FSImage save code writes the LAYOUT_VERSION at the head of the file, along with some other headers, and then dumps the directory into the file. Instead, it should write a special IMAGE_IN_PROGRESS entry for the layout version, dump all of the data, then seek back to the head of the file to write the proper LAYOUT_VERSION. This would make it very easy to detect the case where the FSImage save got interrupted."
Warm HA NameNode going Hot,HDFS-2064,"This is the design for automatic hot HA for HDFS NameNode. It involves use of HA software and LoadReplicator - external to Hadoop components, which substantially simplify the architecture by separating HA- from Hadoop-specific problems. Without the external components it provides warm standby with manual failover."
Datanode blockId layout upgrade threads should be daemon thread,HDFS-7666,"This jira is to mark the layout upgrade thread as daemon thread.

{code}
     int numLinkWorkers = datanode.getConf().getInt(
         DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS_KEY,
         DFSConfigKeys.DFS_DATANODE_BLOCK_ID_LAYOUT_UPGRADE_THREADS);
    ExecutorService linkWorkers = Executors.newFixedThreadPool(numLinkWorkers);
{code}"
"TestEditLog assumes that FSNamesystem.getFSNamesystem().dir is non-null, even after the FSNameSystem is closed",HDFS-135,"In my modified services, I'm setting {{FSNameSystem.dir}} to {{null}} on {{close()}}:
{code}
        if(dir != null) {
         dir.close();
         dir =  null;
        }
{code}

This breaks TestEditLog
{code}
java.lang.NullPointerException
at org.apache.hadoop.hdfs.server.namenode.FSEditLog.loadFSEdits(FSEditLog.java:620)
at org.apache.hadoop.hdfs.server.namenode.TestEditLog.testEditLog(TestEditLog.java:148)
{code}

There are two possible conclusions here. 
# Setting dir=null in {{FSNameSystem.close()}} is a regression and should be fixed
# The test contains some assumptions that are not valid

I will leave it to others to decide; I will try and fix the code whichever approach is chosen. Personally, I'd go for setting dir=null as it is cleaner, but there is clearly some risk of backward's compatibility problems, at least in test code
"
Not all datanodes are displayed on the namenode http tab,HDFS-7117,"On a single machine, I have three ""fake nodes"" (each node use different dfs.datanode.address, dfs.datanode.ipc.address, dfs.datanode.http.address)

- node1 starts the namenode and a datanode
- node2 starts a datanode
- node3 starts a datanode

In the namenode http console, on the overview, I can see 3 live nodes:

{code}
http://localhost:50070/dfshealth.html#tab-overview
{code}

but, when clicking on the ""Live Nodes"":

{code}
http://localhost:50070/dfshealth.html#tab-datanode
{code}

I can see only one node row."
Verify initializations of LocatedBlock/RecoveringBlock,HDFS-5423,"Tracking Jira to make sure we verify initialization of LocatedBlock and RecoveringBlock, possibly reorg the constructors to make missing initialization of StorageIDs less likely."
NN -> JN communication should use reusable authentication methods,HDFS-7580,"It appears that NNs talk to JNs via general SaslRPC in secure mode, causing all requests to be carried out with a kerberos authentication. This can cause delays and occasionally NN failures if the KDC used does not respond in its default timeout period (30s, whereas the QJM writes come with default of 20s)."
HDFS dangerously uses @Beta methods from very old versions of Guava,HDFS-7040,"HDFS uses LimitInputStream from Guava. This was introduced as @Beta and is risky for any application to use.

The problem is further exacerbated by Hadoop's dependency on Guava version 11.0.2, which is quite old for an active project (Feb. 2012).

Because Guava is very stable, projects which depend on Hadoop and use Guava themselves, can use up through Guava version 14.x

However, in version 14, Guava deprecated LimitInputStream and provided a replacement. Because they make no guarantees about compatibility about @Beta classes, they removed it in version 15.

What should be done: Hadoop should updated its dependency on Guava to at least version 14 (currently Guava is on version 19). This should have little impact on users, because Guava is so stable.

HDFS should then be patched to use the provided alternative to LimitInputStream, so that downstream packagers, users, and application developers requiring more recent versions of Guava (to fix bugs, to use new features, etc.) will be able to swap out the Guava dependency without breaking Hadoop.

Alternative: While Hadoop cannot predict the marking and removal of deprecated code, it can, and should, avoid the use of @Beta classes and methods that do not offer guarantees. If the dependency cannot be bumped, then it should be relatively trivial to provide an internal class with the same functionality, that does not rely on the older version of Guava."
start-dfs.sh does not start remote DataNode due to escape characters,HDFS-6239,"start-dfs.sh fails to start remote data nodes and task nodes, though it is possible to start them manually through hadoop-daemon.sh.

I've been able to debug and find the root cause the bug, and I thought it was a trivial fix, but I do not know how to do it. Can't figure out a way to handle this seemingly trivial bug.

hadoop-daemons.sh calls slave.sh:

exec ""$bin/slaves.sh"" --config $HADOOP_CONF_DIR cd ""$HADOOP_HOME"" \; ""$bin/hadoop-daemon.sh"" --config $HADOOP_CONF_DIR ""$@""

This is the issue when I debug using bash -x: In slaves.sh, the \; becomes ';'

+ ssh xxxx.xx.xxxx.xxx cd /afs/xx.xxxx.xxx/x/x/x/xx/xxxxx/libexec/.. ';' /afs/xx.xxxx.xxx/x/x/x/xx/xxxx/bin/hadoop-daemon.sh --config /afs/xx.xxxx.xxx/x/x/x/xx/xxxx/libexec/../conf start datanode

The problem is ';' . Because the semi-colon is surrounded by quotes, it doesn't execute the code after that. I manually ran the above command, and as expected the data node did not start. When I removed the quotes around the semi-colon, everything works. Please note that you can see the issue only when you do bash -x. If you echo the statement, the quotes around the semi-colon are not visible.

This issue is always reproducible for me, and because of it, I have to manually start daemons on each machine. "
we should add a wait for non-safe mode and call dfsadmin -report in start-dfs,HDFS-2256,I think we should add a call to wait for safe mode exit and print the dfs report to show upgrades that are in progress.
NameNode: implement Global ACL Set as a memory optimization.,HDFS-5620,The {{AclManager}} can maintain a Global ACL Set to store all distinct ACLs in use by the file system.  All inodes that have the same ACL entries can share the same ACL instance.
Disable check for jsvc on windows,HDFS-3749,Jsvc doesn't make sense on windows and thus we should not require the datanode to start up under it on that platform.
Remove unused TokenRenewer implementation from WebHdfsFileSystem and HftpFileSystem,HDFS-4010,"WebHdfsFileSystem and HftpFileSystem implement TokenRenewer without using anywhere.

As we are in the process of migrating them to not use tokens, this code should be removed."
WebHdfsFileSystem and HftpFileSystem don't need delegation tokens,HDFS-4009,"Parent JIRA to track the work of removing delegation tokens from these filesystems. 

This JIRA has evolved from the initial issue of these filesystems not stopping the DelegationTokenRenewer thread they were creating.

After further investigation, Daryn pointed out - ""If you can get a token, you don't need a token""! Hence, these filesystems shouldn't use delegation tokens.

Evolution of the JIRA is listed below:
Update 2:
DelegationTokenRenewer is not required. The filesystems that are using it already have Krb tickets and do not need tokens. Remove DelegationTokenRenewer and all the related logic from WebHdfs and Hftp filesystems.

Update1:
DelegationTokenRenewer should be Singleton - the instance and renewer threads should be created/started lazily. The filesystems using the renewer shouldn't need to explicity start/stop the renewer, and only register/de-register for token renewal.

Initial issue:
HftpFileSystem and WebHdfsFileSystem should stop the DelegationTokenRenewer thread when they are closed. "
Add configurable maximum block count for datanode,HDFS-6088,"Currently datanode resources are protected by the free space check and the balancer.  But datanodes can run out of memory simply storing too many blocks. If the sizes of blocks are small, datanodes will appear to have plenty of space to put more blocks.

I propose adding a configurable max block count to datanode. Since datanodes can have different heap configurations, it will make sense to make it datanode-level, rather than something enforced by namenode."
getBlockLocationsUpdateTimes missing handle exception may cause fsLock dead lock,HDFS-7253,"One day my active namenode hanged and I dumped the program stacks by jstack.In the stacks file, I saw most threads were waiting FSNamesystem.fsLock, both  readLock and writeLock were unacquirable, but no thread was holding writeLock.
I tried to access the web interface of this namenode but was blocked. and I tried to failover the active node to another namenode manually (zkfs did not discover this node was hanging) but it was also failed. So I killed this namenode trying to recover the production environment, then the failover was triggered, standby nn transited to active, and then, the new active namenode hanged.
My following steps are useless and can be ignored. At last, I thought it was caused by an incorrect lock handling in FSNamesystem.getBlockLocationsUpdateTimes, which I will describe in the first comment.
"
TestWebHdfsFileSystemContract fails occassionally,HDFS-7070,"org.apache.hadoop.hdfs.web.TestWebHdfsFileSystemContract.testResponseCode
and  org.apache.hadoop.hdfs.web.TestWebHdfsFileSystemContract.testRenameDirToSelf 
failed recently.

Need to determine whether it's  introduced by some latest code change due to file descriptor leak; or it's a similar issue as HDFS-6694 reported.


E.g. https://builds.apache.org/job/PreCommit-HDFS-Build/8026/testReport/org.apache.hadoop.hdfs.web/TestWebHdfsFileSystemContract/testResponseCode/.

{code}
2014-09-15 12:52:18,866 INFO  datanode.DataNode (DataXceiver.java:writeBlock(749)) - opWriteBlock BP-23833599-67.195.81.147-1410785517350:blk_1073741827_1461 received exception java.io.IOException: Cannot run program ""stat"": java.io.IOException: error=24, Too many open files
2014-09-15 12:52:18,867 ERROR datanode.DataNode (DataXceiver.java:run(243)) - 127.0.0.1:47221:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:38112 dst: /127.0.0.1:47221
java.io.IOException: Cannot run program ""stat"": java.io.IOException: error=24, Too many open files
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:470)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:485)
	at org.apache.hadoop.util.Shell.run(Shell.java:455)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:702)
	at org.apache.hadoop.fs.HardLink.getLinkCount(HardLink.java:495)
	at org.apache.hadoop.hdfs.server.datanode.ReplicaInfo.unlinkBlock(ReplicaInfo.java:288)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.append(FsDatasetImpl.java:702)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.append(FsDatasetImpl.java:680)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.append(FsDatasetImpl.java:101)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.<init>(BlockReceiver.java:193)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:604)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:126)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:72)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:225)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: java.io.IOException: error=24, Too many open files
	at java.lang.UNIXProcess.<init>(UNIXProcess.java:148)
	at java.lang.ProcessImpl.start(ProcessImpl.java:65)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:452)
	... 14 more
2014-09-15 12:52:18,867 INFO  hdfs.DFSClient (DFSOutputStream.java:createBlockOutputStream(1400)) - Exception in createBlockOutputStream
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2101)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1210)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:530)
2014-09-15 12:52:18,870 WARN  hdfs.DFSClient (DFSOutputStream.java:run(883)) - DFSOutputStream ResponseProcessor exception  for block BP-23833599-67.195.81.147-1410785517350:blk_1073741827_1461
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2099)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:176)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:798)
2014-09-15 12:52:18,870 WARN  hdfs.DFSClient (DFSOutputStream.java:run(627)) - DataStreamer Exception
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.DFSOutputStream$Packet.writeTo(DFSOutputStream.java:273)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:579)
{code}


"
Tests fail on windows due to BindException.,HDFS-11,"On Windows a series of tests fail starting from {{TestAbandonBlock}}.
The reason is that name-nodes or data-nodes cannot start because of the ""BindException: Address already in use: connect"".
Seems like some of the tests that run before {{TestAbandonBlock}} are not releasing resources or not closing sockets."
setLowResourceMaxIdleTime(10000) in HttpServer2 is dead,HDFS-7234,"In HttpSever2.java, the code call {{ret.setLowResourceMaxIdleTime(10000);}} but it does not set the {{LowResourceConnections}} in jetty, thus the setting becomes useless."
Fix incorrect layout version caused by bad merge,HDFS-7160,The layout version was not correctly updated while merging from trunk.
Implement compression in the HTTP server of SNN / SBN instead of FSImage,HDFS-5722,"The current FSImage format support compression, there is a field in the header which specifies the compression codec used to compress the data in the image. The main motivation was to reduce the number of bytes to be transferred between SNN / SBN / NN.

The main disadvantage, however, is that it requires the client to access the FSImage in strictly sequential order. This might not fit well with the new design of FSImage. For example, serializing the data in protobuf allows the client to quickly skip data that it does not understand. The compression built-in the format, however, complicates the calculation of offsets and lengths. Recovering from a corrupted, compressed FSImage is also non-trivial as off-the-shelf tools like bzip2recover is inapplicable.

This jira proposes to move the compression from the format of the FSImage to the transport layer, namely, the HTTP server of SNN / SBN. This design simplifies the format of FSImage, opens up the opportunity to quickly navigate through the FSImage, and eases the process of recovery. It also retains the benefits of reducing the number of bytes to be transferred across the wire since there are compression on the transport layer.

"
the shell script error for Cygwin on windows7,HDFS-4198,See the following [comment|https://issues.apache.org/jira/browse/HDFS-4198?focusedCommentId=13498818&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13498818] for detailed description.
Can namenode recover from a edit log file?,HDFS-7043,"I have delete all the data dir on the namenode,causing lost of all the data.
I can only recover a edit.new file from the disk, whtich is quite huge(12GB),
Maybe most of recent operation is on the edit log
Is it possible to recover data from this edit log file?

Try to add all the file or direcotry on every edit log to recover the filesystem?

anyone could help?
many thx"
Refactor Raid so it can work better with new erasure codes,HDFS-3543,"Refactor Raid so it can work with
https://issues.apache.org/jira/browse/MAPREDUCE-3361"
Path#makeQualified copies authority when scheme does not match,HDFS-7031,"I have an application that calls {{makeQualified}} that amounts to this:

{code:java}
new Path(""file:/some/local/path"").makeQualified(
    URI.create(""hdfs://nn:8020""), new Path(""/""));
{code}

This unexpectedly produces {{file://nn:8020/some/local/path}}, using the authority section from the default URI even though the path that is being qualified doesn't have a scheme that matches the default URI.

In {{Path}}, there is a check to see if the default URI should be used:
{code:java}
    if (scheme != null &&
        (authority != null || defaultUri.getAuthority() == null))
      return path;
{code}

I think this should be:
{code:java}
    // if the scheme matches and there is no authority, use the default
    if (scheme != null && scheme.equals(defaultUri.getScheme()) &&
        (authority != null || defaultUri.getAuthority() == null))
      return path;
{code}
"
Stub implementation of getrlimit for Windows.,HDFS-5204,The HDFS-4949 feature branch adds a JNI wrapper over the {{getrlimit}} function.  This function does not exist on Windows.  We need to provide a stub implementation so that the codebase can compile on Windows.
Update datanode replacement policy to make writes more robust,HDFS-6016,"As discussed in HDFS-5924, writers that are down to only one node due to node failures can suffer if a DN does not restart in time. We do not worry about writes that began with single replica. "
libhdfs with gdb got SEGV,HDFS-866,"I'm now trying to integrate HDFS into our server programs using libhdfs.

I found that hdfs_write and hdfs_read (sample program in libhdfs directory) got segv with the gdb execution.
Without gdb, it runs OK. Valgrind also outputs nothing.

By printf debug, the SEGV seems to be occurred in invokeMethod() function in hdfsRead() and hdfsWrite().

Following is the information.

* Code
IHDFS trunk code (revision  894610)

* Java
java version ""1.6.0_16""
Java(TM) SE Runtime Environment (build 1.6.0_16-b01)
Java HotSpot(TM) 64-Bit Server VM (build 14.2-b01, mixed mode)

* Kernel
Linux pfisedueindexer 2.6.28-14-generic #47-Ubuntu SMP Sat Jul 25 01:19:55 UTC 2009 x86_64 GNU/Linux

* GCC
4.3.3

* GDB
GNU gdb 6.8-debian

* libhdfs
CFLAGS=""-g -D_REENTRANT""
LDFLAGS=""-g""

* gdb log
{quote}
kzk@pfisedueindexer:~/apacheprojects/hdfs/src/c++/libhdfs$ gdb ./hdfs_write 
GNU gdb 6.8-debian
Copyright (C) 2008 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type ""show copying""
and ""show warranty"" for details.
This GDB was configured as ""x86_64-linux-gnu""...
(gdb) r testfile 100000000 1000000
Starting program: /home/kzk/apacheprojects/hdfs/src/c++/libhdfs/hdfs_write testfile 100000000 1000000
[Thread debugging using libthread_db enabled]
[New Thread 0x7fa8971f36f0 (LWP 10490)]
[New Thread 0x7fa7d241f950 (LWP 10493)]
[New Thread 0x7fa7d231e950 (LWP 10494)]
[New Thread 0x7fa7d221d950 (LWP 10495)]
[New Thread 0x7fa7d211c950 (LWP 10496)]
[New Thread 0x7fa7d201b950 (LWP 10497)]
[New Thread 0x7fa7d1f1a950 (LWP 10498)]
[New Thread 0x7fa7d1e19950 (LWP 10499)]
[New Thread 0x7fa7d1d18950 (LWP 10500)]
[New Thread 0x7fa7d193f950 (LWP 10501)]
[New Thread 0x7fa7d183e950 (LWP 10502)]
[New Thread 0x7fa7d173d950 (LWP 10503)]
[New Thread 0x7fa7d15bb950 (LWP 10504)]
[New Thread 0x7fa7d14ba950 (LWP 10505)]
[New Thread 0x7fa7d13b9950 (LWP 10506)]
[New Thread 0x7fa7d12b8950 (LWP 10507)]
[New Thread 0x7fa7d11b7950 (LWP 10508)]
[New Thread 0x7fa7d0ffc950 (LWP 10509)]
[Thread 0x7fa7d0ffc950 (LWP 10509) exited]
[New Thread 0x7fa7d0ffc950 (LWP 10511)]
[Thread 0x7fa7d0ffc950 (LWP 10511) exited]
[New Thread 0x7fa7d0ffc950 (LWP 10512)]
[New Thread 0x7fa7d0efb950 (LWP 10514)]
[Thread 0x7fa7d0ffc950 (LWP 10512) exited]
[Thread 0x7fa7d0efb950 (LWP 10514) exited]
[New Thread 0x7fa7d0ffc950 (LWP 10515)]
[Thread 0x7fa7d0ffc950 (LWP 10515) exited]
[New Thread 0x7fa7d0ffc950 (LWP 10517)]
[Thread 0x7fa7d0ffc950 (LWP 10517) exited]
[New Thread 0x7fa7d0ffc950 (LWP 10518)]
[New Thread 0x7fa7d0efb950 (LWP 10522)]
[New Thread 0x7fa7d0acd950 (LWP 10523)]
[New Thread 0x7fa7d09cc950 (LWP 10524)]
[New Thread 0x7fa7d07d6950 (LWP 10527)]
[Thread 0x7fa7d07d6950 (LWP 10527) exited]
[New Thread 0x7fa7d07d6950 (LWP 10530)]

Program received signal SIGSEGV, Segmentation fault.
[Switching to Thread 0x7fa7d07d6950 (LWP 10530)]
0x00007fa891cb45d4 in ?? ()
(gdb) bt
#0  0x00007fa891cb45d4 in ?? ()
#1  0x00007fa856dc5c08 in ?? ()
#2  0x00007fa7d2f0d630 in ?? ()
#3  0x0000000000000000 in ?? ()


(gdb) thread apply all bt
Thread 29 (Thread 0x7fa7d07d6950 (LWP 10530)):
#0  0x00007fa891cb45d4 in ?? ()
#1  0x00007fa856dc5c08 in ?? ()
#2  0x00007fa7d2f0d630 in ?? ()
#3  0x0000000000000000 in ?? ()

Thread 27 (Thread 0x7fa7d09cc950 (LWP 10524)):
#0  0x00007fa895fb756d in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa896962b46 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa896960bbf in os::sleep () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa8967f0028 in JVM_Sleep () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa891c34f50 in ?? ()
#5  0x00000125dfcbbb3b in ?? ()
#6  0x00007fa8967da917 in JVM_CurrentTimeMillis () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#7  0x00007fa891c29a22 in ?? ()
#8  0x00007fa7d2e673f0 in ?? ()
#9  0x00007fa891c31a58 in ?? ()
#10 0x00000000000003e8 in ?? ()
#11 0x00007fa891c31a57 in ?? ()
#12 0x00007fa7d09cba58 in ?? ()
#13 0x00007fa7d350860f in ?? ()
#14 0x00007fa7d09cbac0 in ?? ()
#15 0x00007fa7d3508c90 in ?? ()
#16 0x0000000000000000 in ?? ()

Thread 26 (Thread 0x7fa7d0acd950 (LWP 10523)):
#0  0x00007fa895fb9e5b in write () from /lib/libpthread.so.0
#1  0x00007fa7d0ad2470 in Java_sun_nio_ch_FileDispatcher_write0 () from /usr/lib/jvm/java-6-sun-1.6.0.16/jre/lib/amd64/libnio.so
#2  0x00007fa891c34f50 in ?? ()
#3  0x00007fa7d0acc4d0 in ?? ()
#4  0x0000000000000000 in ?? ()

Thread 25 (Thread 0x7fa7d0efb950 (LWP 10522)):
#0  0x00007fa895fb756d in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa896962b46 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa896a17212 in ObjectMonitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa896a14b13 in ObjectSynchronizer::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa8967dc5fb in JVM_MonitorWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa891c34f50 in ?? ()
#6  0x00000125dfcbbdbb in ?? ()
#7  0x00007fa8967da917 in JVM_CurrentTimeMillis () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#8  0x00007fa891c29a22 in ?? ()
#9  0x0000000000000000 in ?? ()
---Type <return> to continue, or q <return> to quit---

Thread 24 (Thread 0x7fa7d0ffc950 (LWP 10518)):
#0  0x00007fa895fb756d in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa896962b46 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa896a17212 in ObjectMonitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa896a14b13 in ObjectSynchronizer::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa8967dc5fb in JVM_MonitorWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa891c34f50 in ?? ()
#6  0x0000000001d64380 in ?? ()
#7  0x0000000001d64758 in ?? ()
#8  0x00007fa7c40f3000 in ?? ()
#9  0x00007fa891c3541d in ?? ()
#10 0x00007fa7d0ffb950 in ?? ()
#11 0x0000000000000000 in ?? ()

Thread 17 (Thread 0x7fa7d11b7950 (LWP 10508)):
#0  0x00007fa895fb756d in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa896962b46 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa896960d3d in os::sleep () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa896a46e3b in WatcherThread::run () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa89696326f in java_start () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa895fb33ba in start_thread () from /lib/libpthread.so.0
#6  0x00007fa895d1ffcd in clone () from /lib/libc.so.6
#7  0x0000000000000000 in ?? ()

Thread 16 (Thread 0x7fa7d12b8950 (LWP 10507)):
#0  0x00007fa895fb72e9 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa8969629f7 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa89693ba92 in Monitor::IWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa89693c25e in Monitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa8968fdbf2 in LowMemoryDetector::low_memory_detector_thread_entry () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa896a47cb1 in JavaThread::run () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#6  0x00007fa89696326f in java_start () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#7  0x00007fa895fb33ba in start_thread () from /lib/libpthread.so.0
#8  0x00007fa895d1ffcd in clone () from /lib/libc.so.6
#9  0x0000000000000000 in ?? ()

Thread 15 (Thread 0x7fa7d13b9950 (LWP 10506)):
#0  0x00007fa895fb72e9 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa8969629f7 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa89693ba92 in Monitor::IWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa89693c0fa in Monitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa896650de3 in CompileQueue::get () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa896652882 in CompileBroker::compiler_thread_loop () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
---Type <return> to continue, or q <return> to quit---
#6  0x00007fa896a4e1e9 in compiler_thread_entry () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#7  0x00007fa896a47cb1 in JavaThread::run () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#8  0x00007fa89696326f in java_start () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#9  0x00007fa895fb33ba in start_thread () from /lib/libpthread.so.0
#10 0x00007fa895d1ffcd in clone () from /lib/libc.so.6
#11 0x0000000000000000 in ?? ()

Thread 14 (Thread 0x7fa7d14ba950 (LWP 10505)):
#0  0x00007fa895fb72e9 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa8969629f7 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa89693ba92 in Monitor::IWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa89693c0fa in Monitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa896650de3 in CompileQueue::get () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa896652882 in CompileBroker::compiler_thread_loop () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#6  0x00007fa896a4e1e9 in compiler_thread_entry () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#7  0x00007fa896a47cb1 in JavaThread::run () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#8  0x00007fa89696326f in java_start () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#9  0x00007fa895fb33ba in start_thread () from /lib/libpthread.so.0
#10 0x00007fa895d1ffcd in clone () from /lib/libc.so.6
#11 0x0000000000000000 in ?? ()

Thread 13 (Thread 0x7fa7d15bb950 (LWP 10504)):
#0  0x00007fa895fb9281 in sem_wait () from /lib/libpthread.so.0
#1  0x00007fa896963635 in check_pending_signals () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa89695c537 in signal_thread_entry () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa896a47cb1 in JavaThread::run () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa89696326f in java_start () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa895fb33ba in start_thread () from /lib/libpthread.so.0
#6  0x00007fa895d1ffcd in clone () from /lib/libc.so.6
#7  0x0000000000000000 in ?? ()

Thread 12 (Thread 0x7fa7d173d950 (LWP 10503)):
#0  0x00007fa895fb72e9 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa8969629f7 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa896a175da in ObjectMonitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa896a14b13 in ObjectSynchronizer::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa8967dc5fb in JVM_MonitorWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa891c34f50 in ?? ()
#6  0x00007fa7d173cb68 in ?? ()
#7  0x00007fa7cc001000 in ?? ()
#8  0x00007fa856cf03b8 in ?? ()
#9  0x00007fa7d173cb58 in ?? ()
#10 0x00007fa7d173cae0 in ?? ()
#11 0x0000000000000000 in ?? ()
---Type <return> to continue, or q <return> to quit---

Thread 11 (Thread 0x7fa7d183e950 (LWP 10502)):
#0  0x00007fa895fb72e9 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa8969629f7 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa896a175da in ObjectMonitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa896a14b13 in ObjectSynchronizer::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa8967dc5fb in JVM_MonitorWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa891c34f50 in ?? ()
#6  0x00007fa7d183db18 in ?? ()
#7  0x0000000001e17000 in ?? ()
#8  0x00007fa7d183d9f0 in ?? ()
#9  0x00007fa7d183d9c8 in ?? ()
#10 0x0000000000000000 in ?? ()

Thread 10 (Thread 0x7fa7d193f950 (LWP 10501)):
#0  0x00007fa895fb756d in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa896962b46 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa89693bbcb in Monitor::IWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa89693c25e in Monitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa896a96873 in VMThread::loop () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa896a9646e in VMThread::run () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#6  0x00007fa89696326f in java_start () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#7  0x00007fa895fb33ba in start_thread () from /lib/libpthread.so.0
#8  0x00007fa895d1ffcd in clone () from /lib/libc.so.6
#9  0x0000000000000000 in ?? ()

Thread 9 (Thread 0x7fa7d1d18950 (LWP 10500)):
#0  0x00007fa895fb72e9 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa8969629f7 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa89693ba92 in Monitor::IWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa89693c25e in Monitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa896705405 in GCTaskManager::get_task () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa8967066b3 in GCTaskThread::run () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#6  0x00007fa89696326f in java_start () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#7  0x00007fa895fb33ba in start_thread () from /lib/libpthread.so.0
#8  0x00007fa895d1ffcd in clone () from /lib/libc.so.6
#9  0x0000000000000000 in ?? ()

Thread 8 (Thread 0x7fa7d1e19950 (LWP 10499)):
#0  0x00007fa895fb72e9 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa8969629f7 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa89693ba92 in Monitor::IWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa89693c25e in Monitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa896705405 in GCTaskManager::get_task () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
---Type <return> to continue, or q <return> to quit---
#5  0x00007fa8967066b3 in GCTaskThread::run () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#6  0x00007fa89696326f in java_start () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#7  0x00007fa895fb33ba in start_thread () from /lib/libpthread.so.0
#8  0x00007fa895d1ffcd in clone () from /lib/libc.so.6
#9  0x0000000000000000 in ?? ()

Thread 7 (Thread 0x7fa7d1f1a950 (LWP 10498)):
#0  0x00007fa895fb72e9 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa8969629f7 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa89693ba92 in Monitor::IWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa89693c25e in Monitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa896705405 in GCTaskManager::get_task () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa8967066b3 in GCTaskThread::run () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#6  0x00007fa89696326f in java_start () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#7  0x00007fa895fb33ba in start_thread () from /lib/libpthread.so.0
#8  0x00007fa895d1ffcd in clone () from /lib/libc.so.6
#9  0x0000000000000000 in ?? ()

Thread 6 (Thread 0x7fa7d201b950 (LWP 10497)):
#0  0x00007fa895fb72e9 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa8969629f7 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa89693ba92 in Monitor::IWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa89693c25e in Monitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa896705405 in GCTaskManager::get_task () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa8967066b3 in GCTaskThread::run () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#6  0x00007fa89696326f in java_start () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#7  0x00007fa895fb33ba in start_thread () from /lib/libpthread.so.0
#8  0x00007fa895d1ffcd in clone () from /lib/libc.so.6
#9  0x0000000000000000 in ?? ()

Thread 5 (Thread 0x7fa7d211c950 (LWP 10496)):
#0  0x00007fa895fb72e9 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa8969629f7 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa89693ba92 in Monitor::IWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa89693c25e in Monitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa896705405 in GCTaskManager::get_task () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa8967066b3 in GCTaskThread::run () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#6  0x00007fa89696326f in java_start () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#7  0x00007fa895fb33ba in start_thread () from /lib/libpthread.so.0
#8  0x00007fa895d1ffcd in clone () from /lib/libc.so.6
#9  0x0000000000000000 in ?? ()

Thread 4 (Thread 0x7fa7d221d950 (LWP 10495)):
#0  0x00007fa895fb72e9 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
---Type <return> to continue, or q <return> to quit---
#1  0x00007fa8969629f7 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa89693ba92 in Monitor::IWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa89693c25e in Monitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa896705405 in GCTaskManager::get_task () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa8967066b3 in GCTaskThread::run () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#6  0x00007fa89696326f in java_start () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#7  0x00007fa895fb33ba in start_thread () from /lib/libpthread.so.0
#8  0x00007fa895d1ffcd in clone () from /lib/libc.so.6
#9  0x0000000000000000 in ?? ()

Thread 3 (Thread 0x7fa7d231e950 (LWP 10494)):
#0  0x00007fa895fb72e9 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa8969629f7 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa89693ba92 in Monitor::IWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa89693c25e in Monitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa896705405 in GCTaskManager::get_task () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa8967066b3 in GCTaskThread::run () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#6  0x00007fa89696326f in java_start () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#7  0x00007fa895fb33ba in start_thread () from /lib/libpthread.so.0
#8  0x00007fa895d1ffcd in clone () from /lib/libc.so.6
#9  0x0000000000000000 in ?? ()

Thread 2 (Thread 0x7fa7d241f950 (LWP 10493)):
#0  0x00007fa895fb72e9 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa8969629f7 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa89693ba92 in Monitor::IWait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa89693c25e in Monitor::wait () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa896705405 in GCTaskManager::get_task () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa8967066b3 in GCTaskThread::run () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#6  0x00007fa89696326f in java_start () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#7  0x00007fa895fb33ba in start_thread () from /lib/libpthread.so.0
#8  0x00007fa895d1ffcd in clone () from /lib/libc.so.6
#9  0x0000000000000000 in ?? ()

Thread 1 (Thread 0x7fa8971f36f0 (LWP 10490)):
#0  0x00007fa895fb72e9 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#1  0x00007fa8969629f7 in os::PlatformEvent::park () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#2  0x00007fa896a16192 in ObjectMonitor::EnterI () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#3  0x00007fa896a16752 in ObjectMonitor::enter () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#4  0x00007fa896a147eb in ObjectSynchronizer::slow_enter () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#5  0x00007fa896a146e4 in ObjectSynchronizer::fast_enter () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#6  0x00007fa8969d5449 in SharedRuntime::complete_monitor_locking_C () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
#7  0x00007fa891c87efe in ?? ()
#8  0x00007fa856cc7190 in ?? ()
---Type <return> to continue, or q <return> to quit---
#9  0x00007fa891cb6548 in ?? ()
#10 0x00007fa856cc72a0 in ?? ()
#11 0x0000020000000000 in ?? ()
#12 0x00007fa856cc74b8 in ?? ()
#13 0x0000000001d6c800 in ?? ()
#14 0x00007fff9f209680 in ?? ()
#15 0x00007fa896a147eb in ObjectSynchronizer::slow_enter () from /usr/lib/jvm/java-6-sun/jre/lib/amd64/server/libjvm.so
Backtrace stopped: previous frame inner to this frame (corrupt stack?)
#0  0x00007fa891cb45d4 in ?? ()
{quote}"
DFSClient should use IV generated based on the configured CipherSuite with codecs used,HDFS-6737,"Seems like we are using IV as like Encrypted data encryption key iv. But the underlying Codec's cipher suite may expect different iv length. So, we should generate IV from the Coec's cipher suite configured.

{code}
 final CryptoInputStream cryptoIn =
          new CryptoInputStream(dfsis, CryptoCodec.getInstance(conf, 
              feInfo.getCipherSuite()), feInfo.getEncryptedDataEncryptionKey(),
              feInfo.getIV());
{code}

So, instead of using feinfo.getIV(), we should generate like

{code}
byte[] iv = new byte[codec.getCipherSuite().getAlgorithmBlockSize()]; 
codec.generateSecureRandom(iv);
{code}

"
make datanodes do graceful shutdown,HDFS-1350,"we found that the Datanode doesn't do a graceful shutdown and a block can be corrupted (data + checksum amounts off)

we can make the DN do a graceful shutdown in case there are open files. if this presents a problem to a timely shutdown, we can make a it a parameter of how long to wait for the full graceful shutdown before just exiting
"
StreamFile is a datanode servlet but it is misplaced in the namenode package,HDFS-1294,The StreamFile servlet is only used by datanode but not namenode.
"Generation Stamp mismatches, leading to failed append",HDFS-1231,"- Summary: the recoverBlock is not atomic, leading retrial fails when 
facing a failure.
 
- Setup:
+ # available datanodes = 3
+ # disks / datanode = 1
+ # failures = 2
+ failure type = crash
+ When/where failure happens = (see below)
 
- Details:
Suppos"
CRC does not match when retrying appending a partial block,HDFS-1228,"- Summary: when appending to partial block, if is possible that
retrial when facing an exception fails due to a checksum mismatch.
Append operation is not atomic (either complete or fail completely).
 
- Setup:
+ # available datanodes = 2
+# disks / datan"
SocketException: Protocol not available for some JVMs,HDFS-1115,"Here, input is a folder containing all .xml files from ./conf  
Then trying the command:
./bin/hadoop fs -copyFromLocal input input

The following message is displayed: 
{noformat}
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Operation not supported
INFO hdfs.DFSClient: Abandoning block blk_-1884214035513073759_1010
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Protocol not available
INFO hdfs.DFSClient: Abandoning block blk_5533397873275401028_1010
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Protocol not available
INFO hdfs.DFSClient: Abandoning block blk_-237603871573204731_1011
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Protocol not available
INFO hdfs.DFSClient: Abandoning block blk_-8668593183126057334_1011
WARN hdfs.DFSClient: DataStreamer Exception: java.io.IOException: Unable to create new block.
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2845)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:2102)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2288)

WARN hdfs.DFSClient: Error Recovery for block blk_-8668593183126057334_1011 bad datanode[0] nodes == null
WARN hdfs.DFSClient: Could not get block locations. Source file ""/user/max/input/core-site.xml"" - Aborting...
copyFromLocal: Protocol not available
ERROR hdfs.DFSClient: Exception closing file /user/max/input/core-site.xml : java.net.SocketException: Protocol not available
java.net.SocketException: Protocol not available
        at sun.nio.ch.Net.getIntOption0(Native Method)
        at sun.nio.ch.Net.getIntOption(Net.java:178)
        at sun.nio.ch.SocketChannelImpl$1.getInt(SocketChannelImpl.java:419)
        at sun.nio.ch.SocketOptsImpl.getInt(SocketOptsImpl.java:60)
        at sun.nio.ch.SocketOptsImpl.sendBufferSize(SocketOptsImpl.java:156)
        at sun.nio.ch.SocketOptsImpl$IP$TCP.sendBufferSize(SocketOptsImpl.java:286)
        at sun.nio.ch.OptionAdaptor.getSendBufferSize(OptionAdaptor.java:129)
        at sun.nio.ch.SocketAdaptor.getSendBufferSize(SocketAdaptor.java:328)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.createBlockOutputStream(DFSClient.java:2873)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2826)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:2102)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2288)
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Operation not supported
INFO hdfs.DFSClient: Abandoning block blk_-1884214035513073759_1010
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Protocol not available
INFO hdfs.DFSClient: Abandoning block blk_5533397873275401028_1010
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Protocol not available
INFO hdfs.DFSClient: Abandoning block blk_-237603871573204731_1011
INFO hdfs.DFSClient: Exception in createBlockOutputStream java.net.SocketException: Protocol not available
INFO hdfs.DFSClient: Abandoning block blk_-8668593183126057334_1011
WARN hdfs.DFSClient: DataStreamer Exception: java.io.IOException: Unable to create new block.
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2845)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:2102)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2288)

WARN hdfs.DFSClient: Error Recovery for block blk_-8668593183126057334_1011 bad datanode[0] nodes == null
WARN hdfs.DFSClient: Could not get block locations. Source file ""/user/max/input/core-site.xml"" - Aborting...
copyFromLocal: Protocol not available
ERROR hdfs.DFSClient: Exception closing file /user/max/input/core-site.xml : java.net.SocketException: Protocol not available
java.net.SocketException: Protocol not available
        at sun.nio.ch.Net.getIntOption0(Native Method)
        at sun.nio.ch.Net.getIntOption(Net.java:178)
        at sun.nio.ch.SocketChannelImpl$1.getInt(SocketChannelImpl.java:419)
        at sun.nio.ch.SocketOptsImpl.getInt(SocketOptsImpl.java:60)
        at sun.nio.ch.SocketOptsImpl.sendBufferSize(SocketOptsImpl.java:156)
        at sun.nio.ch.SocketOptsImpl$IP$TCP.sendBufferSize(SocketOptsImpl.java:286)
        at sun.nio.ch.OptionAdaptor.getSendBufferSize(OptionAdaptor.java:129)
        at sun.nio.ch.SocketAdaptor.getSendBufferSize(SocketAdaptor.java:328)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.createBlockOutputStream(DFSClient.java:2873)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2826)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:2102)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2288)
{noformat}
However, only empty files are created on HDFS."
Write HDFS wire protocols in AVRO IDL,HDFS-1069,"As part of the the move to AVRO and wire compatibility, write all HDFS protocols in AVRO IDL"
Make HDFS tests pass using Avro RPC,HDFS-1066,"Some HDFS tests fail when Avro RPCs are enabled.

To see this, try 'ant test-core -Dtest.hdfs.rpc.engine=org.apache.hadoop.ipc.AvroRpcEngine'

This is an umbrella issue.  Sub-tasks will be added for particular classes of failures."
"Append/flush should support concurrent ""tailer"" use case",HDFS-1060,"Several people have a usecase for a writer logging edits and using hflush() while one or more readers ""tails"" the file by periodically reopening and seeking to get the latest bytes. Currently there are several bugs. Using this ticket as a supertask for those bugs (and to add test cases for this use case)"
completeFile loops forever if the block's only replica has become corrupt,HDFS-1059,"If a writer is appending to a block with replication factor 1, and that block has become corrupt, a reader will report the corruption to the NN. Then when the writer tries to complete the file, it will loop forever with an error like:

    [junit] 2010-03-21 17:40:08,093 INFO  namenode.FSNamesystem (FSNamesystem.java:checkFileProgress(1613)) - BLOCK* NameSystem.checkFileProgress: block blk_-4256412191814117589_1001{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[127.0.0.1:56782|RBW]]} has not reached minimal replication 1
    [junit] 2010-03-21 17:40:08,495 INFO  hdfs.DFSClient (DFSOutputStream.java:completeFile(1435)) - Could not complete file /TestReadWhileWriting/file1 retrying...

Should add tests that cover the case of a writer appending to a block that is corrupt while a reader accesses it."
Potential NN deadlock in processDistributedUpgradeCommand,HDFS-862,"Haven't seen this in practice, but the lock order is inconsistent. processReport locks FSNamesystem, then calls UpgradeManager.startUpgrade, getUpgradeState, and getUpgradeStatus (each of which locks the UpgradeManager). FSNameSystem.processDistributedUpgradeCommand calls upgradeManager.processUpgradeCommand which is synchronized on UpgradeManager, which can call FSNameSystem.leaveSafeMode which synchronizes on FSNamesystem."
Move Ivy related build targets from the list of public targets,HDFS-807,"Current list of  build targets is quite long. It contains targets that aren't normally executed by a user, e.g.
{noformat}
 ivy-download                To download ivy
 ivy-report                  Generate
 ivy-retrieve                Retrieve Ivy-managed artifacts
 ivy-retrieve-checkstyle     Retrieve Ivy-managed artifacts for the checkstyle configurations
 ivy-retrieve-common         Retrieve Ivy-managed artifacts for the compile configurations
 ivy-retrieve-javadoc        Retrieve Ivy-managed artifacts for the javadoc configurations
 ivy-retrieve-jdiff          Retrieve Ivy-managed artifacts for the javadoc configurations
 ivy-retrieve-releaseaudit   Retrieve Ivy-managed artifacts for the compile configurations
 ivy-retrieve-test           Retrieve Ivy-managed artifacts for the test configurations
{noformat}

I'd suggest to take these out of the list of top-level build targets"
Add SureLogic annotations' jar into Ivy and Eclipse configs,HDFS-801,"In order to use SureLogic analysis tools and allow their concurrency analysis annotations in HDFS code the annotations library has to be automatically pulled from a Maven repo. Also, it has to be added to Eclipse .classpath template."
HDFS should enforce a max block size,HDFS-583,"When DataNode creates a replica, it should enforce a max block size, so clients can't go crazy. One way of enforcing this is to make BlockWritesStreams to be filter steams that check the block size."
Display the most recent GC info on NN webUI,HDFS-6747,It will be handy if the recent GC information is available on NN webUI. admins don't need to dig out GC logs.
Update startup scripts to start Checkpoint node instead of SecondaryNameNode,HDFS-272,"Start up script {{start-dfs.sh}} should start Checkpoint node instead of SecondaryNameNode.
It should provide an option to start Checkpoint or Backup node or secondary.
The default should be checkpoint."
Expose NN and DN hooks to service plugins,HDFS-460,This is the other half of the old HADOOP-5640 (Allow ServicePlugins to hook callbacks into key service events). It adds hooks to the NN and DN to expose certain events to plugins.
Create unencrypted streams interface,HDFS-6554,There needs to be an interface to encrypted files that streams the unencrypted data.
Implement seek for HftpFileSystem,HDFS-269,Support seek in the HftpFileSystem. This is useful for a host of applications that need to access data from a hadoop cluster running a different version of hadoop.
Support manually fsck in DataNode,HDFS-366,"Now DataNode only support scan all blocks periodically.  Our site need a tool to check some blocks and files manually. 

My current design is to add a parameter to DFSck to indicate deeply and manually fsck request, then let NameNode collect property block identifies and sent them to associated DataNode. 
I'll let DataBlockScanner runs in two ways: periodically ( original one ) and manually. 

Any suggestions on this are welcome. "
DF should use used + available as the capacity of this volume,HDFS-4,"Generally speaking, UNIX tends to keep certain percentage of disk space reserved for root used only (can be changed via tune2fs or when mkfs). Therefore, Hadoop's DF class should not use the 1st number in df output as the capacity of this volume. Instead, it should use used+available as its capacity.

Otherwise, datanode may think this volume is not full but in fact it is.

The code in question is src/core/org/apache/hadoop/fs/DF.java, method parseExecResult()
"
ZKFailoverController failed to recognize the quorum is not met,HDFS-6706,"Thanks Kenny Zhang for finding this problem.
The zkfc cannot be startup due to ha.zookeeper.quorum is not met. ""zkfc -format"" doesn't log the real problem. And then user will see the error message instead of the real issue when starting zkfc:
2014-07-01 17:08:17,528 FATAL ha.ZKFailoverController (ZKFailoverController.java:doRun(213)) - Unable to start failover controller. Parent znode does not exist.
Run with -formatZK flag to initialize ZooKeeper.

2014-07-01 16:00:48,678 FATAL ha.ZKFailoverController (ZKFailoverController.java:fatalError(365)) - Fatal error occurred:Received create error from Zookeeper. code:NONODE for path /hadoop-ha/prodcluster/ActiveStandbyElectorLock
2014-07-01 17:24:44,202 - INFO ProcessThread(sid:2 cport:-1)::PrepRequestProcessor@627 - Got user-level KeeperException when processing sessionid:0x346f36191250005 type:create cxid:0x2 zxid:0xf00000033 txntype:-1 reqpath:n/a Error Path:/hadoop-ha/prodcluster/ActiveStandbyElectorLock Error:KeeperErrorCode = NodeExists for /hadoop-ha/prodcluster/ActiveStandbyElectorLock

To reproduce the problem:
1. use HDFS cluster with automatic HA enable and set the ha.zookeeper.quorum to 3.
2. start two zookeeper servers.
3. do ""hdfs zkfc -format"", and then ""hdfs zkfc""
"
Group changes cause FUSE-DFS I/O error,HDFS-2249,"If a user utilizes the FUSE mount, then has a group change, they will be unable to write into FUSE using the new group information.

To duplicate (assuming user brian starts only in group brian):
1) Write file into FUSE (or do any other action)
2) Add user brian to group brian2
3) chown anything brian owns to brian:brian2.  The error message that gets passed along is:

[brian@red ~]$ chown brian:brian2 /mnt/hadoop/user/brian/test_group_perms2
chown: changing ownership of `/mnt/hadoop/user/brian/test_group_perms2': Input/output error

I believe this is due to the fact that group information is only looked up at NN connection time?

If you then remount the FUSE mount, the chown command succeeds."
"in branch-1, libhdfs makes jni lib calls after setting errno in some places",HDFS-465,"errno can be affected by other lib calls, so should always be set right before return stmt and never before making other library calls."
Automate test for Hadoop-4597,HDFS-406,Just do it.
Synthetic Load Generator for NameNode testing -- Next Generation,HDFS-408,"A review of the Synthetic Load Generator identified several candidates for improvement. None are so urgent as to stand in the way of the present facility, but all are of sufficient interest to merit inclusion in this note.
"
Transparent archival and restore of files from HDFS,HDFS-220,"There should be a facility to migrate old files away from a production cluster. Access to those files from applications should continue to work transparently, without changing application code, but maybe with reduced performance. The policy engine  that does this could be layered on HDFS rather than being built into HDFS itself."
Cross-system causal tracing within Hadoop,HDFS-232,"Much of Hadoop's behavior is client-driven, with clients responsible for contacting individual datanodes to read and write data, as well as dividing up work for map and reduce tasks.  In a large deployment with many concurrent users, identifying the effects of individual clients on the infrastructure is a challenge.  The use of data pipelining in HDFS and Map/Reduce make it hard to follow the effects of a given client request through the system.

This proposal is to instrument the HDFS, IPC, and Map/Reduce layers of Hadoop with X-Trace.  X-Trace is an open-source framework for capturing causality of events in a distributed system.  It can correlate operations making up a single user request, even if those operations span multiple machines.  As an example, you could use X-Trace to follow an HDFS write operation as it is pipelined through intermediate nodes.  Additionally, you could trace a single Map/Reduce job and see how it is decomposed into lower-layer HDFS operations.

Matei Zaharia and Andy Konwinski initially integrated X-Trace with a local copy of the 0.14 release, and I've brought that code up to release 0.17.  Performing the integration involves modifying the IPC protocol, inter-datanode protocol, and some data structures in the map/reduce layer to include 20-byte long tracing metadata.  With release 0.18, the generated traces could be collected with Chukwa.

I've attached some example traces of HDFS and IPC layers from the 0.17 patch to this JIRA issue.

More information about X-Trace is available from http://www.x-trace.net/ as well as in a paper that appeared at NSDI 2007, available online at http://www.usenix.org/events/nsdi07/tech/fonseca.html"
Unable to access data from non hadoop application (Version mismatch in DataNode),HDFS-65,"Hi, I'm trying to access the hdfs of my hadoop cluster in a non hadoop application. Hadoop 0.17.1 is running on standart ports (The same error also occured on earlier verisons). The code however will fail, as there is a version conflict.


This is the code I use:

FileSystem fileSystem = null;
                String hdfsurl = ""hdfs://localhost:50010"";
fileSystem = new DistributedFileSystem();

                try {
                        fileSystem.initialize(new URI(hdfsurl), new Configuration());
                } catch (Exception e) {
                        e.printStackTrace();
                        System.out.println(""init error:"");
                        System.exit(1);

                }


which fails with the exception:


java.net.SocketTimeoutException: timed out waiting for rpc response
        at org.apache.hadoop.ipc.Client.call(Client.java:559)
        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:212)
        at org.apache.hadoop.dfs.$Proxy0.getProtocolVersion(Unknown Source)
        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:313)
        at org.apache.hadoop.dfs.DFSClient.createRPCNamenode(DFSClient.java:102)
        at org.apache.hadoop.dfs.DFSClient.<init>(DFSClient.java:178)
        at org.apache.hadoop.dfs.DistributedFileSystem.initialize(DistributedFileSystem.java:68)
        at com.iterend.spider.conf.Config.getRemoteFileSystem(Config.java:72)
        at tests.RemoteFileSystemTest.main(RemoteFileSystemTest.java:22)
init error:


The haddop logfile contains the following error:
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = bluelu-PC/192.168.1.130
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 0.17.1
STARTUP_MSG:   build = http://svn.apache.org/repos/asf/hadoop/core/branches/branch-0.17 -r 669344; compiled by 'hadoopqa' on Thu Jun 19 01:18:25 UTC 2008 
2008-07-10 23:05:47,840 INFO org.apache.hadoop.dfs.Storage: Storage directory \hadoop\tmp\hadoop-sshd_server\dfs\data is not formatted.
2008-07-10 23:05:47,840 INFO org.apache.hadoop.dfs.Storage: Formatting ...
2008-07-10 23:05:47,928 INFO org.apache.hadoop.dfs.DataNode: Registered FSDatasetStatusMBean
2008-07-10 23:05:47,929 INFO org.apache.hadoop.dfs.DataNode: Opened server at 50010
2008-07-10 23:05:47,933 INFO org.apache.hadoop.dfs.DataNode: Balancing bandwith is 1048576 bytes/s
2008-07-10 23:05:48,128 INFO org.mortbay.util.Credential: Checking Resource aliases
2008-07-10 23:05:48,344 INFO org.mortbay.http.HttpServer: Version Jetty/5.1.4
2008-07-10 23:05:48,346 INFO org.mortbay.util.Container: Started HttpContext[/static,/static]
2008-07-10 23:05:48,346 INFO org.mortbay.util.Container: Started HttpContext[/logs,/logs]
2008-07-10 23:05:49,047 INFO org.mortbay.util.Container: Started org.mortbay.jetty.servlet.WebApplicationHandler@15bc6c8
2008-07-10 23:05:49,244 INFO org.mortbay.util.Container: Started WebApplicationContext[/,/]
2008-07-10 23:05:49,247 INFO org.mortbay.http.SocketListener: Started SocketListener on 0.0.0.0:50075
2008-07-10 23:05:49,247 INFO org.mortbay.util.Container: Started org.mortbay.jetty.Server@47a0d4
2008-07-10 23:05:49,257 INFO org.apache.hadoop.metrics.jvm.JvmMetrics: Initializing JVM Metrics with processName=DataNode, sessionId=null
2008-07-10 23:05:49,535 INFO org.apache.hadoop.dfs.DataNode: New storage id DS-2117780943-192.168.1.130-50010-1215723949510 is assigned to data-node 127.0.0.1:50010
2008-07-10 23:05:49,586 INFO org.apache.hadoop.dfs.DataNode: 127.0.0.1:50010In DataNode.run, data = FSDataset{dirpath='c:\hadoop\tmp\hadoop-sshd_server\dfs\data\current'}
2008-07-10 23:05:49,586 INFO org.apache.hadoop.dfs.DataNode: using BLOCKREPORT_INTERVAL of 3600000msec Initial delay: 60000msec
2008-07-10 23:06:04,636 INFO org.apache.hadoop.dfs.DataNode: BlockReport of 0 blocks got processed in 11 msecs
2008-07-10 23:19:54,512 ERROR org.apache.hadoop.dfs.DataNode: 127.0.0.1:50010:DataXceiver: java.io.IOException: Version Mismatch
        at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:961)
        at java.lang.Thread.run(Thread.java:619)


When compiling my own jar from the 0.17-1, I see that the distributed version has the revision number compiled into version number, instead of using the one from the source code (26738 vs 9). Skipping this check triggers another exception:

2008-07-17 17:28:51,268 ERROR org.apache.hadoop.dfs.DataNode: 127.0.0.1:50010:DataXceiver: java.io.IOException: Unknown opcode 112 in data stream
	at org.apache.hadoop.dfs.DataNode$DataXceiver.run(DataNode.java:1002)
	at java.lang.Thread.run(Thread.java:619)


What do I do different from a hadoop application accessing hdfs?
"
Remove code related to conversion of name-node and data-node storage directories to the format introduced in hadoop 0.13,HDFS-293,"Hadoop 0.18 does not support direct HDFS upgrades from versions 0.13 or earlier as stated in HADOOP-2797.
A 2 step upgrade is required in this case first from 0.x <= 0.13 to one of version 0.14 through 0.17 and then to 0.18.
This implies that current hdfs does not need to support code related to conversions of the old (pre 0.13) storage layout to the current one introduced in 0.13 (see. HADOOP-702).
"
Name collision for AccessControlException.,HDFS-337,"There is a name collision in org.apache.hadoop.fs.permission.AccessControlException and java.security.AccessControlException.
Since  java.security.AccessControlException is not an IOException we cannot throw it directly as we do with FileNotFoundException.
Therefore, the only choice is to rename the hadoop AccessControlException to e.g., PermissionException (or AccessDeniedException).
To provide compatibility we can inherit PermissionException from AccessControlException, and deprecate the latter.
"
Explore usage of the sendfile api via java.nio.channels.FileChannel.transfer{To|From} for i/o in datanodes,HDFS-281,"We could potentially gain a lot of performance by using the *sendfile* system call:

$ man sendfile
{noformat}
DESCRIPTION
       This  call  copies  data between one file descriptor and another.  Either or both of these file descriptors may refer to a socket (but see below).
       in_fd should be a file descriptor opened for reading and out_fd should be a descriptor opened for writing.  offset is  a  pointer  to  a  variable
       holding  the input file pointer position from which sendfile() will start reading data.  When sendfile() returns, this variable will be set to the
       offset of the byte following the last byte that was read.  count is the number of bytes to copy between file descriptors.

       Because this copying is done within the kernel, sendfile() does not need to spend time transferring data to and from user space.
{noformat}

The nio package offers this via the java.nio.channels.FileChannel.transfer{To|From} apis:
http://java.sun.com/j2se/1.5.0/docs/api/java/nio/channels/FileChannel.html#transferFrom(java.nio.channels.ReadableByteChannel,%20long,%20long)
http://java.sun.com/j2se/1.5.0/docs/api/java/nio/channels/FileChannel.html#transferTo(long,%20long,%20java.nio.channels.WritableByteChannel)

From the javadocs:
{noformat}
     This method is potentially much more efficient than a simple loop that reads from this channel and writes to the target channel. Many operating systems can transfer bytes directly from the filesystem cache to the target channel without actually copying them.
{noformat}

----

Hence, this could well-worth exploring for doing io at the datanodes..."
DFS Upgrade should process dfs.data.dirs in parallel,HDFS-270,"I just upgraded from 0.14.2 to 0.15.0, and things went very smoothly, if a little slowly.

The main reason the upgrade took so long was the block upgrades on the datanodes. Each of our datanodes has 3 drives listed for the dfs.data.dir parameter. From looking at the logs, it is fairly clear that the upgrade procedure does not attempt to upgrade all listed dfs.data.dir's in parallel.

I think even if all of your dfs.data.dir's are on the same physical device, there would still be an advantage to performing the upgrade process in parallel. The less downtime, the better: especially if it is potentially 20 minutes versus 60 minutes."
"DFS web UI does should have ""TAIL this block"" option",HDFS-188,"It should be available either in addition or instead ""TAIL this file"" as it covers it.
In case of 1 block files, the two are the same.
"
AuthenticationToken will be ignored if the cookie value contains '@',HDFS-6548,"if the cookie value is something like ""email=xyz@abc.com"", HDFS will ignore the AuthenticationToken and reject the request.

2014-06-05 19:12:40,654 WARN org.apache.hadoop.security.authentication.server.AuthenticationFilter: AuthenticationToken ignored: org.apache.hadoop.security.authentication.util.SignerException: Invalid signed text: u

This is caused by fix for HADOOP-10379 Protect authentication cookies with the HttpOnly and Secure flags
it constructs cookie header manually instead of using Cookie class so the value is not double quoted."
TestWebHdfsWithMultipleNameNodes failed with ConcurrentModificationException,HDFS-6428,"TestWebHdfsWithMultipleNameNodes failed as follows:

{code}
Running org.apache.hadoop.hdfs.web.TestWebHdfsWithMultipleNameNodes
Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 8.643 sec <<< FAILURE! - in org.apache.hadoop.hdfs.web.TestWebHdfsWithMultipleNameNodes
org.apache.hadoop.hdfs.web.TestWebHdfsWithMultipleNameNodes  Time elapsed: 3.771 sec  <<< ERROR!
java.util.ConcurrentModificationException: null
        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:894)
        at java.util.HashMap$EntryIterator.next(HashMap.java:934)
        at java.util.HashMap$EntryIterator.next(HashMap.java:932)
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.shutdown(FsVolumeImpl.java:251)
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.shutdown(FsVolumeList.java:249)
        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.shutdown(FsDatasetImpl.java:1389)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:1304)
        at org.apache.hadoop.hdfs.MiniDFSCluster.shutdownDataNodes(MiniDFSCluster.java:1555)
        at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:1530)
        at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:1514)
        at org.apache.hadoop.hdfs.web.TestWebHdfsWithMultipleNameNodes.shutdownCluster(TestWebHdfsWithMultipleNameNodes.java:99)
{code}


"
Mavenize hdfs contribs,HDFS-2097,Same as HADOOP-6671 for hdfs contribs
Various syntax and style cleanups,HDFS-6319,"Fix various style issues like if(, while(, [i.e. lack of a space after the keyword],
Extra whitespace and newlines
if (...) return ... [lack of {}'s]
"
"Branch 0.23 Patch for ""Block Replication Policy Implementation May Skip Higher-Priority Blocks for Lower-Priority Blocks""",HDFS-4696,This JIRA tracks the solution to HDFS-4366 for the 0.23 branch.
"ch{mod,own,grp} -R to do recursion at the name node",HDFS-241,Performance. No need to maintain {{distch}}.
Create options to search files/dirs in OfflineImageViewer,HDFS-5990,"Add some query options to liststatus operation in WebImageViewer to search files/dirs in a fsimage.
An example query is as follows:
{code}
curl -i http://localhost:5978/?op=liststatus&owner=root&group=supergroup&minsize=1&maxsize=1048576&recursive=true
{code}
"
JobTracker blocked on TIMED_WAITING DFSOutputStream.waitForAckedSeqno() running,HDFS-6139,"We're using CDH 4.2.1.  The following is a part of threaddump on our JobTracker.

{code}
""IPC Server handler 249 on 8021"" daemon prio=10 tid=0x00007fce80e2c000 nid=0x718e in Object.wait() [0x00007fc92afe6000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x00007fc95f5ffba8> (a java.util.LinkedList)
        at org.apache.hadoop.hdfs.DFSOutputStream.waitForAckedSeqno(DFSOutputStream.java:1708)
        - locked <0x00007fc95f5ffba8> (a java.util.LinkedList)
        at org.apache.hadoop.hdfs.DFSOutputStream.flushInternal(DFSOutputStream.java:1694)
        at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:1778)
        - locked <0x00007fc95f5ff898> (a org.apache.hadoop.hdfs.DFSOutputStream)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:66)
        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:99)
        at org.apache.hadoop.mapred.JobTracker.submitJob(JobTracker.java:3562)
        - locked <0x00007fc9652787e0> (a org.apache.hadoop.mapred.JobTracker)
        at org.apache.hadoop.mapred.JobTracker.submitJob(JobTracker.java:3475)
        at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.WritableRpcEngine$Server$WritableRpcInvoker.call(WritableRpcEngine.java:474)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1002)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1695)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1691)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1689)
... ...
""Thread-9489990"" daemon prio=10 tid=0x00007fce6c01b000 nid=0x14e1 runnable [0x00007fc8f38f7000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
        - locked <0x00007fc9631201d0> (a sun.nio.ch.Util$1)
        - locked <0x00007fc9631201e8> (a java.util.Collections$UnmodifiableSet)
        - locked <0x00007fc963120158> (a sun.nio.ch.EPollSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
        at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:336)
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:158)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:156)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:129)
        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:117)
        at java.io.FilterInputStream.read(FilterInputStream.java:66)
        at java.io.FilterInputStream.read(FilterInputStream.java:66)
        at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:169)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1105)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1039)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:487)
{code}"
DataNode exceptions reading local disk,HDFS-193,"We get 100s of exceptions at WARN level per day indicating errors while trying to read local blocks.  When this occurs, I've checked on the local box's dfs.data.dir and the block is not present.  Here is a relevant snippet from the logs regarding the missing block.  It *looks* like the DataNode deletes the block and then tries to read it again later.

NOTE: this is for the jar file as up to 8 hosts have this exception for one block and our data repl factor is only 3.
"
ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(1,HDFS-2076,"see sir
datanode log socket and datasteam problem unable to upload text file to DFS i deleted tmp folders dfs and mapred again i formated ""hadoop namenode -format""
start-all.sh done then
dfs folder contains:
data node ,name node,secondarynamenode
mapred: empty
about space:-----------------
linux-8ysi:/etc/hadoop/hadoop-0.20.2 # df -h
Filesystem Size Used Avail Use% Mounted on
/dev/sda5 25G 16G 7.4G 69% /
udev 987M 212K 986M 1% /dev
/dev/sda7 42G 5.5G 34G 14% /home
-------------------------------------------
http://localhost:50070/dfshealth.jsp------------------

NameNode 'localhost:54310'
Started: Wed Jun 15 04:13:14 IST 2011
Version: 0.20.2, r911707
Compiled: Fri Feb 19 08:07:34 UTC 2010 by chrisdo
Upgrades: There are no upgrades in progress.

Browse the filesystem
Namenode Logs
Cluster Summary
10 files and directories, 0 blocks = 10 total. Heap Size is 15.5 MB / 966.69 MB (1%)
Configured Capacity : 24.61 GB
DFS Used : 24 KB
Non DFS Used : 17.23 GB
DFS Remaining : 7.38 GB
DFS Used% : 0 %
DFS Remaining% : 29.99 %
Live Nodes : 1
Dead Nodes : 0

NameNode Storage:
Storage Directory Type State
/tmp/Testinghadoop/dfs/name IMAGE_AND_EDITS Active

Hadoop, 2011.
----------------------------------------
core-site.xml
---------------------------------
<?xml version=""1.0""?>
<?xml-stylesheet type=""text/xsl"" href=""configuration.xsl""?>

<!-- Put site-specific property overrides in this file. -->

<configuration>

<property>
<name>hadoop.tmp.dir</name>
<value>/tmp/Testinghadoop/</value>
<description>A base for other temporary directories.</description>
</property>

<property>
<name>fs.default.name</name>
<value>hdfs://localhost:54310</value>
<description>The name of the default file system. A URI whose
scheme and authority determine the FileSystem implementation. The
uri's scheme determines the config property (fs.SCHEME.impl) naming
the FileSystem implementation class. The uri's authority is used to
determine the host, port, etc. for a filesystem.</description>
</property>

</configuration>
------------------------------------------------
hdfs-site.xml
----------------------------------
<?xml version=""1.0""?>
<?xml-stylesheet type=""text/xsl"" href=""configuration.xsl""?>

<!-- Put site-specific property overrides in this file. -->

<configuration>

<property>
<name>dfs.permissions</name>
<value>true</value>
<description>
If ""true"", enable permission checking in HDFS.
If ""false"", permission checking is turned off,
but all other behavior is unchanged.
Switching from one parameter value to the other does not change the mode,
owner, or group of files or directories.
</description>
</property>

<property>
<name>dfs.replication</name>
<value>1</value>
<description>Default block replication.
The actual number of replications can be specified when the file is created.
The default is used if replication is not specified in create time.
</description>
</property>

</configuration>
---------------------------------------
mapred-site.xml
----------------------------------
<?xml version=""1.0""?>
<?xml-stylesheet type=""text/xsl"" href=""configuration.xsl""?>

<!-- Put site-specific property overrides in this file. -->

<configuration>
<property>
<name>mapred.job.tracker</name>
<value>localhost:54311</value>
<description>The host and port that the MapReduce job tracker runs
at. If ""local"", then jobs are run in-process as a single map
and reduce task.
</description>
</property>
</configuration>
----------------------------------------------------------------------------------

please give suggetions about this error:
------------------------------------------------------------------------------------------------------------------
linux-8ysi:/etc/hadoop/hadoop-0.20.2/conf # hadoop fsck /
RUN_JAVA
/usr/java/jre1.6.0_25/bin/java
.Status: HEALTHY
Total size: 0 B
Total dirs: 7
Total files: 1 (Files currently being written: 1)
Total blocks (validated): 0
Minimally replicated blocks: 0
Over-replicated blocks: 0
Under-replicated blocks: 0
Mis-replicated blocks: 0
Default replication factor: 1
Average block replication: 0.0
Corrupt blocks: 0
Missing replicas: 0
Number of data-nodes: 1
Number of racks: 1


The filesystem under path '/' is HEALTHY

linux-8ysi:/etc/hadoop/hadoop-0.20.2/conf # hadoop dfsadmin -report
RUN_JAVA
/usr/java/jre1.6.0_25/bin/java
Configured Capacity: 26425618432 (24.61 GB)
Present Capacity: 7923564544 (7.38 GB)
DFS Remaining: 7923539968 (7.38 GB)
DFS Used: 24576 (24 KB)
DFS Used%: 0%
Under replicated blocks: 0
Blocks with corrupt replicas: 0
Missing blocks: 0

-------------------------------------------------
Datanodes available: 1 (1 total, 0 dead)

Name: 127.0.0.1:50010
Decommission Status : Normal
Configured Capacity: 26425618432 (24.61 GB)
DFS Used: 24576 (24 KB)
Non DFS Used: 18502053888 (17.23 GB)
DFS Remaining: 7923539968(7.38 GB)
DFS Used%: 0%
DFS Remaining%: 29.98%
Last contact: Wed Jun 15 05:54:00 IST 2011

i got this error:
----------------------------

linux-8ysi:/etc/hadoop/hadoop-0.20.2 # hadoop dfs -put spo.txt In
RUN_JAVA
/usr/java/jre1.6.0_25/bin/java
11/06/15 04:50:18 WARN hdfs.DFSClient: DataStreamer Exception: org.apache.hadoop.ipc.RemoteException: java.io.IOException: File /user/root/In/spo.txt could only be replicated to 0 nodes, instead of 1
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1271)
at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:422)
at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
at java.lang.reflect.Method.invoke(Unknown Source)
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Unknown Source)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)

at org.apache.hadoop.ipc.Client.call(Client.java:740)
at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
at $Proxy0.addBlock(Unknown Source)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
at java.lang.reflect.Method.invoke(Unknown Source)
at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
at $Proxy0.addBlock(Unknown Source)
at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.locateFollowingBlock(DFSClient.java:2937)
at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2819)
at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:2102)
at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2288)

11/06/15 04:50:18 WARN hdfs.DFSClient: Error Recovery for block null bad datanode[0] nodes == null
11/06/15 04:50:18 WARN hdfs.DFSClient: Could not get block locations. Source file ""/user/root/In/spo.txt"" - Aborting...
put: java.io.IOException: File /user/root/In/spo.txt could only be replicated to 0 nodes, instead of 1
11/06/15 04:50:18 ERROR hdfs.DFSClient: Exception closing file /user/root/In/spo.txt : org.apache.hadoop.ipc.RemoteException: java.io.IOException: File /user/root/In/spo.txt could only be replicated to 0 nodes, instead of 1
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1271)
at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:422)
at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
at java.lang.reflect.Method.invoke(Unknown Source)
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Unknown Source)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)

org.apache.hadoop.ipc.RemoteException: java.io.IOException: File /user/root/In/spo.txt could only be replicated to 0 nodes, instead of 1
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1271)
at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:422)
at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
at java.lang.reflect.Method.invoke(Unknown Source)
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Unknown Source)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)

at org.apache.hadoop.ipc.Client.call(Client.java:740)
at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
at $Proxy0.addBlock(Unknown Source)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
at java.lang.reflect.Method.invoke(Unknown Source)
at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
at $Proxy0.addBlock(Unknown Source)
at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.locateFollowingBlock(DFSClient.java:2937)
at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:2819)
at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2000(DFSClient.java:2102)
at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2288)

regards
Ranga Swamy
8904524975 "
"File not being replicated, even when #of DNs >0",HDFS-675,"One of my tests is now failing, possibly a race condition: 
java.io.IOException: File /test-filename could only be replicated to 0 nodes, instead of 1. ( there are currently 1 live data nodes in the cluster)"
Rolling upgrae exception,HDFS-6113,"I've a hadoop-2.3 running non-securable on the cluster. then I built a trunk instance, also non securable.

NN1 - active
NN2 - standby
DN1 - datanode 
DN2 - datanode
JN1,JN2,JN3 - Journal and ZK

then on the NN2:
{code}
hadoop-dameon.sh stop namenode
hadoop-dameon.sh stop zkfc
{code}

then:
change the environment variables to the new hadoop.(trunk version)

then:

{code}
hadoop-dameon.sh start namenode
{code}

NN2 throws exception:
{code}
org.apache.hadoop.hdfs.qjournal.client.QuorumException: Could not journal CTime for one more JournalNodes. 1 exceptions thrown:
10.100.91.33:8485: Failed on local exception: java.io.EOFException; Host Details : local host is: ""10-204-8-136/10.204.8.136""; destination host is: ""jn33.com"":8485;
        at org.apache.hadoop.hdfs.qjournal.client.QuorumException.create(QuorumException.java:81)
        at org.apache.hadoop.hdfs.qjournal.client.QuorumCall.rethrowException(QuorumCall.java:223)
        at org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.getJournalCTime(QuorumJournalManager.java:631)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.getSharedLogCTime(FSEditLog.java:1383)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.initEditLog(FSImage.java:738)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:600)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.doUpgrade(FSImage.java:360)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:258)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:894)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:653)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:444)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:500)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:656)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:641)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1294)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1360)
{code}


JN throws Exception:
{code}
2014-03-18 12:19:01,960 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8485: readAndProcess threw exception java.io.IOException: Unable to read authentication method from client 10.204.8.136. Count of bytes read: 0
java.io.IOException: Unable to read authentication method
	at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1344)
	at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:761)
	at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:560)
	at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:535)
2014-03-18 12:19:01,960 DEBUG org.apache.hadoop.ipc.Server: IPC Server listener on 8485: disconnecting client 10.204.8.136:39063. Number of active connections: 1
{code}
"
Configurable DataXceiver thread stack size,HDFS-4814,"By default, Java creates a thread with 512kB stack size.  In Datanode, we may have 4096 or more DataXceiver threads.  These threads do not require such large stack size and unnecessarily occupy a large amount of memory."
TestFsLimits#testDefaultMaxComponentLength Fails on branch-2,HDFS-6104,"testDefaultMaxComponentLength fails intermittently with the following error
{noformat}
java.lang.AssertionError: expected:<0> but was:<255>
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.failNotEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:128)
	at org.junit.Assert.assertEquals(Assert.java:472)
	at org.junit.Assert.assertEquals(Assert.java:456)
	at org.apache.hadoop.hdfs.server.namenode.TestFsLimits.testDefaultMaxComponentLength(TestFsLimits.java:90)
{noformat}

On doing some research, I found that this is actually a JDK7 issue.
The test always fails when it runs after any test that runs addChildWithName() method"
Mismatch in number of bytes already moved and number of bytes being moved in balancer report,HDFS-3295,"Scenario:
Replication factor = 1,fs.defaultFS=hdfs://namenodeip:port,dfs.namenode.rpc-address=namenodeip:port.
step 1: started DN1.Pumped 4.67GB of data
step 2: started DN2.
step 3: issued the balancer cmd(./start-balancer.sh -threshold 1)

Totally  848.41 MB  has been moved to 2DN and took 4 iterations to move the blocks to 2DN.

But in balancer output in all the iterations the number of bytes already moved is alway 0KB  and there is mismatch in the Bytes being moved with the bytes left to move

Balancer output for 2nd Iteration
=================================
{noformat}
Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved
Apr 18, 2012 12:41:20 PM          0                 0 KB           848.41 MB          151.18 MB
Apr 18, 2012 1:06:46 PM           1                 0 KB           646.02 MB          151.18 MB
Apr 18, 2012 1:10:28 PM           2                 0 KB           417.43 MB          151.18 MB
Apr 18, 2012 1:14:04 PM           3                 0 KB           223.84 MB          151.18 MB
Apr 18, 2012 1:18:10 PM           4                 0 KB            30.34 MB          151.18 MB{noformat}

In the above balancer output

鈥ytes already moved is 0KB
鈥?In 3 rd iterarion Bytes left to move is 223.84 MB but Bytes being moved is 151.18 MB . So in next iteration the bytes left to move should be 223.84 MB - 151.18 MB 
"
Consider supporting a mechanism to allow datanodes to drain outstanding work during rolling upgrade,HDFS-5446,"Rebuilding write pipelines is expensive and this can happen many times during a rolling restart of datanodes (i.e. during a rolling upgrade). It seems like it might help if datanodes could be told to drain current work while rejecting new requests - possibly with a new response indicating the node is temporarily unavailable (it's not broken, it's just going through a maintenance phase where it shouldn't accept new work). 

Waiting just a few seconds is normally enough to clear up a good percentage of the open requests without error, thus reducing the overhead associated with restarting lots of datanodes in rapid succession.

Obviously would need a timeout to make sure the datanode doesn't wait forever.
"
TestDNFencingWithReplication fails on branch2,HDFS-5829,"{noformat}
Running org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencingWithReplication
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.097 sec <<< FAILURE!
testFencingStress(org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencingWithReplication)  Time elapsed: 6 sec  <<< ERROR!
java.lang.ExceptionInInitializerError
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencingWithReplication.<clinit>(TestDNFencingWithReplication.java:49)
	... 28 more
{noformat}"
"When SSL is enabled , the Namenode WEBUI redirects to Infosecport, which could be 0",HDFS-5660,"(case 1) When SSL is enabled by setting ""hadoop.ssl.enabled"", SSL will be enabled on the regular port (infoport) on Datanode. 

 (case 2) When SSL on HDFS is enabled by setting ""dfs.https.enable"", SSL will be enabled on a separate port (infoSecurePort)  on Datanode. 

if SSL is enabled , the Namenode always redirects to infoSecurePort. ""infoSecurePort"" will be 0 in case 1 above.

This breaks the file browsing via web.
"
Remove compression support from FSImage,HDFS-5725,"As proposed in HDFS-5722, this jira removes the support of compression in the FSImage format."
HDFS file append failing in multinode cluster,HDFS-4600,"NOTE: the following only happens in a fully distributed setup (core-site.xml and hdfs-site.xml are attached)

Steps to reproduce:

{noformat}
$ javac -cp /usr/lib/hadoop/client/\* X.java
$ echo aaaaa > a.txt
$ hadoop fs -ls /tmp/a.txt
ls: `/tmp/a.txt': No such file or directory
$ HADOOP_CLASSPATH=`pwd` hadoop X /tmp/a.txt
13/03/13 16:05:14 WARN hdfs.DFSClient: DataStreamer Exception
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[10.10.37.16:50010, 10.80.134.126:50010], original=[10.10.37.16:50010, 10.80.134.126:50010]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:793)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:858)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:964)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:470)
Exception in thread ""main"" java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[10.10.37.16:50010, 10.80.134.126:50010], original=[10.10.37.16:50010, 10.80.134.126:50010]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:793)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:858)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:964)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:470)
13/03/13 16:05:14 ERROR hdfs.DFSClient: Failed to close file /tmp/a.txt
java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[10.10.37.16:50010, 10.80.134.126:50010], original=[10.10.37.16:50010, 10.80.134.126:50010]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.findNewDatanode(DFSOutputStream.java:793)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:858)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:964)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:470)
{noformat}

Given that the file actually does get created:
{noformat}
$ hadoop fs -ls /tmp/a.txt
Found 1 items
-rw-r--r--   3 root hadoop          6 2013-03-13 16:05 /tmp/a.txt
{noformat}

this feels like a regression in APPEND's functionality.

"
Datanode should have compatibility mode for sending combined block reports,HDFS-5200,We may want to consider adding a compatibility mode to the Datanode wherein it can send a combined block report (as is done today). HDFS-4988 will modify this behavior so that the Datanode will send one block report per Storage Directory. We will look at this when we get to compatibility/upgrade testing for the feature.
NameNode: change startup progress to track loading INode ACL Map.,HDFS-5622,Define a new startup progress {{StepType}} for loading INode ACL Map entries and use it to track progress during {{Phase#LOADING_FSIMAGE}}.
NameNode: implement AclManager as abstraction over INode ACL Map.,HDFS-5595,Complete an initial implementation of {{AclManager}} to enable further development tasks.  This will be a basic implementation using the INode ACL Map to track associations between inodes and ACLs.  This will not fully implement all of the optimizations discussed in the design doc.  Further optimization work will be tracked in separate tasks.
HA namenode with QJM created from org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider should implement Closeable,HDFS-5566,"When using hbase-0.96 with hadoop-2.2.0, stopping master/regionserver node will result in {{Cannot close proxy - is not Closeable or does not provide closeable invocation}}.

[Mail Archive|https://drive.google.com/file/d/0B22pkxoqCdvWSGFIaEpfR3lnT2M/edit?usp=sharing]

My hadoop-2.2.0 configured as HA namenode with QJM, the configuration is like this:
{code:xml}
  <property>
    <name>dfs.nameservices</name>
    <value>hadoopdev</value>
  </property>
  <property>
    <name>dfs.ha.namenodes.hadoopdev</name>
    <value>nn1,nn2</value>
  </property>
  <property>
    <name>dfs.namenode.rpc-address.hadoopdev.nn1</name>
    <value>fphd9.ctpilot1.com:9000</value>
  </property>
  <property>
    <name>dfs.namenode.http-address.hadoopdev.nn1</name>
    <value>fphd9.ctpilot1.com:50070</value>
  </property>
  <property>
    <name>dfs.namenode.rpc-address.hadoopdev.nn2</name>
    <value>fphd10.ctpilot1.com:9000</value>
  </property>
  <property>
    <name>dfs.namenode.http-address.hadoopdev.nn2</name>
    <value>fphd10.ctpilot1.com:50070</value>
  </property>
  <property>
    <name>dfs.namenode.shared.edits.dir</name>
    <value>qjournal://fphd8.ctpilot1.com:8485;fphd9.ctpilot1.com:8485;fphd10.ctpilot1.com:8485/hadoopdev</value>
  </property>
  <property>
    <name>dfs.client.failover.proxy.provider.hadoopdev</name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
  </property>
  <property>
    <name>dfs.ha.fencing.methods</name>
    <value>shell(/bin/true)</value>
  </property>
  <property>
    <name>dfs.journalnode.edits.dir</name>
    <value>/data/hadoop/hadoop-data-2/journal</value>
  </property>
  <property>
    <name>dfs.ha.automatic-failover.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>ha.zookeeper.quorum</name>
    <value>fphd1.ctpilot1.com:2222</value>
  </property>
{code}

I traced the code and found out that when stopping the hbase master node, it will try invoke method ""close"" on namenode, but the instance that created from {{org.apache.hadoop.hdfs.NameNodeProxies.createProxy}} with failoverProxyProviderClass {{org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider}} do not have the Closeable interface.

If we use the Non-HA case, the created instance will be {{org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB}} that implement Closeable.

TL;DR;
With hbase connecting to hadoop HA namenode, when stopping the hbase master or regionserver, it couldn't find the {{close}} method to gracefully close namenode session."
WebHDFS: add support for recursive flag in ACL operations.,HDFS-5611,Implement and test handling of recursive flag for all ACL operations in WebHDFS.
libHDFS: add support for recursive flag in ACL functions.,HDFS-5607,Implement and test handling of recursive flag for all ACL functions in libHDFS.
DistributedFileSystem: add support for recursive flag in ACL methods.,HDFS-5599,Implement and test handling of recursive flag for all ACL methods in {{DistributedFileSystem}}.
HftpFileSystem should try both KSSL and SPNEGO when authentication is required,HDFS-3699,"See discussion in HDFS-2617 (Replaced Kerberized SSL for image transfer and fsck with SPNEGO-based solution).

To handle the transition from Hadoop1.0 systems running KSSL authentication to Hadoop systems running SPNEGO, it would be good to fix the client in both 1 and 2 to try SPNEGO and then fall back to try KSSL.  

This will allow organizations that are running a lot of Hadoop 1.0 to gradually transition over, without needing to convert all clusters at the same time.  They would first need to update their 1.0 HFTP clients (and 2.0/0.23 if they are already running those) and then they could copy data between clusters without needing to move all clusters to SPNEGO in a big bang.

"
Incorrect exit code when copying a file bigger than given quota,HDFS-4891,"Exit code is incorrect for hdfs command.

===Repro step===
1. Set quota on a directory in HDFS.
2. Get a file of which the size is bigger than given quota
3. Do $ hdfs fs -copyFromLocal command to copy the file to HDFS
4. There will be an exception. That is expected error message.
   Exit code will be zero. This is incorrect."
fuse_dfs: ERROR: could not connect open file fuse_impls_open.c:54,HDFS-5072,"Here are some command lines on CentOS 6.4
sudo ./fuse_dfs_wrapper.sh dfs://172.16.0.80:9000 /mnt/hdfs
sudo -u hadoop bin/hadoop dfs -mkdir /test
sudo -u hadoop bin/hadoop dfs -chown -R root:root /test

I can create file and directories from following command lines
sudo bin/hadoop dfs -copyFromLocal /tmp/vod/* /test
sudo touch /mnt/hdfs/test/test.txt

And then I created samba share \\172.16.0.80\hdfs for /mnt/hdfs,
On window system, go to the share folder \\172.16.0.80\hdfs\test via root user,
I can create directory, copy files from samba, also can rename file on the samba,
but when I copy file into samba, it popup one window and said I/O error.

I checked the /var/log/messages, found

fuse_dfs: ERROR: could not connect open file fuse_impls_open.c:54

I'm guess it's a bad build, but wondering if there might be another cause."
Convert snapshot user guide to APT from XDOC,HDFS-4747,"To be consistent with the rest of the HDFS docs, the snapshots user guide should use APT instead of XDOC."
NameSystem.addStoredBlock: addStoredBlock request received for blk_-8546297170266610178_1223147 on 192.168.10.44:40010 size 134217728 but was rejected: Block not in blockMap with any generation stamp,HDFS-5057,"In some cases, the following symptoms occur?

---------------------------------------------------------------------------









2013-08-02 00:09:16,426 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: addStoredBlock request received for blk_-8546297170266610178_1223147 on 192.168.10.44:40010 size 134217728 but was rejected: Block not in blockMap with any generation stamp
2013-08-02 00:09:16,426 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addToInvalidates: blk_-8546297170266610178 to 192.168.10.44:40010
2013-08-02 00:09:16,429 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: addStoredBlock request received for blk_-8546297170266610178_1223147 on 192.168.10.23:40010 size 134217728 but was rejected: Block not in blockMap with any generation stamp
2013-08-02 00:09:16,429 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.addToInvalidates: blk_-8546297170266610178 to 192.168.10.23:40010
2013-08-02 00:09:16,468 ERROR org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:hdfsuser cause:org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hdfsroot/20130802/1214110.0 File does not exist. [Lease.  Holder: DFSClient_1545724836, pendingcreates: 4]
2013-08-02 00:09:16,468 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 40000, call addBlock(/hdfsroot/20130802/1214110.0, DFSClient_1545724836, null) from 192.168.10.23:60071: error: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hdfsroot/20130802/1214110.0 File does not exist. [Lease.  Holder: DFSClient_1545724836, pendingcreates: 4]
org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException: No lease on /hdfsroot/20130802/1214110.0 File does not exist. [Lease.  Holder: DFSClient_1545724836, pendingcreates: 4]
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1720)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1711)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1619)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:736)
        at sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:578)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1393)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1389)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1149)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1387)
		
		
		
		"
Error while running custom writable code  - DFSClient_NONMAPREDUCE_549327626_1 doesnot have any open file,HDFS-5015,"hi , 
We are facing an below error while running custom writable code, our driver code is using below code . logwritable is the name of custom writable interface.

--Error message at the code execution is as follows
DFSClient_NONMAPREDUCE_549327626_1 doesnot have any open file

--Driver code - 
job.setMapOutputKeyClass(logwritable.class);
job.setMapOutputValueClass(logwritable.class);
job.setNumReducerTasks(0);

--logwritable class implements Writable interface
--Definition of mapper is as follows

public class MAPPER extends Mapper<Text,IntWritable,Logwritable, Logwritabe>{ 
....
contaxt.write(new logwritable(), new logwritable());
}

Any help to resolve above issue will be great help
Thanks
Allan "
HDFSSeek API fails to seek to position when file is opened in write mode.,HDFS-4991,"Hi,

hdfsSeek API fails to seek to position when file is opened in write mode. I studied in documentation that hdfsSeek is only supported when file is opened in read mode.

We have a requirement of replacing the file resided on hadoop environment.

Is there any possibility of having HDFSSeek to be supported when file is opened in write mode?

Regards,
Dayakar"
Add class to manage JournalList,HDFS-3182,See the comment for details of the JournalList ZooKeeper znode.
HDFS permission check is incorrect,HDFS-4918,"HDFS permisson check is incorrect, even if dfs.permissions is set false. it does look like this was caused by snap shot.
"
Access to HDFS,HDFS-4844,"Hello,

I'm currently working on the hadoop framework.

I'm trying to publish the statistics from the stored data (in hdfs) in my web interface.
I want to know if there is a possibility to access the filesystem hdfs by a script Perl ?
Is there a perl lib to download ?


"
Encapsulate arguments to BlockReaderFactory in a class,HDFS-4352,Encapsulate the arguments to BlockReaderFactory in a class to avoid having to pass around 10+ arguments to a few different functions.
Per directory trash settings / trash override,HDFS-4683,"With the migration of trash settings to server side, it becomes more complicated for applications built on top of HDFS to properly deal with their trash. Applications like HBase and Accumulo already have a fair amount of trash management, adding the HDFS Trash will simply put more stress on DFS. But fully disabling the trash is overkill, as there still may be use for it in other uses of hadoop.

I would like to request either:
A. per directory or user trash settings, so that applications which work in a specific directory or use a specific user can continue to ignore the trash.
B. An updated DistributedFileSystem delete() call which allows you to force ignoring the trash. I'm not sure how feasible this is due to the FileSystem API, but it may be possible."
Create a fsckraid tool to verify the consistency of erasure codes for HDFS-503,HDFS-582,"HDFS-503 should also have a tool to test the consistency of the parity files generated, so that data corruption can be detected and treated."
"Add a lifecycle interface for Hadoop components: namenodes, job clients, etc.",HDFS-326,"I'd like to propose we have a standard interface for hadoop components, the things that get started or stopped when you bring up a namenode. currently, some of these classes have a stop() or shutdown() method, with no standard name/interface, but no way of seeing if they are live, checking their health of shutting them down reliably. Indeed, there is a tendency for the spawned threads to not want to die; to require the entire process to be killed to stop the workers. 

Having a standard interface would make it easier for 
 * management tools to manage the different things
 * monitoring the state of things
 * subclassing

The latter is interesting as right now TaskTracker and JobTracker start up threads in their constructor; that's very dangerous as subclasses may have their methods called before they are full initialised. Adding this interface would be the right time to clean up the startup process so that subclassing is less risky."
Wrong server principal is used for rpc calls to namenode if HA is enabled,HDFS-4713,"When various components are connecting to a namenode in a HA-enabled environment, a wrong server principal may be picked up.  This result in SASL failure, since the client-side used a wrong service ticket for the connection.
"
Loading data from HDFS to tape,HDFS-4731,"I want to load my HDFS data directly to a tape or external storage device.

Please let me know if there is any way to do this."
Permission checker should not expose resolved paths,HDFS-4703,"Namenode currently prints inode information in AccessControlException. When path provided by user is different from the actual path is resolved to a different path in namenode (some examples as in snapshot paths, HDFS-4434), this might expose information about resolved path.

In this jira, in permission checker, I propose adding a separate field for path to print in AccessControlException."
getting error while configuring the hadoop,HDFS-4682,"i try to configuring hadoop in windows but i am getting error ... i followed the address .....
http://blog.sqltrainer.com/2012/01/installing-and-configuring-apache.html"
Update the groupId property in the pom.xml to use ASF org instead of kr.ac.korea.dbserver,HDFS-4644,"As part of incubating process in ASF, we need to change the groupId to reflect the ASF org."
eclipse plugin for hadoop 2.0.0-alpha,HDFS-4624,Is there an eclipse plug in available for hadoop 2.0.0-alpha? i am currently working on a project to device a solution for small files problem and i am using hdfs federation. I want to integrate our web server with hdfs. So I need eclipse plugin for this version. Please help me out.
Enable replicating and pinning files to a data node,HDFS-2004,Some HDFS applications require that a given file is on the local DataNode.  The functionality created here will allow pinning the file to any DataNode.
Namenode HA using Backup Namenode as Hot Standby,HDFS-2124,"This is to share our experiences on building Automatic, Hot High Availability for Namenode, leveraging the existing Backup Namenode."
"Namenode supports Active and Standby modes; Standby Namenode should be able to able to take up the Active role dynamically, without losing any data.",HDFS-2164,"Namenode should have the capability to play dual roles, either as Active or Standby, as HA Agent decides. The Standby Namenode should contain the instance of Backup Namenode inside. When HA Agent forces a switch over, the Backup Namenode instance should be turned to a normal Namenode instance."
GetImage failed,HDFS-4501,"fsimage and editslog are not updating 
Following are the logs 

SNN Logs:
----------------
2013-02-14 17:29:56,975 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of transactions: 0 Total time for transactions(ms): 0Number of transactions batched in Syncs: 0 Number of syncs: 0 SyncTimes(ms): 0
2013-02-14 17:29:57,039 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Downloaded file fsimage size 10181 bytes.
2013-02-14 17:29:57,042 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Downloaded file edits size 521 bytes.
2013-02-14 17:29:57,042 INFO org.apache.hadoop.hdfs.util.GSet: VM type       = 64-bit
2013-02-14 17:29:57,042 INFO org.apache.hadoop.hdfs.util.GSet: 2% max memory = 17.77875 MB
2013-02-14 17:29:57,042 INFO org.apache.hadoop.hdfs.util.GSet: capacity      = 2^21 = 2097152 entries
2013-02-14 17:29:57,042 INFO org.apache.hadoop.hdfs.util.GSet: recommended=2097152, actual=2097152
2013-02-14 17:29:57,044 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner=hduser
2013-02-14 17:29:57,044 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup=supergroup
2013-02-14 17:29:57,044 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled=true
2013-02-14 17:29:57,044 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.block.invalidate.limit=100
2013-02-14 17:29:57,044 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)
2013-02-14 17:29:57,044 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2013-02-14 17:29:57,045 INFO org.apache.hadoop.hdfs.server.common.Storage: Number of files = 89
2013-02-14 17:29:57,059 INFO org.apache.hadoop.hdfs.server.common.Storage: Number of files under construction = 0
2013-02-14 17:29:57,061 INFO org.apache.hadoop.hdfs.server.common.Storage: Edits file /app/hadoop/tmp/dfs/namesecondary/current/edits of size 521 edits # 7 loaded in 0 seconds.
2013-02-14 17:29:57,061 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of transactions: 0 Total time for transactions(ms): 0Number of transactions batched in Syncs: 0 Number of syncs: 0 SyncTimes(ms): 0
2013-02-14 17:29:57,121 INFO org.apache.hadoop.hdfs.server.common.Storage: Image file of size 10181 saved in 0 seconds.
2013-02-14 17:29:57,673 INFO org.apache.hadoop.hdfs.server.common.Storage: Image file of size 10181 saved in 0 seconds.
2013-02-14 17:29:58,121 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Posted URL ramesh:50070putimage=1&port=50090&machine=0.0.0.0&token=-32:1989419481:0:1360842594000:1360842284984
2013-02-14 17:29:58,128 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint:
2013-02-14 17:29:58,129 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: java.io.FileNotFoundException: http://ramesh:50070/getimage?putimage=1&port=50090&machine=0.0.0.0&token=-32:1989419481:0:1360842594000:1360842284984
        at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1613)
        at org.apache.hadoop.hdfs.server.namenode.TransferFsImage.getFileClient(TransferFsImage.java:160)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.putFSImage(SecondaryNameNode.java:377)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:418)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:312)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:275)
        at java.lang.Thread.run(Thread.java:722)


NN Logs:
-----------

2013-02-14 18:15:08,127 ERROR org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:hduser cause:java.net.ConnectException: Connection refused
2013-02-14 18:15:08,128 ERROR org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:hduser cause:java.net.ConnectException: Connection refused
2013-02-14 18:15:08,129 WARN org.mortbay.log: /getimage: java.io.IOException: GetImage failed. java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:198)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:391)
	at java.net.Socket.connect(Socket.java:579)
	at java.net.Socket.connect(Socket.java:528)
	at sun.net.NetworkClient.doConnect(NetworkClient.java:180)
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:378)
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:473)
	at sun.net.www.http.HttpClient.<init>(HttpClient.java:203)
	at sun.net.www.http.HttpClient.New(HttpClient.java:290)
	at sun.net.www.http.HttpClient.New(HttpClient.java:306)
	at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:995)
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:931)
	at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:849)
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1299)
	at org.apache.hadoop.hdfs.server.namenode.TransferFsImage.getFileClient(TransferFsImage.java:160)
	at org.apache.hadoop.hdfs.server.namenode.GetImageServlet$1$1.run(GetImageServlet.java:88)
	at org.apache.hadoop.hdfs.server.namenode.GetImageServlet$1$1.run(GetImageServlet.java:85)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)
	at org.apache.hadoop.hdfs.server.namenode.GetImageServlet$1.run(GetImageServlet.java:85)
	at org.apache.hadoop.hdfs.server.namenode.GetImageServlet$1.run(GetImageServlet.java:70)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)
	at org.apache.hadoop.hdfs.server.namenode.GetImageServlet.doGet(GetImageServlet.java:70)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
	at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:835)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)


Please help...."
"When storageID of dfs.data.dir of being inconsistent, restart datanode will be failure.",HDFS-4343,"A datanode has multiple storage directories configured using dfs.data.dir. When the storageID in the VERSION files in these directories, the datanode fails to startup. Consider a scenario, when old data in a storage directory is not cleared, the storage ID from it will not match with storage ID of in other storage storage directories. In this situation, the DataNode will quit and restart fails."
DFSInputStream.reportCheckSumFailure do not report all corrupted blocks,HDFS-4263,"DFSInputStream.reportCheckSumFailure do not report all corrupted blocks, it seems the code forgets to loop through the corruptedBlockMap."
Add support for scheduled automatic snapshots,HDFS-4166,This jira will track the work related to supporting automatic scheduled snapshots. 
?Formatting HDFS running into errors :( - Many thanks ,HDFS-4109,"Hi,

I am trying to format the Hadoop file system with:

bin/hadoop namenode -format

But I received this error in Cygwin:

/home/anjames/bin/../conf/hadoop-env.sh: line 8: $鈥橽r鈥? command not found
/home/anjames/bin/../conf/hadoop-env.sh: line 14: $鈥橽r鈥? command not found
/home/anjames/bin/../conf/hadoop-env.sh: line 17: $鈥橽r鈥? command not found
/home/anjames/bin/../conf/hadoop-env.sh: line 25: $鈥橽r鈥? command not found
/bin/java; No  such file or directoryjre7
/bin/java; No  such file or directoryjre7
/bin/java; cannot execute: No such file or directory

I had previous modified the following conf files the cygwin/home/anjames directory
1. core-site.xml 
2. mapred-site.xml 
3. hdfs-site.xml 

4. hadoop-env.sh

-I updated this file using the instructions: ""uncomment the JAVA_HOME export command, and set the path to your Java home (typically C:/Program Files/Java/{java-home}""

i.e. In the ""hadoop-env.sh"" file, I took out the ""#"" infront of JAVA_HOME comment and changed the path as follows:

export JAVA_HOME=C:\Progra~1\Java\jre7



The hadoop-env.sh file is now:

----------------------------------------------------------------

# Set Hadoop-specific environment variables here.


# The only required environment variable is JAVA_HOME.  All others are
# optional.  When running a distributed configuration it is best to
# set JAVA_HOME in this file, so that it is correctly defined on
# remote nodes.


# The java implementation to use.  
export JAVA_HOME=C:\Progra~1\Java\jre7 ###<-----uncommented and revised code

# Extra Java CLASSPATH elements.  Optional.

# export HADOOP_CLASSPATH=


# The maximum amount of heap to use, in MB. Default is 1000.

# export HADOOP_HEAPSIZE=2000

# Extra Java runtime options.  Empty by default.
# export HADOOP_OPTS=-server


# Command specific options appended to HADOOP_OPTS when specified
export HADOOP_NAMENODE_OPTS=""-Dcom.sun.management.jmxremote $HADOOP_NAMENODE_OPTS""
export HADOOP_SECONDARYNAMENODE_OPTS=""-Dcom.sun.management.jmxremote $HADOOP_SECONDARYNAMENODE_OPTS""
export HADOOP_DATANODE_OPTS=""-Dcom.sun.management.jmxremote $HADOOP_DATANODE_OPTS""
export HADOOP_BALANCER_OPTS=""-Dcom.sun.management.jmxremote $HADOOP_BALANCER_OPTS""
export JAVA_HOME=C:\Progra~1\Java\jre7
HADOOP_JOBTRACKER_OPTS=""-Dcom.sun.management.jmxremote $HADOOP_JOBTRACKER_OPTS""
# export HADOOP_TASKTRACKER_OPTS=
# The following applies to multiple commands (fs, dfs, fsck, distcp etc)
# export HADOOP_CLIENT_OPTS

# Extra ssh options.  Empty by default.
# export HADOOP_SSH_OPTS=""-o ConnectTimeout=1 -o SendEnv=HADOOP_CONF_DIR""

# Where log files are stored.  $HADOOP_HOME/logs by default.
# export HADOOP_LOG_DIR=${HADOOP_HOME}/logs

# File naming remote slave hosts.  $HADOOP_HOME/conf/slaves by default.
# export HADOOP_SLAVES=${HADOOP_HOME}/conf/slaves

# host:path where hadoop code should be rsync'd from.  Unset by default.
# export HADOOP_MASTER=master:/home/$USER/src/hadoop

# Seconds to sleep between slave commands.  Unset by default.  This
# can be useful in large clusters, where, e.g., slave rsyncs can
# otherwise arrive faster than the master can service them.
# export HADOOP_SLAVE_SLEEP=0.1

# The directory where pid files are stored. /tmp by default.
# export HADOOP_PID_DIR=/var/hadoop/pids

# A string representing this instance of hadoop. $USER by default.
# export HADOOP_IDENT_STRING=$USER

# The scheduling priority for daemon processes.  See 'man nice'.
# export HADOOP_NICENESS=10


------------------------------

I'm trying to get back in the programming swing with a Big Data Analytics course, so any help is much appreciated its been a while, many thanks. 

"
TestJspHelper#testGetUgi fails with NPE,HDFS-3654,Looks like my recent change in HDFS-3639 can occasionally cause this test to fail. 
Incompatible change between hadoop-1 and hadoop-2 when the dfs.hosts and dfs.hosts.exclude files are not present,HDFS-3977,"While testing hadoop-1 and hadoop-2 the following was noticed

if the files in the properties dfs.hosts and dfs.hosts.exclude do not exist

in hadoop-1 namenode format and start went through successfully.

in hadoop-2 we get a file not found exception and both the format and the namenode start commands fail.


We should be logging a warning in the case when the file is not found so that we are compatible with hadoop-1"
NN is getting shutdown by throwing IllegalArgumentException ,HDFS-3467,"Scenario:
=========
Configure fs.default.name
Format Namenode
Start NameNode.

Here it is getting shutdown ..like following..

{noformat}
2012-05-25 21:09:30,810 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Exception in namenode join
java.lang.IllegalArgumentException: Invalid URI for NameNode address (check fs.defaultFS): file:/// has no authority.
	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:295)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:283)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.getRpcServerAddress(NameNode.java:336)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loginAsNameNodeUser(NameNode.java:388)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:400)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:570)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:551)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1114)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1170)
2012-05-25 21:09:30,812 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
{noformat}

This is happening in hadoop-2.0.1 But it's not like that in trunk."
Gracefully handle OutOfMemoryErrors,HDFS-2911,"We should gracefully handle j.l.OutOfMemoryError exceptions in the NN or DN. We should catch them in a high-level handler, cleanly fail the RPC (vs sending back the OOM stackrace) or background thread, and shutdown the NN or DN. Currently the process is left in a not well-test tested state (continuously fails RPCs and internal threads, may or may not recover and doesn't shutdown gracefully)."
Remove name.node.address servlet attribute,HDFS-3437,Per HDFS-3434 we should be able to get rid of NAMENODE_ADDRESS_ATTRIBUTE_KEY since we always call DfsServlet#createNameNodeProxy within the NN.
QJM: Add a few missing imports in TestEditLog and TestFSEditLogLoader,HDFS-3820,Both of these test files are missing a few imports required to make the compile.
TestNameNodeMetrics fails intermittently,HDFS-540,TestNameNodeMetrics has strict timing constraint that relies on block management functionality and can fail intermittently.
Enable the trash feature by default,HDFS-2740,"Currently trash is disabled out of box. I do not think it'd be of high surprise to anyone (but surely a relief when *hit happens) to have trash enabled by default, with the usually recommended periods of 1-day.

Thoughts?"
XML-based metrics as JSP servlet for NameNode,HDFS-453,"In HADOOP-4559, a general REST API for reporting metrics was proposed but work seems to have stalled. In the interim, we have a simple XML translation of the existing NameNode status page which provides the same metrics as the human-readable page. This is a relatively lightweight addition to provide some machine-understandable metrics reporting."
Add concat to FsShell,HDFS-950,Would be nice if concat (HDFS-222) was exposed up to FsShell so users don't have to use hadoop jar.
webhdfs able to set access and mod times of paths to a value in future,HDFS-2435,I am able to use the settimes api and set the access and modification time of paths to a value in future. Should that be allowed?
Wget retrieval of fsimage fails,HDFS-3588,"jlord$ wget http://localhost:50070/getimage?getimage=1 
--2012-06-28 17:45:48-- http://localhost:50070/getimage?getimage=1 
Resolving localhost... 127.0.0.1, ::1, fe80::1 
Connecting to localhost|127.0.0.1|:50070... connected. 
HTTP request sent, awaiting response... 410 GetImage failed. java.io.IOException: Invalid request has no txid parameter \tat org.apache.hadoop.hdfs.server.namenode.GetImageServlet$GetImageParams.parseLongParam(GetImageServlet.java:414) \tat org.apache.hadoop.hdfs.server.namenode.GetImageServlet$GetImageParams.<init>(GetImageServlet.java:338) \tat org.apache.hadoop.hdfs.server.namenode.GetImageServlet.doGet(GetImageServlet.java:85) \tat javax.servlet.http.HttpServlet.service(HttpServlet.java:707) \tat javax.servlet.http.HttpServlet.service(HttpServlet.java:820) \tat org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511) \tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221) \tat org.apache.hadoop.http.HttpServe"
Update DatanodeManager to resolve network location with new API from HADOOP-8304 (DNSToSwitchMapping should add interface to resolve individual host besides a list of host),HDFS-3324,HADOOP-8304 (DNSToSwitchMapping should add interface to resolve individual host besides a list of host) will induce a new API to resolve individual host rather than a list of host. Here is update on HDFS part to use new API.
While Balancing more than 10 Blocks are being moved from one DN even though the maximum number of blocks to be moved in an iterations is hard coded to 5,HDFS-3377,"Replication factor= 1,block size is default value
Step 1: Start NN,DN1
Step 2: Pump 5 GB of data.
Step 3: Start DN2 and issue balancer with threshold value 1

In the balancer report and the NN logs displays that more than 8 blocks are being moved from DN1 to DN2 in one iterations But MAX_NUM_CONCURRENT_MOVES in one iterations is hard coded to 5.
Balancer report for 1st iteration:
=================================
{noformat}
HOST-XX-XX-XX-XX:/home/Andreina/NewHadoop2nd/hadoop-2.0.0-SNAPSHOT/bin # ./hdfs balancer -threshold 1
12/05/03 17:31:28 INFO balancer.Balancer: Using a threshold of 1.0
12/05/03 17:31:28 INFO balancer.Balancer: namenodes = [hdfs://HOST-XX-XX-XX-XX:9002]
12/05/03 17:31:28 INFO balancer.Balancer: p         = Balancer.Parameters[BalancingPolicy.Node, threshold=1.0]
Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved
12/05/03 17:31:30 INFO net.NetworkTopology: Adding a new node: /datacenter1/rack1/YY.YY.YY.YY:50176
12/05/03 17:31:30 INFO net.NetworkTopology: Adding a new node: /datacenter1/rack1/XX.XX.XX.XX:50076
12/05/03 17:31:30 INFO balancer.Balancer: 1 over-utilized: [Source[XX.XX.XX.XX:50076, utilization=5.018416429773605]]
12/05/03 17:31:30 INFO balancer.Balancer: 1 underutilized: [BalancerDatanode[YY.YY.YY.YY:50176, utilization=3.272819804269012E-5]]
12/05/03 17:31:30 INFO balancer.Balancer: Need to move 1.06 GB to make the cluster balanced.
12/05/03 17:31:30 INFO balancer.Balancer: Decided to move 716.13 MB bytes from XX.XX.XX.XX:50076 to YY.YY.YY.YY:50176
12/05/03 17:31:30 INFO balancer.Balancer: Will move 716.13 MB in this iteration
May 3, 2012 5:31:30 PM            0                 0 KB             1.06 GB          716.13 MB
12/05/03 17:35:29 INFO balancer.Balancer: Moving block -5275260117334749945 from XX.XX.XX.XX:50076 to YY.YY.YY.YY:50176 through XX.XX.XX.XX:50076 is succeeded.
12/05/03 17:36:31 INFO balancer.Balancer: Moving block -8079758341763366944 from XX.XX.XX.XX:50076 to YY.YY.YY.YY:50176 through XX.XX.XX.XX:50076 is succeeded.
12/05/03 17:37:12 INFO balancer.Balancer: Moving block -7395554712490186313 from XX.XX.XX.XX:50076 to YY.YY.YY.YY:50176 through XX.XX.XX.XX:50076 is succeeded.
12/05/03 17:37:45 INFO balancer.Balancer: Moving block 7805443002654525130 from XX.XX.XX.XX:50076 to YY.YY.YY.YY:50176 through XX.XX.XX.XX:50076 is succeeded.
12/05/03 17:38:15 INFO balancer.Balancer: Moving block 1864290085256894184 from XX.XX.XX.XX:50076 to YY.YY.YY.YY:50176 through XX.XX.XX.XX:50076 is succeeded.
12/05/03 17:40:30 INFO balancer.Balancer: Moving block 23322655230037442 from XX.XX.XX.XX:50076 to YY.YY.YY.YY:50176 through XX.XX.XX.XX:50076 is succeeded.
12/05/03 17:41:24 INFO balancer.Balancer: Moving block -8839566903692469634 from XX.XX.XX.XX:50076 to YY.YY.YY.YY:50176 through XX.XX.XX.XX:50076 is succeeded.
12/05/03 17:43:03 INFO balancer.Balancer: Moving block 7304385435779271887 from XX.XX.XX.XX:50076 to YY.YY.YY.YY:50176 through XX.XX.XX.XX:50076 is succeeded.
12/05/03 17:43:48 INFO balancer.Balancer: Moving block -7242009026552182303 from XX.XX.XX.XX:50076 to YY.YY.YY.YY:50176 through XX.XX.XX.XX:50076 is succeeded.
12/05/03 17:44:06 INFO balancer.Balancer: Moving block -2449309138254106767 from XX.XX.XX.XX:50076 to YY.YY.YY.YY:50176 through XX.XX.XX.XX:50076 is succeeded.
12/05/03 17:44:55 INFO balancer.Balancer: Moving block 500930296233438046 from XX.XX.XX.XX:50076 to YY.YY.YY.YY:50176 through XX.XX.XX.XX:50076 is succeeded.
12/05/03 17:45:04 INFO balancer.Balancer: Moving block 2642725820310610865 from XX.XX.XX.XX:50076 to YY.YY.YY.YY:50176 through XX.XX.XX.XX:50076 is succeeded.{noformat}"
"When a file is deleted, its blocks remain in the blocksmap till the next block report from Datanode",HDFS-140,"When a file is deleted, the namenode sends out block deletions messages to the appropriate datanodes. However, the namenode does not delete these blocks from the blocksmap. Instead, the processing of the next block report from the datanode causes these blocks to get removed from the blocksmap.

If we desire to make block report processing less frequent, this issue needs to be addressed. Also, this introduces indeterministic behaviout to a a few unit tests. Another factor to consider is to ensure that duplicate block detection is not compromised.
"
"dfsadmin -refreshServiceAcl fails Kerb authentication with valid Kerb ticket, other subcommands succeed",HDFS-3001,"With a valid hdfs kerberos ticket, the dfsadmin subcommand '-refreshServiceAcl' still fails on Kerb authentication. Please see the comment for more details.
"
Why open method in class DFSClient would compare old LocatedBlocks and new LocatedBlocks?,HDFS-404,"This is in the package of org.apache.hadoop.hdfs, DFSClient.openInfo():
if (locatedBlocks != null) {
        Iterator<LocatedBlock> oldIter = locatedBlocks.getLocatedBlocks().iterator();
        Iterator<LocatedBlock> newIter = newInfo.getLocatedBlocks().iterator();
        while (oldIter.hasNext() && newIter.hasNext()) {
          if (! oldIter.next().getBlock().equals(newIter.next().getBlock())) {
            throw new IOException(""Blocklist for "" + src + "" has changed!"");
          }
        }
      }
Why we need compare old LocatedBlocks and new LocatedBlocks, and in what case it happen?
Why not ""this.locatedBlocks = newInfo"" directly?"
BookKeeper Journal Manager is not retrying to connect to BK when BookKeeper is not available for write.,HDFS-3392,"Scenario:

1. Start 3 bookKeeper and 3 zookeeper.
2. Start one NN as active & second NN as standby.
3. Write some file.
4. Stop all BookKeepers.

Issue:
Bookkeeper Journal Manager is not retrying to connect to BK when Bookkeeper is not available for write and Active namenode is shutdown.
"
Start-all.sh Error ,HDFS-3430,"Hi,
m new to Hadoop and trying to run hadoop on standalone Linux machine but faing some error please help me the Error is as follow :- 

2012-05-16 13:03:11,155 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = sra_hadoop.com/192.168.1.62
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 1.0.2-SNAPSHOT
STARTUP_MSG:   build =  -r ; compiled by 'root' on Wed May 16 12:30:17 IST 2012
************************************************************/
2012-05-16 13:03:11,363 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2012-05-16 13:03:11,379 INFO org.apache.hadoop.metrics2.impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.
2012-05-16 13:03:11,380 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2012-05-16 13:03:11,381 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2012-05-16 13:03:11,390 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.lang.IllegalArgumentException: Does not contain a valid host:port authority: hdfs://sra_hadoop:9000
        at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:162)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:198)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:228)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:262)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:496)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1279)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1288)

2012-05-16 13:03:11,391 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at sra_hadoop.com/192.168.1.62
"
fix FSEditLog#verifyEndOfLog,HDFS-3340,FSEditLog#verifyEndOfLog may incorrectly report corruption in some cases.  It should be fixed.
DN metrics should include per-disk utilization,HDFS-2999,We should have per-dfs.data.dir metrics in the DN's metrics report.
Httpfs: Cannot build because of wrong pom file,HDFS-3311,"On httpfs master 43f595d77d9d42ade5220bfba55ec13e558afb7a
{code}
hoop]$ mvn clean package site assembly:single
...
[WARNING] Unable to create Maven project from repository.
org.apache.maven.project.InvalidProjectModelException: 1 problem was encountered while building the effective model for com.sun.jersey:jersey-server:1.4
[FATAL] Non-parseable POM /home/yuki/.m2/repository/com/sun/jersey/jersey-project/1.4/jersey-project-1.4.pom: end tag name </body> must match start tag name <hr> from line 5 (position: TEXT seen ...</center>\r\n</body>... @6:8)  @ /home/yuki/.m2/repository/com/sun/jersey/jersey-project/1.4/jersey-project-1.4.pom, line 6, column 8
 for project com.sun.jersey:jersey-server:1.4 for project com.sun.jersey:jersey-server:1.4
	at org.apache.maven.project.DefaultMavenProjectBuilder.transformError(DefaultMavenProjectBuilder.java:193)
	at 
...
{code}
and
{code}
hoop]$ cat /home/yuki/.m2/repository/com/sun/jersey/jersey-project/1.4/jersey-project-1.4.pom

<html>
<head><title>301 Moved Permanently</title></head>
<body bgcolor=""white"">
<center><h1>301 Moved Permanently</h1></center>
<hr><center>nginx/0.6.39</center>
</body>
</html>

{code}
"
Add GETMERGE operation to httpfs,HDFS-2833,"Add to a convenience operation GETMERGE to httpfs. 

This will simplify for external system accessing over HTTP to consume the output of an MR job a single stream.

It would have the same semantics as the 'hadoop fs -getmerge' command."
Hadoop-1.0.2 is taking the wrong class path during setup ,HDFS-3239,"/usr/libexec/../bin/hadoop: line 321: /usr/lib/jvm/java-6-sun/bin/java: No such file or directory
/usr/libexec/../bin/hadoop: line 387: /usr/lib/jvm/java-6-sun/bin/java: No such file or directory
"
clean cache and can't start hadoop,HDFS-3117,"i use command cache >3 /proc/sys/vm/drop_caches to clean cache
Now i can't start hadoop.
thanks"
Build contrib,HDFS-3112,"I build capacity-shechuder contrib in src/contrib/capacity-scheduler
> ant package

but error is:

   [javac]                          ^
    [javac] /setup/hadoop-1.0.0/src/contrib/capacity-scheduler/src/java/org/apache/hadoop/mapred/CapacitySchedulerQueue.java:754: cannot find symbol
    [javac] symbol  : class JobInProgress
    [javac] location: class org.apache.hadoop.mapred.CapacitySchedulerQueue.UserInfo
    [javac]     public void jobAdded(JobSchedulingInfo jobSchedInfo, JobInProgress job) {
    [javac]                                                          ^
    [javac] /setup/hadoop-1.0.0/src/contrib/capacity-scheduler/src/java/org/apache/hadoop/mapred/CapacitySchedulerQueue.java:758: cannot find symbol
    [javac] symbol  : class JobSchedulingInfo
    [javac] location: class org.apache.hadoop.mapred.CapacitySchedulerQueue.User

   ........

   [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] 100 errors

BUILD FAILED
/setup/hadoop-1.0.0/src/contrib/build.xml:30: The following error occurred while executing this line:
/setup/hadoop-1.0.0/src/contrib/build-contrib.xml:185: Compile failed; see the compiler error output for details.

HADOOP_CLASSPATH=/setup/hadoop-1.0.0/lib/

thanks
"
Append: The condition is incorrect for checking whether the last block is full,HDFS-1624,"When the last block is full, the free space should be 0 but not equal to block size.
{code}
//In DFSOutputStream.DataStreamer.DataStreamer(..),
      if (freeInLastBlock == blockSize) {
        throw new IOException(""The last block for file "" + 
            src + "" is full."");
      }
{code}"
HA: enable hadoop security authorization for haadmin / protocols,HDFS-2926,Per HDFS-2917 we can enable Acls on haadmin via adding Acl support to the protocols.
Define a DFSClient interface for wire compatibility,HDFS-2155,"Define a DFSClient interface so that different versions of client can be loaded for talking to the corresponding version of server.  For more details, see HADOOP-7347."
"Move OfflineImageViewer, OfflineEditsViewer into the same package as the NameNode",HDFS-2990,"Since OfflineImageviewer and OfflineEditsVeiwer are in different namespaces than the NameNode, we can't ruuse a lot of the code from the NameNode without making things public that we probably don't want to make public.  Let's move them into the NameNode namespace to avoid this problem.  These tools will always be tightly tied to the NameNode anyway (they are parsing the same on-disk structures, after all), so that is where they belong."
some improvements to the manual NN metadata recovery tools,HDFS-2971,"Some improvements to the manual NN metadata recovery tools.

Specifically, we want the Offline Edit Viewer (oev) tool to prints out the highest generation stamp that was encountered when processing the edit log.

We also want OEV to look for large gaps in the generation stamp, as these can indicate corruption.  The minimum gap to look for should be configurable with -G or --genStampGap."
"Add service lifecycle to the HDFS classes: NameNode, Datanode, etc",HDFS-545,This is the HDFS portion of the service lifecycle changes: integrating the HDFS services: Namenode (and subclasses) and the Datanode with the Service base class. 
HA: TestDFSUtil is failing,HDFS-2960,"TestDFSUtil is failing.

{noformat}
org.junit.ComparisonFailure: expected:<ns1-nn1.example.com[]:8020> but was:<ns1-nn1.example.com[/220.250.64.24]:8020>
	at org.junit.Assert.assertEquals(Assert.java:123)
	at org.junit.Assert.assertEquals(Assert.java:145)
	at org.apache.hadoop.hdfs.TestDFSUtil.testHANameNodesWithFederation(TestDFSUtil.java:411)
{noformat}

"
Allow NNThroughputBenchmark to stress IPC layer,HDFS-2945,"Currently, the NNThroughputBenchmark acts only as a benchmark of the NN itself. It accesses the NN directly rather than going via IPC. It would be nice to allow it to run in another mode where all access goes through the IPC layer, so that changes in IPC serialization performance can be repeatably/easily measured."
hudson ignored a test failure while generating the junit test report.,HDFS-502,"http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-vesta.apache.org/25/console
console test logs show a test failure.

exec] [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 3.745 sec
[exec] [junit] Test org.apache.hadoop.hdfs.server.datanode.TestInterDatanodeProtocol FAILED

TestInterDatanodeProtocol test failed for some reason; and hudson while parsing the xml results didn't consider the test failure.
http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-vesta.apache.org/25/testReport/

I'm not sure if this is something to do with hudson junit plugin or  with hudson itself.

"
"Standby namenode gets a ""cannot lock storage"" exception during startup",HDFS-2865,"Standby NN is restarted. This is a follow-on to hdfs-2863. In this setup, dfs.edits.dir is different from dfs.shared.edits.dir. During startup, standby NN fails to acquire lock on the dfs.edits.dir. If standby NN is restarted again, it seems to work fine."
HA: Client should fail if a failover occurs which switches block pool ID,HDFS-2811,Making sure that the client is talking to an NN with the same block pool ID as the one it was previously talking to seems like a good sanity check.
HA : An alternative approach to clients handling  Namenode failover.,HDFS-2713,"This is the approach for client failover which we adopted when we developed HA for Hadoop. I would like to propose thia approach for others to review & include in the HA implementation, if found useful.

This is similar to the ConfiguredProxyProvider in the sense that the it takes the address of both the Namenodes as the input. The major differences I can see from the current implementation are
1) During failover, user threads can be controlled very accurately about *the time they wait for active namenode* to be available, awaiting the retry. Beyond this, the threads will not be made to wait; DFS Client will throw an Exception indicating that the operation has failed.
2) Failover happens in a seperate thread, not in the client application threads. The thread will keep trying to find the Active Namenode until it succeeds. 
3) This also means that irrespective of whether the operation's RetryAction is RETRY_FAILOVER or FAIL, the user thread can trigger the client's failover. "
Add option to fsck that checks the crc,HDFS-319,"If I'm not mistaken the fsck command doesn't actually check the crc of the blocks.
I just had a problem where a few blocks managed to get the size 0, fsck didn't report this but
my hadoop programs started failing because some of the input data was corrupted.

An option in the fsck program to actually check the crc would be nice, even though it would take a while to run."
HA: support 2NN with SBN,HDFS-2736,"HDFS-2291 adds support for making the SBN capable of checkpointing, seems like we may also need to support the 2NN checkpointing as well. Eg if we fail over to the SBN does it continue to checkpoint? If not the log grows unbounded until the old primary comes back, if so does that create performance problems since the primary wasn't previously checkpointing?"
Add support for the standby in the bin scripts,HDFS-2732,"We need to update the bin scripts to support SBNs. Two ideas:

Modify start-dfs.sh to start another copy of the NN if HA is configured. We could introduce a file similar to masters (2NN hosts) called standbys which lists the SBN hosts, and start-dfs.sh would automatically make the NN it starts active (and leave the NNs listed in standby as is).

Or simpler, we could just provide a start-namenode.sh script that a user can run to start the SBN on another host themselves. The user would manually tell the other NN to be active via HAAdmin (or start-dfs.sh could do that automatically, ie assume the NN it starts should be the primary)."
Streaming task stuck in MapTask$DirectMapOutputCollector.close,HDFS-105,Observed a streaming task stuck in MapTask$DirectMapOutputCollector.close
Regression: TestInjectionForSimulatedStorage fails with IllegalMonitorStateException,HDFS-146,"org.apache.hadoop.hdfs.TestInjectionForSimulatedStorage.testInjection fails with IllegalMonitorStateException

Stacktrace
java.lang.IllegalMonitorStateException
	at java.lang.Object.notifyAll(Native Method)
	at org.apache.hadoop.ipc.Server.stop(Server.java:1110)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown(DataNode.java:574)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdownDataNodes(MiniDFSCluster.java:569)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:553)
	at org.apache.hadoop.hdfs.TestInjectionForSimulatedStorage.testInjection(TestInjectionForSimulatedStorage.java:195)

No errors show up in the standard output, but there are a few warnings.
http://hudson.zones.apache.org/hudson/view/Hadoop/job/Hadoop-trunk/749/testReport/org.apache.hadoop.hdfs/TestInjectionForSimulatedStorage/testInjection/"
Unit test failed: TestInjectionForSimulatedStorage,HDFS-44,"Unit test failed: TestInjectionForSimulatedStorage failed in the nightly build with a timeout:

tail from the console:

[junit] 2007-12-12 12:02:18,674 INFO  dfs.TestInjectionForSimulatedStorage (TestInjectionForSimulatedStorage.java:waitForBlockReplication(89)) - Not enough replicas for 4th block blk_4235117719756274078 yet. Expecting 4, got 5.
    [junit] 2007-12-12 12:02:19,184 INFO  dfs.TestInjectionForSimulatedStorage (TestInjectionForSimulatedStorage.java:waitForBlockReplication(89)) - Not enough replicas for 4th block blk_4235117719756274078 yet. Expecting 4, got 5.
    [junit] 2007-12-12 12:02:19,694 INFO  dfs.TestInjectionForSimulatedStorage (TestInjectionForSimulatedStorage.java:waitForBlockReplication(89)) - Not enough replicas for 4th block blk_4235117719756274078 yet. Expecting 4, got 5.
    [junit] 2007-12-12 12:02:20,204 INFO  dfs.TestInjectionForSimulatedStorage (TestInjectionForSimulatedStorage.java:waitForBlockReplication(89)) - Not enough replicas for 4th block blk_4235117719756274078 yet. Expecting 4, got 5.
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] Test org.apache.hadoop.dfs.TestInjectionForSimulatedStorage FAILED (timeout)

Complete console log:
http://lucene.zones.apache.org:8080/hudson/job/Hadoop-Nightly/330/console"
TestInjectionForSimulatedStorage fails once in a while,HDFS-104,"TestInjectionForSimulatedStorage fails once in a while.
"
high cpu usage in ReplicationMonitor thread ,HDFS-102,"We had a namenode stuck in CPU 99% and it  was showing a slow response time.
(dfs.namenode.handler.count was still set to 10.)

ReplicationMonitor thread was using the most CPU time.
Jstack showed,

""org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor@1c7b0f4d"" daemon prio=10 tid=0x0000002d90690800 nid=0x4855 runnable [0x0000000041941000..0x0000000041941b30]
   java.lang.Thread.State: RUNNABLE
  at java.util.AbstractList$Itr.remove(AbstractList.java:360)
  at org.apache.hadoop.dfs.FSNamesystem.blocksToInvalidate(FSNamesystem.java:2475)
  - locked <0x0000002a9f522038> (a org.apache.hadoop.dfs.FSNamesystem)
  at org.apache.hadoop.dfs.FSNamesystem.computeDatanodeWork(FSNamesystem.java:1775)
  at org.apache.hadoop.dfs.FSNamesystem$ReplicationMonitor.run(FSNamesystem.java:1713)
  at java.lang.Thread.run(Thread.java:619)
"
NameNode should not serve up a bad edits log,HDFS-93,"A NameNode disk failure (apparently) resulted in the NameNode serving a bad edits log to the Secondary NameNode. The SNN observed the problem (good!), but had no alternative but to ask again for the log, and again get the same bad replica.

1. The NN could/should have observed the same fault as the SNN.
2. If a replica is known to be bad, the NN should serve a different replica, if available.
3. The SNN should have a way to report replica failure to the NN."
Change all references of dfs to hdfs in configs,HDFS-55,"After code restructuring dfs has been changed to hdfs, but I see config variables with dfs.<something> eg dfs.http.address. Should we change everything to hdfs?"
DistributedFileSystem.listPaths with some paths causes directory to be cleared,HDFS-58,"I am currently writing a Ruby wrapper to the Java DFS client libraries via JNI. While attempting to test the listPaths method of the FileSystem class, I discovered that passing a Path URI like ""hdfs://tf11:7276/user/rapleaf"" results in the /user/rapleaf directory being cleared of all contents. A path URI like ""hdfs://tf11:7276/user/rapleaf/*"" will list the contents of the directory without damage. 

I have verified this by creating directories and listing via the bin/hadoop dfs -ls command. 

Obviously passing an incorrectly formatted string a method that should be read-only should not have destructive effects. Also, the actual required path syntax for listings should be recorded in the documentation."
Datanode shutdown is called multiple times ,HDFS-61,"- When DataNode gets {{IncorrectVersionException}} in {{DataNode.offerService()}} {{DataNode.shutdown()}} is called
- In {{DataNode.processCommand()}} when DataNode gets DNA_SHUTDOWN, {{DataNode.shutdown()}} is called

{{DataNode.shutdown()}} is again cal"
Unhandled exceptions in DFSClient,HDFS-19,"DFSOutputStream.handleSocketException() does not handle exceptions thrown inside it
by abandonBlock(). I'd propose to retry abandonBlock() in case of timeout.
In case of DFSOutputStream.close() the exception in handleSocketException() will result in
calling abandonFileInProgress().
In a similar case of DFSOutputStream.flush() the file will not be abandoned.
Exceptions thrown by abandonFileInProgress() are not handled either.

Feels like we need a general mechanism for handling all these things."
Check that network topology is updated when new data-nodes are joining the cluster,HDFS-5,There is a suspicion that network topology is not updated if new racks are added to the cluster. We should investigate and either confirm or rule out this.
DFS logging in NameSystem.pendingTransfer consumes all disk space,HDFS-10,"Sometimes the namenode goes crazy.  I see this in my logs:

2007-04-28 02:40:46,992 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.pendingTransfer: ask x.y.z.243:50010 to replicate blk_-9064654741761822118 to datanode(s) x.y.z.247:50010
2007-04-28 02:40:46,992 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.pendingTransfer: ask x.y.z.243:50010 to replicate blk_-8996500637974689840 to datanode(s) x.y.yz.225:50010
2007-04-28 02:40:46,992 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.pendingTransfer: ask x.y.z.227:50010 to replicate blk_-8870980160272831217 to datanode(s) x.y.z.244:50010
2007-04-28 02:40:46,992 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.pendingTransfer: ask x.y.z.227:50010 to replicate blk_-8721101562083234290 to datanode(s) x.y.z.250:50010
2007-04-28 02:40:46,992 INFO org.apache.hadoop.dfs.StateChange: BLOCK* NameSystem.pendingTransfer: ask x.y.z.250:50010 to replicate blk_-9044741671491162229 to datanode(s) x.y.z.244:50010

There are on the order of 10k/sec until the machine runs out of disk space.

I notice that in FSNamesystem.java, about 10 lines above this line is logged, there is a comment:

        //
        // Move the block-replication into a ""pending"" state.
        // The reason we use 'pending' is so we can retry
        // replications that fail after an appropriate amount of time.
        // (REMIND - mjc - this timer is not yet implemented.)
        //
"
unresponsive namenode because of not finding places to replicate,HDFS-21,"We have a 80 node cluster where many nodes started to fail such it went down to 59 live nodes. Originally we had our set of applications 60 times replicated. The cluster size went below the required replication number, and started to become increasingly less responsive, spewing out the following messages at a high rate:

WARN org.apache.hadoop.fs.FSNamesystem: Not able to place enough replicas, still in need of 2

"
test-contrib target fails on hdfsproxy tests,HDFS-667,"hdfsproxy test fails only on hadoop-20 branch.
http://hudson.zones.apache.org/hudson/view/Hadoop/job/Hadoop-20-Build/33/console"
TestDFSShell is failing on trunk,HDFS-1819,"The commit of HADOOP-7202 now requires that classes that extend {{FsCommand}} implement the {{void run(PathData)}} method. The {{Count}} class was changed to extend {{FsCommand}}, but renamed the {{run}} method and did not provide a replacement."
IOE in org.apache.hadoop.hdfs.server.namenode.GetImageServlet,HDFS-2549,"Cluster is running but i see in logs this exception:

2011-11-10 01:57:32,851 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of transactions: 9 Total time for transactions(ms): 1Number of transactions batched in Syncs: 0 Number of syncs: 6 SyncTimes(ms): 103                                                                                                 
2011-11-10 01:57:34,403 INFO org.apache.hadoop.hdfs.server.namenode.GetImageServlet: Downloaded new fsimage with checksum: 59f76fc93e2ab11ff9bb966e37bbb696                                                                                                                                                                    
2011-11-10 01:57:34,414 WARN org.mortbay.log: /getimage: java.io.IOException: GetImage failed. java.io.IOException: Actual checksum of transferred fsimage: 59f76fc93e2ab11ff9bb966e37bbb696 does not match expected checksum: null                                                                                            
        at org.apache.hadoop.hdfs.server.namenode.GetImageServlet$1$1.run(GetImageServlet.java:102)                                                                                                                                                                                                                            
        at org.apache.hadoop.hdfs.server.namenode.GetImageServlet$1$1.run(GetImageServlet.java:95)                                                                                                                                                                                                                             
        at java.security.AccessController.doPrivileged(Native Method)                                                                                                                                                                                                                                                          
        at javax.security.auth.Subject.doAs(Subject.java:396)                                                                                                                                                                                                                                                                  
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)                                                                                                                                                                                                                                
        at org.apache.hadoop.hdfs.server.namenode.GetImageServlet$1.run(GetImageServlet.java:95)                                                                                                                                                                                                                               
        at org.apache.hadoop.hdfs.server.namenode.GetImageServlet$1.run(GetImageServlet.java:78)                                                                                                                                                                                                                               
        at java.security.AccessController.doPrivileged(Native Method)                                                                                                                                                                                                                                                          
        at javax.security.auth.Subject.doAs(Subject.java:396)                                                                                                                                                                                                                                                                  
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)                                                                                                                                                                                                                                
        at org.apache.hadoop.hdfs.server.namenode.GetImageServlet.doGet(GetImageServlet.java:78)                                                                                                                                                                                                                               
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)                                                                                                                                                                                                                                                        
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)                                                                                                                                                                                                                                                        
        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)                                                                                                                                                                                                                                              
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)                                                                                                                                                                                                                             
        at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:835)                                                                                                                                                                                                                                  
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)                                                                                                                                                                                                                             
        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)                                                                                                                                                                                                                                            
        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)                                                                                                                                                                                                                                         
        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)                                                                                                                                                                                                                                            
        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)                                                                                                                                                                                                                                            
        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)                                                                                                                                                                                                                                               
        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)                                                                                                                                                                                                                        
        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)                                                                                                                                                                                                                                            
        at org.mortbay.jetty.Server.handle(Server.java:326)                                                                                                                                                                                                                                                                    
        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)                                                                                                                                                                                                                                             
        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)                                                                                                                                                                                                                             
        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)                                                                                                                                                                                                                                                         
        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)                                                                                                                                                                                                                                                    
        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)                                                                                                                                                                                                                                                    
        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)                                                                                                                                                                                                                                        
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)                                                                                                                                                                    "
TSocket timed out reading 4 bytes error against HadoopThriftServer from Hadoop-0.20.2 in Perl,HDFS-2550,"For the past few weeks I have randomly been receiving errors via the Perl binding of ThriftFS TSocket cannot read 4 bytes.  I thought that it was a symptom of too many clients against the server (say 16 doing a mix of file read and write, as well as listStatus), but in the past couple of days, I have started getting them all the time, even with only 1 client trying to read.  The Perl client error is:

{noformat}
$VAR1 = bless( {
                 'code' => 0,
                 'message' => 'TSocket: timed out reading 4 bytes from bigwws001:9090'
               }, 'Thrift::TException' );
{noformat}

This typically happens in conjunction with errors in other clients, thus initially leading me to believe that it was really a timeout issue.  But after it started occurring with only 1 client running and well within the 10 sec timeout I had set on the TSocket within my Perl client, I checked on the exceptions in the ThriftFS server log:

{noformat}
java.lang.StringIndexOutOfBoundsException: String index out of range: -1
        at java.lang.String.checkBounds(String.java:397)
        at java.lang.String.<init>(String.java:442)
        at org.apache.hadoop.thriftfs.HadoopThriftServer$HadoopThriftHandler.read(HadoopThriftServer.java:307)
        at org.apache.hadoop.thriftfs.api.ThriftHadoopFileSystem$Processor$read.process(Unknown Source)
        at org.apache.hadoop.thriftfs.api.ThriftHadoopFileSystem$Processor.process(Unknown Source)
        at com.facebook.thrift.server.TThreadPoolServer$WorkerProcess.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}


This change seems to have ""solved"" the problem, but this is very much a hack since I do not know the code (maybe throwing an IOException is the right thing to do, and then let it turn into a ThriftIOException?).

{noformat}
--- src/contrib/thriftfs/src/java/org/apache/hadoop/thriftfs/HadoopThriftServer.java-orig       2011-11-11 09:18:44.000000000 -0600
+++ src/contrib/thriftfs/src/java/org/apache/hadoop/thriftfs/HadoopThriftServer.java    2011-11-11 09:00:47.000000000 -0600
@@ -303,8 +303,9 @@
         }
         byte[] tmp = new byte[length];
         int numbytes = in.read(offset, tmp, 0, length);
-        HadoopThriftHandler.LOG.debug(""read done: "" + tout.id);
-        return new String(tmp, 0, numbytes, ""UTF-8"");
+        HadoopThriftHandler.LOG.debug(""read done: "" + tout.id +
+                                     "" numbytes: "" + numbytes);
+        return new String(tmp, 0, numbytes > 0 ? numbytes : 0, ""UTF-8"");
       } catch (IOException e) {
         throw new ThriftIOException(e.getMessage());
       }

{noformat}
"
with hadoop.security.group.mapping settings two extra INFO with every hadoop command,HDFS-2446,"If we add following config parameter in core-site.xml in all the compute nodes and client side, we get extra two INFO message in STDOUT for every hadoop command

core-site.xml
=============
...
  <property>
    <name>hadoop.security.group.mapping</name>
    <value>org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping</value>
    <description>
    Class for user to group mapping (get groups for a given user) for ACL
    </description>
  </property>
....

$hadoop dfs -lsr /tmp
11/10/12 19:04:58 INFO util.NativeCodeLoader: Loaded the native-hadoop library
11/10/12 19:04:58 INFO security.JniBasedUnixGroupsMapping: Using JniBasedUnixGroupsMapping for Group resolution
drwx------   - hadoopqa hdfs          0 2011-10-12 19:04 /tmp/file1
drwx------   - hadoopqa hdfs          0 2011-10-12 19:04 /tmp/file2
$hadoop dfs -rmr -skipTrash /tmp/file2
11/10/12 19:04:54 INFO util.NativeCodeLoader: Loaded the native-hadoop library
11/10/12 19:04:54 INFO security.JniBasedUnixGroupsMapping: Using JniBasedUnixGroupsMapping for Group resolution
Deleted hdfs://<NN Hostname>/tmp/file2"
Fix the 2 release audit warnings in trunk.,HDFS-2448,"/home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/resources/META-INF/services/org.apache.hadoop.security.token.TokenRenewer
/home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/META-INF/services/org.apache.hadoop.security.token.TokenRenewer
Lines that start with ????? in the release audit report indicate files that do not have an Apache license header.
So, we can include them in rat configuration item in pom.xml to exclude from release audit warnings.

"
"The elephant should remember names, not numbers.",HDFS-34,"The name node and the data node should not cache the resolution of host names, as doing so prevents the use of DNS CNAMEs for any sort of fail over capability."
Post users:  need admin-only access to HDFS,HDFS-226,"When user support gets added to HDFS, administrators are going to need to be able to set the namenode such that it only allows connections/interactions from the administrative user.  This is particularly important after upgrades and for other administrative work that may require the changing of user/group ownership, permissions, location of files within the HDFS, etc."
Name node doesn't always properly recognize health of data node,HDFS-577,The one-way communication (data node -> name node) for node health does not guarantee that the data node is actually healthy.
Name node will exit safe mode w/0 blocks even if data nodes are broken,HDFS-580,"If one brings up a freshly formatted name node against older data nodes with an incompatible storage id (such that the datanodes fail with Directory /mnt/u001/dfs-data is in an inconsistent state: is incompatible with others.), the name node will still come out of safe mode.  Writes will partially succeed--entries are created, but all are zero length.
"
HDFS should support SNMP,HDFS-607,HDFS should provide key statistics over a standard protocol such as SNMP.  This would allow for much easier integration into common software packages that are already established in the industry.
Topology is permanently cached,HDFS-870,Replacing the topology script requires a namenode bounce because the NN caches the information permanently.  It should really either expire it periodically or expire on -refreshNodes.
utility to list all files less than X replication,HDFS-1049,"It would be great to have a utility that lists all files that have a replication less than X.  While fsck provides this output and it isn't that tricky to parse, it would still be nice if Hadoop had this functionality out of the box."
Bouncing the namenode causes JMX deadnode count to drop to 0,HDFS-2275,The namenode JMX metrics and the namenode web UI disagree on the number of dead nodes.  See comments.
hftp: not able to delete content using hftp url,HDFS-2374,"issue command 

bin/hadoop dfs -rmr hftp://nn:port/path

Command fails with the following exception

11/09/27 03:53:43 WARN fs.Trash: Can't create trash directory: hftp://nn:port/user/some_user/.Trash/Current/path
Problem with Trash.Not supported. Consider using -skipTrash option"
app master configuration web UI link under the Job menu opens up application menu,HDFS-2357,"If you go to the app master web UI for a particular job.  The job menu on the left side displays links for overview, counters, configuration, etc..

If you click on the configuration one, it closes the job menu and opens the application menu on that left side.  It shouldn't do this.  It should leave the job menu open."
Improve metrics for measuring NN startup costs.,HDFS-1729,"Current logging and metrics are insufficient to diagnose latency problems in cluster startup.  Add:
1. better logs in both Datanode and Namenode for Initial Block Report processing, to help distinguish between block
report processing problems and RPC/queuing problems;
2. new logs to measure cost of scanning all blocks for over/under/invalid replicas, which occurs in Namenode just
before exiting safe mode;
3. new logs to measure cost of processing the under/invalid replica queues (created by the above mentioned scan), which
occurs just after exiting safe mode, and is said to take 100% of CPU."
HDFS logs not being rotated,HDFS-2302,"In commit c5edca2b15eca7c0bd568a0017f699ac91b8aebf, the logs for the namenode, datanode and secondarynamenode are being written to .out files and are not being rotated after one day. IMHO rotation of logs is important"
Call to localhost/127.0.0.1:54310 failed on connection exception: Connection refused,HDFS-2295,"when I try it
dm@master:/usr/local/hadoop-0.20.2$ bin/hadoop dfs -ls hdfs://localhost:54310/
It cast these exceptions
11/08/29 11:34:29 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:54310. Already tried 0 time(s).
11/08/29 11:34:30 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:54310. Already tried 1 time(s).
11/08/29 11:34:31 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:54310. Already tried 2 time(s).
11/08/29 11:34:32 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:54310. Already tried 3 time(s).
11/08/29 11:34:33 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:54310. Already tried 4 time(s).
11/08/29 11:34:34 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:54310. Already tried 5 time(s).
11/08/29 11:34:35 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:54310. Already tried 6 time(s).
11/08/29 11:34:36 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:54310. Already tried 7 time(s).
11/08/29 11:34:37 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:54310. Already tried 8 time(s).
11/08/29 11:34:38 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:54310. Already tried 9 time(s).
ls: Call to localhost/127.0.0.1:54310 failed on connection exception: java.net.ConnectException: Connection refused
Our namenode is listening to the 54310 port.
<name>fs.default.name</name>
<value>hdfs://master:54310</value>

"
Add the ability to set dfs.block.size for a given file from the command line,HDFS-2293,"See the thread [here|http://www.mail-archive.com/hdfs-user@hadoop.apache.org/msg01459.html], in which Allen correctly points out that since dfs.block.size is a client-side config, one could just set a different value in a custom HADOOP_CONF_DIR. We might as well make that easy."
Accelerate chmod task in ant files,HDFS-1834,"The chmod tasks have a parallel=""false"" attribute attached to them.  This causes ant to shell out for each and every item in the fileset which slows down parts of the build."
dfs.blocksize and dfs.block.size are ignored,HDFS-2216,"Hi everybody, 
I have a big problem with the blocksize configuration I tried different configurations at the hdfs-site config (datanodes and namenodes have the same configuration)

i tried 
<property>
  <name>dfs.block.size</name>
  <value>128m</value>
  <final>true</final>
</property>
-----------------------
<property>
  <name>dfs.blocksize</name>
  <value>128m</value>
  <final>true</final>
</property>
<property>
  <name>dfs.block.size</name>
  <value>134217728</value>
  <final>true</final>
</property>
-----------------------
<property>
  <name>dfs.blocksize</name>
  <value>134217728</value>
  <final>true</final>
</property>

But in all cases when I run a map-reduce job i found that the amount of slots is proportional to 64M blocks and if a run a du -hs in all datanode a found that the block files are 65M

For example 

65M	blk_720821373677199742
520K	blk_720821373677199742_13833.meta
65M	blk_-7294849724164540020
520K	blk_-7294849724164540020_7314.meta
65M	blk_7468624346905314857
520K	blk_7468624346905314857_7312.meta
65M	blk_7638666943543421576
520K	blk_7638666943543421576_7312.meta
65M	blk_7830551307355288414
520K	blk_7830551307355288414_7314.meta
65M	blk_7844142950685471855
520K	blk_7844142950685471855_7312.meta
65M	blk_7978753697206960302
520K	blk_7978753697206960302_7312.meta
65M	blk_-7997715050017508513
520K	blk_-7997715050017508513_7313.meta
65M	blk_-8085168141075809653
520K	blk_-8085168141075809653_7314.meta
65M	blk_8250324684742742886
520K	blk_8250324684742742886_7314.meta
65M	blk_839132493383742510
520K	blk_839132493383742510_7312.meta
65M	blk_847712434829366950
520K	blk_847712434829366950_7314.meta
65M	blk_-8735461258629196142

but if I run ""hadoop fs -stat %o file"" I get 134217728

Do yo know if I am doing same wrong? or It's souns a bug

Thanks"
Add simple HTTP GET support for single cluster case and for both Jetty and Tomcat based proxy ,HDFS-474,This is a follow-up to HADOOP-5366. Currently simple HTTP GET support only works for tomcat-based proxy and only when it is set up to proxy for multiple HDFS clusters (see HADOOP-5363). This feature supports users using commonly available HTTP clients like wget or curl to fetch files from proxy. It would be good to port this feature to both Jetty and Tomcat based proxies in cases where the proxy is set up to proxy for a single HDFS cluster. Note that currently Jetty-based proxy can't be set up to proxy for multiple HDFS clusters.
TestProxyUtil failing test-patch builds,HDFS-936,"TestProxy has been consistently failing HDFS test-patch builds the last day or two:

junit.framework.AssertionFailedError: null
	at org.apache.hadoop.hdfsproxy.TestProxyUtil.testSendCommand(TestProxyUtil.java:43)"
test-cactus fails with timeout error on trunk,HDFS-911,"test-cactus target is failing on trunk:
{noformat}
test-cactus:
     [echo]  Free Ports: startup-17053 / http-17054 / https-17055
     [echo] Please take a deep breath while Cargo gets the Tomcat for running the servlet tests...
     [copy] Copying 1 file to /private/tmp/zok/hadoop-hdfs/build/contrib/hdfsproxy/target/tomcat-config/conf
   [cactus] -----------------------------------------------------------------
   [cactus] Running tests against Tomcat 5.x @ http://localhost:17054
   [cactus] -----------------------------------------------------------------
   [cactus] Deploying [/private/tmp/zok/hadoop-hdfs/build/contrib/hdfsproxy/target/test.war] to [/private/tmp/zok/hadoop-hdfs/build/contrib/hdfsproxy/target/tomcat-config/
webapps]...
   [cactus] Tomcat 5.x starting...
   [cactus] Tomcat 5.x started on port [17054]

BUILD FAILED
/private/tmp/zok/hadoop-hdfs/src/contrib/hdfsproxy/build.xml:292: Failed to start the container after more than [180000] ms. Trying to connect to the [http://localhost:170
54/test/ServletRedirector?Cactus_Service=RUN_TEST] test URL yielded a [-1] error code. Please run in debug mode for more details about the error.{noformat}"
Restarting the namenode when the secondary namenode is checkpointing seems to remove everything from /,HDFS-2177,This was again discovered by Arpit Gupta! Restarting the namenode when the secondary namenode is checkpointing seems to remove everything from /
Using the hadoop-deamon.sh script to start nodes leads to a depricated warning ,HDFS-2122,hadoop-daemon.sh calls common/bin/hadoop for hdfs/bin/hdfs tasks and so common/bin/hadoop complains its deprecated for those uses.
HDFS FileStatus.getPermission() should return 777 when dfs.permissions=false,HDFS-17,"Generic permission checking code should still work correctly when dfs.permissions=false.  Currently FileStatus#getPermission() returns the actual permission when dfs.permissions=false on the namenode, which is incorrect, since all accesses are permitted in this case."
Web UI fails with non-default port,HDFS-1196,"On trunk using hdfs://localhost:9000 for fs.default.name going to the web UI http://localhost:9000 fails with the following stack trace: 

10/06/08 23:07:19 INFO namenode.NameNode: NameNode up at: localhost/127.0.0.1:9000
10/06/08 23:08:45 INFO ipc.Server: IPC Server listener on 9000: readAndProcess threw exception java.io.IOException: Unable to read authentication method. Count of bytes read: 0
java.io.IOException: Unable to read authentication method
	at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1092)
	at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:525)
	at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:332)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)

Use port 50070 works (but says ""NameNode 'localhost:9000'"").
"
Hadoop does not scale as expected,HDFS-2091,"The more nodes I add to this application, the slower it goes. This is the app's map,

 public void map(IntWritable linearPos, FloatWritable heat, Context context
                            ) throws IOException, InterruptedException {

       int myLinearPos = linearPos.get();
       //Distribute my value to the previous and the next
       linearPos.set(myLinearPos - 1);
       context.write(linearPos, heat);
       linearPos.set(myLinearPos + 1);
       context.write(linearPos, heat);
       //Distribute my value to the cells above and below
       linearPos.set(myLinearPos - MatrixData.Length());
       context.write(linearPos, heat);
       linearPos.set(myLinearPos + MatrixData.Length());
       context.write(linearPos, heat);
    }//end map

and this is the reduce,

public void reduce(IntWritable linearPos, Iterable<FloatWritable> fwValues,
                     Context context) throws IOException, InterruptedException {

       //Handle first and last ""cold"" boundaries
       if(linearPos.get()<0 || linearPos.get()>MatrixData.LinearSize()){
          return;
       }

       if(linearPos.get()==MatrixData.HeatSourceLinearPos()){
          context.write(linearPos, new FloatWritable(MatrixData.HeatSourceTemperature()));
          return;
       }

       float result = 0.0f;
       //Add all the values
       for(FloatWritable heat : fwValues) {
          result += heat.get();
       }

      context.write(linearPos, new FloatWritable(result/4) );
}

For example, with 6 nodes I get a running time of 15minutes, and with 4 nodes I get a running time of 8minutes!.
This is how I generated the input,

 public static void main(String[] args) throws IOException {
     //Write file in the local dir
     String uri = ""/home/beto/mySeq"";

     Configuration conf = new Configuration();
     FileSystem fs = FileSystem.get(URI.create(uri), conf);
     Path path = new Path(uri);

     IntWritable key = new IntWritable();
     FloatWritable value = new FloatWritable(0.0f);

     SequenceFile.Writer writer = null;
     try {
       writer = SequenceFile.createWriter(fs, conf, path, key.getClass(), value.getClass());

     int step = MatrixData.LinearSize()/10;
     int limit = step;
     for (int i = 0; i <= MatrixData.LinearSize(); i++) {
        key.set(i);
        if(i>limit){
             System.out.println(""*"");
             limit +=step;
        }
          if(i==MatrixData.HeatSourceLinearPos()) {
            writer.append(key, new FloatWritable(MatrixData.HeatSourceTemperature()));
            continue;
          }

        writer.append(key, value);

      }
    } finally {
      IOUtils.closeStream(writer);
    }
  }


I'm basically solving a heat transfer problem in a squared section. Pretty simple. The input data is being stored as a (key, value) pairs, read in this way, processed, and written again in the same format.
Any thoughts?

Alberto.
"
Can't read binary data off HDFS via thrift API,HDFS-1169,"Trying to access binary data stored in HDFS (in my case, TypedByte files generated by Dumbo) via thrift talking to org.apache.hadoop.thriftfs.HadoopThriftServer, the data I get back is mangled. For example, when I read a file which contains the value 0xa2, it's coming back as 0xef 0xbf 0xbd, also known as the Unicode replacement character.

I think this is because the read method in HadoopThriftServer.java is trying to convert the data read from HDFS into UTF-8 via the String() constructor. 

This essentially makes the HDFS thrift API useless for me :-(.

Not being an expert on Thrift, but would it be possible to modify the API so that it uses the binary type listed on http://wiki.apache.org/thrift/ThriftTypes?"
thriftfs jar cleanup,HDFS-1484,"Thriftfs has some jars checked into the source directory (src/contrib/thriftfs/lib). It contains libthrift.jar, which we should get via ivy,  and hadoopthriftapi.jar which is a checked in (out of date?) binary of the generated java code. This file could probably be removed entirely. "
Add Thrift interface to JobTracker/TaskTracker,HDFS-419,"We currently have Thrift interfaces for accessing the NN and the DFS, but no access to the Mapred system. I'm currently working on instrumenting the JT with a Thrift plugin. If anyone has any thoughts in this area, please comment on this JIRA.

Open questions:
* Is job submission a practical goal to accomplish via Thrift? My thought is that this might be a goal for a second JIRA after basic monitoring/reporting is working.
* Does this belong in contrib/thriftfs? I propose renaming contrib/thriftfs to contrib/thrift, as it will no longer be FS-specific."
Move new per-daemon Thrift contrib into a new contrib project/package layout,HDFS-418,The discussion in HADOOP-4707 has moved away from the original intent of that issue. Opening this one to continue the discussion of this rename/move.
Add conf to classpath in start_thrift_server.sh,HDFS-789,"In the current script start_thrift_server.sh, the conf folder is not in classpath, so when user start the thrift server, it actually use the local file system.
So I create this issue to put the hdfs configuration file to classpath."
Hadoop Namenode not starting up.,HDFS-1864,"1. Checked to make sure hadoop was running properly. Discovered that we suppose to run 'jps' and make sure there is a namenode process. 

2. Documentation said, if namenode does not exist - then run 

/etc/init.d/hadoop-0.20-namenode start 

/etc/init.d/hadoop-0.20-namenode status - namenode process fails 

 EQX hdfs@hadoop-master:/usr/lib/hadoop/bin$ /etc/init.d/hadoop-0.20-namenode status 
namenode dead but pid file exists 


3. Searched for pid files. We deleted pid files. 

4. All over stats fell off. As direct of result, looking at the process list - and there appeared to be a stalled process that was killed. 

kill -9 

for the following process: 

EQX root@hadoop-master:/etc/init.d# ps aux | grep namenode 
hdfs 5038 0.2 1.0 3617440 526704 ? Sl Mar31 74:02 /usr/java/default/bin/java -Dproc_namenode -Xmx3000m -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote -Dhadoop.log.dir=/usr/lib/hadoop/logs -Dhadoop.log.file=hadoop-hdfs-namenode-hadoop-master.rockyou.com.log -Dhadoop.home.dir=/usr/lib/hadoop -Dhadoop.id.str=hdfs -Dhadoop.root.logger=INFO,DRFA -Djava.library.path=/usr/lib/hadoop/lib/native/Linux-amd64-64 -Dhadoop.policy.file=hadoop-policy.xml -classpath /usr/lib/hadoop/conf:/usr/java/default/lib/tools.jar:/usr/lib/hadoop:/usr/lib/hadoop/hadoop-core-0.20.2+737.jar:/usr/lib/hadoop/lib/aspectjrt-1.6.5.jar:/usr/lib/hadoop/lib/aspectjtools-1.6.5.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/commons-daemon-1.0.1.jar:/usr/lib/hadoop/lib/commons-el-1.0.jar:/usr/lib/hadoop/lib/commons-httpclient-3.0.1.jar:/usr/lib/hadoop/lib/commons-logging-1.0.4.jar:/usr/lib/hadoop/lib/commons-logging-api-1.0.4.jar:/usr/lib/hadoop/lib/commons-net-1.4.1.jar:/usr/lib/hadoop/lib/core-3.1.1.jar:/usr/lib/hadoop/lib/hadoop-fairscheduler-0.20.2+737.jar:/usr/lib/hadoop/lib/hadoop-lzo-0.4.8.jar:/usr/lib/hadoop/lib/hadoop-lzo.jar:/usr/lib/hadoop/lib/hsqldb-1.8.0.10.jar:/usr/lib/hadoop/lib/hue-plugins-1.1.0.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.5.2.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.5.2.jar:/usr/lib/hadoop/lib/jasper-compiler-5.5.12.jar:/usr/lib/hadoop/lib/jasper-runtime-5.5.12.jar:/usr/lib/hadoop/lib/jets3t-0.6.1.jar:/usr/lib/hadoop/lib/jetty-6.1.14.jar:/usr/lib/hadoop/lib/jetty-util-6.1.14.jar:/usr/lib/hadoop/lib/junit-4.5.jar:/usr/lib/hadoop/lib/kfs-0.2.2.jar:/usr/lib/hadoop/lib/log4j-1.2.15.jar:/usr/lib/hadoop/lib/mockito-all-1.8.2.jar:/usr/lib/hadoop/lib/mysql-connector-java-5.0.8-bin.jar:/usr/lib/hadoop/lib/oro-2.0.8.jar:/usr/lib/hadoop/lib/servlet-api-2.5-6.1.14.jar:/usr/lib/hadoop/lib/slf4j-api-1.4.3.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.4.3.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/jsp-2.1/jsp-2.1.jar:/usr/lib/hadoop/lib/jsp-2.1/jsp-api-2.1.jar::/usr/local/lib/mysql-connector-java-5.1.7-bin.jar:/usr/local/lib/mail.jar:/usr/local/lib/mysql-connector-java-5.1.7-bin.jar:/usr/local/lib/mail.jar:/usr/local/lib/mysql-connector-java-5.1.7-bin.jar:/usr/local/lib/mail.jar:/usr/local/lib/mysql-connector-java-5.1.7-bin.jar:/usr/local/lib/mail.jar:/usr/local/lib/mysql-connector-java-5.1.7-bin.jar:/usr/local/lib/mail.jar org.apache.hadoop.hdfs.server.namenode.NameNode 
root 16449 0.0 0.0 61136 744 pts/4 S+ 16:29 0:00 grep namenode 
EQX root@hadoop-master:/etc/init.d# kill -9 5038



 We starting looking at log output - we discovered the namenode startup process is throwing a null pointer exception. 

STARTUP_MSG: build = -r 98c55c28258aa6f42250569bd7fa431ac657bdbd; compiled by 'root' on Mon Oct 11 13:14:05 EDT 2010 
************************************************************/ 
2011-04-25 21:16:47,841 INFO org.apache.hadoop.metrics.jvm.JvmMetrics: Initializing JVM Metrics with processName=NameNode, sessionId=null 
2011-04-25 21:16:47,949 INFO org.apache.hadoop.hdfs.server.namenode.metrics.NameNodeMetrics: Initializing NameNodeMeterics using context object:org.apache.hadoop.metrics.ganglia.GangliaContext31 
2011-04-25 21:16:47,982 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner=hdfs 
2011-04-25 21:16:47,982 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup=root 
2011-04-25 21:16:47,982 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled=true 
2011-04-25 21:16:47,987 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s) 
2011-04-25 21:16:48,301 INFO org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMetrics: Initializing FSNamesystemMetrics using context object:org.apache.hadoop.metrics.ganglia.GangliaContext31 
2011-04-25 21:16:48,302 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemStatusMBean 
2011-04-25 21:16:48,328 INFO org.apache.hadoop.hdfs.server.common.Storage: Number of files = 237791 
2011-04-25 21:16:51,699 INFO org.apache.hadoop.hdfs.server.common.Storage: Number of files under construction = 0 
2011-04-25 21:16:51,699 INFO org.apache.hadoop.hdfs.server.common.Storage: Image file of size 42758182 loaded in 3 seconds. 
2011-04-25 21:16:51,701 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.lang.NullPointerException 
at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addChild(FSDirectory.java:1088) 

at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addChild(FSDirectory.java:1100) 
at org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedMkdir(FSDirectory.java:987) 

at org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedMkdir(FSDirectory.java:974) 

at org.apache.hadoop.hdfs.server.namenode.FSEditLog.loadFSEdits(FSEditLog.java:718) 

at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSEdits(FSImage.java:1034) 
at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:845) 

at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:379) 
at org.apache.hadoop.hdfs.server.namenode.FSDirectory.loadFSImage(FSDirectory.java:99) 

at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initialize(FSNamesystem.java:343) 
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:317) 

at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:214) 
at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:394) 

at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1148) 

at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1157) 
"
hdfsproxy tests fails test patch builds. ,HDFS-471,"org.apache.hadoop.hdfsproxy.TestHdfsProxy.testHdfsProxyInterface

Failing for the past 19 builds (Since #469 ) 
Took 25 sec.
Error Message

Stacktrace
java.lang.NullPointerException
	at org.apache.commons.cli.GnuParser.flatten(GnuParser.java:110)
	at org.apache.commons.cli.Parser.parse(Parser.java:143)
	at org.apache.hadoop.util.GenericOptionsParser.parseGeneralOptions(GenericOptionsParser.java:374)
	at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:153)
	at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:138)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1314)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:414)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:278)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:119)
	at org.apache.hadoop.hdfsproxy.TestHdfsProxy.testHdfsProxyInterface(TestHdfsProxy.java:209)

Link: http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-vesta.apache.org/487/testReport/org.apache.hadoop.hdfsproxy/TestHdfsProxy/testHdfsProxyInterface/"
CHANGES.txt in the last three branches diverged,HDFS-1082,"Particularly, CHANGES.txt in hdfs trunk and 0.21 don't reflect that 0.20.2 has been released, there is no section for 0.20.3, and the diff on the fixed issues is not uniform."
"In secure mode, Datanodes should shutdown if they come up on non-privileged ports",HDFS-1589,"It's a security hole, need to fix it asap."
Number of Under-Replicated Blocks information posted on WebUI is  inconsistent with CLI Fsck report. ,HDFS-810,Number of Under-Replicated Blocks show on WebUI is inconsistent with Under replicated blocks shown on Fsck.
Failed to execute fsck with -move option,HDFS-110,"I received the following error when running fsck with -move option. The dfs was started by a user while the fsck was ran by a different user that does not have the write access to the hadoop dfs data directory.

- moving to /lost+found: /data.txt
java.io.FileNotFoundException: hadoop-dfs-data-dir/tmp/client-8234960199756230677 (Permission denied)
at java.io.FileOutputStream.open(Native Method)
at java.io.FileOutputStream.<init>(FileOutputStream.java:179)
at java.io.FileOutputStream.<init>(FileOutputStream.java:131)
at org.apache.hadoop.dfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:546)
at org.apache.hadoop.dfs.DFSClient.create(DFSClient.java:99)
at org.apache.hadoop.dfs.DFSck.lostFoundMove(DFSck.java:222)
at org.apache.hadoop.dfs.DFSck.check(DFSck.java:178)
at org.apache.hadoop.dfs.DFSck.check(DFSck.java:124)
at org.apache.hadoop.dfs.DFSck.fsck(DFSck.java:112)
at org.apache.hadoop.dfs.DFSck.main(DFSck.java:433)
Failed to move /data.txt to /lost+found: hadoop-dfs-data-dir/tmp/client-8234960199756230677 (Permission denied)
"
"create(file, true) appears to be violating atomicity",HDFS-1563,"Will upload a unittest to reveal this bug.

In a word, when a thread is doing create(file, true) on existing file, there are chances that another thread will get 'false' for exists(file) during the period."
TestFileAppend4 fails,HDFS-1306,"Following tests are failing on trunk:
TestFileAppend4.testRecoverFinalizedBlock 
TestFileAppend4.testCompleteOtherLeaseHoldersFile 
"
Make FSVolumeSet in FSDataSet pluggable,HDFS-1405,"I am trying to submit a patch for HDFS-1362, which enable online add or remove a volume of a serving DataNode.

I managed to create the patch  avoiding modify exist class, but FSVolumeSet is an exception, the member ""volumes"" is an array, while I need a List to facilitate the hotplug procedure. Thus I have 2 possible solutions:
* Provide a abstract class as its super class, with the definition of all its methods. OR
* Modify the volumes' definition in FSVolumeSet, as a Gereral Type Collection<>, which may use different implementation in subclasses.

I will supply a patch for the first method for review"
Reduce the time required for a checkpoint,HDFS-1162,The checkpoint time increases linearly with the number of files in the cluster. This is a problem with large clusters.
RaidNode should fix missing blocks directly on Data Node,HDFS-1171,"RaidNode currently does not fix missing blocks. The missing blocks have to be fixed manually.

This task proposes that recovery be more automated:
1. RaidNode periodically fetches a list of corrupt files from the NameNode
2. If the corrupt files has a RAID parity file, RaidNode identifies missing block(s) in the file and recomputes the block(s) using the parity file and other good blocks
3. RaidNode sends the generated block contents to a DataNode
   a. RaidNode chooses a DataNode with the most available space to send the block. "
Need a command line option in RaidShell to fix blocks using raid,HDFS-1453,"RaidShell currently has an option to recover a file and return the path to the recovered file. The administrator can then rename the recovered file to the damaged file.

The problem with this is that the file metadata is altered, specifically the modification time. Instead we need a way to just repair the damaged blocks and send the fixed blocks to a data node.

Once this is done, we can put automation around it."
Group name is not properly set in inodes,HDFS-1415,In NameNode.create() and NameNode.mkdirs() do not pass the group name. So it is not properly set.
creating a file in hdfs should not automatically create the parent directories,HDFS-98,"I think it would be better if HDFS didn't automatically create directories for the user. In particular, in clean up code, it would be nice if deleting a directory couldn't be undone by mistake by a process that hasn't been killed yet creating a new file."
Client logic for 1st phase and 2nd phase failover are different,HDFS-1237,"- Setup:
number of datanodes = 4
replication factor = 2 (2 datanodes in the pipeline)
number of failure injected = 2
failure type: crash
Where/When failures happen: There are two scenarios: First, is when two datanodes crash at the same time in the first "
Hadoop distcp tool fails if file path contains special characters + & !,HDFS-31,"Copying folders containing + & ! characters between hdfs (using hftp) does not work in distcp

For example: 
Copying  folder ""string1+string2""  at ""namenode.address.com"", hftp port myport to ""/myotherhome/folder"" on ""myothermachine"" does not work 

myothermachine prompt>>> hadoop --config ~/mycluster/ distcp  ""hftp://namenode.address.com:myport/myhome/dir/string1+string2""  /myotherhome/folder/
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Error results for hadoop job1:
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
08/07/16 00:27:39 INFO tools.DistCp: srcPaths=[hftp://namenode.address.com:myport/myhome/dir/string1+string2]
08/07/16 00:27:39 INFO tools.DistCp: destPath=/myotherhome/folder/
08/07/16 00:27:41 INFO tools.DistCp: srcCount=2
08/07/16 00:27:42 INFO mapred.JobClient: Running job: job1
08/07/16 00:27:43 INFO mapred.JobClient:  map 0% reduce 0%
08/07/16 00:27:58 INFO mapred.JobClient: Task Id : attempt_1_m_000000_0, Status : FAILED
java.io.IOException: Copied: 0 Skipped: 0 Failed: 1
        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.close(DistCp.java:538)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:226)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2208)

08/07/16 00:28:14 INFO mapred.JobClient: Task Id : attempt_1_m_000000_1, Status : FAILED
java.io.IOException: Copied: 0 Skipped: 0 Failed: 1
        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.close(DistCp.java:538)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:226)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2208)

08/07/16 00:28:28 INFO mapred.JobClient: Task Id : attempt_1_m_000000_2, Status : FAILED
java.io.IOException: Copied: 0 Skipped: 0 Failed: 1
        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.close(DistCp.java:538)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:226)
        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2208)

With failures, global counters are inaccurate; consider running with -i
Copy failed: java.io.IOException: Job failed!
        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1053)
        at org.apache.hadoop.tools.DistCp.copy(DistCp.java:615)
        at org.apache.hadoop.tools.DistCp.run(DistCp.java:764)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
        at org.apache.hadoop.tools.DistCp.main(DistCp.java:784)
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Error log for the map task which failed
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
INFO org.apache.hadoop.tools.DistCp: FAIL string1+string2/myjobtrackermachine.com-joblog.tar.gz : java.io.IOException: Server returned HTTP response code: 500 for URL: http://mymachine.com:myport/streamFile?filename=/myhome/dir/string1+string2/myjobtrackermachine.com-joblog.tar.gz&ugi=myid,mygroup
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1241)
	at org.apache.hadoop.dfs.HftpFileSystem.open(HftpFileSystem.java:117)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:371)
	at org.apache.hadoop.tools.DistCp$CopyFilesMapper.copy(DistCp.java:377)
	at org.apache.hadoop.tools.DistCp$CopyFilesMapper.map(DistCp.java:504)
	at org.apache.hadoop.tools.DistCp$CopyFilesMapper.map(DistCp.java:279)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:226)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2208)
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
Make it possible for BlockPlacementPolicy to return null,HDFS-1351,"The idea is to modify FSNamesystem.chooseExcessReplicates code, so it can accept a null return from chooseReplicaToDelete which will indicate that NameNode should not be deleting extra replicas.

One possible usecase - if there are nodes being added to the cluster that might have corrupt replicas on them you do not want to delete other replicas until the block scanner finished scanning every block on the datanode.

This will require additional work on the implementation of the BlockPlacementPolicy, but with this JIRA I just wanted to create a basis for future improvements."
Corrupted blocks get deleted but not replicated,HDFS-86,"When I test the patch to HADOOP-1345 on a two node dfs cluster, I see that dfs correctly delete the corrupted replica and successfully retry reading from the other correct replica, but the block does not get replicated. The block remains with only 1 replica until the next block report comes in.

In my testcase, since the dfs cluster has only 2 datanodes, the target of replication is the same as the target of block invalidation.  After poking the logs, I found out that the namenode sent the replication request before the block invalidation request. 

This is because the namenode does not invalidate a block well. In FSNamesystem.invalidateBlock, it first puts the invalidate request in a queue and then immediately removes the replica from its state, which triggers the choosing a target for the block. When requests are sent back to the target datanode as a reply to a heartbeat message, the replication requests have higher priority than the invalidate requests.

This problem could be solved if a namenode removes an invalidated replica from its state only after the invalidate request is sent to the datanode."
NameNode.getBlockLocations throws NPE when offset > filesize and file is not empty,HDFS-513,"in BlockManager.getBlockLocations, if the offset is past the end of a non-empty file, it returns null. In FSNamesystem.getBlockLocationsInternal, this null is passed through to inode.createLocatedBlocks, so it ends up with a LocatedBlocks instance whose .blocks is null. This is then iterated over in FSNamesystem.getBlockLocations, and throws an NPE.

Instead, I think BlockManager.getBlockLocations should return Collections.emptyList in the past-EOF case. This would result in an empty list response from NN.getBlockLocations which matches the behavior of an empty file. If this sounds like the appropriate fix I""ll attach the patch."
Namenode should return lease recovery request with other requests,HDFS-320,"HADOOP-5034 modified NN to return both replication and deletion requests to DN in one reply to a heartbeat. However, the lease recovery request is still sent separately by itself. Is there a reason for this? If not, I suggest we combine them together. This will make it less confusing when adding new types of requests, which are combinable as well."
DFSClient incorrectly asks for new block if primary crashes during first recoverBlock,HDFS-1229,"Setup:
--------
+ # available datanodes = 2
+ # disks / datanode = 1
+ # failures = 1
+ failure type = crash
+ When/where failure happens = during primary's recoverBlock
 
Details:
----------
Say client is appending to block X1 in 2 datanodes: dn1 and dn2.
First it needs to make sure both dn1 and dn2  agree on the new GS of the block.
1) Client first creates DFSOutputStream by calling
 
>OutputStream result = new DFSOutputStream(src, buffersize, progress,
>                                            lastBlock, stat, conf.getInt(""io.bytes.per.checksum"", 512));
 
in DFSClient.append()
 
2) The above DFSOutputStream constructor in turn calls processDataNodeError(true, true)
(i.e, hasError = true, isAppend = true), and starts the DataStreammer
 
> processDatanodeError(true, true);  /* let's call this PDNE 1 */
> streamer.start();
 
Note that DataStreammer.run() also calls processDatanodeError()
> while (!closed && clientRunning) {
>  ...
>      boolean doSleep = processDatanodeError(hasError, false); /let's call this PDNE 2*/
 
3) Now in the PDNE 1, we have following code:
 
> blockStream = null;
> blockReplyStream = null;
> ...
> while (!success && clientRunning) {
> ...
>    try {
>         primary = createClientDatanodeProtocolProxy(primaryNode, conf);
>         newBlock = primary.recoverBlock(block, isAppend, newnodes); /*exception here*/
>         ...
>    catch (IOException e) {
>         ...
>         if (recoveryErrorCount > maxRecoveryErrorCount) { 
>         // this condition is false
>         }
>         ...
>         return true;
>    } // end catch
>    finally {...}
>    
>    this.hasError = false;
>    lastException = null;
>    errorIndex = 0;
>    success = createBlockOutputStream(nodes, clientName, true);
>    }
>    ...
 
Because dn1 crashes during client call to recoverBlock, we have an exception.
Hence, go to the catch block, in which processDatanodeError returns true
before setting hasError to false. Also, because createBlockOutputStream() is not called
(due to an early return), blockStream is still null.
 
4) Now PDNE 1 has finished, we come to streamer.start(), which calls PDNE 2.
Because hasError = false, PDNE 2 returns false immediately without doing anything

> if (!hasError) { return false; }
 
5) still in the DataStreamer.run(), after returning false from PDNE 2, we still have
blockStream = null, hence the following code is executed:

if (blockStream == null) {
   nodes = nextBlockOutputStream(src);
   this.setName(""DataStreamer for file "" + src + "" block "" + block);
   response = new ResponseProcessor(nodes);
   response.start();
}
 
nextBlockOutputStream which asks namenode to allocate new Block is called.
(This is not good, because we are appending, not writing).
Namenode gives it new Block ID and a set of datanodes, including crashed dn1.
this leads to createOutputStream() fails because it tries to contact the dn1 first.
(which has crashed). The client retries 5 times without any success,
because every time, it asks namenode for new block! Again we see
that the retry logic at client is weird!

*This bug was found by our Failure Testing Service framework:
http://www.eecs.berkeley.edu/Pubs/TechRpts/2010/EECS-2010-98.html
For questions, please email us: Thanh Do (thanhdo@cs.wisc.edu) and 
Haryadi Gunawi (haryadi@eecs.berkeley.edu)*"
Impact in NameNode scalability because heartbeat processing acquires the global lock,HDFS-81,"The heartbeat processing code recently got rearranged via HADOOP-3254. This caused the NameNode heartbeat processing code to acquire the FSNamesystem global lock for every heartbeat processing. This could impact scalability of the namenode.

This problem is present in 0.17.x and 0.18.x release only. It is not present in 0.16.x and on trunk.
"
All datanodes are bad in 2nd phase,HDFS-1239,"- Setups:
number of datanodes = 2
replication factor = 2
Type of failure: transient fault (a java i/o call throws an exception or return false)
Number of failures = 2
when/where failures happen = during the 2nd phase of the pipeline, each happens at each "
NameNode startup failed,HDFS-18,"After bouncing the cluster namenode refuses to start and gives the error: FSNamesystem initialization failed. Also says: saveLeases found path /tmp/temp623789763/tmp659456056/_temporary/_attempt_200904211331_0010_r_000002_0/part-00002 but no matching entry in namespace. Recovery from checkpoint resulted in wide spread corruption which made it necessary to format the dfs.
This is opened as a result from these threads: http://www.mail-archive.com/core-user@hadoop.apache.org/msg09397.html, http://www.mail-archive.com/core-user@hadoop.apache.org/msg09663.html
"
Client uselessly retries recoverBlock 5 times,HDFS-1236,"Summary:
Client uselessly retries recoverBlock 5 times
The same behavior is also seen in append protocol (HDFS-1229)

The setup:
+ # available datanodes = 4
+ Replication factor = 2 (hence there are 2 datanodes in the pipeline)
+ Failure type = Bad disk at datanode (not crashes)
+ # failures = 2
+ # disks / datanode = 1
+ Where/when the failures happen: This is a scenario where each disk of the two datanodes in the pipeline go bad at the same time during the 2nd phase of the pipeline (the data transfer phase).
 
Details:
 
In this case, the client will call processDatanodeError
which will call datanode.recoverBlock to those two datanodes.
But since these two datanodes have bad disks (although they're still alive),
then recoverBlock() will fail.
For this one, the client's retry logic ends when streamer is closed (close == true).
But before this happen, the client will retry 5 times
(maxRecoveryErrorCount) and will fail all the time, until
it finishes.  What is interesting is that
during each retry, there is a wait of 1 second in
DataStreamer.run (i.e. dataQueue.wait(1000)).
So it will be a 5-second total wait before declaring it fails.
 
This is a different bug than HDFS-1235, where the client retries
3 times for 6 seconds (resulting in 25 seconds wait time).
In this experiment, what we get for the total wait time is only
12 seconds (not sure why it is 12). So the DFSClient quits without
contacting the namenode again (say to ask for a new set of
two datanodes).
So interestingly we find another
bug that shows client retry logic is complex and not deterministic
depending on where and when failures happen.

This bug was found by our Failure Testing Service framework:
http://www.eecs.berkeley.edu/Pubs/TechRpts/2010/EECS-2010-98.html
For questions, please email us: Thanh Do (thanhdo@cs.wisc.edu) and
Haryadi Gunawi (haryadi@eecs.berkeley.edu)"
Bad retry logic at DFSClient,HDFS-1233,"- Summary: failover bug, bad retry logic at DFSClient, cannot failover to the 2nd disk
 
- Setups:
+ # available datanodes = 1
+ # disks / datanode = 2
+ # failures = 1
+ failure type = bad disk
+ When/where failure happens = (see below)
 
- Details:

The"
Incorrect hadoop dependency in 0.21,HDFS-1180,"The  build is broken due to a dependency on ""hadoop-common 0.21"". In version 0.21 the ""common"" artifact is called ""hadoop-core."""
Deprecate dfs.permissions.superusergroup in favor of hadoop.cluster.administrators,HDFS-1008,"HADOOP-6568 added the configuration {{hadoop.cluster.administrators}} through which admins can configure who the superusers/supergroups for the cluster are. HDFS itself already has {{dfs.permissions.superusergroup}} (which is just a single group). As agreed upon at HADOOP-6568, this should be deprecated in favor of {{hadoop.cluster.administrators}}."
Fix Javadoc for DistributedCache usage,HDFS-1098,"The Javadoc for using DistributedCache isn't up-to-date with code:
 *     JobConf job = new JobConf();
 *     DistributedCache.addCacheFile(new URI(""/myapp/lookup.dat#lookup.dat""), 
 *                                   job);

This is the new API:
  public static void addCacheFile(URI uri, Configuration conf) {

Javadoc should (partially) reflect the actual usage from TaskRunner:
            p[i] = DistributedCache.getLocalCache(files[i], conf,
                                                  new Path(baseDir),
                                                  fileStatus,
                                                  false, Long.parseLong(
                                                           fileTimestamps[i]),
                                                  new Path(workDir.
                                                        getAbsolutePath()),
                                                  false);
          }
          DistributedCache.setLocalFiles(conf, stringifyPathArray(p));"
Block report processing should compare gneration stamp,HDFS-168,"If a reported block has a different generation stamp then the one stored in the NameNode, the reported block will be considered as invalid.  This is incorrect since blocks with larger generation stamp are valid."
TestHDFSCLI.refreshServiceAcl failed,HDFS-848,"{noformat}
                     Test ID: [597]
            Test Description: [refreshServiceAcl: refreshing security authorization policy for namenode]
 
               Test Commands: [-fs NAMENODE -refreshServiceAcl ]
 
 
                  Comparator: [ExactComparator]
          Comparision result:   [fail]
             Expected output:   []
               Actual output:   [refreshServiceAcl: org.apache.hadoop.security.authorize.AuthorizationException: java.security.AccessControlException: access denied ConnectionPermission(org.apache.hadoop.security.authorize.RefreshAuthorizationPolicyProtocol)
]
{noformat}
I did ""ant veryclean"" before running ""ant run-test-hdfs -Dtestcase=TestHDFSCLI""."
Secondary name node won't start,HDFS-109,"The secondary name node was unable to start. 

Early investigation suggests that the secondary node was not provisioned the same as the primary node, and entered a swapping regime. 

This report can be dismissed if provisioning is the complete explanation."
TestOfflineImageViewer fails,HDFS-683,"{noformat}
Failed reading valid file: No image processor to read version -21 is available.
{noformat}
See [build #56|http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/56/testReport/org.apache.hadoop.hdfs.tools.offlineImageViewer/TestOfflineImageViewer/testOIV/] for more details."
Simplify the codes in the replica related classes,HDFS-628,"In the replica related classes (e.g. ReplicaBeingWritten, ReplicaInPipeline, etc.), there are too many constructors and unnecessary parameter passing."
"dfs startup error, 0 datanodes in ",HDFS-100," Web site shows:
NameNode 'master.cloud:9000'
Started:  Thu Dec 18 17:10:35 CST 2008  
Version:  0.17.2.1, r684969  
Compiled:  Wed Aug 20 22:29:32 UTC 2008 by oom  
Upgrades:  There are no upgrades in progress.  


Browse the filesystem 
--------------------------------------------------------------------------------

Cluster Summary
Safe mode is ON. The ratio of reported blocks 0.0000 has not reached the threshold 0.9990. Safe mode will be turned off automatically.
21 files and directories, 6 blocks = 27 total. Heap Size is 4.94 MB / 992.31 MB (0%) 

Capacity : 0 KB 
DFS Remaining : 0 KB 
DFS Used : 0 KB 
DFS Used% : 0 % 
Live Nodes  : 0 
Dead Nodes  : 0 




--------------------------------------------------------------------------------
There are no datanodes in the cluster 

On blog of namenode, it shows:
2008-12-18 17:10:35,204 INFO org.apache.hadoop.dfs.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master.cloud/10.100.4.226
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 0.17.2.1
STARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/core/branches/branch-0.17 -r 684969; compiled by 'oom' on Wed Aug 20 22:29:32 UTC 2008
************************************************************/
2008-12-18 17:10:35,337 INFO org.apache.hadoop.ipc.metrics.RpcMetrics: Initializing RPC Metrics with hostName=NameNode, port=9000
2008-12-18 17:10:35,344 INFO org.apache.hadoop.dfs.NameNode: Namenode up at: master.cloud/10.100.4.226:9000
2008-12-18 17:10:35,348 INFO org.apache.hadoop.metrics.jvm.JvmMetrics: Initializing JVM Metrics with processName=NameNode, sessionId=null
2008-12-18 17:10:35,351 INFO org.apache.hadoop.dfs.NameNodeMetrics: Initializing NameNodeMeterics using context object:org.apache.hadoop.metrics.spi.NullContext
2008-12-18 17:10:35,436 INFO org.apache.hadoop.fs.FSNamesystem: fsOwner=user,users,ftp,sshd
2008-12-18 17:10:35,437 INFO org.apache.hadoop.fs.FSNamesystem: supergroup=supergroup
2008-12-18 17:10:35,437 INFO org.apache.hadoop.fs.FSNamesystem: isPermissionEnabled=true
2008-12-18 17:10:35,576 INFO org.apache.hadoop.fs.FSNamesystem: Finished loading FSImage in 181 msecs
2008-12-18 17:10:35,585 INFO org.apache.hadoop.dfs.StateChange: STATE* Safe mode ON. 
The ratio of reported blocks 0.0000 has not reached the threshold 0.9990. Safe mode will be turned off automatically.
2008-12-18 17:10:35,595 INFO org.apache.hadoop.fs.FSNamesystem: Registered FSNamesystemStatusMBean
2008-12-18 17:10:35,727 INFO org.mortbay.util.Credential: Checking Resource aliases
2008-12-18 17:10:35,870 INFO org.mortbay.http.HttpServer: Version Jetty/5.1.4
2008-12-18 17:10:35,871 INFO org.mortbay.util.Container: Started HttpContext[/static,/static]
2008-12-18 17:10:35,871 INFO org.mortbay.util.Container: Started HttpContext[/logs,/logs]
2008-12-18 17:10:36,260 INFO org.mortbay.util.Container: Started org.mortbay.jetty.servlet.WebApplicationHandler@b60b93
2008-12-18 17:10:36,307 INFO org.mortbay.util.Container: Started WebApplicationContext[/,/]
2008-12-18 17:10:36,309 INFO org.mortbay.http.SocketListener: Started SocketListener on 0.0.0.0:50070
2008-12-18 17:10:36,310 INFO org.mortbay.util.Container: Started org.mortbay.jetty.Server@1bd7848
2008-12-18 17:10:36,310 INFO org.apache.hadoop.fs.FSNamesystem: Web-server up at: 0.0.0.0:50070
2008-12-18 17:10:36,310 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2008-12-18 17:10:36,312 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2008-12-18 17:10:36,316 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000: starting
2008-12-18 17:10:36,317 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000: starting
2008-12-18 17:10:36,317 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000: starting
2008-12-18 17:10:36,320 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000: starting
2008-12-18 17:10:36,321 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000: starting
2008-12-18 17:10:36,321 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000: starting
2008-12-18 17:10:36,321 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000: starting
2008-12-18 17:10:36,321 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000: starting
2008-12-18 17:10:36,322 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000: starting
2008-12-18 17:10:36,374 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000: starting


and in the slaves blog, i find a strange thing. 

************************************************************/
2008-12-18 17:11:47,627 INFO org.apache.hadoop.dfs.DataNode: STARTUP_MSG:
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = slave3.cloud/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 0.17.2.1
STARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/core/branches/branch-0.17 -r 684969; compiled by 'oom' on Wed Aug 20 22:29:32 UTC 2008
************************************************************/
2008-12-18 17:11:48,267 ERROR org.apache.hadoop.dfs.DataNode: java.io.IOException: Incompatible namespaceIDs in /home/user/hadoop/tmp/dfs/data: namenode namespaceID = 1098832880; datanode namespaceID = 464592288
        at org.apache.hadoop.dfs.DataStorage.doTransition(DataStorage.java:298)
        at org.apache.hadoop.dfs.DataStorage.recoverTransitionRead(DataStorage.java:142)
        at org.apache.hadoop.dfs.DataNode.startDataNode(DataNode.java:258)
        at org.apache.hadoop.dfs.DataNode.<init>(DataNode.java:176)
        at org.apache.hadoop.dfs.DataNode.makeInstance(DataNode.java:2795)
        at org.apache.hadoop.dfs.DataNode.instantiateDataNode(DataNode.java:2750)
        at org.apache.hadoop.dfs.DataNode.createDataNode(DataNode.java:2758)
        at org.apache.hadoop.dfs.DataNode.main(DataNode.java:2880)

2008-12-18 17:11:48,269 INFO org.apache.hadoop.dfs.DataNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at slave3.cloud/127.0.0.1
************************************************************/
"
files disappearing on dfs,HDFS-33,"We have rare instances where we see files disappearing after they have been renamed.

Because of the rarity, it is hard to reproduce. In the latest instance, the destination files (900 of them) existed before the renaming operation with the same filenames and got deleted just before the new files got renamed. It is possible, but not sure, that all instances of file-disappearing are of this nature."
"On a busy cluster, it is possible for the client to believe it cannot fetch a block when the client or datanodes are running slowly",HDFS-262,"On a heavily loaded node, the communication between a DFSClient can time out or fail leading DFSClient to believe the datanode is non-responsive even though the datanode is, in fact, healthy. It may run through all the retries for that datanode leading DFSClient to mark the datanode ""dead"".  

This can continue as DFSClient iterates through the other datanodes for the block it is looking for, and then DFSClient will declare that it can't find any servers for that block (even though all n (where n = replication factor) datanodes are healthy (but slow) and have valid copies of the block.

It is also possible that the process running the DFSClient is too slow and misses (or times out) responses from the data node, resulting in the DFSClient believing that the datanode is dead.

Another possibility is that the block has been moved from one or more datanodes since DFSClient$DFSInputStream.chooseDataNode() found the locations of the block.

When the retries for each datanode and all datanodes are exhausted, DFSClient$DFSInputStream.chooseDataNode() issues the warning:

{code}
          if (nodes == null || nodes.length == 0) {
            LOG.info(""No node available for block: "" + blockInfo);
          }
          LOG.info(""Could not obtain block "" + block.getBlock() + "" from any node:  "" + ie);
{code}

It would be an improvement, and not impact performance under normal conditions if  when DFSClient decides that it cannot find the block anywhere, for it to retry finding the block by calling 

{code}
private static LocatedBlocks callGetBlockLocations()
{code}
 
*once* , to attempt to recover from machine(s) being too busy, or the block being relocated since the initial call to callGetBlockLocations(). If the second attempt to find the block based on what the namenode told DFSClient,  then issue the messages and give up by throwing the exception it does today.
"
Strange behavior with bin/hadoop dfs -mv,HDFS-643,"I had a very strange experience today with the bin/hadoop dfs -mv command. I accidentally passed 3 parameters, like so:

{code}
bin/hadoop dfs -mv /1 /2 /3
{code}

My intent was to mv /1 to /3, but mis-pasted some stuff. However, to my surprise, the result I got was that /1 had been moved to /2, and /2 had then been moved to /3! This seems like totally confusing semantics - I would have expected a 3-parameter move to either ignore the 3rd parameter or error out altogether. Needless to say, a solid 10 minutes of confused scrambling commenced."
data node startup problem,HDFS-407,"Hi,
I have set up a cluster using dom0( a.k.a Host OS ) and one domU ( a.k.a Guest OS ) both are running on Fedora -8 32-bit .
Para Virtualization used: XEN Version 3.1.0-13

HostOS host name is hadoop10 configured as name-node/job tracker and it will acts as data-node/task tracker as well in order to spread the data storage and GuestOS name is hadoop11 it is configured as data-node/task tracker

i'm facing new problem...trying to start HDFS daemons but i'm getting following errors:
1) Permission denied error in line number [3] below
2) hadoop11: no datanode to stop at line number [8]

but i can able to SSH to hadoop11( GuestOS/slave )
---------------------------------------------------------------------

[hadoop@hadoop10 hadoop-0.18.0]$ bin/start-dfs.sh
[1] starting namenode, logging to /home/hadoop/hadoop-0.18.0/bin/../logs/hadoop-hadoop-namenode-hadoop10.out
[2] hadoop11: starting datanode, logging to /home/hadoop/hadoop-0.18.0/bin/../logs/hadoop-hadoop-datanode-hadoop11.out
[3] hadoop11: /home/hadoop/hadoop-0.18.0/bin/hadoop-daemon.sh: line 118: /tmp/hadoop-hadoop-datanode.pid: Permission denied
[4] hadoop10: starting datanode, logging to /home/hadoop/hadoop-0.18.0/bin/../logs/hadoop-hadoop-datanode-hadoop10.out
[5] hadoop10: starting secondarynamenode, logging to /home/hadoop/hadoop-0.18.0/bin/../logs/hadoop-hadoop-secondarynamenode-hadoop10.out
[6] [hadoop@hadoop10 hadoop-0.18.0]$ bin/stop-dfs.sh
[7] stopping namenode
[8] hadoop11: no datanode to stop
[9] hadoop10: stopping datanode
[10] hadoop10: stopping secondarynamenode
[11] [hadoop@hadoop10 hadoop-0.18.0]$ ssh hadoop11    ----------- |
[12] Last login: Tue Oct  7 18:07:20 2008 from hadoop10    -----------|---------> I can able to SSH to slave ( hadoop11 ) from master ( hadoop10)
[13] [hadoop@hadoop11 ~]$                              --------------------------------|

------------------------------------------------------------------------------------------------

Whats going wrong over here??  
"
What will we encounter if we add a lot of nodes into the current cluster?,HDFS-541,"Our cluster has 200 nodes now. In order to improve its ability, we hope to add 60 nodes into the current cluster. However, we all don't know what will happen if we add so many nodes at the same time. Could you give me some tips and notes? During the process, which part shall we pay much attention on?
Thank you!
"
Upgrade build to use findbugs-1.3.8,HDFS-507,"Current findbugs reports a false warning:

{quote}
OBL     Method  org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader.readHeader(File) may fail to clean up stream or resource of type java.io.InputStream
{quote}

Suggested version findbugs-1.3.8 does not report this false positive."
Convert the file including 10 minutes run's tests into the test harness test suite,HDFS-505,"This issue is created to track the conversion of external testlist for 10 minutes tests into JUnit test suite.

It would be consistent through the bigger Hadoop project to have 10 minutes tests to be collected in a JUnit's test suite instead of an external file (as of now). 

The reason is simple: with the file we'd have two points of maintenance: tests source code and one an auxiliary text files. Besides, a general approach is to use JUnit's suites for tagging purposes, thus having a suite here would be more uniform.
"
error : too many fetch failures,HDFS-486,"i configured the hadoop cluster environment with one physical mahine as data node and 7(2physical and 5 virtual machines) as namenode
when i submitted the sort job(from hadoop-core-example)
it finished with following error, can you explain why this is happening and how to solve this?


output from terminal ************************************************************

[root@hadoop1 hadoop-0.18.3]# bin/hadoop jar hadoop-0.18.3-examples.jar sort input13 output1
Running on 7 nodes to sort from hdfs://hadoop1:8022/user/root/input13 into hdfs://hadoop1:8022/user/root/output1 with 12 reduces.
Job started: Fri Jul 10 14:00:19 IST 2009
09/07/10 14:00:19 INFO mapred.FileInputFormat: Total input paths to process : 1
09/07/10 14:00:19 INFO mapred.FileInputFormat: Total input paths to process : 1
09/07/10 14:00:19 INFO mapred.JobClient: Running job: job_200907101344_0002
09/07/10 14:00:20 INFO mapred.JobClient:  map 0% reduce 0%
09/07/10 14:00:24 INFO mapred.JobClient:  map 6% reduce 0%
09/07/10 14:00:25 INFO mapred.JobClient:  map 12% reduce 0%
09/07/10 14:00:28 INFO mapred.JobClient:  map 31% reduce 0%
09/07/10 14:00:29 INFO mapred.JobClient:  map 50% reduce 0%
09/07/10 14:00:33 INFO mapred.JobClient:  map 66% reduce 0%
09/07/10 14:00:34 INFO mapred.JobClient:  map 72% reduce 0%
09/07/10 14:00:35 INFO mapred.JobClient:  map 75% reduce 0%
09/07/10 14:00:37 INFO mapred.JobClient:  map 75% reduce 1%
09/07/10 14:00:38 INFO mapred.JobClient:  map 78% reduce 5%
09/07/10 14:00:39 INFO mapred.JobClient:  map 89% reduce 10%
09/07/10 14:00:40 INFO mapred.JobClient:  map 89% reduce 11%
09/07/10 14:00:41 INFO mapred.JobClient:  map 90% reduce 11%
09/07/10 14:00:42 INFO mapred.JobClient:  map 99% reduce 14%
09/07/10 14:00:43 INFO mapred.JobClient:  map 99% reduce 16%
09/07/10 14:00:44 INFO mapred.JobClient:  map 99% reduce 18%
09/07/10 14:00:45 INFO mapred.JobClient:  map 99% reduce 19%
09/07/10 14:00:47 INFO mapred.JobClient:  map 99% reduce 22%
09/07/10 14:00:48 INFO mapred.JobClient:  map 100% reduce 22%
09/07/10 14:00:50 INFO mapred.JobClient:  map 100% reduce 24%
09/07/10 14:00:52 INFO mapred.JobClient:  map 100% reduce 25%
09/07/10 14:00:53 INFO mapred.JobClient:  map 100% reduce 26%
09/07/10 14:00:54 INFO mapred.JobClient:  map 100% reduce 27%
09/07/10 14:00:58 INFO mapred.JobClient:  map 100% reduce 33%
09/07/10 14:01:00 INFO mapred.JobClient:  map 100% reduce 34%
09/07/10 14:01:03 INFO mapred.JobClient:  map 100% reduce 39%
09/07/10 14:03:29 INFO mapred.JobClient:  map 100% reduce 40%
09/07/10 14:03:42 INFO mapred.JobClient:  map 100% reduce 41%
09/07/10 14:03:50 INFO mapred.JobClient:  map 100% reduce 47%
09/07/10 14:03:51 INFO mapred.JobClient:  map 100% reduce 50%
09/07/10 14:03:52 INFO mapred.JobClient:  map 100% reduce 56%
09/07/10 14:03:57 INFO mapred.JobClient:  map 100% reduce 57%
09/07/10 14:04:00 INFO mapred.JobClient:  map 100% reduce 58%
09/07/10 14:04:05 INFO mapred.JobClient:  map 100% reduce 59%
09/07/10 14:04:07 INFO mapred.JobClient:  map 100% reduce 60%
09/07/10 14:04:09 INFO mapred.JobClient:  map 100% reduce 66%
09/07/10 14:04:10 INFO mapred.JobClient:  map 100% reduce 67%
09/07/10 14:04:12 INFO mapred.JobClient:  map 100% reduce 68%
09/07/10 14:04:13 INFO mapred.JobClient:  map 100% reduce 69%
09/07/10 14:04:14 INFO mapred.JobClient:  map 100% reduce 70%
09/07/10 14:04:20 INFO mapred.JobClient:  map 100% reduce 79%
09/07/10 14:04:21 INFO mapred.JobClient:  map 100% reduce 80%
09/07/10 14:04:22 INFO mapred.JobClient:  map 100% reduce 81%
09/07/10 14:04:23 INFO mapred.JobClient:  map 100% reduce 82%
09/07/10 14:04:33 INFO mapred.JobClient:  map 100% reduce 87%
09/07/10 14:04:42 INFO mapred.JobClient: Task Id : attempt_200907101344_0002_m_000013_0, Status : FAILED
Too many fetch-failures
09/07/10 14:04:44 INFO mapred.JobClient:  map 93% reduce 87%
09/07/10 14:04:50 INFO mapred.JobClient:  map 100% reduce 87%
09/07/10 14:05:06 INFO mapred.JobClient:  map 100% reduce 93%
09/07/10 14:05:36 INFO mapred.JobClient:  map 100% reduce 94%
09/07/10 14:06:17 INFO mapred.JobClient: Job complete: job_200907101344_0002
09/07/10 14:06:17 INFO mapred.JobClient: Counters: 17
09/07/10 14:06:17 INFO mapred.JobClient:   File Systems
09/07/10 14:06:17 INFO mapred.JobClient:     HDFS bytes read=1077612760
09/07/10 14:06:17 INFO mapred.JobClient:     HDFS bytes written=1077285377
09/07/10 14:06:17 INFO mapred.JobClient:     Local bytes read=1083539214
09/07/10 14:06:17 INFO mapred.JobClient:     Local bytes written=2167083496
09/07/10 14:06:17 INFO mapred.JobClient:   Job Counters 
09/07/10 14:06:17 INFO mapred.JobClient:     Launched reduce tasks=18
09/07/10 14:06:17 INFO mapred.JobClient:     Rack-local map tasks=2
09/07/10 14:06:17 INFO mapred.JobClient:     Launched map tasks=18
09/07/10 14:06:17 INFO mapred.JobClient:     Data-local map tasks=15
09/07/10 14:06:17 INFO mapred.JobClient:   Map-Reduce Framework
09/07/10 14:06:17 INFO mapred.JobClient:     Reduce input groups=102341
09/07/10 14:06:17 INFO mapred.JobClient:     Combine output records=0
09/07/10 14:06:17 INFO mapred.JobClient:     Map input records=102341
09/07/10 14:06:17 INFO mapred.JobClient:     Reduce output records=102341
09/07/10 14:06:17 INFO mapred.JobClient:     Map output bytes=1074564177
09/07/10 14:06:17 INFO mapred.JobClient:     Map input bytes=1077284885
09/07/10 14:06:17 INFO mapred.JobClient:     Combine input records=0
09/07/10 14:06:17 INFO mapred.JobClient:     Map output records=102341
09/07/10 14:06:17 INFO mapred.JobClient:     Reduce input records=102341
Job ended: Fri Jul 10 14:06:17 IST 2009
The job took 357 seconds.

"
error : too many fetch failures,HDFS-485,"i have a hadoop cluster configured with 1 physcical machine as name node and 7 data nodes(2 physcical+5 virtual).
When a sort job (hadoop-core-examples) is submitted it completes with the following error:can anyone tell me why and how to solve this issue.

hadoop version:0.18.3

O/P from terminal********************************
[root@hadoop1 hadoop-0.18.3]# bin/hadoop jar hadoop-0.18.3-examples.jar sort input13 output1
Running on 7 nodes to sort from hdfs://hadoop1:8022/user/root/input13 into hdfs://hadoop1:8022/user/root/output1 with 12 reduces.
Job started: Fri Jul 10 14:00:19 IST 2009
09/07/10 14:00:19 INFO mapred.FileInputFormat: Total input paths to process : 1
09/07/10 14:00:19 INFO mapred.FileInputFormat: Total input paths to process : 1
09/07/10 14:00:19 INFO mapred.JobClient: Running job: job_200907101344_0002
09/07/10 14:00:20 INFO mapred.JobClient:  map 0% reduce 0%
09/07/10 14:00:24 INFO mapred.JobClient:  map 6% reduce 0%
09/07/10 14:00:25 INFO mapred.JobClient:  map 12% reduce 0%
09/07/10 14:00:28 INFO mapred.JobClient:  map 31% reduce 0%
09/07/10 14:00:29 INFO mapred.JobClient:  map 50% reduce 0%
09/07/10 14:00:33 INFO mapred.JobClient:  map 66% reduce 0%
09/07/10 14:00:34 INFO mapred.JobClient:  map 72% reduce 0%
09/07/10 14:00:35 INFO mapred.JobClient:  map 75% reduce 0%
09/07/10 14:00:37 INFO mapred.JobClient:  map 75% reduce 1%
09/07/10 14:00:38 INFO mapred.JobClient:  map 78% reduce 5%
09/07/10 14:00:39 INFO mapred.JobClient:  map 89% reduce 10%
09/07/10 14:00:40 INFO mapred.JobClient:  map 89% reduce 11%
09/07/10 14:00:41 INFO mapred.JobClient:  map 90% reduce 11%
09/07/10 14:00:42 INFO mapred.JobClient:  map 99% reduce 14%
09/07/10 14:00:43 INFO mapred.JobClient:  map 99% reduce 16%
09/07/10 14:00:44 INFO mapred.JobClient:  map 99% reduce 18%
09/07/10 14:00:45 INFO mapred.JobClient:  map 99% reduce 19%
09/07/10 14:00:47 INFO mapred.JobClient:  map 99% reduce 22%
09/07/10 14:00:48 INFO mapred.JobClient:  map 100% reduce 22%
09/07/10 14:00:50 INFO mapred.JobClient:  map 100% reduce 24%
09/07/10 14:00:52 INFO mapred.JobClient:  map 100% reduce 25%
09/07/10 14:00:53 INFO mapred.JobClient:  map 100% reduce 26%
09/07/10 14:00:54 INFO mapred.JobClient:  map 100% reduce 27%
09/07/10 14:00:58 INFO mapred.JobClient:  map 100% reduce 33%
09/07/10 14:01:00 INFO mapred.JobClient:  map 100% reduce 34%
09/07/10 14:01:03 INFO mapred.JobClient:  map 100% reduce 39%
09/07/10 14:03:29 INFO mapred.JobClient:  map 100% reduce 40%
09/07/10 14:03:42 INFO mapred.JobClient:  map 100% reduce 41%
09/07/10 14:03:50 INFO mapred.JobClient:  map 100% reduce 47%
09/07/10 14:03:51 INFO mapred.JobClient:  map 100% reduce 50%
09/07/10 14:03:52 INFO mapred.JobClient:  map 100% reduce 56%
09/07/10 14:03:57 INFO mapred.JobClient:  map 100% reduce 57%
09/07/10 14:04:00 INFO mapred.JobClient:  map 100% reduce 58%
09/07/10 14:04:05 INFO mapred.JobClient:  map 100% reduce 59%
09/07/10 14:04:07 INFO mapred.JobClient:  map 100% reduce 60%
09/07/10 14:04:09 INFO mapred.JobClient:  map 100% reduce 66%
09/07/10 14:04:10 INFO mapred.JobClient:  map 100% reduce 67%
09/07/10 14:04:12 INFO mapred.JobClient:  map 100% reduce 68%
09/07/10 14:04:13 INFO mapred.JobClient:  map 100% reduce 69%
09/07/10 14:04:14 INFO mapred.JobClient:  map 100% reduce 70%
09/07/10 14:04:20 INFO mapred.JobClient:  map 100% reduce 79%
09/07/10 14:04:21 INFO mapred.JobClient:  map 100% reduce 80%
09/07/10 14:04:22 INFO mapred.JobClient:  map 100% reduce 81%
09/07/10 14:04:23 INFO mapred.JobClient:  map 100% reduce 82%
09/07/10 14:04:33 INFO mapred.JobClient:  map 100% reduce 87%
09/07/10 14:04:42 INFO mapred.JobClient: Task Id : attempt_200907101344_0002_m_000013_0, Status : FAILED
Too many fetch-failures
09/07/10 14:04:44 INFO mapred.JobClient:  map 93% reduce 87%
09/07/10 14:04:50 INFO mapred.JobClient:  map 100% reduce 87%
09/07/10 14:05:06 INFO mapred.JobClient:  map 100% reduce 93%
09/07/10 14:05:36 INFO mapred.JobClient:  map 100% reduce 94%
09/07/10 14:06:17 INFO mapred.JobClient: Job complete: job_200907101344_0002
09/07/10 14:06:17 INFO mapred.JobClient: Counters: 17
09/07/10 14:06:17 INFO mapred.JobClient:   File Systems
09/07/10 14:06:17 INFO mapred.JobClient:     HDFS bytes read=1077612760
09/07/10 14:06:17 INFO mapred.JobClient:     HDFS bytes written=1077285377
09/07/10 14:06:17 INFO mapred.JobClient:     Local bytes read=1083539214
09/07/10 14:06:17 INFO mapred.JobClient:     Local bytes written=2167083496
09/07/10 14:06:17 INFO mapred.JobClient:   Job Counters 
09/07/10 14:06:17 INFO mapred.JobClient:     Launched reduce tasks=18
09/07/10 14:06:17 INFO mapred.JobClient:     Rack-local map tasks=2
09/07/10 14:06:17 INFO mapred.JobClient:     Launched map tasks=18
09/07/10 14:06:17 INFO mapred.JobClient:     Data-local map tasks=15
09/07/10 14:06:17 INFO mapred.JobClient:   Map-Reduce Framework
09/07/10 14:06:17 INFO mapred.JobClient:     Reduce input groups=102341
09/07/10 14:06:17 INFO mapred.JobClient:     Combine output records=0
09/07/10 14:06:17 INFO mapred.JobClient:     Map input records=102341
09/07/10 14:06:17 INFO mapred.JobClient:     Reduce output records=102341
09/07/10 14:06:17 INFO mapred.JobClient:     Map output bytes=1074564177
09/07/10 14:06:17 INFO mapred.JobClient:     Map input bytes=1077284885
09/07/10 14:06:17 INFO mapred.JobClient:     Combine input records=0
09/07/10 14:06:17 INFO mapred.JobClient:     Map output records=102341
09/07/10 14:06:17 INFO mapred.JobClient:     Reduce input records=102341
Job ended: Fri Jul 10 14:06:17 IST 2009
The job took 357 seconds.
"
Block Report Optimization: Queue block reports,HDFS-393,Use queuing to improve efficiency of block reports.
Add query whether storage has been initialized to DN's JMXBean,HDFS-14030,"During the short period while the DN is starting and the storage is not yet available we get the following errors:
{noformat}
ERROR org.apache.hadoop.jmx.JMXJsonServlet: getting attribute VolumeInfo of Hadoop:service=DataNode,name=DataNodeInfo threw an exception
javax.management.RuntimeMBeanException: java.lang.NullPointerException: Storage not yet initialized
{noformat}
I suggest to add an option to the DataNodeMXBean to query whether the storage has been initialized or not to surpass the exception."
TestDataNodeUUID#testUUIDRegeneration times out on Windows,HDFS-13568,"{color:#d04437}[INFO] Running org.apache.hadoop.hdfs.server.datanode.TestDataNodeUUID{color}
{color:#d04437}[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 10.059 s <<< FAILURE! - in org.apache.hadoop.hdfs.server.datanode.TestDataNodeUUID{color}
{color:#d04437}[ERROR] testUUIDRegeneration(org.apache.hadoop.hdfs.server.datanode.TestDataNodeUUID) Time elapsed: 10.006 s <<< ERROR!{color}
{color:#d04437}java.lang.Exception: test timed out after 10000 milliseconds{color}
{color:#d04437} at java.net.Inet4AddressImpl.getHostByAddr(Native Method){color}
{color:#d04437} at java.net.InetAddress$2.getHostByAddr(InetAddress.java:932){color}
{color:#d04437} at java.net.InetAddress.getHostFromNameService(InetAddress.java:617){color}
{color:#d04437} at java.net.InetAddress.getCanonicalHostName(InetAddress.java:588){color}
{color:#d04437} at org.apache.hadoop.net.DNS.resolveLocalHostname(DNS.java:284){color}
{color:#d04437} at org.apache.hadoop.net.DNS.<clinit>(DNS.java:61){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NNStorage.newBlockPoolID(NNStorage.java:989){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NNStorage.newNamespaceInfo(NNStorage.java:599){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.FSImage.format(FSImage.java:168){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1172){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:403){color}
{color:#d04437} at org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:234){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1080){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:883){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:514){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:473){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.datanode.TestDataNodeUUID.testUUIDRegeneration(TestDataNodeUUID.java:90){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62){color}
{color:#d04437} at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43){color}
{color:#d04437} at java.lang.reflect.Method.invoke(Method.java:498){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47){color}
{color:#d04437} at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44){color}
{color:#d04437} at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17){color}
{color:#d04437} at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74){color}

{color:#d04437}[INFO]{color}
{color:#d04437}[INFO] Results:{color}
{color:#d04437}[INFO]{color}
{color:#d04437}[ERROR] Errors:{color}
{color:#d04437}[ERROR] TestDataNodeUUID.testUUIDRegeneration:90 鈺?test timed out after 10000 millise...{color}
{color:#d04437}[INFO]{color}
{color:#d04437}[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0{color}

聽

{color:#333333}[Windows daily build|https://builds.apache.org/job/hadoop-trunk-win/467/testReport/org.apache.hadoop.hdfs.server.datanode/TestDataNodeUUID/testUUIDRegeneration/] also聽times out on this test.{color}"
TestPipelinesFailover times out on Windows,HDFS-13562,"testCompleteFileAfterCrashFailover times out, causing other tests to fail because they cannot start cluster:

{color:#d04437}[INFO] Running org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover{color}
{color:#d04437}[ERROR] Tests run: 8, Failures: 0, Errors: 8, Skipped: 0, Time elapsed: 30.813 s <<< FAILURE! - in org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover{color}
{color:#d04437}[ERROR] testCompleteFileAfterCrashFailover(org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover) Time elapsed: 30.009 s <<< ERROR!{color}
{color:#d04437}java.lang.Exception: test timed out after 30000 milliseconds{color}
{color:#d04437} at java.net.Inet4AddressImpl.getHostByAddr(Native Method){color}
{color:#d04437} at java.net.InetAddress$2.getHostByAddr(InetAddress.java:932){color}
{color:#d04437} at java.net.InetAddress.getHostFromNameService(InetAddress.java:617){color}
{color:#d04437} at java.net.InetAddress.getCanonicalHostName(InetAddress.java:588){color}
{color:#d04437} at org.apache.hadoop.security.SecurityUtil.getLocalHostName(SecurityUtil.java:256){color}
{color:#d04437} at org.apache.hadoop.security.SecurityUtil.replacePattern(SecurityUtil.java:224){color}
{color:#d04437} at org.apache.hadoop.security.SecurityUtil.getServerPrincipal(SecurityUtil.java:179){color}
{color:#d04437} at org.apache.hadoop.security.AuthenticationFilterInitializer.getFilterConfigMap(AuthenticationFilterInitializer.java:90){color}
{color:#d04437} at org.apache.hadoop.http.HttpServer2.getFilterProperties(HttpServer2.java:521){color}
{color:#d04437} at org.apache.hadoop.http.HttpServer2.constructSecretProvider(HttpServer2.java:511){color}
{color:#d04437} at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:400){color}
{color:#d04437} at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:115){color}
{color:#d04437} at org.apache.hadoop.http.HttpServer2$Builder.build(HttpServer2.java:336){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer.<init>(DatanodeHttpServer.java:131){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.datanode.DataNode.startInfoServer(DataNode.java:962){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1370){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:495){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2695){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2598){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1554){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:904){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:514){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:473){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover.doWriteOverFailoverTest(TestPipelinesFailover.java:143){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover.testCompleteFileAfterCrashFailover(TestPipelinesFailover.java:128){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62){color}
{color:#d04437} at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43){color}
{color:#d04437} at java.lang.reflect.Method.invoke(Method.java:498){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47){color}
{color:#d04437} at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44){color}
{color:#d04437} at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17){color}
{color:#d04437} at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74){color}

{color:#d04437}[ERROR] testWriteOverGracefulFailoverWithDnFail(org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover) Time elapsed: 0.055 s <<< ERROR!{color}
{color:#d04437}java.io.IOException: Could not fully delete D:\OSS\hadoop-branch-2\hadoop-hdfs-project\hadoop-hdfs\target\test\data\dfs\name1{color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1047){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:883){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:514){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:473){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover.doTestWriteOverFailoverWithDnFail(TestPipelinesFailover.java:221){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover.testWriteOverGracefulFailoverWithDnFail(TestPipelinesFailover.java:203){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62){color}
{color:#d04437} at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43){color}
{color:#d04437} at java.lang.reflect.Method.invoke(Method.java:498){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47){color}
{color:#d04437} at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44){color}
{color:#d04437} at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17){color}
{color:#d04437} at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74){color}

{color:#d04437}...{color}

聽

{color:#d04437}[INFO]{color}

{color:#d04437}[INFO] Results:{color}
{color:#d04437}[INFO]{color}
{color:#d04437}[ERROR] Errors:{color}
{color:#d04437}[ERROR] TestPipelinesFailover.testAllocateBlockAfterCrashFailover:122->doWriteOverFailoverTest:143 鈺?IO{color}
{color:#d04437}[ERROR] TestPipelinesFailover.testCompleteFileAfterCrashFailover:128->doWriteOverFailoverTest:143 鈺梴color}
{color:#d04437}[ERROR] TestPipelinesFailover.testFailoverRightBeforeCommitSynchronization:338 鈺?IO Co...{color}
{color:#d04437}[ERROR] TestPipelinesFailover.testLeaseRecoveryAfterFailover:283 鈺?IO Could not fully ...{color}
{color:#d04437}[ERROR] TestPipelinesFailover.testPipelineRecoveryStress:455 鈺?IO Could not fully dele...{color}
{color:#d04437}[ERROR] TestPipelinesFailover.testWriteOverCrashFailoverWithDnFail:208->doTestWriteOverFailoverWithDnFail:221 鈺?IO{color}
{color:#d04437}[ERROR] TestPipelinesFailover.testWriteOverGracefulFailover:116->doWriteOverFailoverTest:143 鈺?IO{color}
{color:#d04437}[ERROR] TestPipelinesFailover.testWriteOverGracefulFailoverWithDnFail:203->doTestWriteOverFailoverWithDnFail:221 鈺?IO{color}
{color:#d04437}[INFO]{color}
{color:#d04437}[ERROR] Tests run: 8, Failures: 0, Errors: 8, Skipped: 0{color}"
"TestFileAppend.testAppendCorruptedBlock,TestFileAppend.testConcurrentAppendRead time out on Windows",HDFS-13552,"{color:#d04437}[INFO] Running org.apache.hadoop.hdfs.TestFileAppend{color}
{color:#d04437}[ERROR] Tests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 20.073 s <<< FAILURE! - in org.apache.hadoop.hdfs.TestFileAppend{color}
{color:#d04437}[ERROR] testConcurrentAppendRead(org.apache.hadoop.hdfs.TestFileAppend) Time elapsed: 10.005 s <<< ERROR!{color}
{color:#d04437}java.lang.Exception: test timed out after 10000 milliseconds{color}
{color:#d04437} at java.net.Inet4AddressImpl.getHostByAddr(Native Method){color}
{color:#d04437} at java.net.InetAddress$2.getHostByAddr(InetAddress.java:932){color}
{color:#d04437} at java.net.InetAddress.getHostFromNameService(InetAddress.java:617){color}
{color:#d04437} at java.net.InetAddress.getCanonicalHostName(InetAddress.java:588){color}
{color:#d04437} at org.apache.hadoop.net.DNS.resolveLocalHostname(DNS.java:284){color}
{color:#d04437} at org.apache.hadoop.net.DNS.<clinit>(DNS.java:61){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NNStorage.newBlockPoolID(NNStorage.java:989){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NNStorage.newNamespaceInfo(NNStorage.java:599){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.FSImage.format(FSImage.java:168){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1172){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:403){color}
{color:#d04437} at org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:234){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1080){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:883){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:514){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:473){color}
{color:#d04437} at org.apache.hadoop.hdfs.TestFileAppend.testConcurrentAppendRead(TestFileAppend.java:701){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62){color}
{color:#d04437} at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43){color}
{color:#d04437} at java.lang.reflect.Method.invoke(Method.java:498){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47){color}
{color:#d04437} at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44){color}
{color:#d04437} at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17){color}
{color:#d04437} at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74){color}

{color:#d04437}[ERROR] testAppendCorruptedBlock(org.apache.hadoop.hdfs.TestFileAppend) Time elapsed: 10.001 s <<< ERROR!{color}
{color:#d04437}java.lang.Exception: test timed out after 10000 milliseconds{color}
{color:#d04437} at java.net.Inet4AddressImpl.getHostByAddr(Native Method){color}
{color:#d04437} at java.net.InetAddress$2.getHostByAddr(InetAddress.java:932){color}
{color:#d04437} at java.net.InetAddress.getHostFromNameService(InetAddress.java:617){color}
{color:#d04437} at java.net.InetAddress.getCanonicalHostName(InetAddress.java:588){color}
{color:#d04437} at org.apache.hadoop.security.SecurityUtil.getLocalHostName(SecurityUtil.java:256){color}
{color:#d04437} at org.apache.hadoop.security.SecurityUtil.replacePattern(SecurityUtil.java:224){color}
{color:#d04437} at org.apache.hadoop.security.SecurityUtil.getServerPrincipal(SecurityUtil.java:179){color}
{color:#d04437} at org.apache.hadoop.security.AuthenticationFilterInitializer.getFilterConfigMap(AuthenticationFilterInitializer.java:90){color}
{color:#d04437} at org.apache.hadoop.http.HttpServer2.getFilterProperties(HttpServer2.java:521){color}
{color:#d04437} at org.apache.hadoop.http.HttpServer2.constructSecretProvider(HttpServer2.java:511){color}
{color:#d04437} at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:400){color}
{color:#d04437} at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:115){color}
{color:#d04437} at org.apache.hadoop.http.HttpServer2$Builder.build(HttpServer2.java:336){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:162){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:889){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:725){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:953){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:932){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1215){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1090){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:883){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:514){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:473){color}
{color:#d04437} at org.apache.hadoop.hdfs.TestFileAppend.testAppendCorruptedBlock(TestFileAppend.java:674){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62){color}
{color:#d04437} at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43){color}
{color:#d04437} at java.lang.reflect.Method.invoke(Method.java:498){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47){color}
{color:#d04437} at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44){color}
{color:#d04437} at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17){color}
{color:#d04437} at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74){color}

{color:#d04437}[INFO]{color}
{color:#d04437}[INFO] Results:{color}
{color:#d04437}[INFO]{color}
{color:#d04437}[ERROR] Errors:{color}
{color:#d04437}[ERROR] TestFileAppend.testAppendCorruptedBlock:674 鈺?test timed out after 10000 mill...{color}
{color:#d04437}[ERROR] TestFileAppend.testConcurrentAppendRead:701 鈺?test timed out after 10000 mill...{color}
{color:#d04437}[INFO]{color}
{color:#d04437}[ERROR] Tests run: 2, Failures: 0, Errors: 2, Skipped: 0{color}"
TestDataNodeMultipleRegistrations#testClusterIdMismatchAtStartupWithHA times out intermittently on Windows,HDFS-13569,"On Windows,聽TestDataNodeMultipleRegistrations#testClusterIdMismatchAtStartupWithHA may time out and cause subsequent tests to fail.

{color:#d04437}[INFO] Running org.apache.hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations{color}
{color:#d04437}[ERROR] Tests run: 6, Failures: 0, Errors: 5, Skipped: 0, Time elapsed: 65.082 s <<< FAILURE! - in org.apache.hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations{color}
{color:#d04437}[ERROR] testClusterIdMismatchAtStartupWithHA(org.apache.hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations) Time elapsed: 20.003 s <<< ERROR!{color}
{color:#d04437}java.lang.Exception: test timed out after 20000 milliseconds{color}
{color:#d04437} at java.net.NetworkInterface.getAll(Native Method){color}
{color:#d04437} at java.net.NetworkInterface.getNetworkInterfaces(NetworkInterface.java:343){color}
{color:#d04437} at org.apache.htrace.core.TracerId.getBestIpString(TracerId.java:179){color}
{color:#d04437} at org.apache.htrace.core.TracerId.processShellVar(TracerId.java:145){color}
{color:#d04437} at org.apache.htrace.core.TracerId.<init>(TracerId.java:116){color}
{color:#d04437} at org.apache.htrace.core.Tracer$Builder.build(Tracer.java:159){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:940){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:932){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1215){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1090){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:883){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:514){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:473){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations.testClusterIdMismatchAtStartupWithHA(TestDataNodeMultipleRegistrations.java:248){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62){color}
{color:#d04437} at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43){color}
{color:#d04437} at java.lang.reflect.Method.invoke(Method.java:498){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47){color}
{color:#d04437} at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44){color}
{color:#d04437} at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17){color}
{color:#d04437} at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74){color}

{color:#d04437}[ERROR] test2NNRegistration(org.apache.hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations) Time elapsed: 0.029 s <<< ERROR!{color}
{color:#d04437}java.io.IOException: Could not fully delete E:\OSS\hadoop-branch-2\hadoop-hdfs-project\hadoop-hdfs\target\test\data\dfs\name1{color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1047){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:883){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:514){color}
{color:#d04437} at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:473){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations.test2NNRegistration(TestDataNodeMultipleRegistrations.java:69){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62){color}
{color:#d04437} at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43){color}
{color:#d04437} at java.lang.reflect.Method.invoke(Method.java:498){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47){color}
{color:#d04437} at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44){color}
{color:#d04437} at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17){color}
{color:#d04437} at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26){color}
{color:#d04437} at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271){color}
{color:#d04437} at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70){color}
{color:#d04437} at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50){color}
{color:#d04437} at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238){color}
{color:#d04437} at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63){color}
{color:#d04437} at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236){color}
{color:#d04437} at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53){color}
{color:#d04437} at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229){color}
{color:#d04437} at org.junit.runners.ParentRunner.run(ParentRunner.java:309){color}
{color:#d04437} at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365){color}
{color:#d04437} at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273){color}
{color:#d04437} at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238){color}
{color:#d04437} at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159){color}
{color:#d04437} at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:379){color}
{color:#d04437} at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:340){color}
{color:#d04437} at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:125){color}
{color:#d04437} at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:413){color}

聽

{color:#d04437}...{color}

{color:#d04437}[INFO]{color}
{color:#d04437}[INFO] Results:{color}
{color:#d04437}[INFO]{color}
{color:#d04437}[ERROR] Errors:{color}
{color:#d04437}[ERROR] TestDataNodeMultipleRegistrations.test2NNRegistration:69 鈺?IO Could not fully ...{color}
{color:#d04437}[ERROR] TestDataNodeMultipleRegistrations.testClusterIdMismatch:204 鈺?IO Could not ful...{color}
{color:#d04437}[ERROR] TestDataNodeMultipleRegistrations.testClusterIdMismatchAtStartupWithHA:248 鈺?...{color}
{color:#d04437}[ERROR] TestDataNodeMultipleRegistrations.testDNWithInvalidStorageWithHA:273 鈺?IO Coul...{color}
{color:#d04437}[ERROR] TestDataNodeMultipleRegistrations.testMiniDFSClusterWithMultipleNN:314 鈺?IO Co...{color}
{color:#d04437}[INFO]{color}
{color:#d04437}[ERROR] Tests run: 6, Failures: 0, Errors: 5, Skipped: 0{color}"
TestTransferFsImage#testGetImageTimeout times out intermittently on Windows,HDFS-13561,"{color:#d04437}[INFO] Running org.apache.hadoop.hdfs.server.namenode.TestTransferFsImage{color}
{color:#d04437}[ERROR] Tests run: 4, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 32.695 s <<< FAILURE! - in org.apache.hadoop.hdfs.server.namenode.TestTransferFsImage{color}
{color:#d04437}[ERROR] testGetImageTimeout(org.apache.hadoop.hdfs.server.namenode.TestTransferFsImage) Time elapsed: 5.002 s <<< ERROR!{color}
{color:#d04437}java.lang.Exception: test timed out after 5000 milliseconds{color}
{color:#d04437} at java.net.SocketInputStream.socketRead0(Native Method){color}
{color:#d04437} at java.net.SocketInputStream.socketRead(SocketInputStream.java:116){color}
{color:#d04437} at java.net.SocketInputStream.read(SocketInputStream.java:171){color}
{color:#d04437} at java.net.SocketInputStream.read(SocketInputStream.java:141){color}
{color:#d04437} at java.io.BufferedInputStream.fill(BufferedInputStream.java:246){color}
{color:#d04437} at java.io.BufferedInputStream.read1(BufferedInputStream.java:286){color}
{color:#d04437} at java.io.BufferedInputStream.read(BufferedInputStream.java:345){color}
{color:#d04437} at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704){color}
{color:#d04437} at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647){color}
{color:#d04437} at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1569){color}
{color:#d04437} at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1474){color}
{color:#d04437} at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.TransferFsImage.doGetUrl(TransferFsImage.java:433){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.TransferFsImage.getFileClient(TransferFsImage.java:418){color}
{color:#d04437} at org.apache.hadoop.hdfs.server.namenode.TestTransferFsImage.testGetImageTimeout(TestTransferFsImage.java:133){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method){color}
{color:#d04437} at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62){color}
{color:#d04437} at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43){color}
{color:#d04437} at java.lang.reflect.Method.invoke(Method.java:498){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47){color}
{color:#d04437} at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12){color}
{color:#d04437} at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44){color}
{color:#d04437} at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17){color}
{color:#d04437} at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74){color}

{color:#d04437}[INFO]{color}
{color:#d04437}[INFO] Results:{color}
{color:#d04437}[INFO]{color}
{color:#d04437}[ERROR] Errors:{color}
{color:#d04437}[ERROR] TestTransferFsImage.testGetImageTimeout:133 鈺?test timed out after 5000 milli...{color}
{color:#d04437}[INFO]{color}
{color:#d04437}[ERROR] Tests run: 4, Failures: 0, Errors: 1, Skipped: 0{color}"
TestNetworkTopology#testInvalidNetworkTopologiesNotCachedInHdfs times out on Windows,HDFS-13555,"Although TestNetworkTopology#testInvalidNetworkTopologiesNotCachedInHdfs has 180s timeout, it is overridden by global timeout
{code:java}
@Rule
 public Timeout testTimeout = new Timeout(30000);{code}
{color:#d04437}[INFO] Running org.apache.hadoop.net.TestNetworkTopology{color}
{color:#d04437}[ERROR] Tests run: 15, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 30.741 s <<< FAILURE! - in org.apache.hadoop.net.TestNetworkTopology{color}
{color:#d04437}[ERROR] testInvalidNetworkTopologiesNotCachedInHdfs(org.apache.hadoop.net.TestNetworkTopology) Time elapsed: 30.009 s <<< ERROR!{color}
{color:#d04437}java.lang.Exception: test timed out after 30000 milliseconds{color}
{color:#d04437} at java.lang.Object.wait(Native Method){color}
{color:#d04437} at java.lang.Thread.join(Thread.java:1257){color}
{color:#d04437} at org.junit.internal.runners.statements.FailOnTimeout.evaluateStatement(FailOnTimeout.java:26){color}
{color:#d04437} at org.junit.internal.runners.statements.FailOnTimeout.evaluate(FailOnTimeout.java:17){color}
{color:#d04437} at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26){color}
{color:#d04437} at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74){color}

{color:#d04437}[INFO]{color}
{color:#d04437}[INFO] Results:{color}
{color:#d04437}[INFO]{color}
{color:#d04437}[ERROR] Errors:{color}
{color:#d04437}[ERROR] TestNetworkTopology>Object.wait:-2 鈺?test timed out after 30000 milliseconds{color}
{color:#d04437}[INFO]{color}
{color:#d04437}[ERROR] Tests run: 15, Failures: 0, Errors: 1, Skipped: 0{color}"
Why we don't add a harder lease expiration limit.,HDFS-13288,"Currently there exists a soft expire timeout(1 minutes by default) and hard expire timeout(60 minutes by default).聽

On our production environment. Some client began writing a file long time(more than one year) ago, when writing finished and tried to close the output stream, the client failed closing it (for some IOException. etc. ).聽 But the client process is a background service, it doesn't exit. So the lease doesn't released for more than one year.

The problem is that, the lease for the file is occupied, we have to call recover lease on the file when doing demission or appending operation.

聽

So I am聽wondering why we don't add a more harder lease expire timeout, when a lease lasts too long (maybe one month),聽 revoke it.聽

聽"
IllegalStateException: Unable to finalize edits file,HDFS-13206,"I noticed the following in hbase test output running against hadoop3:
{code}
2018-02-28 18:40:18,491 ERROR [Time-limited test] namenode.JournalSet(402): Error: finalize log segment 1, 658 failed for (journal JournalAndStream(mgr=FileJournalManager(root=/mnt/disk2/a/2-hbase/hbase-server/target/test-data/5670112c-31f1-43b0-af31-c1182e142e63/cluster_8f993609-c3a1-4fb4-8b3d-0e642261deb1/dfs/name-0-1), stream=null))
java.lang.IllegalStateException: Unable to finalize edits file /mnt/disk2/a/2-hbase/hbase-server/target/test-data/5670112c-31f1-43b0-af31-c1182e142e63/cluster_8f993609-c3a1-4fb4-8b3d-0e642261deb1/dfs/name-0-1/current/edits_inprogress_0000000000000000001
  at org.apache.hadoop.hdfs.server.namenode.FileJournalManager.finalizeLogSegment(FileJournalManager.java:153)
  at org.apache.hadoop.hdfs.server.namenode.JournalSet$2.apply(JournalSet.java:224)
  at org.apache.hadoop.hdfs.server.namenode.JournalSet.mapJournalsAndReportErrors(JournalSet.java:385)
  at org.apache.hadoop.hdfs.server.namenode.JournalSet.finalizeLogSegment(JournalSet.java:219)
  at org.apache.hadoop.hdfs.server.namenode.FSEditLog.endCurrentLogSegment(FSEditLog.java:1427)
  at org.apache.hadoop.hdfs.server.namenode.FSEditLog.close(FSEditLog.java:398)
  at org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync.close(FSEditLogAsync.java:110)
  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.stopActiveServices(FSNamesystem.java:1320)
  at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.stopActiveServices(NameNode.java:1909)
  at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.exitState(ActiveState.java:70)
  at org.apache.hadoop.hdfs.server.namenode.NameNode.stop(NameNode.java:1013)
  at org.apache.hadoop.hdfs.MiniDFSCluster.stopAndJoinNameNode(MiniDFSCluster.java:2047)
  at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:1987)
  at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:1958)
  at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:1951)
  at org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniDFSCluster(HBaseTestingUtility.java:767)
  at org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster(HBaseTestingUtility.java:1109)
  at org.apache.hadoop.hbase.master.balancer.TestFavoredNodeTableImport.stopCluster(TestFavoredNodeTableImport.java:71)
{code}"
Allow dfs.data.transfer.protection default to hadoop.rpc.protection,HDFS-6859,"Currently administrator needs to configure both _dfs.data.transfer.protection_ and _hadoop.rpc.protection_ to specify _QOP_ for rpc and data transfer protocols. In some cases, the values for these two properties will be same. In those cases, it may be easier to allow dfs.data.transfer.protection default to hadoop.rpc.protection.
This also ensures that an admin will get QOP as _Authentication_ if admin  does not specify either of those values.

Separate jiras  (HDFS-6858 and HDFS-6859) are created for dfs.data.transfer.saslproperties.resolver.class and dfs.data.transfer.protection respectively.
"
The DFSUsed value bigger than the Capacity,HDFS-13034,"||Node||Last contact||Admin State||Capacity||Used||Non DFS Used||Remaining||Blocks||Block pool used||Failed Volumes||Version||
|A|0|In Service|20.65 TB|18.26 TB|0 B|1.27 TB|24330|2.57 TB (12.42%)|0|2.7.1|
|B|2|In Service|5.47 TB|12.78 TB|0 B|1.46 TB|27657|2.65 TB (48.37%)|0|2.7.1|"
StreamCapability enums are not displayed in javadoc,HDFS-12604,"http://hadoop.apache.org/docs/r3.0.0-beta1/api/org/apache/hadoop/fs/StreamCapabilities.html

{{StreamCapability#HFLUSH}} and {{StreamCapability#HSYNC}} are not displayed in the doc."
backport HDFS-3568 (add security to fuse_dfs) to branch-1,HDFS-3638,"Backport HDFS-3568 to branch-1.  This will give fuse_dfs support for Kerberos authentication, allowing FUSE to be used in a secure cluster."
handling of corrupt blocks not suitable for commodity hardware,HDFS-12649,"Hadoop's documentation tells me it's suitable for commodity hardware in the sense that hardware failures are expected to happen frequently. However, there is currently no automatic handling of corrupted blocks, which seems a bit contradictory to me.

See: https://stackoverflow.com/questions/19205057/how-to-fix-corrupt-hdfs-files

This is even problematic for data integrity as the redundancy is not kept at the desired level without manual intervention and therefore in a timely manner. If there is a corrupted block, I would at least expect that the namenode forces the creation of an additional good replica to keep up the redundancy level, ie. the redundancy level should never include corrupted data... which it currently does:

    ""UnderReplicatedBlocks"" : 0,
    ""CorruptBlocks"" : 2,

(namenode /jmx http dump)"
TestCheckpoint#testCheckpoint may fail due to Bad value assertion,HDFS-5834,"I saw the following when running test suite on Linux:
{code}
testCheckpoint(org.apache.hadoop.hdfs.server.namenode.TestCheckpoint)  Time elapsed: 3.058 sec  <<< FAILURE!
java.lang.AssertionError: Bad value for metric GetImageNumOps
Expected: gt(0)
     got: <0L>

        at org.junit.Assert.assertThat(Assert.java:780)
        at org.apache.hadoop.test.MetricsAsserts.assertCounterGt(MetricsAsserts.java:318)
        at org.apache.hadoop.hdfs.server.namenode.TestCheckpoint.testCheckpoint(TestCheckpoint.java:1058)
{code}"
TestHttpsFileSystem intermittently fails with Port in use error,HDFS-5718,"From https://builds.apache.org/job/Hadoop-Hdfs-trunk/1634/testReport/junit/org.apache.hadoop.hdfs.web/TestHttpsFileSystem/org_apache_hadoop_hdfs_web_TestHttpsFileSystem/ :
{code}
java.net.BindException: Port in use: localhost:50475
	at java.net.PlainSocketImpl.socketBind(Native Method)
	at java.net.PlainSocketImpl.bind(PlainSocketImpl.java:383)
	at java.net.ServerSocket.bind(ServerSocket.java:328)
	at java.net.ServerSocket.<init>(ServerSocket.java:194)
	at javax.net.ssl.SSLServerSocket.<init>(SSLServerSocket.java:106)
	at com.sun.net.ssl.internal.ssl.SSLServerSocketImpl.<init>(SSLServerSocketImpl.java:108)
	at com.sun.net.ssl.internal.ssl.SSLServerSocketFactoryImpl.createServerSocket(SSLServerSocketFactoryImpl.java:72)
	at org.mortbay.jetty.security.SslSocketConnector.newServerSocket(SslSocketConnector.java:478)
	at org.mortbay.jetty.bio.SocketConnector.open(SocketConnector.java:73)
	at org.apache.hadoop.http.HttpServer.openListeners(HttpServer.java:973)
	at org.apache.hadoop.http.HttpServer.start(HttpServer.java:914)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startInfoServer(DataNode.java:412)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:769)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:315)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1846)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1746)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1203)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:673)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:342)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:323)
	at org.apache.hadoop.hdfs.web.TestHttpsFileSystem.setUp(TestHttpsFileSystem.java:64)
{code}
This could have been caused by concurrent test(s)."
Ozone: TestBlockDeletingService#testBlockDeletionTimeout sometimes timeout,HDFS-12401,"{code}
testBlockDeletionTimeout(org.apache.hadoop.ozone.container.common.TestBlockDeletingService)  Time elapsed: 100.383 sec  <<< ERROR!
java.util.concurrent.TimeoutException: Timed out waiting for condition. Thread diagnostics:
{code}"
Block Storage : make the server address config more concise,HDFS-12041,"Currently there are a few places where the address are read from config like such 
{code}
    String cbmIPAddress = ozoneConf.get(
        DFS_CBLOCK_JSCSI_CBLOCK_SERVER_ADDRESS_KEY,
        DFS_CBLOCK_JSCSI_CBLOCK_SERVER_ADDRESS_DEFAULT
    );
    int cbmPort = ozoneConf.getInt(
        DFS_CBLOCK_JSCSI_PORT_KEY,
        DFS_CBLOCK_JSCSI_PORT_DEFAULT
    );
{code}
Similarly for jscsi address config. Maybe we should consider merge these to one single key config in form of host:port."
TestWebHdfsTimeouts fails due to null SocketTimeoutException,HDFS-11059,"TestWebHdfsTimeouts expects SocketTimeoutException with ""connect timed out"" or ""Read timed out"" message but fails when encountering ""null"" message sometimes. Occurred 4 out of 100 tests.

{{SocksSocketImpl#remainingMillis}} may send null SocketTimeoutException:
{code}
    private static int remainingMillis(long deadlineMillis) throws IOException {
        if (deadlineMillis == 0L)
            return 0;

        final long remaining = deadlineMillis - System.currentTimeMillis();
        if (remaining > 0)
            return (int) remaining;

        throw new SocketTimeoutException();   <<<<==
    }
{code}"
Include event for AddBlock in Inotify Event Stream,HDFS-10608,"It would be nice to have an AddBlockEvent in the INotify pipeline.  Based on discussions from mailing list:

http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-dev/201607.mbox/%3C1467743792.4040080.657624289.7BE240AD%40webmail.messagingengine.com%3E"
Dead Code in DFS Util for DFSUtil#substituteForWildcardAddress,HDFS-8609,"Dead code after JDK 1.4

{code}
    otherHttpAddr = DFSUtil.getInfoServerWithDefaultHost(
        otherIpcAddr.getHostName(), otherNode, scheme).toURL();
{code}

In {{DFSUtil#substituteForWildcardAddress}} 
{code}
 if (addr != null && addr.isAnyLocalAddress()) {
...
}
{code}

addr.isAnyLocalAddress() will always return false.

Always the url will be formed with address which is configured  in hdfs-site.xml .Same will affect bootStrap from NN and ssl certificate check"
combine FsShell.copyToLocal to ChecksumFileSystem.copyToLocalFile,HDFS-291,#NAME?
Remove deprecated FileSystem#getDefault* and getServerDefault methods that don't take a Path argument ,HDFS-11228,"FileSystem#getServerDefaults(), #getDefaultReplication, #getDefaultBlockSize were deprecated by HADOOP-8422 and the fix version is 2.0.2-alpha. They can be removed in Hadoop 3."
Improve BKJM documentation,HDFS-4613,"The documentation tags BKJM as experimental, and I don't think it is experimental any longer. It is also missing a link of HDFS HA with BKJM on the left-hand side menu of the documentation."
FSDirAttrOp#setOwner throws ACE with misleading message,HDFS-10378,"Calling {{setOwner}} as a non-super user does trigger {{AccessControlException}}, however, the message ""Permission denied. user=user1967821757 is not the owner of inode=child"" is wrong. Expect this message: ""Non-super user cannot change owner"".

Output of patched unit test {{TestPermission.testFilePermission}}:
{noformat}
2016-05-06 16:45:44,915 [main] INFO  security.TestPermission (TestPermission.java:testFilePermission(280)) - GOOD: got org.apache.hadoop.security.AccessControlException: Permission denied. user=user1967821757 is not the owner of inode=child1
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkOwner(FSPermissionChecker.java:273)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:250)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1642)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1626)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkOwner(FSDirectory.java:1595)
	at org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.setOwner(FSDirAttrOp.java:88)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setOwner(FSNamesystem.java:1717)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setOwner(NameNodeRpcServer.java:835)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setOwner(ClientNamenodeProtocolServerSideTranslatorPB.java:481)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:665)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2423)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2419)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1755)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2417)
{noformat}

Will upload the unit test patch shortly."
Create accessor methods for DataNode#data and DataNode#isBlockTokenEnabled,HDFS-9028,Currently both DataNode#data and DataNode#isBlockTokenEnabled instance variables are package scoped with no accessor methods. This makes mocking in unit tests difficult. This jira is to make them private scoped with proper getters and setters.
DistributedFileSystem.listLocatedStatus() should return HdfsBlockLocation instead of BlockLocation,HDFS-10466,"https://issues.apache.org/jira/browse/HDFS-202 added a new API listLocatedStatus() to get all files' status with block locations for a directory. This is great that we don't need to call FileSystem.getFileBlockLocations() for each file. it's much faster (about 8-10 times).
However, the returned LocatedFileStatus only contains basic BlockLocation instead of HdfsBlockLocation, the LocatedBlock details are stripped out.

It should do the similar as DFSClient.getBlockLocations(), return HdfsBlockLocation which provide full block location details.

The implementation of DistributedFileSystem. listLocatedStatus() retrieves HdfsLocatedFileStatus which contains all information, but when convert it to LocatedFileStatus, it doesn't keep LocatedBlock data. It's a simple (and compatible) change to make to keep the LocatedBlock details."
FsShell get command does not support writing to stdout,HDFS-4869,"In FsShell the put command supports using ""\-"" in place of stdin, but this functionality (""\-"" in place of stdout) is broken in the get command."
"""hdfs dfs"" command usage still shows ""hadoop fs""",HDFS-3435,"""hdfs dfs"" command usage still shows ""hadoop fs"".
{code}
$ hdfs dfs
Usage: hadoop fs [generic options]
{code}

The command should be ""hdfs dfs""."
"Rename ""path.based"" caching configuration options",HDFS-5551,"Some configuration options still have the ""path.based"" moniker, missed during the big rename removing this naming convention."
libhdfs doesn't work with jamVM,HDFS-4387,"Building and running tests on OpenJDK 7 on Ubuntu 12.10 fails with {{mvn test -Pnative}}.  The output is hard to decipher but the underlying issue is that {{test_libhdfs_native}} segfaults at startup.

{noformat}
(gdb) run
Starting program: /mnt/trunk/hadoop-hdfs-project/hadoop-hdfs/target/native/test_libhdfs_threaded
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/lib/x86_64-linux-gnu/libthread_db.so.1"".

Program received signal SIGSEGV, Segmentation fault.
0x00007ffff739a897 in attachJNIThread (name=0x0, is_daemon=is_daemon@entry=0 '\000', group=0x0) at thread.c:768
768 thread.c: No such file or directory.
(gdb) where
#0 0x00007ffff739a897 in attachJNIThread (name=0x0, is_daemon=is_daemon@entry=0 '\000', group=0x0) at thread.c:768
#1 0x00007ffff7395020 in attachCurrentThread (is_daemon=0, args=0x0, penv=0x7fffffffddb8) at jni.c:1454
#2 Jam_AttachCurrentThread (vm=<optimized out>, penv=0x7fffffffddb8, args=0x0) at jni.c:1466
#3 0x00007ffff7bcf979 in getGlobalJNIEnv () at /mnt/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/jni_helper.c:527
#4 getJNIEnv () at /mnt/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/jni_helper.c:585
#5 0x0000000000402512 in nmdCreate (conf=conf@entry=0x7fffffffdeb0) at /mnt/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/native_mini_dfs.c:49
#6 0x00000000004016e1 in main () at /mnt/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/test_libhdfs_threaded.c:283
{noformat}"
Some usage of GetConf tool not compatible with federation,HDFS-4202,"In the start/stop scripts, we use the ""getconf"" tool to look up certain config keys to discover hosts we need to start/stop daemons on. However, some of these configs are optionally suffixed by a nameservice or nameservice.namenode identifier. In these cases, the start/stop scripts wouldn't correctly see the configs, and therefore would fail to start all the necessary daemons for the federated cluster."
Hdfs audit shouldn't log mkdir operaton if the directory already exists.,HDFS-10305,"Currently Hdfs audit logs mkdir operation even if the directory already exists.
This creates confusion while analyzing audit logs."
Is DataNode aware of the name of the file that it is going to store?,HDFS-10252,"I am going through the HDFS Namenode and Datanode code and I am trying to see if the DataNode is aware of the names of the files that are stored in it (and other metadata as well).

Assuming that we have the most simple case: 
1 NameNode
1 DataNode
1 single machine running HDFS with replication factor 1. 

and considering the way HDFS works a use case could be: 
A client requests to write a file from local to HDFS (for example: ""hdfs dfs -put file /file"")
He first communicates with NameNode and gets where this file should be stored.
Then, after receiving an answer, he requests to the DataNode to store that file.

(At that point I am going to be a little more specific about the code)
The DataNode has a DataXceiverServer class which runs and waits for requests. When a request comes, it starts a DataXceiver thread and try to serve that request. What I would like to know is, if at that specific point the DataNode knows the name of the file that it is going to store. I spent hours of debugging but I could not find it. Is it somewhere there, or only the NameNode knows the name of that file?"
Tests that use KeyStoreUtil must call KeyStoreUtil.cleanupSSLConfig(),HDFS-9309,"When KeyStoreUtil.setupSSLConfig() is called, several files are created (ssl-server.xml, ssl-client.xml, trustKS.jks, clientKS.jks, serverKS.jks). However, if they are not deleted upon exit, weird thing can happen to any subsequent tests.

For example, if ssl-client.xml is not delete, but trustKS.jks is deleted, TestWebHDFSOAuth2.listStatusReturnsAsExpected will fail with message:
{noformat}
java.io.IOException: Unable to load OAuth2 connection factory.
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.apache.hadoop.security.ssl.ReloadingX509TrustManager.loadTrustManager(ReloadingX509TrustManager.java:164)
	at org.apache.hadoop.security.ssl.ReloadingX509TrustManager.<init>(ReloadingX509TrustManager.java:81)
	at org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.init(FileBasedKeyStoresFactory.java:215)
	at org.apache.hadoop.security.ssl.SSLFactory.init(SSLFactory.java:131)
	at org.apache.hadoop.hdfs.web.URLConnectionFactory.newSslConnConfigurator(URLConnectionFactory.java:138)
	at org.apache.hadoop.hdfs.web.URLConnectionFactory.newOAuth2URLConnectionFactory(URLConnectionFactory.java:112)
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.initialize(WebHdfsFileSystem.java:163)
	at org.apache.hadoop.hdfs.web.TestWebHDFSOAuth2.listStatusReturnsAsExpected(TestWebHDFSOAuth2.java:147)
{noformat}

There are currently several tests that do not clean up:

{noformat}

130 鉁?weichiu@weichiu ~/trunk (trunk) $ grep -rnw . -e 'KeyStoreTestUtil\.setupSSLConfig' | cut -d: -f1 |xargs grep -L ""KeyStoreTestUtil\.cleanupSSLConfig""
./hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/TestKMS.java
./hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/webapp/TestTimelineWebServicesWithSSL.java
./hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsTokens.java
./hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferTestCase.java
./hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/TestSecureNNWithQJM.java
./hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRespectsBindHostKeys.java
./hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/fs/http/client/TestHttpFSFWithSWebhdfsFileSystem.java
{noformat}

This JIRA is the effort to remove the bug."
SnapshotDiffReport$DiffReportEntry$hashCode should use type field,HDFS-9573,"DiffReportEntry.equals() uses field ""type"", but DiffReportEntry.hashCode() doesn't. This breaks the rules on equals and hashCode:
* if a class overrides equals, it must override hashCode
* when they are both overridden, equals and hashCode must use the same set of fields"
"jsvc should be listed as a dependency for ""package"" and ""bin-package""",HDFS-3316,"The dependency on jsvc of targets ""package"" and ""bin-package"" would be much clearer if made explicit, as in the proposed patch.  (However, the larger issue of HADOOP-8364 should be addressed first.)"
Adding new target to build.xml to run test-core without compiling,HDFS-1494,"While testing Apache Harmony Select (lightweight version of Harmony) with Hadoop hdfs we had to first build with Harmony and then test using Harmony Select using the test-core target. This was done in an effort to investigate any issues with Harmony Select in running common. However, the test-core target also compiles the classes which we are unable to do with Harmony Select. A new target is proposed that only runs the tests without compiling them."
Inconsistent log level practice,HDFS-8840,"In method ""checkLogsAvailableForRead()"" of class: hadoop-2.7.1-src\hadoop-hdfs-project\hadoop-hdfs\src\main\java\org\apache\hadoop\hdfs\server\namenode\ha\BootstrapStandby.java

The log level is not correct, after checking ""LOG.isDebugEnabled()"", we should use ""LOG.debug(msg, e);"", while now we use "" LOG.fatal(msg, e);"". Log level is inconsistent.

the source code of this method is:
private boolean checkLogsAvailableForRead(FSImage image, long imageTxId, long curTxIdOnOtherNode) {

  ...
    } catch (IOException e) {
   ...
      if (LOG.isDebugEnabled()) {
        LOG.fatal(msg, e);
      } else {
        LOG.fatal(msg);
      }
      return false;
    }
  }
"
Random VolumeChoosingPolicy,HDFS-9551,Please find attached a new implementation of VolumeChoosingPolicy.  This implementation chooses volumes at random to place blocks.  It is thread-safe and un-synchronized so there is less thread contention.
When dfs.block.size is configured to 0 the block which is created in rbw is never deleted,HDFS-3356,"dfs.block.size=0
step 1: start NN and DN
step 2: write a file ""a.txt""
The block is created in rbw and since the blocksize is 0 write fails and the file is not closed. DN sents in the block report , number of blocks as 1
Even after the DN has sent the block report and directory scan has been done , the block is not invalidated for ever.

But In earlier version when the block.size is configured to 0 default value will be taken and write will be successful.
NN logs:
========
{noformat}
2012-04-24 19:54:27,089 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* processReport: from DatanodeRegistration(.18.40.117, storageID=DS-452047493-xx.xx.xx.xx-50076-1335277451277, infoPort=50075, ipcPort=50077, storageInfo=lv=-40;cid=CID-742fda5f-68f7-40a5-9d52-a2a15facc6af;nsid=797082741;c=0), blocks: 0, processing time: 0 msecs
2012-04-24 19:54:29,689 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /1._COPYING_. BP-1612285678-xx.xx.xx.xx-1335277427136 blk_-262107679534121671_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[xx.xx.xx.xx:50076|RBW]]}
2012-04-24 19:54:30,113 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* processReport: from DatanodeRegistration(xx.xx.xx.xx, storageID=DS-452047493-xx.xx.xx.xx-50076-1335277451277, infoPort=50075, ipcPort=50077, storageInfo=lv=-40;cid=CID-742fda5f-68f7-40a5-9d52-a2a15facc6af;nsid=797082741;c=0), blocks: 1, processing time: 0 msecs{noformat}

Exception message while writing a file:
=======================================
{noformat}
./hdfs dfs -put hadoop /1
12/04/24 19:54:30 WARN hdfs.DFSClient: DataStreamer Exception
java.io.IOException: BlockSize 0 is smaller than data size.  Offset of packet in block 4745 Aborting file /1._COPYING_
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:467)
put: BlockSize 0 is smaller than data size.  Offset of packet in block 4745 Aborting file /1._COPYING_
12/04/24 19:54:30 ERROR hdfs.DFSClient: Failed to close file /1._COPYING_
java.io.IOException: BlockSize 0 is smaller than data size.  Offset of packet in block 4745 Aborting file /1._COPYING_
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:467){noformat}"
Implement a unix-like cat utility,HDFS-9272,"Implement the basic functionality of ""cat"" and have it build as a separate executable.

2 Reasons for this:
We don't have any real integration tests at the moment so something simple to verify that the library actually works against a real cluster is useful.

Eventually I'll make more utilities like stat, mkdir etc.  Once there are enough of them it will be simple to make a C++ implementation of the hadoop fs command line interface that doesn't take the latency hit of spinning up a JVM."
TestOfflineImageViewer.outputOfLSVisitor fails for certain usernames,HDFS-6540,"TestOfflineImageViewer.outputOfLSVisitor() fails if the username contains ""-"" (dash). A dash is a valid character in a username."
Non Authenticated Data node Allowed to Join HDFS,HDFS-8906,"An attacker with network access to a Hadoop cluster can create a spoof datanode that the namenode will accept into the cluster without authentication, allowing the attacker to run MapReduce jobs on the cluster in order to steal data.  The spoof datanode is created by adding the namenode RSA SSH public key to the known hosts directory, starting Hadoop services, setting the IP address to be the same as a legitimate node on the Hadoop cluster and sending the namenode a heartbeat message with an empty namespace ID.  This will cause the namenode to think that the spoof datanode is a node that had previously crashed and lost its data.  The namenode will then connect to the spoof datanode using its SSH credentials and start replicating data on the spoof datanode, incorporating the spoof datanode into the cluster.  Once incorporated, the spoof node can start issuing MapReduce jobs to retrieve cluster data."
DataNode should be marked as final to prevent subclassing,HDFS-190,"Reviewing the DataNode core, it starts a thread in its constructor calling back in to the Run() method. This is generally perceived as very dangerous, as if DataNode were ever subclassed, the subclass would start to be invoked in the run() method before its own constructor had finished working.

1. Consider splitting the constructor from the start() operation.
2. If this cannot be changed, mark DataNode as final so nobody can subclass it.  Though if the latter were done, it would be convenient to have a method to let external management components poll for the health of the node, and to pick up reasons for the node shutting down."
QUEUE_WITH_CORRUPT_BLOCKS is no longer needed,HDFS-9344,"After the change of HDFS-9205, the {{QUEUE_WITH_CORRUPT_BLOCKS}} queue in {{UnderReplicatedBlocks}} is no longer needed."
Are there any official performance tests or reports using WebHDFS,HDFS-9212,I'd like to know if there are any performance tests or reports when reading and writing files using WebHDFS rest api. Or any design-time numbers?
DataXceiver per accept seems to be a bottleneck in HBase/YCSB test,HDFS-2243,"I am running the YCSB benchmark against HBase, sometimes against a single node, sometimes against a cluster of 6 systems. As the load increases into thousands of TPS, especially on the single node, I can see that the datanode runs very high system time and seems to be bottlenecked by how fast it can create the threads to handle the new connections in DataXceiverServer.run. By ""perf top"" I can see the process spends about 12% of all its time in pthread_create, and in hprof profiles I can see there are tens of thousands of threads created in just a few minutes of test execution.

Does anyone else observe this bottleneck? Is there a major challenge to using a thread pool of DataXceivers in this situation?
"
need log out an extra info in DFSOutputstream,HDFS-4770,need log out an extra info in DFSOutputstream
UserGroupInformation.loginUserFromKeytab will hang forever if keytab file length  is less than 6 byte.,HDFS-6674,"The jstack is as follows:
   java.lang.Thread.State: RUNNABLE
	at java.io.FileInputStream.available(Native Method)
	at java.io.BufferedInputStream.available(BufferedInputStream.java:399)
	- locked <0x0000000745585330> (a sun.security.krb5.internal.ktab.KeyTabInputStream)
	at sun.security.krb5.internal.ktab.KeyTab.load(KeyTab.java:257)
	at sun.security.krb5.internal.ktab.KeyTab.<init>(KeyTab.java:97)
	at sun.security.krb5.internal.ktab.KeyTab.getInstance0(KeyTab.java:124)
	- locked <0x0000000745586560> (a java.lang.Class for sun.security.krb5.internal.ktab.KeyTab)
	at sun.security.krb5.internal.ktab.KeyTab.getInstance(KeyTab.java:157)
	at javax.security.auth.kerberos.KeyTab.takeSnapshot(KeyTab.java:119)
	at javax.security.auth.kerberos.KeyTab.getEncryptionKeys(KeyTab.java:192)
	at javax.security.auth.kerberos.JavaxSecurityAuthKerberosAccessImpl.keyTabGetEncryptionKeys(JavaxSecurityAuthKerberosAccessImpl.java:36)
	at sun.security.jgss.krb5.Krb5Util.keysFromJavaxKeyTab(Krb5Util.java:381)
	at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:701)
	at com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:584)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at javax.security.auth.login.LoginContext.invoke(LoginContext.java:784)
	at javax.security.auth.login.LoginContext.access$000(LoginContext.java:203)
	at javax.security.auth.login.LoginContext$5.run(LoginContext.java:721)
	at javax.security.auth.login.LoginContext$5.run(LoginContext.java:719)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.login.LoginContext.invokeCreatorPriv(LoginContext.java:718)
	at javax.security.auth.login.LoginContext.login(LoginContext.java:590)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:679)"
"HDFS mover stuck in loop trying to move corrupt block with no other valid replicas, doesn't move rest of other data blocks",HDFS-8341,"HDFS mover gets stuck looping on a block that fails to move and doesn't migrate the rest of the blocks.

This is preventing recovery of data from a decomissioning external storage tier used for archive (we've had problems with that proprietary ""hyperscale"" storage product which is why a couple blocks here and there have checksum problems or premature eof as shown below), but this should not prevent moving all the other blocks to recover our data:
{code}hdfs mover -p /apps/hive/warehouse/<custom_scrubbed>
15/05/07 14:52:50 INFO mover.Mover: namenodes = {hdfs://nameservice1=[/apps/hive/warehouse/<custom_scrubbed>]}
15/05/07 14:52:51 INFO balancer.KeyManager: Block token params received from NN: update interval=10hrs, 0sec, token lifetime=10hrs, 0sec
15/05/07 14:52:51 INFO block.BlockTokenSecretManager: Setting block keys
15/05/07 14:52:51 INFO balancer.KeyManager: Update block keys every 2hrs, 30mins, 0sec
15/05/07 14:52:52 INFO block.BlockTokenSecretManager: Setting block keys
15/05/07 14:52:52 INFO net.NetworkTopology: Adding a new node: /default-rack/<ip>:1019
15/05/07 14:52:52 INFO net.NetworkTopology: Adding a new node: /default-rack/<ip>:1019
15/05/07 14:52:52 INFO net.NetworkTopology: Adding a new node: /default-rack/<ip>:1019
15/05/07 14:52:52 INFO net.NetworkTopology: Adding a new node: /default-rack/<ip>:1019
15/05/07 14:52:52 INFO net.NetworkTopology: Adding a new node: /default-rack/<ip>:1019
15/05/07 14:52:52 INFO net.NetworkTopology: Adding a new node: /default-rack/<ip>:1019
15/05/07 14:52:52 WARN balancer.Dispatcher: Failed to move blk_1075156654_1438349 with size=134217728 from <ip>:1019:ARCHIVE to <ip>:1019:DISK through <ip>:1019: block move is failed: opReplaceBlock BP-120244285-<ip>-1417023863606:blk_1075156654_1438349 received exception java.io.EOFException: Premature EOF: no length prefix available
<NOW IT STARTS LOOPING ON SAME BLOCK>
15/05/07 14:53:31 INFO net.NetworkTopology: Adding a new node: /default-rack/<ip>:1019
15/05/07 14:53:31 INFO net.NetworkTopology: Adding a new node: /default-rack/<ip>:1019
15/05/07 14:53:31 INFO net.NetworkTopology: Adding a new node: /default-rack/<ip>:1019
15/05/07 14:53:31 INFO net.NetworkTopology: Adding a new node: /default-rack/<ip>:1019
15/05/07 14:53:31 INFO net.NetworkTopology: Adding a new node: /default-rack/<ip>:1019
15/05/07 14:53:31 INFO net.NetworkTopology: Adding a new node: /default-rack/<ip>:1019
15/05/07 14:53:31 WARN balancer.Dispatcher: Failed to move blk_1075156654_1438349 with size=134217728 from <ip>:1019:ARCHIVE to <ip>:1019:DISK through <ip>:1019: block move is failed: opReplaceBlock BP-120244285-<ip>-1417023863606:blk_1075156654_1438349 received exception java.io.EOFException: Premature EOF: no length prefix available
...<repeat indefinitely>...
{code}"
The dncp_block_verification log can be compressed,HDFS-4224,"On some systems, I noticed that when the scanner runs, the dncp_block_verification.log.curr file under the block pool gets quite large (several GBs). Although this is rolled away, we could also configure compression upon it (a codec that may work without natives, would be a good default) and save on I/O and space."
Race condition on yieldCount in FSDirectory.java,HDFS-9054,"getContentSummaryInt only held read lock, and it called fsd.addYieldCount which may cause race condition:
{code}
  private static ContentSummary getContentSummaryInt(FSDirectory fsd,
      INodesInPath iip) throws IOException {
    fsd.readLock();
    try {
      INode targetNode = iip.getLastINode();
      if (targetNode == null) {
        throw new FileNotFoundException(""File does not exist: "" + iip.getPath());
      }
      else {
        // Make it relinquish locks everytime contentCountLimit entries are
        // processed. 0 means disabled. I.e. blocking for the entire duration.
        ContentSummaryComputationContext cscc =
            new ContentSummaryComputationContext(fsd, fsd.getFSNamesystem(),
                fsd.getContentCountLimit(), fsd.getContentSleepMicroSec());
        ContentSummary cs = targetNode.computeAndConvertContentSummary(cscc);
        fsd.addYieldCount(cscc.getYieldCount());
        return cs;
      }
    } finally {
      fsd.readUnlock();
    }
  }
{code}"
Small improvement for DatanodeManager#sortLocatedBlocks,HDFS-9032,"-This is a minor improvement: In sortLocatedBlocks, (in most cases) if locations of located block don't contain decommissioned/stale datanode, no need to call Arrays.sort. Also we can make the comparator as the class variable.-"
File is not closed in OfflineImageViewerPB#run(),HDFS-6290,"{code}
      } else if (processor.equals(""XML"")) {
        new PBImageXmlWriter(conf, out).visit(new RandomAccessFile(inputFile,
            ""r""));
{code}
The RandomAccessFile instance should be closed before the method returns."
Remove unnecessary log from method FSNamesystem.getCorruptFiles,HDFS-8861,"The log in FSNamesystem.getCorruptFiles will print out too many messages mixed with other log entries, which makes whole log quite verbose, hard to understood and analyzed, especially in those cases where SuperuserPrivilege check and Operation check are not satisfied in frequent calls of listCorruptFileBlocks."
TestDatanodeBlockScanner#testBlockCorruptionRecoveryPolicy2 times out   ,HDFS-3660,Saw this on a recent jenkins run.
rename favoriteAndExcludedNodes to excludedNodes for clarity,HDFS-7973,"two reasons:
1. name {{favoriteAndExcludedNodes}} is easily confusing.
2. {{DatanodeStorageInfo[] chooseTarget(..)}} should like {{private Node chooseTarget(..)}} use name {{oldExcludedNodes}} to backup {{excludedNodes}}. We should keep the consistency."
favoredNodes should accept ip addr,HDFS-8700,"NameNode accepts two forms of FavoredNodes, {{ip:port}} and {{host:port}}

DFSClient#create(..) only uses {{host:port}}, if favoredNodes is created by 
{code}
new InetSocketAddress(ip, port)
{code}
DFSClient will attempt a reverse lookup locally to get {{host:port}}, instead of sending {{ip:port}} directly to NameNode.

It's not a problem in production. But it's a problem in MiniDFSCluster.
MiniDFSCluster use fake hostname ""host1.foo.com"" to start DataNodes.
DFSClient doesn't use StaticMapping. So if DFSClient do reverse lookup, ""127.0.0.1:50470"" becomes ""localhost:50470""."
Links resolving either from active/standby should be same (example clicking on datanodes from Standby),HDFS-5319,"click live nodes from standby namenode will throw exception ""Operation category READ is not supported in state standby"""
Some hdfs admin operations from client should have audit logs ,HDFS-8504,"Below  ""hdfs  dfsadmin xxx"" commands should have audit logs printed, because of those operations are helpful for administrator to check what  happened to hdfs service.
*hdfs dfsadmin commands*
{noformat}
hdfs dfsadmin -safemode enter
hdfs dfsadmin -safemode leave
hdfs dfsadmin -rollEdits
hdfs dfsadmin -refreshNodes
hdfs dfsadmin -refreshServiceAcl
hdfs dfsadmin -refreshUserToGroupsMappings
hdfs dfsadmin -refreshSuperUserGroupsConfiguration
hdfs dfsadmin -refreshCallQueue
hdfs dfsadmin -shutdownDatanode
{noformat}"
BlockPoolSliceScanner#getNewBlockScanTime does not handle numbers > 31 bits properly,HDFS-3488,"This code does not handle the case where period > 2**31 properly:

{code}
    long period = Math.min(scanPeriod, 
                           Math.max(blockMap.size(),1) * 600 * 1000L);
    int periodInt = Math.abs((int)period);
    return System.currentTimeMillis() - scanPeriod + 
        DFSUtil.getRandom().nextInt(periodInt);
{code}

So, for example, if period = 0x100000000, we'll map that to 0, and so forth."
Fix some issue in DFSInputstream,HDFS-4273,"Following issues in DFSInputStream are addressed in this jira:
1. read may not retry enough in some cases cause early failure
Assume the following call logic
{noformat} 
readWithStrategy()
  -> blockSeekTo()
  -> readBuffer()
     -> reader.doRead()
     -> seekToNewSource() add currentNode to deadnode, wish to get a different datanode
        -> blockSeekTo()
           -> chooseDataNode()
              -> block missing, clear deadNodes and pick the currentNode again
        seekToNewSource() return false
     readBuffer() re-throw the exception quit loop
readWithStrategy() got the exception,  and may fail the read call before tried MaxBlockAcquireFailures.
{noformat} 

2. In multi-threaded scenario(like hbase), DFSInputStream.failures has race condition, it is cleared to 0 when it is still used by other thread. So it is possible that  some read thread may never quit. Change failures to local variable solve this issue.

3. If local datanode is added to deadNodes, it will not be removed from deadNodes if DN is back alive. We need a way to remove local datanode from deadNodes when the local datanode is become live."
Add additional fields to the JMX output on NameNode,HDFS-2203,"When accessing the JMX data via http (http://namenode:50070/jmx) there are a couple of useful fields missing from this bean:

""name"" : ""Hadoop:service=NameNode,name=NameNodeInfo"",
""modelerType"" : ""org.apache.hadoop.hdfs.server.namenode.FSNamesystem"",

Please add the number of blocks and the configured capacity. "
Add the current time to all the pages in the user interface,HDFS-274,"Adding current time to all of the pages, so that current time of the machine serving the page is displayed in the UI

As discussed in the hadoop-dev mailing list by Arkady Borkovsky :

""it would be so nice to add the CURRENT TIME to all the pages.
For naive users like myself, understanding universal time is very difficult.  So knowing what the cluster thinks about current time makes it so much easier to understand when a job has actually started or ended.... ""

"
Refactor the checking the minimum replication logic in BlockManager,HDFS-7905,"This is some refactoring separated from the HDFS-7285 branch. In the current BlockManager, the ""checking the minimum replication"" code is repeated in several places. It will be better to wrap it into a single function. This also make the further extension (for Erasure Coded blocks) easier and cleaner."
Remove obsolete -ns options in in DFSHAAdmin.java,HDFS-7808,"After HDFS-7324 fix following piece of code become unused. It should be removed.
{code}
    int i = 0;
    String cmd = argv[i++];

    if (""-ns"".equals(cmd)) {
      if (i == argv.length) {
        errOut.println(""Missing nameservice ID"");
        printUsage(errOut);
        return -1;
      }
      nameserviceId = argv[i++];
      if (i >= argv.length) {
        errOut.println(""Missing command"");
        printUsage(errOut);
        return -1;
      }
      argv = Arrays.copyOfRange(argv, i, argv.length);
    }
{code}"
FSImage should specify which dirs are missing when refusing to come up,HDFS-979,"When {{FSImage}} can't come up as either it has no data or edit dirs, it tells me this
{code}
java.io.IOException: All specified directories are not accessible or do not exist.
{code}
What it doesn't do is say which of the two attributes are missing. This would be beneficial to anyone trying to track down the problem. Also, I don't think the message is correct. It's bailing out because dataDirs.size() == 0 || editsDirs.size() == 0 , because a list is empty -not because the dirs aren't there, as there hasn't been any validation yet.

More useful would be
# Explicit mention of which attributes are null
# Declare that this is because they are not in the config"
DFSAdmin fetchImage command should initialize security credentials,HDFS-3349,The `hdfs dfsadmin -fetchImage' command should fetch the fsimage using the appropriate credentials if security is enabled.
TestBalancerWithSaslDataTransfer fails in trunk,HDFS-6946,"From build #1849 :
{code}
REGRESSION:  org.apache.hadoop.hdfs.server.balancer.TestBalancerWithSaslDataTransfer.testBalancer0Integrity

Error Message:
Cluster failed to reached expected values of totalSpace (current: 750, expected: 750), or usedSpace (current: 140, expected: 150), in more than 40000 msec.

Stack Trace:
java.util.concurrent.TimeoutException: Cluster failed to reached expected values of totalSpace (current: 750, expected: 750), or usedSpace (current: 140, expected: 150), in more than 40000 msec.
        at org.apache.hadoop.hdfs.server.balancer.TestBalancer.waitForHeartBeat(TestBalancer.java:253)
        at org.apache.hadoop.hdfs.server.balancer.TestBalancer.runBalancer(TestBalancer.java:578)
        at org.apache.hadoop.hdfs.server.balancer.TestBalancer.doTest(TestBalancer.java:551)
        at org.apache.hadoop.hdfs.server.balancer.TestBalancer.doTest(TestBalancer.java:437)
        at org.apache.hadoop.hdfs.server.balancer.TestBalancer.oneNodeTest(TestBalancer.java:645)
        at org.apache.hadoop.hdfs.server.balancer.TestBalancer.testBalancer0Internal(TestBalancer.java:759)
        at org.apache.hadoop.hdfs.server.balancer.TestBalancerWithSaslDataTransfer.testBalancer0Integrity(TestBalancerWithSaslDataTransfer.java:34)
{code}"
Build behind a proxy failing to fetch commons-daemon,HDFS-2381,"The bit of Ant code in the HDFS project to D/L the commons-daemon binary is failing as it cannot reach archive.apache.org, even though Ant and Maven have their proxies setup. I will allege that M2 isn't passing proxy information down properly. "
"FSVolumeList#initializeReplicaMaps(..) not doing anything, it can be removed",HDFS-5903,"{code}  void initializeReplicaMaps(ReplicaMap globalReplicaMap) throws IOException {
    for (FsVolumeImpl v : volumes) {
      v.getVolumeMap(globalReplicaMap);
    }
  }{code}

This method has been called at the time of initialization even before the blockpools are added. So its useless to call this method.

Anyway replica map will be updated for each of the blockpool during {{addBlockPool(..)}} by calling {{FSVolumesList#getAllVolumesMap(..)}}"
open and getFileInfo APIs treat paths inconsistently wrt protocol,HDFS-7895,"When open() is called with regular HDFS path, hdfs://blah/blah/blah, it appears to work.
However, getFileInfo doesn't
{noformat}
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.fs.InvalidPathException): Invalid path name Invalid file name: hdfs://localhost:9000/apps/hive/warehouse/tpch_2.db/lineitem_orc/000001_0
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4128)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:838)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:821)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)

        at org.apache.hadoop.ipc.Client.call(Client.java:1468)
        at org.apache.hadoop.ipc.Client.call(Client.java:1399)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
        at com.sun.proxy.$Proxy16.getFileInfo(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:752)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
        at com.sun.proxy.$Proxy17.getFileInfo(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1988)
{noformat}

1) this seems inconsistent.
2) not clear why the validation should reject what looks like a good HDFS path. At least, client code should clean this stuff up on the way.

[~prasanth_j] has the details, I just filed a bug so I could mention how buggy HDFS is to [~jingzhao] :)
"
TestHttpFSServer fails occasionally in trunk,HDFS-6177,"From https://builds.apache.org/job/Hadoop-hdfs-trunk/1716/consoleFull :
{code}
Running org.apache.hadoop.fs.http.server.TestHttpFSServer
Tests run: 7, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 8.424 sec <<< FAILURE! - in org.apache.hadoop.fs.http.server.TestHttpFSServer
testDelegationTokenOperations(org.apache.hadoop.fs.http.server.TestHttpFSServer)  Time elapsed: 0.559 sec  <<< FAILURE!
java.lang.AssertionError: expected:<401> but was:<403>
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.failNotEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:128)
	at org.junit.Assert.assertEquals(Assert.java:472)
	at org.junit.Assert.assertEquals(Assert.java:456)
	at org.apache.hadoop.fs.http.server.TestHttpFSServer.testDelegationTokenOperations(TestHttpFSServer.java:352)
{code}"
TestCrcCorruption#testCorruptionDuringWrt sometimes fails in trunk,HDFS-6501,"From https://builds.apache.org/job/Hadoop-Hdfs-trunk/1767/ :
{code}
REGRESSION:  org.apache.hadoop.hdfs.TestCrcCorruption.testCorruptionDuringWrt

Error Message:
test timed out after 50000 milliseconds

Stack Trace:
java.lang.Exception: test timed out after 50000 milliseconds
        at java.lang.Object.wait(Native Method)
        at org.apache.hadoop.hdfs.DFSOutputStream.waitForAckedSeqno(DFSOutputStream.java:2024)
        at org.apache.hadoop.hdfs.DFSOutputStream.flushInternal(DFSOutputStream.java:2008)
        at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2107)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:98)
        at org.apache.hadoop.hdfs.TestCrcCorruption.testCorruptionDuringWrt(TestCrcCorruption.java:133)
{code}"
TestNamenodeCapacityReport fails intermittently,HDFS-6726,"From https://builds.apache.org/job/Hadoop-Hdfs-trunk/1812/testReport/junit/org.apache.hadoop.hdfs.server.namenode/TestNamenodeCapacityReport/testXceiverCount/ :
{code}
java.io.IOException: Unable to close file because the last block does not have enough number of replicas.
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2141)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2109)
	at org.apache.hadoop.hdfs.server.namenode.TestNamenodeCapacityReport.testXceiverCount(TestNamenodeCapacityReport.java:281)
{code}
There were multiple occurrences of 'Broken pipe', 'Connection reset by peer' and 'Premature EOF from inputStream' exceptions in test output"
TestDataNodeMetrics fails in trunk,HDFS-7220,"From https://builds.apache.org/job/Hadoop-Hdfs-trunk/1896/testReport/junit/org.apache.hadoop.hdfs.server.datanode/TestDataNodeMetrics/testSendDataPacketMetrics/ :
{code}
java.lang.NoClassDefFoundError: org/apache/hadoop/util/IntrusiveCollection$IntrusiveIterator
	at org.apache.hadoop.util.IntrusiveCollection.iterator(IntrusiveCollection.java:213)
	at org.apache.hadoop.util.IntrusiveCollection.clear(IntrusiveCollection.java:368)
	at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.clearPendingCachingCommands(DatanodeManager.java:1590)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.stopActiveServices(FSNamesystem.java:1262)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.close(FSNamesystem.java:1590)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.stopCommonServices(NameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.stop(NameNode.java:823)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:1717)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:1696)
	at org.apache.hadoop.hdfs.server.datanode.TestDataNodeMetrics.testSendDataPacketMetrics(TestDataNodeMetrics.java:94)
{code}"
TestCacheDirectives#testExceedsCapacity sometimes fails in trunk,HDFS-7571,"From https://builds.apache.org/job/Hadoop-Hdfs-trunk/1985/ :
{code}
REGRESSION:  org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.testExceedsCapacity

Error Message:
Pending cached list of 127.0.0.1:47332 is not empty, [{blockId=1073741841, replication=1, mark=true}]

Stack Trace:
java.lang.AssertionError: Pending cached list of 127.0.0.1:47332 is not empty, [{blockId=1073741841, replication=1, mark=true}]
        at org.junit.Assert.fail(Assert.java:88)
        at org.junit.Assert.assertTrue(Assert.java:41)
        at org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.checkPendingCachedEmpty(TestCacheDirectives.java:1420)
        at org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.testExceedsCapacity(TestCacheDirectives.java:1443)
{code}"
TestDFSUpgradeWithHA sometimes fails in trunk,HDFS-7289,"From trunk build #1912:
{code}
REGRESSION:  org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA.testFinalizeFromSecondNameNodeWithJournalNodes

Error Message:
java.lang.RuntimeException: java.net.SocketTimeoutException: Read timed out

Stack Trace:
java.io.IOException: java.lang.RuntimeException: java.net.SocketTimeoutException: Read timed out
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.read(SocketInputStream.java:129)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:258)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:317)
        at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:698)
        at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:641)
        at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1218)
        at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:379)
        at org.apache.hadoop.hdfs.server.namenode.TransferFsImage.doGetUrl(TransferFsImage.java:410)
        at org.apache.hadoop.hdfs.server.namenode.TransferFsImage.getFileClient(TransferFsImage.java:395)
        at org.apache.hadoop.hdfs.server.namenode.TransferFsImage.downloadImageToStorage(TransferFsImage.java:114)
        at org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby.doRun(BootstrapStandby.java:213)
        at org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby.access$000(BootstrapStandby.java:69)
        at org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby$1.run(BootstrapStandby.java:107)
        at org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby$1.run(BootstrapStandby.java:103)
        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:414)
        at org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby.run(BootstrapStandby.java:103)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)
        at org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby.run(BootstrapStandby.java:315)
        at org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA.testFinalizeFromSecondNameNodeWithJournalNodes(TestDFSUpgradeWithHA.java:493)
{code}"
TestLeaseRecovery2 sometimes fails in trunk,HDFS-7311,"From https://builds.apache.org/job/Hadoop-Hdfs-trunk/1917/ :
{code}
REGRESSION:  org.apache.hadoop.hdfs.TestLeaseRecovery2.testHardLeaseRecovery

Error Message:
Call From asf909.gq1.ygridcore.net/67.195.81.153 to localhost:55061 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused

Stack Trace:
java.net.ConnectException: Call From asf909.gq1.ygridcore.net/67.195.81.153 to localhost:55061 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:599)
        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
        at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)
        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)
        at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
        at org.apache.hadoop.ipc.Client.call(Client.java:1438)
        at org.apache.hadoop.ipc.Client.call(Client.java:1399)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
        at com.sun.proxy.$Proxy19.create(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:295)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:101)
        at com.sun.proxy.$Proxy20.create(Unknown Source)
        at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1694)
        at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1654)
        at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1579)
        at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:397)
        at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:393)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:393)
        at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:337)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)
        at org.apache.hadoop.hdfs.TestLeaseRecovery2.testHardLeaseRecovery(TestLeaseRecovery2.java:276)


FAILED:  org.apache.hadoop.hdfs.TestLeaseRecovery2.org.apache.hadoop.hdfs.TestLeaseRecovery2

Error Message:
Test resulted in an unexpected exit

Stack Trace:
java.lang.AssertionError: Test resulted in an unexpected exit
        at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:1709)
        at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:1696)
        at org.apache.hadoop.hdfs.TestLeaseRecovery2.tearDown(TestLeaseRecovery2.java:105)
{code}"
TestEncryptionZonesWithKMS fails against Java 8,HDFS-7422,"From https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/12/ :
{code}
REGRESSION:  org.apache.hadoop.hdfs.TestEncryptionZonesWithKMS.testReadWriteUsingWebHdfs

Error Message:
Stream closed.

Stack Trace:
java.io.IOException: Stream closed.
        at sun.reflect.GeneratedConstructorAccessor58.newInstance(Unknown Source)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
        at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toIOException(WebHdfsFileSystem.java:385)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$600(WebHdfsFileSystem.java:91)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.shouldRetry(WebHdfsFileSystem.java:656)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.runWithRetry(WebHdfsFileSystem.java:622)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.access$100(WebHdfsFileSystem.java:458)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner$1.run(WebHdfsFileSystem.java:487)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1683)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.run(WebHdfsFileSystem.java:483)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$UnresolvedUrlOpener.connect(WebHdfsFileSystem.java:1204)
        at org.apache.hadoop.hdfs.web.ByteRangeInputStream.openInputStream(ByteRangeInputStream.java:120)
        at org.apache.hadoop.hdfs.web.ByteRangeInputStream.getInputStream(ByteRangeInputStream.java:104)
        at org.apache.hadoop.hdfs.web.ByteRangeInputStream.<init>(ByteRangeInputStream.java:89)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$OffsetUrlInputStream.<init>(WebHdfsFileSystem.java:1261)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.open(WebHdfsFileSystem.java:1175)
        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)
        at org.apache.hadoop.hdfs.DFSTestUtil.verifyFilesEqual(DFSTestUtil.java:1399)
        at org.apache.hadoop.hdfs.TestEncryptionZones.testReadWriteUsingWebHdfs(TestEncryptionZones.java:634)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
Caused by: org.apache.hadoop.ipc.RemoteException: Stream closed.
        at org.apache.hadoop.hdfs.web.JsonUtil.toRemoteException(JsonUtil.java:165)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:353)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$200(WebHdfsFileSystem.java:91)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.runWithRetry(WebHdfsFileSystem.java:608)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.access$100(WebHdfsFileSystem.java:458)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner$1.run(WebHdfsFileSystem.java:487)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1683)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.run(WebHdfsFileSystem.java:483)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$UnresolvedUrlOpener.connect(WebHdfsFileSystem.java:1204)
        at org.apache.hadoop.hdfs.web.ByteRangeInputStream.openInputStream(ByteRangeInputStream.java:120)
        at org.apache.hadoop.hdfs.web.ByteRangeInputStream.getInputStream(ByteRangeInputStream.java:104)
        at org.apache.hadoop.hdfs.web.ByteRangeInputStream.<init>(ByteRangeInputStream.java:89)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$OffsetUrlInputStream.<init>(WebHdfsFileSystem.java:1261)
        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.open(WebHdfsFileSystem.java:1175)
        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)
        at org.apache.hadoop.hdfs.DFSTestUtil.verifyFilesEqual(DFSTestUtil.java:1399)
        at org.apache.hadoop.hdfs.TestEncryptionZones.testReadWriteUsingWebHdfs(TestEncryptionZones.java:634)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
{code}"
"If the NameNode has already been formatted, but a QuroumJournal has not, auto-format it on startup",HDFS-4173,"If we have multiple edit log directories, and some of them are formatted, but others are not, we format the unformatted ones.  However, when we implemented QuorumJournalManager, we did not extend this behavior to it.  It makes sense to do this.

One use case is if you want to add a QuorumJournalManager URI ({{journal://}}) to an existing {{NameNode}}, without reformatting everything.  There is currently no easy way to do this, since {{namenode \-format}} will nuke everything, and there's no other way to format the {{JournalNodes}}."
Data Not replicate in ssd drive,HDFS-7593,
Unable to change JAVA_HOME directory in hadoop-setup-conf.sh script.,HDFS-4063,"The JAVA_HOME directory remains unchanged no matter what you enter when you run hadoop-setup-conf.sh to generate hadoop configurations. Please see below example:

*********************************************************
[root@hadoop-slave ~]# /sbin/hadoop-setup-conf.sh
Setup Hadoop Configuration

Where would you like to put config directory? (/etc/hadoop)
Where would you like to put log directory? (/var/log/hadoop)
Where would you like to put pid directory? (/var/run/hadoop)
What is the host of the namenode? (hadoop-slave)
Where would you like to put namenode data directory? (/var/lib/hadoop/hdfs/namenode)
Where would you like to put datanode data directory? (/var/lib/hadoop/hdfs/datanode)
What is the host of the jobtracker? (hadoop-slave)
Where would you like to put jobtracker/tasktracker data directory? (/var/lib/hadoop/mapred)
Where is JAVA_HOME directory? (/usr/java/default) *+/usr/lib/jvm/jre+*
Would you like to create directories/copy conf files to localhost? (Y/n)

Review your choices:

Config directory            : /etc/hadoop
Log directory               : /var/log/hadoop
PID directory               : /var/run/hadoop
Namenode host               : hadoop-slave
Namenode directory          : /var/lib/hadoop/hdfs/namenode
Datanode directory          : /var/lib/hadoop/hdfs/datanode
Jobtracker host             : hadoop-slave
Mapreduce directory         : /var/lib/hadoop/mapred
Task scheduler              : org.apache.hadoop.mapred.JobQueueTaskScheduler
JAVA_HOME directory         : *+/usr/java/default+*
Create dirs/copy conf files : y

Proceed with generate configuration? (y/N) n
User aborted setup, exiting...
*********************************************************

Resolution:
Amend line 509 in file /sbin/hadoop-setup-conf.sh

from:

JAVA_HOME=${USER_USER_JAVA_HOME:-$JAVA_HOME}

to:

JAVA_HOME=${USER_JAVA_HOME:-$JAVA_HOME}

will resolve this issue."
Support reading and writing sequencefile in libhdfs ,HDFS-924,"Some use case may need read and write sequencefile through libhdfs. 

We should provide the reading and writing api for sequencefile in libhdfs."
Include path in the XML output of oiv,HDFS-7428,"When generating the XML output using {noformat}hadoop oiv -p XML{noformat} the path of a file is not printed, just the file name. While the complete path can be derived by parsing INodeDirectorySection and INodeSection and their children, it would be convenient to have the ""absolute path"" present directly like {noformat}INodeSection->inode->path{noformat}"
Setting up of cluster using ssh - Scripts that help in minimising the cluster setup efforts,HDFS-1895,"Sometimes when we have a large number of clusters we may have to specify the password of the different machines that we are using as slaves (datanodes).
 
If the cluster is very huge we may have to repeat this everytime.
So we would  to suggest a way to avoid this
 
1. Generate a SSH key from the name node machine
2. Read the entries from the conf/slaves file, for every entry add the key generated in step 1 to a file of slave machine.
3. Repeat the same for master file also.
 
when you execute step 1 it will prompt for the password.  This is only for the first time.
 
After that whenever you need to start the cluster then password need not be specified.
 
This scenario is valid when we are sure of the cluster that we will be maintaining and we are aware of the credentials of the machine.
 
This will help the cluster administrator.
 

"
Provide a 64bit Native library,HDFS-7038,"As Hadoop will be running on nodes with more then 4GB memory, I do not understand why the build of libraries is done on 32 bit systems.

It will be great to have 64bit provided natively.

Regards
Gurmukh
An Hadoop Lover."
[HDFS-RAID] ExtFSInputStream#read wrapper does not preserve semantics,HDFS-2482,"The ExtFSInputStream#read wrapper has signed byte issues. No need to use a local byte buffer either, IMO."
[HDFS-RAID] DistributedRaidFileSystem cannot handle token delegation,HDFS-2483,"DistributedRaidFileSystem cannot handle token delegation, so it is impossible to specify it as fs.hdfs.impl in a secure configuration."
Atomicity of multi file operations,HDFS-6821,"Looking how HDFS updates the log files in case of chmod 鈥搑 or chown 鈥搑 operations. In these operations, HDFS name node seems to update each file separately; consequently the strace of the operation looks as follows.

append(edits)
fsync(edits)

append(edits)
fsync(edits)
-----------------------
append(edits)
fsync(edits)

append(edits)
fsync(edits)

If a crash happens in the middle of this operation (e.g. at the dashed line in the trace), the system will end up with part of the files updates with the new owner or permissions and part still with the old owner.

Isn鈥檛 it better to log the whole operations (chown -r) as one entry in the edit file?
"
dfs.journalnode.edits.dir should accept URI,HDFS-6091,"Using a URI in dfs.journalnode.edits.dir (such as file:///foo)  throws a ""Journal dir 'file:/foo' should be an absolute path'. "
Misc cleanup/logging improvements for branch-20-append,HDFS-1248,"Last remaining bits of my append branch that didn't fit elsewhere in JIRA (just misc cleanup)
 - Slight cleanup to recoverFile() function in TFA4
 - Improve error messages on OP_READ_BLOCK
 - Some comment cleanup in FSNamesystem
 - Remove toInodeUnderConstruction (not used)
 - Add some checks for null blocks to avoid NPE
 - Only log ""inconsistent size"" warnings at WARN level for non-under-construction blocks.
 - Redundant addStoredBlock calls are also not worthy of WARN level
 - Add some extra information to a warning in ReplicationTargetChooser

This may need HDFS-1057 to be committed first to apply."
"Fedora jdk install creates circular symlinks, causes test-c++-libhdfs not to build",HDFS-901,"installed hadoop; needed javac; installed jdk
export JAVA_HOME=/usr/java/jdk1.6.0_17
ant clean
ant
ant -Dcompile.c++=true -Dlibhdfs=true test-c++-libhdfs
-------------------------------------------------------------------------------
[exec] libtool: link: gcc -g -O2 -DOS_LINUX -DDSO_DLFCN -DCPU=\""i386\"" -m32 -I/usr/java/jdk1.6.0_17/include -I/usr/java/jdk1.6.0_17/include/linux -Wall -Wstrict-prototypes -m32 /usr/java/jdk1.6.0_17/jre1.6.0_17/lib/i386/server -Wl,-x -o hdfs_test hdfs_test.o  -L/usr/java/jdk1.6.0_17/jre/lib/i386/server /home/cook/Desktop/hadoop-0.20.1/build/c++/Linux-i386-32/lib/libhdfs.so -ljvm -ldl -lpthread -Wl,-rpath -Wl,/home/cook/Desktop/hadoop-0.20.1/build/c++/Linux-i386-32/lib -Wl,-rpath -Wl,/home/cook/Desktop/hadoop-0.20.1/build/c++/Linux-i386-32/lib
     [exec] /usr/java/jdk1.6.0_17/jre1.6.0_17/lib/i386/server: file not recognized: Is a directory
     [exec] collect2: ld returned 1 exit status
     [exec] make: *** [hdfs_test] Error 1

BUILD FAILED
note: insertion of /usr/java/jdk1.6.0_17/jre1.6.0_17/lib/i386/server is spurious
-------------------------------------------------------------------------------
tracked bug down as follows:
for some reason, install of jdk creates a symlink from within jdk directory to existing jre impl jre1.6.0_17 and a symlink from within jre directory to jdk.
I suspected the recursion likely messed up a ""find"" somewhere; deleted symlink in jdk then
ant clean
ant
ant -Dcompile.c++=true -Dlibhdfs=true test-c++-libhdfs
BUILD and TEST ran successfully then
just to be sure I put the symlink back; reran above; same failure"
supergroup permission ,HDFS-6761,"<hdfs default directory information>
Permission   Owner     Group           Size     Name
drwxr-wr-x      hdfs     supergroup      0        /user

I created 'testuser' account and added 'testuser' to the supergroup.
When i use 'testuser' account, I try to 'hadoop fs mkdir /user/data' command.

I thought when i run 'hadoop fs mkdir /user/data' command, this result would be shown 'permission denied'

but I can make 'data' directory.

Is this correct?


"
Work out the memory consumption of NN artifacts on a compressed pointer JVM,HDFS-559,"Following up HADOOP-1687, it would be nice to know the size of datatypes in under the java16u14 JVM, which offers compressed pointers.

"
Use custom MAX_SIZE_TO_MOVE value  Balancer,HDFS-369,"Balancer will load ""fs.balancer.max.move.size"" for a custom value of MAX_SIZE_TO_MOVE."
Metrics on Secondary namenode's activity,HDFS-271,"To monitor secondary namenode's activiely, we currently rely on 'delete' metrics when it replays the edits.
(HADOOP-1495) 

Requesting for new metrics that shows secondary namenode's activity.
Maybe the size of the fsimage/edits being read and written."
Secondary Namenode: Limit number of retries when fsimage/edits transfer fails,HDFS-280,"When hitting HADOOP-3980, secondary namenode kept on pulling gigs of fsimage/edits every 10 minutes which slowed down the namenode significantly.    When namenode is down, I'd like the secondary namenode to keep on retrying to connect.  However, when pull/push of large files keep on failing, I'd like a upper limit on the number of retries.  Either shutdown or  sleep for _fs.checkpoint.period_ seconds.
"
Uncaught Exception in DataTransfer.run,HDFS-79,"Minor, but it would be nice if this exception is caught and logged.

I see in .out file of datanode, 

{noformat}
Exception in thread ""org.apache.hadoop.dfs.DataNode$DataTransfer@9d2805"" java.nio.channels.ClosedSelectorException
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:66)
        at sun.nio.ch.SelectorImpl.selectNow(SelectorImpl.java:88)
        at sun.nio.ch.Util.releaseTemporarySelector(Util.java:135)
        at sun.nio.ch.SocketAdaptor.connect(SocketAdaptor.java:118)
        at org.apache.hadoop.dfs.DataNode$DataTransfer.run(DataNode.java:2604)
        at java.lang.Thread.run(Thread.java:619)
{noformat}"
Missing null check in FSImageSerialization#writePermissionStatus(),HDFS-6415,"{code}
    PermissionStatus.write(out, inode.getUserName(), inode.getGroupName(), p);
{code}
getUserName() / getGroupName() may return null.
null check should be added for these two calls."
TestQuorumJournalManager#testChangeWritersLogsOutOfSync2 occasionally fails,HDFS-6083,"From https://builds.apache.org/job/Hadoop-Hdfs-trunk/1695/testReport/junit/org.apache.hadoop.hdfs.qjournal.client/TestQuorumJournalManager/testChangeWritersLogsOutOfSync2/ :
{code}
Leaked thread: ""IPC Client (26533782) connection to /127.0.0.1:57898 from jenkins"" Id=590 RUNNABLE
 at java.lang.System.arraycopy(Native Method)
 at java.lang.ThreadGroup.remove(ThreadGroup.java:885)
 at java.lang.Thread.exit(Thread.java:672)
{code}
The following check should give more time for the threads to shutdown:
{code}
    // Should not leak clients between tests -- this can cause flaky tests.
    // (See HDFS-4643)
    GenericTestUtils.assertNoThreadsMatching("".*IPC Client.*"");
{code}"
Style Hadoop HDFS web ui's with Twitter's bootstrap.,HDFS-4670,A users' first experience of Apache Hadoop is often looking at the web ui.  This should give the user confidence that the project is usable and relatively current.
Create an option to specify a file path for OfflineImageViewer,HDFS-5975,"The output of OfflineImageViewer becomes quite large if an input fsimage is large. I propose '-filePath' option to make the output smaller.

The below command will output the {{ls -R}} of {{/user/root}}.
{code}
hdfs oiv -i input -o output -p Ls -filePath /user/root
{code}"
Interrupting the namenode thread triggers System.exit(),HDFS-8,"My service setup/teardown tests are managing to trigger system exits in the namenode, which seems overkill.

1. Interrupting the thread that is starting the namesystem up raises a java.nio.channels.ClosedByInterruptException.
2. This is caught in FSImage.rollFSImage, and handed off to processIOError
3. This triggers a call to Runtime.getRuntime().exit(-1); ""All storage directories are inaccessible."".

Stack trace to follow. Exiting the JVM is somewhat overkill; if someone has interrupted the thread is is (presumably) because they want to stop the namenode, which may not imply they want to kill the JVM at the same time. Certainly JUnit does not expect it. 

Some possibilities
 -ClosedByInterruptException get handled differently as some form of shutdown request
 -Calls to system exit are factored out into something that can have its behaviour changed by policy options to throw a RuntimeException instead. 
Hosting a Namenode in a security manager that blocks off System.exit() is the simplest workaround; this is fairly simple, but it means that what would be a straight exit does now get turned into an exception, so callers may be surprised by what happens."
Clean up the output of NameDistribution processor,HDFS-5867,"The output of 'hdfs oiv -i INPUT -o OUTPUT -p NameDistribution' is as follows:
{code}
Total unique file names 86
0 names are used by 0 files between 100000-13 times. Heap savings ~0 bytes.
0 names are used by 0 files between 10000-99999 times. Heap savings ~0 bytes.
0 names are used by 0 files between 1000-9999 times. Heap savings ~0 bytes.
0 names are used by 0 files between 100-999 times. Heap savings ~0 bytes.
1 names are used by 13 files between 10-99 times. Heap savings ~372 bytes.
4 names are used by 34 files between 5-9 times. Heap savings ~942 bytes.
2 names are used by 8 files 4 times. Heap savings ~192 bytes.
0 names are used by 0 files 3 times. Heap savings ~0 bytes.
7 names are used by 14 files 2 times. Heap savings ~222 bytes.

Total saved heap ~1728bytes.
{code}
'between 100000-13 times' should be 'over 99999 times' , or the line starting with '0 names' should not output."
port fuse-dfs existing autoconf to hadoop project's autoconf infrastructure,HDFS-431,"Although fuse-dfs has its own autoconf macros and such, better to use one set of macros and in some places the macros could be improved.
"
namenode fails to run on ppc,HDFS-160,"Hadoop starts, but eats 100% CPU. Data- and Secondarynamenodes can not connect. No jobs were run, just trying to start the daemon. using bin/start-dfs.sh.

Using the same simple configuration on an x86-arch - also using Fedora 9 and gcj-1.5.0.0 - works perfectly."
LIBHDFS questions and performance suggestions,HDFS-5541,"Since libhdfs is a ""client"" interface"",  and esspecially because it is a ""C"" interface , it should be assumed that the code will be used accross many different platforms, and many different compilers.

1) The code should be cross platform ( no Linux extras )
2) The code should compile on standard c89 compilers, the
>>>  {least common denominator rule applies here} !! <<  

C  code with  ""c""   extension should follow the rules of the c standard  

All variables must be declared at the begining of scope , and no (//) comments allowed 

>> I just spent a week white-washing the code back to nornal C standards so that it could compile and build accross a wide range of platforms << 

Now on-to  performance questions 

1) If threads are not used why do a thread attach ( when threads are not used all the thread attach nonesense is a waste of time and a performance killer ) 

2) The JVM  init  code should not be imbedded within the context of every function call   .  The  JVM init code should be in a stand-alone  LIBINIT function that is only invoked once.   The JVM * and the JNI * should be global variables for use when no threads are utilized.  

3) When threads are utilized the attach fucntion can use the GLOBAL  jvm * created by the LIBINIT  { WHICH IS INVOKED ONLY ONCE } and thus safely outside the scope of any LOOP that is using the functions 

4) Hash Table and Locking  Why ?????
When threads are used the hash table locking is going to hurt perfromance .  Why not use thread local storage for the hash table,that way no locking is required either with or without threads.   
 
5) FINALLY Windows  Compatibility 

Do not use posix features if they cannot easilly be replaced on other platforms   !!"
Namenode safemode is on and is misleading on webpage,HDFS-5036,"even though namenode is not in safemode , namenode webUI shows safemode is on.
when namenode safemode is get from command line it shows safe mode is off but on the it shows it is on, it is confusing to the users looking at the webUI."
"Retain old edits log, don't retain all minimum required logs",HDFS-4939,"JNStorage.java

{code}

  private static void purgeMatching(File dir, List<Pattern> patterns,
      long minTxIdToKeep) throws IOException {

    for (File f : FileUtil.listFiles(dir)) {
      if (!f.isFile()) continue;

      for (Pattern p : patterns) {
        Matcher matcher = p.matcher(f.getName());
        if (matcher.matches()) {
          // This parsing will always succeed since the group(1) is
          // /\d+/ in the regex itself.
          long txid = Long.valueOf(matcher.group(1));
          if (txid < minTxIdToKeep) {
            LOG.info(""Purging no-longer needed file "" + txid);
            if (!f.delete()) {
              LOG.warn(""Unable to delete no-longer-needed data "" +
                  f);
            }
            break;
          }
        }
      }
    }
  }
{code}

Why break the for loop here? if so, only delete one file for each retain, am I right?
"
Support non-recursive create() in FileSystem API,HDFS-1269,"HDFS-617 added a non-recursive create() api to HDFS, however the FileSystem API was not changed.  This change is necessary for SequenceFile.Writer to support non-recursive creates."
SecondaryNamenode may report incorrect info host name,HDFS-62,"I have set up {{dfs.secondary.http.address}} like this:

{code}
<property>
  <name>dfs.secondary.http.address</name>
  <value>secondary.example.com:50090</value>
</property>
{code}

In my setup {{secondary.example.com}} resolves to an IP address (say, 192.168.0.10) which is not the same as the host's name (as returned by {{InetAddress.getLocalHost().getHostAddress()}}, say 192.168.0.1).

In this situation, edit log related transfers fail. From the namenode log:

{code}
2009-04-05 13:32:39,128 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 192.168.0.10
2009-04-05 13:32:39,168 WARN org.mortbay.log: /getimage: java.io.IOException: GetImage failed. java.net.ConnectException: Connection refused
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
        at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
        at java.net.Socket.connect(Socket.java:519)
        at java.net.Socket.connect(Socket.java:469)
        at sun.net.NetworkClient.doConnect(NetworkClient.java:163)
        at sun.net.www.http.HttpClient.openServer(HttpClient.java:394)
        at sun.net.www.http.HttpClient.openServer(HttpClient.java:529)
        at sun.net.www.http.HttpClient.<init>(HttpClient.java:233)
        at sun.net.www.http.HttpClient.New(HttpClient.java:306)
        at sun.net.www.http.HttpClient.New(HttpClient.java:323)
        at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:837)
        at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:778)
        at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:703)
        at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1026)
        at org.apache.hadoop.hdfs.server.namenode.TransferFsImage.getFileClient(TransferFsImage.java:151)
        ...
{code}

From the secondary namenode log:

{code}
2009-04-05 13:42:39,238 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint: 
2009-04-05 13:42:39,238 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: java.io.FileNotFoundException: http://nn.example.com:50070/getimage?putimage=1&port=50090&machine=
192.168.0.1&token=-19:1243068779:0:1238929357000:1238929031783
        at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1288)
        at org.apache.hadoop.hdfs.server.namenode.TransferFsImage.getFileClient(TransferFsImage.java:151)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.putFSImage(SecondaryNameNode.java:294)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:333)
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:239)
        at java.lang.Thread.run(Thread.java:619)
{code}"
Can not change replication factor of file during moving or deleting.,HDFS-4711,"I don't know is it a feature or a bug. 
According to hdfs dfs -help we can use key -D to set specific options for action;
When we copying or uploading file to hdfs we can override replication factor with -D dfs.replication=N. That's works well.
But it doesn't work for moving or removing(to trash) file.
Steps to reproduce:
Uploading file
hdfs dfs -put somefile /tmp/somefile
Copying with changing replication:
hdfs dfs -D dfs.replication=1 -mv /tmp/somefile /tmp/somefile2

hadoop version:
Hadoop 2.0.0-cdh4.1.2
"
port HDFS-1457 to branch-1.1,HDFS-4535,port HDFS-1457 (configuration option to enable limiting the transfer rate used when sending the image and edits for checkpointing) to branch-1.1
"If there are multiple edit logs with the same fstime, Recovery Mode should load a different one than the normal loading process",HDFS-3947,"If there are multiple edit logs with the same {{fstime}}, Recovery Mode should load a different one than the normal loading process.  This will protect against many common kinds of corruption which affect only one edit log directory.  For example, most I/O errors would fall into this category.

This improvement is not necessary in branch-2 and later branches because in those branches, we have edit log failover (See HDFS-3049).  However, this is an extremely simple and useful thing we can do to make Recovery Mode more helpful in branch-1."
"Command : ""hadoop fs -ls / "" suggesting local directories/files instead hdfs directories/files",HDFS-4474,"hadoop fs -ls / , suggesting local directories (when we press TAB key).
In earlier version, pressing TAB was not suggesting anything but in hadoop-2.0.0 (CDH4) is suggesting local filesystem directories.
 

hadoop@hadoop-VirtualBox:~/CDH4/hadoop-2.0.0-mr1-cdh4.1.3$ hadoop fs -ls /   {Pressing TAB}
bin/        dev/        initrd.img  lost+found/ opt/        run/        srv/        usr/        
boot/       etc/        lib/        media/      proc/       sbin/       sys/        var/        
cdrom/      home/       lib64/      mnt/        root/       selinux/    tmp/        vmlinuz    
"
Provide a way to disable browsing of files from the web UI,HDFS-3801,"A few times we've had requests from users who wish to disable browsing of the filesystem in the web UI completely, while keeping other servlet functionality enabled (such as fsck, etc.). Right now, the cheap way to do this is by blocking out the DN web port (50075) from access by clients, but that also hampers HFTP transfers.

We should instead provide a toggle config for the JSPs to use and disallow browsing if the toggle's enabled. The config can be true by default, to not change the behavior."
EditLogFileInputStream: be more careful about closing streams when we're done with them.,HDFS-3371,"EditLogFileInputStream#EditLogFileInputStream should be more careful about closing streams when there is an exception thrown.  Also, EditLogFileInputStream#close should close all of the streams we opened in the constructor, not just one of them (although the file-backed one is probably the most important)."
File mode bits of some scripts in rpm package are incorrect,HDFS-4069,"These scripts should have execute permission(755). It only happens to rpm package, deb package does not have this problem.

{noformat}-rw-r--r--. 1 root root  2143 Oct  4 22:12 /usr/sbin/slaves.sh
-rw-r--r--. 1 root root  1166 Oct  4 22:12 /usr/sbin/start-all.sh
-rw-r--r--. 1 root root  1065 Oct  4 22:12 /usr/sbin/start-balancer.sh
-rw-r--r--. 1 root root  1745 Oct  4 22:12 /usr/sbin/start-dfs.sh
-rw-r--r--. 1 root root  1145 Oct  4 22:12 /usr/sbin/start-jobhistoryserver.sh
-rw-r--r--. 1 root root  1259 Oct  4 22:12 /usr/sbin/start-mapred.sh
-rw-r--r--. 1 root root  1119 Oct  4 22:12 /usr/sbin/stop-all.sh
-rw-r--r--. 1 root root  1116 Oct  4 22:12 /usr/sbin/stop-balancer.sh
-rw-r--r--. 1 root root  1246 Oct  4 22:12 /usr/sbin/stop-dfs.sh
-rw-r--r--. 1 root root  1131 Oct  4 22:12 /usr/sbin/stop-jobhistoryserver.sh
-rw-r--r--. 1 root root  1168 Oct  4 22:12 /usr/sbin/stop-mapred.sh
-rw-r--r--. 1 root root  4210 Oct  4 22:12 /usr/sbin/update-hadoop-env.sh{noformat} "
BlockMap's corruptNodes count and CorruptReplicas map count is not matching.,HDFS-3162,"Even after invalidating the block, continuosly below log is coming
 
Inconsistent number of corrupt replicas for blk_1332906029734_1719blockMap has 0 but corrupt replicas map has 1"
TestDFSUtil#testHANameNodesWithFederation failed because of misuse toString to generate hostname:port,HDFS-3748,"In the test case TestDFSUtil#testHANameNodesWithFederation, it misused InetSocketAddress.toString() to generate hostname:port format data. It cause the following assert failure when run this test case.
assertEquals(NS1_NN1_HOST, map.get(""ns1"").get(""ns1-nn1"").toString());
assertEquals(NS1_NN2_HOST, map.get(""ns1"").get(""ns1-nn2"").toString());
assertEquals(NS2_NN1_HOST, map.get(""ns2"").get(""ns2-nn1"").toString());
assertEquals(NS2_NN2_HOST, map.get(""ns2"").get(""ns2-nn2"").toString());"
Supporting unzip and untar in hadoop shell,HDFS-3253,As of now hadoop command shell doesnot support unzipping or untaring files in HDFS. But API wise FileUtil already supports this functionality. Work of this jira will be adding untar and unzip functionality to FSShell. 
"webhdfs delete a path that does not exists returns a 200, we should return a 404",HDFS-2429,"Request URI http://NN:50070/webhdfs/some_path_that_does_not_exists?op=DELETE
Request Method: DELETE
Status Line: HTTP/1.1 200 OK
Request Content: {""boolean"":false}"
DatanodeInfo should have a DatanodeID rather than extend it,HDFS-3237,"DatanodeInfo currently extends DatanodeID, the code would be more clear if it had a DatanodeID member instead, as DatanodeInfo is private within the server side and DatanodeID is passed to clients."
JIRA to redesign client side read path,HDFS-2033,This is a JIRA that came up based on comments on HADOOP-7316. This is to study the current design of the client side read path and redesign if necessary.
Some stacktraces are now too lengthy and sometimes no good,HDFS-3366,"This is a high-on-nitpick ticket for the benefit of troubleshooting.

This is partially related to all the PB-changes we've had. And also partially related to Java/JVMs.

Take a case of an AccessControlException, which is pretty common in HDFS permissions layer. We  now get, due to several more calls added at the RPC layer for PB (or maybe something else, if am mistaken):
{code}
Caused by: org.apache.hadoop.security.AccessControlException: org.apache.hadoop.security.AccessControlException: Permission denied: user=yarn, access=WRITE, inode=""/"":hdfs:supergroup:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:205)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:186)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:135)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:4204)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:4175)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:2565)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:2529)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:640)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:42618)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:448)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:891)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1661)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1657)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1204)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1655)

	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:205)
	at $Proxy10.mkdirs(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:84)
	at $Proxy10.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:430)
	at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:1717)
	... 9 more
{code}

The ""9 more"" is what I was looking for, to identify the caller to debug on/find the exact directory. However it now gets eaten away cause just the mkdir-to-exception trace itself has grown quite a bit. Comparing this to 0.20, we have much fewer calls and that helps us see at least the real caller of mkdirs.

I'm actually not sure what causes Java to print ""... X more"" in these form of exception prints, but if thats controllable am all in favor of increasing its amount for HDFS (using new default java opts?). So that when an exception does occur, we don't get a nearly-unusable stacktrace."
Exclude second Ant JAR from classpath in hdfs builds,HDFS-798,"I've no evidence that this is a problem, but I have known it to be in different projects:
{code}
    [junit] WARNING: multiple versions of ant detected in path for junit 
    [junit]          jar:file:/Users/slo/Java/Apache/ant/lib/ant.jar!/org/apache/tools/ant/Project.class
    [junit]      and jar:file:/Users/slo/.ivy2/cache/ant/ant/jars/ant-1.6.5.jar!/org/apache/tools/ant/Project.class
{code}
Somehow Ivy needs to be set up to skip pulling in an old version of Ant in the build -both paranamer-ant and jsp-2.1 declare a dependency on it. If both tools are only ever run under Ant, the ivy.xml file could exclude it, the build file just has to make sure that Ant's own classpath gets passed down."
"HDFS ignores group of a user when creating a file or a directory, and instead inherits",HDFS-3074,"When creating a file or making a directory on HDFS, the namesystem calls pass {{null}} for the group name, thereby having the parent directory permissions inherited onto the file.

This is not how the Linux FS works at least.

For instance, if I have today a user 'foo' with default group 'foo', and I have my HDFS home dir created as ""foo:foo"" by the HDFS admin, all files I create under my directory too will have ""foo"" as group unless I chgrp them myself. This makes sense.

Now, if my admin were to change my local accounts' default/primary group to 'bar' (but did not change so on my homedir on HDFS, and I were to continue writing files to my home directory or any subdirectory that has 'foo' as group, all files still get created with group 'foo' - as if the NN has not realized the primary group of the mapped shell account has already changed.

On linux this is the opposite. My login session's current primary group is what determines the default group on my created files and directories, not the parent dir owner.

If the create and mkdirs call passed UGI's group info (UserGroupInformation.getCurrentUser().getGroupNames()[0] should give primary group?) along into their calls instead of a null in the PermissionsStatus object, perhaps this can be avoided.

Or should we leave this as-is, and instead state that if admins wish their default groups of users to change, they'd have to chgrp all the directories themselves?"
Merge NameNode roles into NodeType.,HDFS-2162,"Currently Namenode has {{NamenodeRole}} with roles NAMENODE, BACKUP and CHECKPOINT. {{NodeType}} has node types NAME_NODE and DATA_NODE. Merge NamenodeRole into NodeType."
imeplement DFSClient on top of thriftfs - this may require DFSClient or DN protocol changes,HDFS-238,"Open up DFS Protocol to allow non-Hadoop DFS clients to implement reads/writes.  Obviously, the NN need not be changed because the thriftfs server will serve up the same metadata - ie it's a bridge to the NN.

This is useful because if we can do this in Java using more open APIs, we could do it in C++ or Python or Perl :)

Doing it in Java first makes sense because we already have the DFSClient - kind of a proof of concept.

"
StatusHttpServer is failing if trying to use hadoop as a standalone jar,HDFS-155,"I just recently tried to upgrade from hadoop .2 and I am getting an error during the startup of StatusHttpServer.  I did set my hadoop.log.dir, but there are still errros.  Here is my stack below.

java.io.FileNotFoundException: E:\projects\marzen\file:\E:\projects\marzen\web\WEB-INF\lib\hadoop.jar!\webapps\datanode
	at org.mortbay.jetty.servlet.WebApplicationContext.resolveWebApp(WebApplicationContext.java:266)
	at org.mortbay.jetty.servlet.WebApplicationContext.doStart(WebApplicationContext.java:449)
	at org.mortbay.util.Container.start(Container.java:72)
	at org.mortbay.http.HttpServer.doStart(HttpServer.java:753)
	at org.mortbay.util.Container.start(Container.java:72)
	at org.apache.hadoop.mapred.StatusHttpServer.start(StatusHttpServer.java:177)
	at org.apache.hadoop.dfs.DataNode.<init>(DataNode.java:167)
	at org.apache.hadoop.dfs.DataNode.makeInstance(DataNode.java:1069)
	at org.apache.hadoop.dfs.DataNode.run(DataNode.java:1003)
	...rest of stack removed...

The root of the issues seems to be in StatusHttpServer.getWebAppsPath().  Its possible that I am using hadoop differently than most people (as a standalone jar in another web app), but I'm not sure how this function could ever work if you were just using hadoop packaged as a standalone jar.  Its trying to get the webapps path using this line: 

URL url = StatusHttpServer.class.getClassLoader().getResource(""webapps"");

Which is returning the following path.  This is a path within the hadoop jar:   file:\E:\projects\marzen\web\WEB-INF\lib\hadoop.jar!\webapps

It then tries to return the canonical path of that wich returns this path:  E:\projects\marzen\file:\E:\projects\marzen\web\WEB-INF\lib\hadoop.jar!\webapps

I don't even think that is a valid path, and the Jetty server certainly doesn't like it.  Is there something I'm doing wrong?  Is it possible nobody else is using hadoop as a standalone jar?  Is this a windows specific bug?  






"
HDFS jira for changes related to HADOOP-7970,HDFS-2783,Create a separate hdfs patch for HDFS changes related to Hadoop-7970 to avoid test failures.
"When all data directory volumes pulled out in DataNode, Its better to shutdown.",HDFS-1863,"When we pulled out all the data directory volumes in DataNode, 
 it is not shuttingdown. Because of this NameNode is keep selecting this DN also for write.
But datanode is saying 'No available volumes' and throwing exception.

Instead of this, we can shutdown the DataNode."
DataNode.setDataNode() considered dangerous,HDFS-973,"I don't have any plans to address this, but it seems to me that having the DataNode save a reference to itself in its constructor by way of {{DataNode.setDataNode(this)}} is hazardous. 

# The reference could be used before the constructor has finished, especially when subclasses are involved
# Callers may assume the DN is actually live
# If startup fails, the DN tries to shut down, but the reference hangs around. Dangerous as well as leaking a reference
# The reference gets retained forever
# It's a singleton that will get confused if >1 DN gets instantiated in-VM

The likely way these problems will surface are in race conditions that are more likely the more cores you have on the machine -production rather than development. This is why it is dangerous.

As part of the service lifecycle patch, I could have this reference only set when the service gets started, set it to null when stopped (and the reference==this). But really the singleton should be removed altogether, somehow. There are methods in DataNode, DataStorage, FSDataset and the namenode that do this, and they should somehow get a reference to any in-VM DN in a cleaner way. For example, servlets can have it set as servlet context."
Handling of deprecated dfs.info.bindAddress and dfs.info.port,HDFS-36,"When checkpointing is triggered in Secondary name node, Secondary name node throws exception while it tries to connect to Namenode's http server in the following two cases:

1) In hadoop-site.xml, if you put only dfs.http.address but not dfs.info.bindAddress and dfs.info.port (Connection Refused Exception)

2) In hadoop-site.xml, if you put only dfs.info.bindAddress and dfs.info.port but not dfs.http.address (SecondaryNameNode.getServerAddress line 148 throws exception since newAddrPort is null)

Temporary Solution: If you put dfs.http.address, dfs.info.bindAddress, and dfs.info.port, then SecondaryNameNode successfully fetches the image and log from Namenode."
One of the DFSClient::create functions ignores parameter,HDFS-497,"DFSClient::create(String src, boolean overwrite, Progressable progress) ignores progress parameter"
"null pointer exception while accessing secondaryname web interface (servlet dfshealth.jsp should not be served from the
secondary Namenode)",HDFS-131,"when I go to the secondary namenode HTTP (dfs.secondary.http.address) in
my browser I see something like this:

        HTTP ERROR: 500
        init
        RequestURI=/dfshealth.jsp
        Powered by Jetty://

And in secondary's log I find these lines:

2008-04-02 11:26:25,357 WARN /: /dfshealth.jsp:
java.lang.NullPointerException
        at org.apache.hadoop.dfs.dfshealth_jsp.<init>(dfshealth_jsp.java:21)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:539)
        at java.lang.Class.newInstance0(Class.java:373)
        at java.lang.Class.newInstance(Class.java:326)
        at org.mortbay.jetty.servlet.Holder.newInstance(Holder.java:199)
        at org.mortbay.jetty.servlet.ServletHolder.getServlet(ServletHolder.java:326)
        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:405)
        at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)
        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)
        at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)
        at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)
        at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)
        at org.mortbay.http.HttpServer.service(HttpServer.java:954)
        at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)
        at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)
        at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)
        at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)
        at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)
        at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)



Responses from core-user@hadoop.apache.org:
 
------------
""dhruba Borthakur"":
The secondary Namenode uses the HTTP interface to pull the fsimage from
the primary. Similarly, the primary Namenode uses the
dfs.secondary.http.address to pull the checkpointed-fsimage back from
the secondary to the primary. So, the definition of
dfs.secondary.http.address is needed.

However, the servlet dfshealth.jsp should not be served from the
secondary Namenode. This servet should be setup in such a way that only
the primary Namenode invokes this servlet.
--------------
 Konstantin Shvachko:
We do not have any secondary nn web interface as of today.
The http server is used for transferring data between the primary and the secondary.
I don't see we can display anything useful on the secondary web UI except for the
current status, config values, and the last checkpoint date/time.
--------------"
"HTTP ERROR: 500 when using ""Go back to dir listing"" link in NameNode web interface",HDFS-71,"I'm running Hadoop 0.13.0 on Windows XP.

Steps to reproduce HTTP 500 error:

1. Go to http://localhost:50070/dfshealth.jsp
2. Click ""Browse the filesystem""
3. Click on any file
4. Click ""Go back to dir listing""

If I do this, I get the following error:

HTTP ERROR: 500

java.io.IOException: Cannot open filename \
	at org.apache.hadoop.dfs.NameNode.open(NameNode.java:263)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:341)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:573)

RequestURI=/browseDirectory.jsp"
"fsimage,fstime not replicated when edit file is empty.",HDFS-41,"Added a second directory to the dfs.name.dir  by 
1) stop dfs
2) restart namenode so that all the edit gets committed to fsimage (edit file becomes empty)
3) stop namenode
4) add second directory to dfs.name.dir in hadoop-site.xml
5) format the new directory
6) restart the dfs

Somehow, edit file was replicated but not the fsimage and fstime.
"
"in FSNamesystem.registerDatanode, dnAddress should be resolved (rarely occured)",HDFS-6,"In FSNamesystem.java registerDatanode(), if the datanode address cannot be
got from the RPC Server, it will use that from the datanode report:

    String dnAddress = Server.getRemoteAddress();
    if (dnAddress == null) {
      // Mostly called inside an RPC.
      // But if not, use address passed by the data-node.
      dnAddress = nodeReg.getHost();
    }      

The getHost() may return the hostname or address, while the Server.getRemoteAddress() 
will return the IP address, which is the dnAddress should be. Thus I think the it should be

    if (dnAddress == null) {
      // Mostly called inside an RPC.
      // But if not, use address passed by the data-node.
      dnAddress = InetAddress.getByName(nodeReg.getHost()).getHostAddress();
    }      

I know it should not be called in most situation, but I indeed use that, and I suppose the 
dnAddress should be an IP address.

"
fsck -files -blocks -locations is a little slow,HDFS-263,"fsck on one subdirectory. 

about 50,000 files (50,000 blocks) 

fsck  /user/aaa            3 seconds
fsck /user/aaa -files   30 seconds
fsck /user/aaa -files -blocks -locations 90 seconds. 

It depends on the network, but could it be a little faster?"
combine sequence read and position read,HDFS-2201,It might be beneficial (especially from maintenance pov) to combine the logic of sequence and position read calls in DFSInputStream.java.
Remove -genclusterid from NameNode startup options,HDFS-1941,"Currently, namenode -genclusterid is a helper utility to generate unique clusterid. This option is useless once namenode -format automatically generates the clusterid."
Programmatically start hdfs processes with JMX port open ,HDFS-1874,Federation web console makes JMX calls to each name node in order to collect statistics from the name nodes.  This requires name node processes to have JMX port open for communication.  We propose a programmatic way to start name node processes with JMX enabled.
dynamically add/subtract dfs.name.dir directories,HDFS-248,It would be very beneficial to be able to add and subtract dfs.name.dir entries on the fly.  This would be used when one drive fails such that another location could be used to keep image and edits redundancy.
dfs.data.dir syntax needs revamping: multiple percentages and weights,HDFS-284,"Currently, all filesystems listed in the dfs.data.dir are treated the same with respected to the space reservation percentages.  This makes sense on homogeneous, dedicated machines, but breaks badly on heterogeneous ones and creates a bit of a support nightmare. 

In a grid with multiple disk sizes, the admin is either leaving space unallocated or is required to slice up the disk.  In addition, if Hadoop isn't the only application running, there may be unexpected collisions. In order to work around this limitation, the administrator must specifically partition up filesystem space such that the reservation 'make sense' for all of the configured file systems.   For example, if someone has 2 small file systems and 2 big ones on a single machine, due to various requirements (such as the OS being mirrored, systems were built from spare parts, server consolidation, whatever).   Reserving 10% might make sense on the small file systems  (say 7G) but 10% may leave a lot more space than desired free on the big ones (say 50G).  

Instead, Hadoop should support a more robust syntax for directory layout.  Ideally, an admin should be able to specify the directory location, the amount of space reserved (in either a percentage or a raw size syntax) for HDFS, as well as a weighting such that some file systems may be preferred over others.  In the example above, the two larger file systems would likely be preferred over the two smaller ones.  Additionally, the reservation on the larger file system might be changed such that it matches the 7G on the smaller file system.

Doing so would allow for much more complex configuration scenarios without having to shuffle a lot of things around at the operating system level."
Make it harder to accidentally close a shared DFSClient,HDFS-925,"Every so often I get stack traces telling me that DFSClient is closed, usually in {{org.apache.hadoop.hdfs.DFSClient.checkOpen() }} . The root cause of this is usually that one thread has closed a shared fsclient while another thread still has a reference to it. If the other thread then asks for a new client it will get one -and the cache repopulated- but if has one already, then I get to see a stack trace. 

It's effectively a race condition between clients in different threads. "
dfs does not support -rmdir,HDFS-639,"Given we have a mkdir, we should have a rmdir.  Using rmr is not a reasonable substitute when you only want to delete empty directories."
add fuse-dfs to src/contrib/build.xml test target,HDFS-414,"since contrib/build.xml test target now specifically includes contrib projects rather than all, fuse-dfs needs to be added.

Note that fuse-dfs' test target is gated on -Dfusedfs=1 and -Dlibhdfs=1, so just running ant test-contrib will not actually trigger it being run.
"
Startup sanity data directory check in main loop.,HDFS-164,"In the overall scheme of things this is probably a nit, but in the run() method of DataXceiveServer in DataNode.java the method ""data.checkDataDir()"" is called right after the socket accept. data.checkDataDir() is a sanity check that makes sure the data directory is setup properly. It is a good sanity check to do when the server is started, but it seems like a bit of a waste to do it after every socket accept. If something happens that causes the sanity check to fail, you would end up finding out about it during the processing of the request anyway."
"Data node should shutdown when a ""critical"" error is returned by the name node",HDFS-70,"Currently data node does not distinguish between critical and non critical exceptions.
Any exception is treated as a signal to sleep and then try again. See
org.apache.hadoop.dfs.DataNode.run()
This is happening because RPC always throws the same RemoteException.
In some cases (like UnregisteredDatanodeException, IncorrectVersionException) the data 
node should shutdown rather than retry.
This logic naturally belongs to the 
org.apache.hadoop.dfs.DataNode.offerService()
but can be reasonably implemented (without examining the RemoteException.className 
field) after HADOOP-266 (2) is fixed."
JspHelper should be a singleton,HDFS-266,"Servlets creating JspHelper instances initialize a set of statics and use the empty instance as a handle to class methods. Instead, JspHelper should either be a singleton with instance fields or its methods should also be static."
Improvements to Hadoop Thrift bindings,HDFS-417,"I have made the following changes to hadoopfs.thrift:

#  Added namespaces for Python, Perl and C++.

# Renamed parameters and struct members to camelCase versions to keep them consistent (in particular FileStatus{blockReplication,blockSize} vs FileStatus.{block_replication,blocksize}).

# Renamed ThriftHadoopFileSystem to FileSystem. From the perspective of a Perl/Python/C++ user, 1) it is already clear that we're using Thrift, and 2) the fact that we're dealing with Hadoop is already explicit in the namespace.  The usage of generated code is more compact and (in my opinion) clearer:
{quote}
        *Perl*:
        use HadoopFS;

        my $client = HadoopFS::FileSystemClient->new(..);

         _instead of:_

        my $client = HadoopFS::ThriftHadoopFileSystemClient->new(..);

        *Python*:

        from hadoopfs import FileSystem

        client = FileSystem.Client(..)

        _instead of_

        from hadoopfs import ThriftHadoopFileSystem

        client = ThriftHadoopFileSystem.Client(..)

        (See also the attached diff [^scripts_hdfs_py.diff] for the
         new version of 'scripts/hdfs.py').

        *C++*:

        hadoopfs::FileSystemClient client(..);

         _instead of_:

        hadoopfs::ThriftHadoopFileSystemClient client(..);
{quote}

# Renamed ThriftHandle to FileHandle: As in 3, it is clear that we're dealing with a Thrift object, and its purpose (to act as a handle for file operations) is clearer.

# Renamed ThriftIOException to IOException, to keep it simpler, and consistent with MalformedInputException.

# Added explicit version tags to fields of ThriftHandle/FileHandle, Pathname, MalformedInputException and ThriftIOException/IOException, to improve compatibility of existing clients with future versions of the interface which might add new fields to those objects (like stack traces for the exception types, for instance).

Those changes are reflected in the attachment [^hadoopfs_thrift.diff].

Changes in generated Java, Python, Perl and C++ code are also attached in [^gen.diff]. They were generated by a Thrift checkout from trunk
([http://svn.apache.org/repos/asf/incubator/thrift/trunk/]) as of revision
719697, plus the following Perl-related patches:

* [https://issues.apache.org/jira/browse/THRIFT-190]
* [https://issues.apache.org/jira/browse/THRIFT-193]
* [https://issues.apache.org/jira/browse/THRIFT-199]

The Thrift jar file [^libthrift.jar] built from that Thrift checkout is also attached, since it's needed to run the Java Thrift server.

I have also added a new target to src/contrib/thriftfs/build.xml to build the Java bindings needed for org.apache.hadoop.thriftfs.HadoopThriftServer.java (see attachment [^build_xml.diff] and modified HadoopThriftServer.java to make use of the new bindings (see attachment [^HadoopThriftServer_java.diff]).

The jar file [^lib/hadoopthriftapi.jar] is also included, although it can be regenerated from the stuff under 'gen-java' and the new 'compile-gen' Ant target.

The whole changeset is also included as [^all.diff]."
Namenode GUI does not show actual memory usage,HDFS-307,"In the namenode GUI, the showed memory usage is not the actual memory usage. Instead, it is the memory currently allocated to Java (Runtime.totalMemory()). That's why we see most of the time, the memory usage is 100%. This is a concern for operation, since this is the only page we can monitor the memory usage of a name node. Showing 100% makes the wrong impression that there is a memory leak in name node, and the name node needs to be restarted.

The current value should show would be Runtime.totalMemory() - Runtime.freeMemory()."
Remove .eclipse.templates directory,HDFS-1710,The {{.eclipse.templates}} directory can be removed.  It contains only a README.txt file.
Use readlink to get absolute paths in the scripts ,HDFS-1569,HDFS side of HADOOP-7089.
Manual tool to test sync against a real cluster,HDFS-1246,"Contributing a tool I've built that writes data against a real cluster, calling sync as fast as it can, and then kill -9s the writer and verifies the data can be recovered."
HAR files used for RAID parity need to have configurable partfile size,HDFS-1175,"RAID parity files are merged into HAR archives periodically. This is required to reduce the number of files that the NameNode has to track. The number of files present in a HAR archive depends on the size of HAR part files - higher the size, lower the number of files.
The size of HAR part files is configurable through the setting har.partfile.size, but that is a global setting. This task introduces a new setting specific to raid.har.partfile.size, that is used in-turn to set har.partfile.size
"
NPE in datanode.handshake(),HDFS-165,It appears possible to raise an NPE in DataNode.handshake() if the startup protocol gets interrupted or fails in some manner
Low Latency distributed reads,HDFS-516,"I created a method for low latency random reads using NIO on the server side and simulated OS paging with LRU caching and lookahead on the client side.  Some applications could include lucene searching (term->doc and doc->offset mappings are likely to be in local cache, thus much faster than nutch's current FsDirectory impl and binary search through record files (bytes at 1/2, 1/4, 1/8 marks are likely to be cached)"
NullPointerException when reading deleted file,HDFS-50,"hdfs://AAA:9999/distcp/destdir/Trash/0803050600/data/part-00018
: java.lang.NullPointerException
  at org.apache.hadoop.dfs.DFSClient$DFSInputStream.getBlockAt(DFSClient.java:919)
  at org.apache.hadoop.dfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:992)
  at org.apache.hadoop.dfs.DFSClient$DFSInputStream.read(DFSClient.java:1112)
  at java.io.DataInputStream.read(DataInputStream.java:83)
  at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.copy(CopyFiles.java:303)
  at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.map(CopyFiles.java:364)
  at org.apache.hadoop.util.CopyFiles$FSCopyFilesMapper.map(CopyFiles.java:219)
  at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
  at org.apache.hadoop.mapred.MapTask.run(MapTask.java:192)
  at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1804)

(Line number for CopyFiles.java is a little off since I'm using my modified version)"
namenode and datanode are not starting,HDFS-14217,"I am new to hadoop ecosystem. I am setting up a cluster with 4 nodes, one master and 3 slave nodes. I have done the ground work of updating all xml files in hadoop/etc/hadoop folder. I saved the slaves file. I formatted the namenode. I then started the cluster with command, sbin/start-dfs.sh. I get to see the lines:

'starting namenode on localhost.... starting datanode on slave1...starting datanode on slave2...starting datanode on slave3... starting secondary namenode..... But when I run jps command in the terminal of masternode, I see only Jps and Secondary Namenode. When I run jps command on the slave nodes, I see only Jps, no datanode running on the slaves. What should I write in my hdfs-site.xml? Currently I set the path for namenode and datanode like... <value>file:/home/user/hadoop_store/hdfs/namenode<\value> and similarly for datanode as well. In the log file for namenode, I see the line saying ""Inconsistent state: storage directory doesn't exist or not accessible for the path /home/user/hadoop_store/hdfs/namenode."
minor change in HDFS Erasure Coding document,HDFS-11607,"The ""Redundant Array of Inexpensive Disks "" in HDFSErasureCoding.md is not appropriate. I think it  should be ""Redundant Arrays of Inexpensive Disks "".
This word means multiple groups of disks, and I have also check this term in papers searched from IEEEXplore

http://ieeexplore.ieee.org/search/searchresult.jsp?newsearch=true&queryText=Redundant%20Arrays%20of%20Inexpensive%20Disks

Since it appears in HDFS Erasure Coding page, a hdfs new feature, it should be accurately described.
"
RequestHedgingInvocationHandler can't be cast to org.apache.hadoop.ipc.RpcInvocationHandler,HDFS-9836,"RequestHedgingInvocationHandler cannot be cast to org.apache.hadoop.ipc.RpcInvocationHandler

Reproduce steps:
1: Set client failover provider as RequestHedgingProxyProvider.
<property>
    <name>dfs.client.failover.proxy.provider.[nameservice]</name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.RequestHedgingProxyProvider</value>
  </property>

2: run hdfs fsck / will get following exceptions.
Exception in thread ""main"" java.lang.ClassCastException: org.apache.hadoop.hdfs.server.namenode.ha.RequestHedgingProxyProvider$RequestHedgingInvocationHandler cannot be cast to org.apache.hadoop.ipc.RpcInvocationHandler
        at org.apache.hadoop.ipc.RPC.getConnectionIdForProxy(RPC.java:613)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.getConnectionId(RetryInvocationHandler.java:281)
        at org.apache.hadoop.ipc.RPC.getConnectionIdForProxy(RPC.java:615)
        at org.apache.hadoop.ipc.RPC.getServerAddress(RPC.java:598)
        at org.apache.hadoop.hdfs.HAUtil.getAddressOfActive(HAUtil.java:380)
        at org.apache.hadoop.hdfs.tools.DFSck.getCurrentNamenodeAddress(DFSck.java:248)
        at org.apache.hadoop.hdfs.tools.DFSck.doWork(DFSck.java:255)
        at org.apache.hadoop.hdfs.tools.DFSck.access$000(DFSck.java:72)
        at org.apache.hadoop.hdfs.tools.DFSck$1.run(DFSck.java:148)
        at org.apache.hadoop.hdfs.tools.DFSck$1.run(DFSck.java:145)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
        at org.apache.hadoop.hdfs.tools.DFSck.run(DFSck.java:144)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)
        at org.apache.hadoop.hdfs.tools.DFSck.main(DFSck.java:360)
"
RoundRobinVolumeChoosingPolicy,HDFS-9492,"This is some general clean-up for: RoundRobinVolumeChoosingPolicy

I have also updated and expanded the unit tests a bit.

There is one error message being generated that I changed.  I felt the previous Exception message was not that helpful and therefore it was possible to trim it down. If the exception message must be enhanced, the entire list of ""volumes"" should be included."
Fix log format in StripedBlockReconstructor,HDFS-12065,"The {{LOG}} is using wrong signature in {{StripedBlockReconstructor}}, and results to the following message without the stack:

{code}
Failed to reconstruct striped block: BP-1026491657-172.31.114.203-1498498077419:blk_-9223372036854759232_5065
java.lang.NullPointerException
{code}"
QJM client tests require a bit more time in some environments,HDFS-4394,"I see hard timeouts of TestQuorumJournalManager and TestQJMWithFaults locally on a dual core laptop. Frequent jstacking shows some minor delay in IPv4 address<->local hostname resolution. These tests start up new ""daemons"" frequently enough for this to be an issue.

With the patch applied, these tests pass. 

I've made a few tries at improving the behavior of the resolver: I've of course insured the 'hosts' file does not have entries which interfere, checked nsswitch.conf sanity, as well as experimented with toggling the java.net.preferIPv4 system property, finally reconfigured dnsmasq to serve local forward and reverse entries, still need the patch. Perhaps I've overlooked another option?"
chmod 777 the .snapshot directory does not error that modification on RO snapshot is disallowed,HDFS-4981,"Snapshots currently are RO, so it's expected that when someone tries to modify the .snapshot directory s/he is denied.

However, if the user tries to chmod 777 the .snapshot directory, the operation does not error. The user should be alerted that modifications are not allowed, even if this operation didn't actually change anything.

Using other modes will trigger the error, though.

{code}
[schu@hdfs-snapshots-1 hdfs]$ sudo -u hdfs hdfs dfs -chmod 777 /user/schu/test_dir_1/.snapshot/
[schu@hdfs-snapshots-1 hdfs]$ sudo -u hdfs hdfs dfs -chmod 755 /user/schu/test_dir_1/.snapshot/
chmod: changing permissions of '/user/schu/test_dir_1/.snapshot': Modification on a read-only snapshot is disallowed
[schu@hdfs-snapshots-1 hdfs]$ sudo -u hdfs hdfs dfs -chmod 435 /user/schu/test_dir_1/.snapshot/
chmod: changing permissions of '/user/schu/test_dir_1/.snapshot': Modification on a read-only snapshot is disallowed
[schu@hdfs-snapshots-1 hdfs]$ sudo -u hdfs hdfs dfs -chown hdfs /user/schu/test_dir_1/.snapshot/
chown: changing ownership of '/user/schu/test_dir_1/.snapshot': Modification on a read-only snapshot is disallowed
[schu@hdfs-snapshots-1 hdfs]$ sudo -u hdfs hdfs dfs -chown schu /user/schu/test_dir_1/.snapshot/
chown: changing ownership of '/user/schu/test_dir_1/.snapshot': Modification on a read-only snapshot is disallowed
[schu@hdfs-snapshots-1 hdfs]$ 
{code}"
A small improvement of SerialNumberManager$SerialNumberMap.get,HDFS-3805,"Instead of use containsKey() + get() to get the value from the HashMap, we can do get() once and check if the result is null"
merge BlockPlacementPolicyWithNodeGroup with default policy,HDFS-8390,"We saw requirements for adding new policies.( HDFS-7613, HDFS-7892, HDFS-8131, maybe  HDFS-4894, HDFS-7068 in the future). Every policy need to support NodeGroup.

Assume we have N policies:
{noformat}
BlockPlacementPolicyDefault
BlockPlacementPolicyCustomized1 extends BlockPlacementPolicyDefault
BlockPlacementPolicyCustomized2 extends BlockPlacementPolicyDefault
BlockPlacementPolicyCustomized3 extends BlockPlacementPolicyDefault
{noformat}
We need to implements another N policies:
{noformat}
BlockPlacementPolicyyWithNodeGroup
BlockPlacementPolicyCustomized1WithNodeGroup extends BlockPlacementPolicyWithNodeGroup
BlockPlacementPolicyCustomized2WithNodeGroup extends BlockPlacementPolicyWithNodeGroup
BlockPlacementPolicyCustomized3WithNodeGroup extends BlockPlacementPolicyWithNodeGroup
{noformat}
We had better merge nodeGroup awareness into default policy. So every new policy only need to extends BlockPlacementPolicyDefault."
"Add a main method to HdfsConfiguration, for debug purposes",HDFS-3621,"Just like Configuration has a main() func that dumps XML out for debug purposes, we should have a similar function under the HdfsConfiguration class that does the same. This is useful in testing out app classpath setups at times."
Add testcases for -n option of FSshell cat,HDFS-2530,Add test cases for HADOOP-7795.
Deep learn about hadoop,HDFS-7631,I want to learn more about hadoop code. If there are any books that can help me.
Some of property descriptions are not given(hdfs-default.xml),HDFS-2892,"Hi..I taken 23.0 release form http://hadoop.apache.org/common/releases.html#11+Nov%2C+2011%3A+release+0.23.0+available

I just gone through all properties provided in the hdfs-default.xml..Some of the property description not mentioned..It's better to give description of property and usage(how to configure ) and Only MapReduce related jars only provided..Please check following two configurations


 *No Description*

{noformat}
<property>
  <name>dfs.datanode.https.address</name>
  <value>0.0.0.0:50475</value>
</property>

<property>
  <name>dfs.namenode.https-address</name>
  <value>0.0.0.0:50470</value>
</property>
{noformat}


 Better to mention example usage (what to configure...format(syntax))in desc,here I did not get what default mean whether this name of n/w interface or something else

 <property>
  <name>dfs.datanode.dns.interface</name>
  <value>default</value>
  <description>The name of the Network Interface from which a data node should 
  report its IP address.
  </description>
 </property>


The following property is commented..If it is not supported better to remove.

<property>
   <name>dfs.cluster.administrators</name>
   <value>ACL for the admins</value>
   <description>This configuration is used to control who can access the
                default servlets in the namenode, etc.
   </description>
</property>




 Small clarification for following property..if some value configured this then NN will be safe mode upto this much time..
May I know usage of the following property...
<property>
  <name>dfs.blockreport.initialDelay</name>  <value>0</value>
  <description>Delay for first block report in seconds.</description>
</property>
"
[JDK8] azurenative test cases fail builds,HDFS-7297,java.util.Base64 conflicts with com.microsoft.windowsazure.storage.core.Base64 in Azure unit tests.
Fix 1 space misalignment in FileSystem class ,HDFS-6669,"This is simple cleanup on FileSystem class to align 1 space char misalignment to help better code updates in IDE.

No code functionality change."
Missing '\n' in the output of 'hdfs oiv --help',HDFS-5864,"In OfflineImageViewer.java, 

{code}
    ""  * NameDistribution: This processor analyzes the file names\n"" +
    ""    in the image and prints total number of file names and how frequently"" +
    ""    file names are reused.\n"" +
{code}

should be

{code}
    ""  * NameDistribution: This processor analyzes the file names\n"" +
    ""    in the image and prints total number of file names and how frequently\n"" +
    ""    file names are reused.\n"" +
{code}"
GSetByHashMap breaks contract of GSet,HDFS-5764,"The contract of GSet says it is ensured to throw NullPointerException if a given argument is null for many methods, but GSetByHashMap doesn't. I think just writing non-null preconditions for GSet are required.
"
"fine tune ""Access token verification failed"" error msg in datanode log",HDFS-4881,"I'd like to issue this ticket is due to we suffered a datanode access token verification failure issue recently. The client is HBase who is accessing the local datanode via DFSClient. The details log snippets as follows...
*regionserver log*
{code}
...
[2013-05-24 08:33:37,553][regionserver8120-compactions-1369288874174][INFO ][org.apache.hadoop.hbase.regionserver.Store]: Started compaction of 1 file(s) in cf=ho, hasReferences=true, into hdfs://sjdc-s-hdd-001.sjdc.ispn.trendmicro.com:8020/user/SPN-hbase/spn.guidcensus.ho/f99c6fb26f488034bf0e6ddd7a647ba4/.tmp, seqid=3, totalSize=4.2g
[2013-05-24 08:33:37,554][regionserver8120-compactions-1369288874174][INFO ][org.apache.hadoop.hdfs.DFSClient]: Access token was invalid when connecting to /10.31.6.49:1004 : org.apache.hadoop.hdfs.security.token.block.InvalidBlockTokenException: Got access token error for OP_READ_BLOCK, self=/10.31.6.49:36530, remote=/10.31.6.49:1004, for file /user/SPN-hbase/spn.guidcensus.ho/a565dd142933e3abf9bec33d59210d1b/ho/c5b37b9dd8801275c8fb160c0fb32ce5c48b56f4, for block 4549293737579979499_205814042
...
{code}

*datanode log*
{code}
...
[2013-05-24 08:33:37,554][DataXceiver for client /10.31.6.49:36530 [Waiting for operation #1]][ERROR][org.apache.hadoop.hdfs.server.datanode.DataNode]: DatanodeRegistration(10.31.6.49:1004, storageID=DS-1953102179-10.31.6.49-1004-       1342490559943, infoPort=1006, ipcPort=50020):DataXceiver
java.io.IOException: Access token verification failed, for client /10.31.6.49:36530 for OP_READ_BLOCK for block blk_4549293737579979499_205814042
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:252)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:175)
...
{code}

After trace o.a.h.hdfs.security.token.block.BlockTokenSecretManager.java, I found that there are more further details error description written in code.
*o.a.h.hdfs.security.token.block.BlockTokenSecretManager.java*
{code}
public void checkAccess(BlockTokenIdentifier id, String userId, Block block,
      AccessMode mode) throws InvalidToken {
    if (LOG.isDebugEnabled()) {
      LOG.debug(""Checking access for user="" + userId + "", block="" + block
          + "", access mode="" + mode + "" using "" + id.toString());
    }
    if (userId != null && !userId.equals(id.getUserId())) {
      throw new InvalidToken(""Block token with "" + id.toString()
          + "" doesn't belong to user "" + userId);
    }
    if (id.getBlockId() != block.getBlockId()) {
      throw new InvalidToken(""Block token with "" + id.toString()
          + "" doesn't apply to block "" + block);
    }
    if (isExpired(id.getExpiryDate())) {
      throw new InvalidToken(""Block token with "" + id.toString()
          + "" is expired."");
    }
    if (!id.getAccessModes().contains(mode)) {
      throw new InvalidToken(""Block token with "" + id.toString()
          + "" doesn't have "" + mode + "" permission"");
    }
  }
{code}

But actually, this InvalidTokenException will not be handled further (but caught), so I can not trace what kind of this access block token verification is...
*o.a.h.hdfs.server.datanode.DataXceiver.java*
{code}
...
if (datanode.isBlockTokenEnabled) {
      try {
        datanode.blockTokenSecretManager.checkAccess(accessToken, null, block,
            BlockTokenSecretManager.AccessMode.READ);
      } catch (InvalidToken e) {
        // the e object not handled further...
        try {
          out.writeShort(DataTransferProtocol.OP_STATUS_ERROR_ACCESS_TOKEN);
          out.flush();
          throw new IOException(""Access token verification failed, for client ""
              + remoteAddress + "" for OP_READ_BLOCK for block "" + block); 
        } finally {
          IOUtils.closeStream(out);
        }   
      }   
    }
...
{code}
"
Remove redundant SuppressWarning annotations in WebHdfsFileSystem,HDFS-5179,"It seems that the annotations of SuppressWarning in this file are no longer needed.

Remove them to eliminate the warnings from the compiler."
misleading comment in CommonConfigurationKeysPublic,HDFS-3994,"{{CommonConfigurationKeysPublic}} contains a potentially misleading comment:

{code}
/** 
 * This class contains constants for configuration keys used
 * in the common code.
 *
 * It includes all publicly documented configuration keys. In general
 * this class should not be used directly (use CommonConfigurationKeys
 * instead)
 */
{code}

This comment suggests that the user use {{CommonConfigurationKeys}}, despite the fact that that class is {{InterfaceAudience.private}} whereas {{CommonConfigurationKeysPublic}} is {{InterfaceAudience.public}}.  Perhaps this should be rephrased."
Add testcases for -n option of FSshell -tail ,HDFS-2279,Add a few test cases for HADOOP-7546.
Replace hardcoded strings with the already defined config keys in DataNode.java ,HDFS-3855,Replace hardcoded strings with the already defined config keys in DataNode.java 
DNA_SHUTDOWN command is never sent by the NN,HDFS-2987,"The DataNode has a code path to handle a DNA_SHUTDOWN command, but in fact this command has never been sent by the NN (it was introduced by HADOOP-641 in 0.8.0!)"
review my networking,HDFS-3327,all my communication
Missing license headers in branch-20-append,HDFS-1266,"We appear to have some files without license headers, we should do a quick pass through and fix them."
thriftfs creates jar file named $\{version\}-thriftfs.jar,HDFS-416,"I am building thrift-fs off of 0.17.2. I accomplished this by downloading the trunk moving it into the 0.17.2.1 source and then running the build process. 
I made the append method throw illegal argument exception. 

After  the file that was produced is named 'hadoop-0.17.2.1/build/contrib/thriftfs/hadoop-${version}-thriftfs.jar'. (The version was not inserted) on my filesystems. This may not be a bug. this may be a result up the mismatched way I built 0.19 and 0.17 components together. I wanted to make sure the component compiled against the version I am running."
Fake jira for illustrating workflow (sorry),HDFS-1916,"The namenode explodes when it eats too much.
Steps to reproduce: a) eat too much. b) explode"
Repeat commonly viewed statistics at the end of dfsadmin -report,HDFS-1545,"Safemode status, datanode's available, and capacity are the most commonly viewed statistics when running dfsadmin -report. They rightly appear first, followed by the statistics of individual datanodes. 

However with a large cluster, these commonly viewed statistics become obscured by the length of the individual datanode statistics. It is a tad ridiculous to have to have to grep or scroll to the top after a long list of nodes. For convenience, these commonly viewed statistics should be repeated at the end of the list."
